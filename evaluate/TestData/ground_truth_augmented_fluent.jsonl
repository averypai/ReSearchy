{"author": "Wisal Mukhtiar, Waliiya Rizwan, Aneela Habib, Yasir Saleem Afridi, Laiq Hasan, Kashif Ahmad", "abstract": "In recent years, social media has been widely explored as a potential source of communication and information in disasters and emergency situations. Several interesting works and case studies of disaster analytics exploring different aspects of natural disasters have been already conducted. Along with the great potential, disaster analytics comes with several challenges mainly due to the nature of social media content. In this paper, we explore one such challenge and propose a text classification framework to deal with Twitter noisy data. More specifically, we employed several transformers both individually and in combination, so as to differentiate between relevant and non-relevant Twitter posts, achieving the highest F1-score of 0.87.", "title": "Relevance Classification of Flood-related Twitter Posts via Multiple Transformers.", "time": "2023-01-01T01:34:15Z", "link": "http://arxiv.org/abs/2301.00320v1", "id": "2301.00320v1_level1"}
{"author": "Wisal Mukhtiar, Waliiya Rizwan, Aneela Habib, Yasir Saleem Afridi, Laiq Hasan, Kashif Ahmad", "abstract": "In recent years, social media has been widely explored as a potential source of communication and information in disasters and emergency situations. Several interesting works and case studies of disaster analytics exploring different aspects of natural disasters have been already conducted. Along with the great potential, disaster analytics comes with several challenges mainly due to the nature of social media content. In this paper, we explore one such challenge and propose a text classification framework to deal with Twitter noisy data. More specifically, we employed several transformers both individually and in combination, so as to differentiate between relevant and non-relevant Twitter posts, achieving the highest F1-score of 0.87.", "title": "Relevance Classification of Flood-related Twitter Posts via Multiple Transformers.", "time": "2023-01-01T01:34:15Z", "link": "http://arxiv.org/abs/2301.00320v1", "id": "2301.00320v1_level2"}
{"author": "Wisal Mukhtiar, Waliiya Rizwan, Aneela Habib, Yasir Saleem Afridi, Laiq Hasan, Kashif Ahmad", "abstract": "social media has been widely explored as a potential source of communication and information in disasters and emergency situations., In recent years. Several interesting works and case studies of disaster analytics exploring different aspects of natural disasters have been already conducted. disaster analytics comes with several challenges mainly due to the nature of social media content., Along with the great potential. achieving the highest F1-score of 0.87., so as to differentiate between relevant and non-relevant Twitter posts, we employed several transformers both individually and in combination, we explore one such challenge and propose a text classification framework to deal with Twitter noisy data. More specifically, In this paper.", "title": "Relevance Classification of Flood-related Twitter Posts via Multiple Transformers.", "time": "2023-01-01T01:34:15Z", "link": "http://arxiv.org/abs/2301.00320v1", "id": "2301.00320v1_level3"}
{"author": "Wisal Mukhtiar, Waliiya Rizwan, Aneela Habib, Yasir Saleem Afridi, Laiq Hasan, Kashif Ahmad", "abstract": "Importantly, In recent years, social media has been widely explored as a potential source of communication and information in disasters and emergency situations. Importantly, Several interesting works and case studies of disaster analytics exploring different aspects of natural disasters have been already conducted. Importantly, Along with the great potential, disaster analytics comes with several challenges mainly due to the nature of social media content. Importantly, In this paper, we explore one such challenge and propose a text classification framework to deal with twitter noisy data. more specifically, we employed several transformers both individually and in combination, so as to differentiate between relevant and non-relevant twitter posts, achieving the highest f1-score of 0.87.", "title": "Importantly, Relevance classification of flood-related twitter posts via multiple transformers.", "time": "2023-01-01T01:34:15Z", "link": "http://arxiv.org/abs/2301.00320v1", "id": "2301.00320v1_level4"}
{"author": "Wisal Mukhtiar, Waliiya Rizwan, Aneela Habib, Yasir Saleem Afridi, Laiq Hasan, Kashif Ahmad", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-01T01:34:15Z", "link": "http://arxiv.org/abs/2301.00320v1", "id": "2301.00320v1_level5"}
{"author": "Muhammad Suleman, Muhammad Asif, Tayyab Zamir, Ayaz Mehmood, Jebran Khan, Nasir Ahmad, Kashif Ahmad", "abstract": "This paper presents our solutions for the MediaEval 2022 task on DisasterMM. The task is composed of two subtasks, namely (i) Relevance Classification of Twitter Posts (RCTP), and (ii) Location Extraction from Twitter Texts (LETT). The RCTP subtask aims at differentiating flood-related and non-relevant social posts while LETT is a Named Entity Recognition (NER) task and aims at the extraction of location information from the text. For RCTP, we proposed four different solutions based on BERT, RoBERTa, Distil BERT, and ALBERT obtaining an F1-score of 0.7934, 0.7970, 0.7613, and 0.7924, respectively. For LETT, we used three frameworks namely BERT, RoBERTa, and Distil BERTA obtaining an F1-score of 0.6256, 0.6744, and 0.6723, respectively.", "title": "Floods Relevancy and Identification of Location from Twitter Posts using NLP Techniques.", "time": "2023-01-01T01:36:32Z", "link": "http://arxiv.org/abs/2301.00321v1", "id": "2301.00321v1_level1"}
{"author": "Muhammad Suleman, Muhammad Asif, Tayyab Zamir, Ayaz Mehmood, Jebran Khan, Nasir Ahmad, Kashif Ahmad", "abstract": "This paper presents our solutions for the MediaEval 2022 task on DisasterMM. The task is composed of two subtasks, namely (i) Relevance Classification of Twitter Posts (RCTP), and (ii) Location Extraction from Twitter Texts (LETT). The RCTP subtask aims at differentiating flood-related and non-relevant social posts while LETT is a Named Entity Recognition (NER) task and aims at the extraction of location information from the text. For RCTP, we proposed four different solutions based on BERT, RoBERTa, Distil BERT, and ALBERT obtaining an F1-score of 0.7934, 0.7970, 0.7613, and 0.7924, respectively. For LETT, we used three models namely BERT, RoBERTa, and Distil BERTA obtaining an F1-score of 0.6256, 0.6744, and 0.6723, respectively.", "title": "Floods Relevancy and Identification of Location from Twitter Posts using NLP Techniques.", "time": "2023-01-01T01:36:32Z", "link": "http://arxiv.org/abs/2301.00321v1", "id": "2301.00321v1_level2"}
{"author": "Muhammad Suleman, Muhammad Asif, Tayyab Zamir, Ayaz Mehmood, Jebran Khan, Nasir Ahmad, Kashif Ahmad", "abstract": "and (ii) Location Extraction from Twitter Texts (LETT). The RCTP subtask aims at differentiating flood-related and non-relevant social posts while LETT is a Named Entity Recognition (NER) task and aims at the extraction of location information from the text., namely (i) Relevance Classification of Twitter Posts (RCTP), This paper presents our solutions for the MediaEval 2022 task on DisasterMM. The task is composed of two subtasks. respectively., and 0.7924, 0.7613, 0.7970, and ALBERT obtaining an F1-score of 0.7934, Distil BERT, RoBERTa, we proposed four different solutions based on BERT, For RCTP. respectively., and 0.6723, 0.6744, and Distil BERTA obtaining an F1-score of 0.6256, RoBERTa, we used three models namely BERT, For LETT.", "title": "Floods Relevancy and Identification of Location from Twitter Posts using NLP Techniques.", "time": "2023-01-01T01:36:32Z", "link": "http://arxiv.org/abs/2301.00321v1", "id": "2301.00321v1_level3"}
{"author": "Muhammad Suleman, Muhammad Asif, Tayyab Zamir, Ayaz Mehmood, Jebran Khan, Nasir Ahmad, Kashif Ahmad", "abstract": "Importantly, This paper presents our solutions for the mediaeval 2022 task on disastermm. the task is composed of two subtasks, namely (i) relevance classification of twitter posts (rctp), and (ii) location extraction from twitter texts (lett). the rctp subtask aims at differentiating flood-related and non-relevant social posts while lett is a named entity recognition (ner) task and aims at the extraction of location information from the text. Importantly, For rctp, we proposed four different solutions based on bert, roberta, distil bert, and albert obtaining an f1-score of 0.7934, 0.7970, 0.7613, and 0.7924, respectively. Importantly, For lett, we used three models namely bert, roberta, and distil berta obtaining an f1-score of 0.6256, 0.6744, and 0.6723, respectively.", "title": "Importantly, Floods relevancy and identification of location from twitter posts using nlp techniques.", "time": "2023-01-01T01:36:32Z", "link": "http://arxiv.org/abs/2301.00321v1", "id": "2301.00321v1_level4"}
{"author": "Muhammad Suleman, Muhammad Asif, Tayyab Zamir, Ayaz Mehmood, Jebran Khan, Nasir Ahmad, Kashif Ahmad", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-01T01:36:32Z", "link": "http://arxiv.org/abs/2301.00321v1", "id": "2301.00321v1_level5"}
{"author": "Xingwu Sun, Hongyin Tang, chengzhong Xu", "abstract": "Neural frameworks with an encoder-decoder framework provide a feasible solution to Question Generation (QG). However, after analyzing the framework vocabulary we find that current frameworks (both RNN-based and pre-training based) have more than 23\\% inflected forms. As a result, the encoder will generate separate embeddings for the inflected forms, leading to a waste of training data and parameters. Even worse, in decoding these frameworks are vulnerable to irrelevant noise and they suffer from high computational costs. In this paper, we propose an approach to enhance the performance of QG by fusing word transformation. Firstly, we identify the inflected forms of words from the input of encoder, and replace them with the root words, letting the encoder pay more attention to the repetitive root words. Secondly, we propose to adapt QG as a combination of the following actions in the encode-decoder framework: generating a question word, copying a word from the source sequence or generating a word transformation type. Such extension can greatly decrease the size of predicted words in the decoder as well as noise. We apply our approach to a typical RNN-based framework and \\textsc{UniLM} to get the improved versions. We conduct extensive experiments on SQuAD and MS MARCO datasets. The experimental results show that the improved versions can significantly outperform the corresponding baselines in terms of BLEU, ROUGE-L and METEOR as well as time cost.", "title": "Inflected Forms Are Redundant in Question Generation Models.", "time": "2023-01-01T13:08:11Z", "link": "http://arxiv.org/abs/2301.00397v1", "id": "2301.00397v1_level1"}
{"author": "Xingwu Sun, Hongyin Tang, chengzhong Xu", "abstract": "Neural models with an encoder-decoder framework provide a feasible solution to Question Generation (QG). However, after analyzing the model vocabulary we find that current models (both RNN-based and pre-training based) have more than 23\\% inflected forms. As a result, the encoder will generate separate embeddings for the inflected forms, leading to a waste of training data and parameters. Even worse, in decoding these models are vulnerable to irrelevant noise and they suffer from high computational costs. In this paper, we propose an approach to enhance the performance of QG by fusing word transformation. Firstly, we identify the inflected forms of words from the input of encoder, and replace them with the root words, letting the encoder pay more attention to the repetitive root words. Secondly, we propose to adapt QG as a combination of the following actions in the encode-decoder framework: generating a question word, copying a word from the source sequence or generating a word transformation type. Such extension can greatly decrease the size of predicted words in the decoder as well as noise. We apply our approach to a typical RNN-based model and \\textsc{UniLM} to get the improved versions. We conduct extensive experiments on SQuAD and MS MARCO datasets. The experimental results show that the improved versions can significantly outperform the corresponding baselines in terms of BLEU, ROUGE-L and METEOR as well as time cost.", "title": "Inflected Forms Are Redundant in Question Generation Models.", "time": "2023-01-01T13:08:11Z", "link": "http://arxiv.org/abs/2301.00397v1", "id": "2301.00397v1_level2"}
{"author": "Xingwu Sun, Hongyin Tang, chengzhong Xu", "abstract": "Neural models with an encoder-decoder framework provide a feasible solution to Question Generation (QG). after analyzing the model vocabulary we find that current models (both RNN-based and pre-training based) have more than 23\\% inflected forms., However. leading to a waste of training data and parameters., the encoder will generate separate embeddings for the inflected forms, As a result. in decoding these models are vulnerable to irrelevant noise and they suffer from high computational costs., Even worse. letting the encoder pay more attention to the repetitive root words., we identify the inflected forms of words from the input of encoder, and replace them with the root words, we propose an approach to enhance the performance of QG by fusing word transformation. Firstly, In this paper. copying a word from the source sequence or generating a word transformation type., we propose to adapt QG as a combination of the following actions in the encode-decoder framework: generating a question word, Secondly. Such extension can greatly decrease the size of predicted words in the decoder as well as noise. We apply our approach to a typical RNN-based model and \\textsc{UniLM} to get the improved versions. We conduct extensive experiments on SQuAD and MS MARCO datasets. ROUGE-L and METEOR as well as time cost., The experimental results show that the improved versions can significantly outperform the corresponding baselines in terms of BLEU.", "title": "Inflected Forms Are Redundant in Question Generation Models.", "time": "2023-01-01T13:08:11Z", "link": "http://arxiv.org/abs/2301.00397v1", "id": "2301.00397v1_level3"}
{"author": "Xingwu Sun, Hongyin Tang, chengzhong Xu", "abstract": "Importantly, Neural models with an encoder-decoder framework provide a feasible solution to question generation (qg). Importantly, However, after analyzing the model vocabulary we find that current models (both rnn-based and pre-training based) have more than 23\\% inflected forms. Importantly, As a result, the encoder will generate separate embeddings for the inflected forms, leading to a waste of training data and parameters. Importantly, Even worse, in decoding these models are vulnerable to irrelevant noise and they suffer from high computational costs. Importantly, In this paper, we propose an approach to enhance the performance of qg by fusing word transformation. firstly, we identify the inflected forms of words from the input of encoder, and replace them with the root words, letting the encoder pay more attention to the repetitive root words. Importantly, Secondly, we propose to adapt qg as a combination of the following actions in the encode-decoder framework: generating a question word, copying a word from the source sequence or generating a word transformation type. Importantly, Such extension can greatly decrease the size of predicted words in the decoder as well as noise. Importantly, We apply our approach to a typical rnn-based model and \\textsc{unilm} to get the improved versions. Importantly, We conduct extensive experiments on squad and ms marco datasets. Importantly, The experimental results show that the improved versions can significantly outperform the corresponding baselines in terms of bleu, rouge-l and meteor as well as time cost.", "title": "Importantly, Inflected forms are redundant in question generation models.", "time": "2023-01-01T13:08:11Z", "link": "http://arxiv.org/abs/2301.00397v1", "id": "2301.00397v1_level4"}
{"author": "Xingwu Sun, Hongyin Tang, chengzhong Xu", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-01T13:08:11Z", "link": "http://arxiv.org/abs/2301.00397v1", "id": "2301.00397v1_level5"}
{"author": "Farshad Noravesh", "abstract": "In the present paper, semantic parsing challenges are briefly introduced and QDMR formalism in semantic parsing is implemented using sequence to sequence framework with attention but uses only part of speech(POS) as a representation of words of a sentence to make the training as simple and as fast as possible and also avoiding curse of dimensionality as well as overfitting. It is shown how semantic operator prediction could be augmented with other frameworks like the CopyNet framework or the recursive neural net framework.", "title": "Semantic Operator Prediction and Applications.", "time": "2023-01-01T13:20:57Z", "link": "http://arxiv.org/abs/2301.00399v1", "id": "2301.00399v1_level1"}
{"author": "Farshad Noravesh", "abstract": "In the present paper, semantic parsing challenges are briefly introduced and QDMR formalism in semantic parsing is implemented using sequence to sequence model with attention but uses only part of speech(POS) as a representation of words of a sentence to make the training as simple and as fast as possible and also avoiding curse of dimensionality as well as overfitting. It is shown how semantic operator prediction could be augmented with other models like the CopyNet model or the recursive neural net model.", "title": "Semantic Operator Prediction and Applications.", "time": "2023-01-01T13:20:57Z", "link": "http://arxiv.org/abs/2301.00399v1", "id": "2301.00399v1_level2"}
{"author": "Farshad Noravesh", "abstract": "semantic parsing challenges are briefly introduced and QDMR formalism in semantic parsing is implemented using sequence to sequence model with attention but uses only part of speech(POS) as a representation of words of a sentence to make the training as simple and as fast as possible and also avoiding curse of dimensionality as well as overfitting., In the present paper. It is shown how semantic operator prediction could be augmented with other models like the CopyNet model or the recursive neural net model.", "title": "Semantic Operator Prediction and Applications.", "time": "2023-01-01T13:20:57Z", "link": "http://arxiv.org/abs/2301.00399v1", "id": "2301.00399v1_level3"}
{"author": "Farshad Noravesh", "abstract": "Importantly, In the present paper, semantic parsing challenges are briefly introduced and qdmr formalism in semantic parsing is implemented using sequence to sequence model with attention but uses only part of speech(pos) as a representation of words of a sentence to make the training as simple and as fast as possible and also avoiding curse of dimensionality as well as overfitting. Importantly, It is shown how semantic operator prediction could be augmented with other models like the copynet model or the recursive neural net model.", "title": "Importantly, Semantic operator prediction and applications.", "time": "2023-01-01T13:20:57Z", "link": "http://arxiv.org/abs/2301.00399v1", "id": "2301.00399v1_level4"}
{"author": "Farshad Noravesh", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-01T13:20:57Z", "link": "http://arxiv.org/abs/2301.00399v1", "id": "2301.00399v1_level5"}
{"author": "Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "To the best of our knowledge, this paper made the first attempt to answer whether word segmentation is necessary for Vietnamese sentiment classification. To do this, we presented five pre-trained monolingual S4- based language frameworks for Vietnamese, including one framework without word segmentation, and four frameworks using RDRsegmenter, uitnlp, pyvi, or underthesea toolkits in the pre-processing data phase. According to comprehensive experimental results on two corpora, including the VLSP2016-SA corpus of technical article reviews from the news and social media and the UIT-VSFC corpus of the educational survey, we have two suggestions. Firstly, using traditional classifiers like Naive Bayes or Support Vector Machines, word segmentation maybe not be necessary for the Vietnamese sentiment classification corpus, which comes from the social domain. Secondly, word segmentation is necessary for Vietnamese sentiment classification when word segmentation is used before using the BPE approach and feeding into the deep learning framework. In this way, the RDRsegmenter is the stable toolkit for word segmentation among the uitnlp, pyvi, and underthesea toolkits.", "title": "Is word segmentation necessary for Vietnamese sentiment classification?.", "time": "2023-01-01T15:04:47Z", "link": "http://arxiv.org/abs/2301.00418v1", "id": "2301.00418v1_level1"}
{"author": "Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "To the best of our knowledge, this paper made the first attempt to answer whether word segmentation is necessary for Vietnamese sentiment classification. To do this, we presented five pre-trained monolingual S4- based language models for Vietnamese, including one model without word segmentation, and four models using RDRsegmenter, uitnlp, pyvi, or underthesea toolkits in the pre-processing data phase. According to comprehensive experimental results on two corpora, including the VLSP2016-SA corpus of technical article reviews from the news and social media and the UIT-VSFC corpus of the educational survey, we have two suggestions. Firstly, using traditional classifiers like Naive Bayes or Support Vector Machines, word segmentation maybe not be necessary for the Vietnamese sentiment classification corpus, which comes from the social domain. Secondly, word segmentation is necessary for Vietnamese sentiment classification when word segmentation is used before using the BPE method and feeding into the deep learning model. In this way, the RDRsegmenter is the stable toolkit for word segmentation among the uitnlp, pyvi, and underthesea toolkits.", "title": "Is word segmentation necessary for Vietnamese sentiment classification?.", "time": "2023-01-01T15:04:47Z", "link": "http://arxiv.org/abs/2301.00418v1", "id": "2301.00418v1_level2"}
{"author": "Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "or underthesea toolkits in the pre-processing data phase., pyvi, uitnlp, and four models using RDRsegmenter, including one model without word segmentation, we presented five pre-trained monolingual S4- based language models for Vietnamese, this paper made the first attempt to answer whether word segmentation is necessary for Vietnamese sentiment classification. To do this, To the best of our knowledge. we have two suggestions., According to comprehensive experimental results on two corpora, including the VLSP2016-SA corpus of technical article reviews from the news and social media and the UIT-VSFC corpus of the educational survey. which comes from the social domain., word segmentation maybe not be necessary for the Vietnamese sentiment classification corpus, using traditional classifiers like Naive Bayes or Support Vector Machines, Firstly. Secondly, word segmentation is necessary for Vietnamese sentiment classification when word segmentation is used before using the BPE method and feeding into the deep learning model. and underthesea toolkits., pyvi, the RDRsegmenter is the stable toolkit for word segmentation among the uitnlp, In this way.", "title": "Is word segmentation necessary for Vietnamese sentiment classification?.", "time": "2023-01-01T15:04:47Z", "link": "http://arxiv.org/abs/2301.00418v1", "id": "2301.00418v1_level3"}
{"author": "Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "Importantly, To the best of our knowledge, this paper made the first attempt to answer whether word segmentation is necessary for vietnamese sentiment classification. to do this, we presented five pre-trained monolingual s4- based language models for vietnamese, including one model without word segmentation, and four models using rdrsegmenter, uitnlp, pyvi, or underthesea toolkits in the pre-processing data phase. Importantly, According to comprehensive experimental results on two corpora, including the vlsp2016-sa corpus of technical article reviews from the news and social media and the uit-vsfc corpus of the educational survey, we have two suggestions. Importantly, Firstly, using traditional classifiers like naive bayes or support vector machines, word segmentation maybe not be necessary for the vietnamese sentiment classification corpus, which comes from the social domain. Importantly, Secondly, word segmentation is necessary for vietnamese sentiment classification when word segmentation is used before using the bpe method and feeding into the deep learning model. Importantly, In this way, the rdrsegmenter is the stable toolkit for word segmentation among the uitnlp, pyvi, and underthesea toolkits.", "title": "Importantly, Is word segmentation necessary for vietnamese sentiment classification?.", "time": "2023-01-01T15:04:47Z", "link": "http://arxiv.org/abs/2301.00418v1", "id": "2301.00418v1_level4"}
{"author": "Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-01T15:04:47Z", "link": "http://arxiv.org/abs/2301.00418v1", "id": "2301.00418v1_level5"}
{"author": "Quoc-Loc Duong, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "RTE is a significant problem and is a reasonably active research community. The proposed research works on the approach to this problem are pretty diverse with many different directions. For Vietnamese, the RTE problem is moderately new, but this problem plays a vital role in natural language understanding systems. Currently, approachs to solve this problem based on contextual word representation learning frameworks have given outstanding results. However, Vietnamese is a semantically rich language. Therefore, in this paper, we want to present an experiment combining semantic word representation through the SRL task with context representation of BERT relative frameworks for the RTE problem. The experimental results give conclusions about the influence and role of semantic representation on Vietnamese in understanding natural language. The experimental results show that the semantic-aware contextual representation framework has about 1% higher performance than the framework that does not incorporate semantic representation. In addition, the effects on the data domain in Vietnamese are also higher than those in English. This result also shows the positive influence of SRL on RTE problem in Vietnamese.", "title": "Leveraging Semantic Representations Combined with Contextual Word Representations for Recognizing Textual Entailment in Vietnamese.", "time": "2023-01-01T15:13:25Z", "link": "http://arxiv.org/abs/2301.00422v1", "id": "2301.00422v1_level1"}
{"author": "Quoc-Loc Duong, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "RTE is a significant problem and is a reasonably active research community. The proposed research works on the approach to this problem are pretty diverse with many different directions. For Vietnamese, the RTE problem is moderately new, but this problem plays a vital role in natural language understanding systems. Currently, methods to solve this problem based on contextual word representation learning models have given outstanding results. However, Vietnamese is a semantically rich language. Therefore, in this paper, we want to present an experiment combining semantic word representation through the SRL task with context representation of BERT relative models for the RTE problem. The experimental results give conclusions about the influence and role of semantic representation on Vietnamese in understanding natural language. The experimental we observe that the semantic-aware contextual representation model has about 1% higher performance than the model that does not incorporate semantic representation. In addition, the effects on the data domain in Vietnamese are also higher than those in English. This result also shows the positive influence of SRL on RTE problem in Vietnamese.", "title": "Leveraging Semantic Representations Combined with Contextual Word Representations for Recognizing Textual Entailment in Vietnamese.", "time": "2023-01-01T15:13:25Z", "link": "http://arxiv.org/abs/2301.00422v1", "id": "2301.00422v1_level2"}
{"author": "Quoc-Loc Duong, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "RTE is a significant problem and is a reasonably active research community. The proposed research works on the approach to this problem are pretty diverse with many different directions. but this problem plays a vital role in natural language understanding systems., the RTE problem is moderately new, For Vietnamese. methods to solve this problem based on contextual word representation learning models have given outstanding results., Currently. However, Vietnamese is a semantically rich language. we want to present an experiment combining semantic word representation through the SRL task with context representation of BERT relative models for the RTE problem. The experimental results give conclusions about the influence and role of semantic representation on Vietnamese in understanding natural language., in this paper, Therefore. The experimental results show that the semantic-aware contextual representation model has about 1% higher performance than the model that does not incorporate semantic representation. the effects on the data domain in Vietnamese are also higher than those in English., In addition. This result also shows the positive influence of SRL on RTE problem in Vietnamese.", "title": "Leveraging Semantic Representations Combined with Contextual Word Representations for Recognizing Textual Entailment in Vietnamese.", "time": "2023-01-01T15:13:25Z", "link": "http://arxiv.org/abs/2301.00422v1", "id": "2301.00422v1_level3"}
{"author": "Quoc-Loc Duong, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "Importantly, Rte is a significant problem and is a reasonably active research community. the proposed research works on the approach to this problem are pretty diverse with many different directions. Importantly, For vietnamese, the rte problem is moderately new, but this problem plays a vital role in natural language understanding systems. Importantly, Currently, methods to solve this problem based on contextual word representation learning models have given outstanding results. Importantly, However, vietnamese is a semantically rich language. Importantly, Therefore, in this paper, we want to present an experiment combining semantic word representation through the srl task with context representation of bert relative models for the rte problem. the experimental results give conclusions about the influence and role of semantic representation on vietnamese in understanding natural language. Importantly, The experimental results show that the semantic-aware contextual representation model has about 1% higher performance than the model that does not incorporate semantic representation. Importantly, In addition, the effects on the data domain in vietnamese are also higher than those in english. Importantly, This result also shows the positive influence of srl on rte problem in vietnamese.", "title": "Importantly, Leveraging semantic representations combined with contextual word representations for recognizing textual entailment in vietnamese.", "time": "2023-01-01T15:13:25Z", "link": "http://arxiv.org/abs/2301.00422v1", "id": "2301.00422v1_level4"}
{"author": "Quoc-Loc Duong, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-01T15:13:25Z", "link": "http://arxiv.org/abs/2301.00422v1", "id": "2301.00422v1_level5"}
{"author": "Hang Thi-Thu Le, Viet-Duc Ho, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "Machine Reading Comprehension has become one of the most advanced and popular research topics in the fields of Natural Language Processing in recent years. The classification of answerability questions is a relatively significant sub-task in machine reading comprehension; however, there haven't been many studies. Retro-Reader is one of the studies that has solved this problem effectively. However, the encoders of most traditional machine reading comprehension frameworks in general and Retro-Reader, in particular, have not been able to exploit the contextual semantic information of the context completely. Inspired by SemBERT, we use semantic role labels from the SRL task to add semantics to pre-trained language frameworks such as mBERT, XLM-R, PhoBERT. This experiment was conducted to compare the influence of semantics on the classification of answerability for the Vietnamese machine reading comprehension. Additionally, we hope this experiment will enhance the encoder for the Retro-Reader framework's Sketchy Reading Module. The improved Retro-Reader framework's encoder with semantics was first applied to the Vietnamese Machine Reading Comprehension task and obtained positive results.", "title": "Integrating Semantic Information into Sketchy Reading Module of Retro-Reader for Vietnamese Machine Reading Comprehension.", "time": "2023-01-01T15:28:27Z", "link": "http://arxiv.org/abs/2301.00429v1", "id": "2301.00429v1_level1"}
{"author": "Hang Thi-Thu Le, Viet-Duc Ho, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "Machine Reading Comprehension has become one of the most advanced and popular research topics in the fields of Natural Language Processing in recent years. The classification of answerability questions is a relatively significant sub-task in machine reading comprehension; however, there haven't been many studies. Retro-Reader is one of the studies that has solved this problem effectively. However, the encoders of most traditional machine reading comprehension models in general and Retro-Reader, in particular, have not been able to exploit the contextual semantic information of the context completely. Inspired by SemBERT, we use semantic role labels from the SRL task to add semantics to pre-trained language models such as mBERT, XLM-R, PhoBERT. This experiment was conducted to compare the influence of semantics on the classification of answerability for the Vietnamese machine reading comprehension. Additionally, we hope this experiment will enhance the encoder for the Retro-Reader model's Sketchy Reading Module. The improved Retro-Reader model's encoder with semantics was first applied to the Vietnamese Machine Reading Comprehension task and obtained positive results.", "title": "Integrating Semantic Information into Sketchy Reading Module of Retro-Reader for Vietnamese Machine Reading Comprehension.", "time": "2023-01-01T15:28:27Z", "link": "http://arxiv.org/abs/2301.00429v1", "id": "2301.00429v1_level2"}
{"author": "Hang Thi-Thu Le, Viet-Duc Ho, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "there haven't been many studies., Machine Reading Comprehension has become one of the most advanced and popular research topics in the fields of Natural Language Processing in recent years. The classification of answerability questions is a relatively significant sub-task in machine reading comprehension; however. Retro-Reader is one of the studies that has solved this problem effectively. PhoBERT., XLM-R, we use semantic role labels from the SRL task to add semantics to pre-trained language models such as mBERT, have not been able to exploit the contextual semantic information of the context completely. Inspired by SemBERT, in particular, the encoders of most traditional machine reading comprehension models in general and Retro-Reader, However. This experiment was conducted to compare the influence of semantics on the classification of answerability for the Vietnamese machine reading comprehension. we hope this experiment will enhance the encoder for the Retro-Reader model's Sketchy Reading Module., Additionally. The improved Retro-Reader model's encoder with semantics was first applied to the Vietnamese Machine Reading Comprehension task and obtained positive results.", "title": "Integrating Semantic Information into Sketchy Reading Module of Retro-Reader for Vietnamese Machine Reading Comprehension.", "time": "2023-01-01T15:28:27Z", "link": "http://arxiv.org/abs/2301.00429v1", "id": "2301.00429v1_level3"}
{"author": "Hang Thi-Thu Le, Viet-Duc Ho, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "Importantly, Machine reading comprehension has become one of the most advanced and popular research topics in the fields of natural language processing in recent years. the classification of answerability questions is a relatively significant sub-task in machine reading comprehension; however, there haven't been many studies. Importantly, Retro-reader is one of the studies that has solved this problem effectively. Importantly, However, the encoders of most traditional machine reading comprehension models in general and retro-reader, in particular, have not been able to exploit the contextual semantic information of the context completely. inspired by sembert, we use semantic role labels from the srl task to add semantics to pre-trained language models such as mbert, xlm-r, phobert. Importantly, This experiment was conducted to compare the influence of semantics on the classification of answerability for the vietnamese machine reading comprehension. Importantly, Additionally, we hope this experiment will enhance the encoder for the retro-reader model's sketchy reading module. Importantly, The improved retro-reader model's encoder with semantics was first applied to the vietnamese machine reading comprehension task and obtained positive results.", "title": "Importantly, Integrating semantic information into sketchy reading module of retro-reader for vietnamese machine reading comprehension.", "time": "2023-01-01T15:28:27Z", "link": "http://arxiv.org/abs/2301.00429v1", "id": "2301.00429v1_level4"}
{"author": "Hang Thi-Thu Le, Viet-Duc Ho, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-01T15:28:27Z", "link": "http://arxiv.org/abs/2301.00429v1", "id": "2301.00429v1_level5"}
{"author": "Sudhansu Bala Das, Divyajoti Panda, Tapas Kumar Mishra, Bidyut Kr. Patra", "abstract": "Machine Translation (MT) system generally aims at automatic representation of source language into target language retaining the originality of context using various Natural Language Processing (NLP) techniques. Among various NLP approachs, Statistical Machine Translation(SMT). SMT uses probabilistic and statistical techniques to analyze information and conversion. This paper canvasses about the development of bilingual SMT frameworks for translating English to fifteen low-resource Indian Languages (ILs) and vice versa. At the outset, all 15 languages are briefed with a short description related to our experimental need. Further, a detailed analysis of Samanantar and OPUS dataset for framework building, along with standard benchmark dataset (Flores-200) for fine-tuning and testing, is done as a part of our experiment. Different preprocessing approaches are proposed in this paper to handle the noise of the dataset. To create the system, MOSES open-source SMT toolkit is explored. Distance reordering is utilized with the aim to understand the rules of grammar and context-dependent adjustments through a phrase reordering categorization framework. In our experiment, the quality of the translation is evaluated using standard metrics such as BLEU, METEOR, and RIBES.", "title": "Statistical Machine Translation for Indic Languages.", "time": "2023-01-02T06:23:12Z", "link": "http://arxiv.org/abs/2301.00539v1", "id": "2301.00539v1_level1"}
{"author": "Sudhansu Bala Das, Divyajoti Panda, Tapas Kumar Mishra, Bidyut Kr. Patra", "abstract": "Machine Translation (MT) system generally aims at automatic representation of source language into target language retaining the originality of context using various Natural Language Processing (NLP) techniques. Among various NLP methods, Statistical Machine Translation(SMT). SMT uses probabilistic and statistical techniques to analyze information and conversion. This paper canvasses about the development of bilingual SMT models for translating English to fifteen low-resource Indian Languages (ILs) and vice versa. At the outset, all 15 languages are briefed with a short description related to our experimental need. Further, a detailed analysis of Samanantar and OPUS dataset for model building, along with standard benchmark dataset (Flores-200) for fine-tuning and testing, is done as a part of our experiment. Different preprocessing approaches are proposed in this paper to handle the noise of the dataset. To create the system, MOSES open-source SMT toolkit is explored. Distance reordering is utilized with the aim to understand the rules of grammar and context-dependent adjustments through a phrase reordering categorization framework. In our experiment, the quality of the translation is evaluated using standard metrics such as BLEU, METEOR, and RIBES.", "title": "Statistical Machine Translation for Indic Languages.", "time": "2023-01-02T06:23:12Z", "link": "http://arxiv.org/abs/2301.00539v1", "id": "2301.00539v1_level2"}
{"author": "Sudhansu Bala Das, Divyajoti Panda, Tapas Kumar Mishra, Bidyut Kr. Patra", "abstract": "Machine Translation (MT) system generally aims at automatic representation of source language into target language retaining the originality of context using various Natural Language Processing (NLP) techniques. Statistical Machine Translation(SMT)., Among various NLP methods. SMT uses probabilistic and statistical techniques to analyze information and conversion. This paper canvasses about the development of bilingual SMT models for translating English to fifteen low-resource Indian Languages (ILs) and vice versa. At the outset, all 15 languages are briefed with a short description related to our experimental need. is done as a part of our experiment., along with standard benchmark dataset (Flores-200) for fine-tuning and testing, a detailed analysis of Samanantar and OPUS dataset for model building, Further. Different preprocessing approaches are proposed in this paper to handle the noise of the dataset. MOSES open-source SMT toolkit is explored. Distance reordering is utilized with the aim to understand the rules of grammar and context-dependent adjustments through a phrase reordering categorization framework., To create the system. and RIBES, METEOR, the quality of the translation is evaluated using standard metrics such as BLEU, In our experiment.", "title": "Statistical Machine Translation for Indic Languages.", "time": "2023-01-02T06:23:12Z", "link": "http://arxiv.org/abs/2301.00539v1", "id": "2301.00539v1_level3"}
{"author": "Sudhansu Bala Das, Divyajoti Panda, Tapas Kumar Mishra, Bidyut Kr. Patra", "abstract": "Importantly, Machine translation (mt) system generally aims at automatic representation of source language into target language retaining the originality of context using various natural language processing (nlp) techniques. Importantly, Among various nlp methods, statistical machine translation(smt). Importantly, Smt uses probabilistic and statistical techniques to analyze information and conversion. Importantly, This paper canvasses about the development of bilingual smt models for translating english to fifteen low-resource indian languages (ils) and vice versa. Importantly, At the outset, all 15 languages are briefed with a short description related to our experimental need. Importantly, Further, a detailed analysis of samanantar and opus dataset for model building, along with standard benchmark dataset (flores-200) for fine-tuning and testing, is done as a part of our experiment. Importantly, Different preprocessing approaches are proposed in this paper to handle the noise of the dataset. Importantly, To create the system, moses open-source smt toolkit is explored. distance reordering is utilized with the aim to understand the rules of grammar and context-dependent adjustments through a phrase reordering categorization framework. Importantly, In our experiment, the quality of the translation is evaluated using standard metrics such as bleu, meteor, and ribes.", "title": "Importantly, Statistical machine translation for indic languages.", "time": "2023-01-02T06:23:12Z", "link": "http://arxiv.org/abs/2301.00539v1", "id": "2301.00539v1_level4"}
{"author": "Sudhansu Bala Das, Divyajoti Panda, Tapas Kumar Mishra, Bidyut Kr. Patra", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-02T06:23:12Z", "link": "http://arxiv.org/abs/2301.00539v1", "id": "2301.00539v1_level5"}
{"author": "Hamed Vahdat-Nejad, Mohammad Ghasem Akbari, Fatemeh Salmani, Faezeh Azizi, Hamid-Reza Nili-Sani", "abstract": "With Twitter's growth and popularity, a huge number of views are shared by users on various topics, making this platform a valuable information source on various political, social, and economic issues. This paper investigates English tweets on the Russia-Ukraine war to analyze trends reflecting users' opinions and sentiments regarding the conflict. The tweets' positive and negative sentiments are analyzed using a BERT-based framework, and the time series associated with the frequency of positive and negative tweets for various countries is calculated. Then, we propose a approach based on the neighborhood average for frameworking and clustering the time series of countries. The clustering results provide valuable insight into public opinion regarding this conflict. Among other things, we can mention the similar thoughts of users from the United States, Canada, the United Kingdom, and most Western European countries versus the shared views of Eastern European, Scandinavian, Asian, and South American nations toward the conflict.", "title": "Russia-Ukraine war: Modeling and Clustering the Sentiments Trends of Various Countries.", "time": "2023-01-02T11:32:47Z", "link": "http://arxiv.org/abs/2301.00604v1", "id": "2301.00604v1_level1"}
{"author": "Hamed Vahdat-Nejad, Mohammad Ghasem Akbari, Fatemeh Salmani, Faezeh Azizi, Hamid-Reza Nili-Sani", "abstract": "With Twitter's growth and popularity, a huge number of views are shared by users on various topics, making this platform a valuable information source on various political, social, and economic issues. This paper investigates English tweets on the Russia-Ukraine war to analyze trends reflecting users' opinions and sentiments regarding the conflict. The tweets' positive and negative sentiments are analyzed using a BERT-based model, and the time series associated with the frequency of positive and negative tweets for various countries is calculated. Then, we propose a method based on the neighborhood average for modeling and clustering the time series of countries. The clustering results provide valuable insight into public opinion regarding this conflict. Among other things, we can mention the similar thoughts of users from the United States, Canada, the United Kingdom, and most Western European countries versus the shared views of Eastern European, Scandinavian, Asian, and South American nations toward the conflict.", "title": "Russia-Ukraine war: Modeling and Clustering the Sentiments Trends of Various Countries.", "time": "2023-01-02T11:32:47Z", "link": "http://arxiv.org/abs/2301.00604v1", "id": "2301.00604v1_level2"}
{"author": "Hamed Vahdat-Nejad, Mohammad Ghasem Akbari, Fatemeh Salmani, Faezeh Azizi, Hamid-Reza Nili-Sani", "abstract": "and economic issues., social, making this platform a valuable information source on various political, a huge number of views are shared by users on various topics, With Twitter's growth and popularity. This paper investigates English tweets on the Russia-Ukraine war to analyze trends reflecting users' opinions and sentiments regarding the conflict. and the time series associated with the frequency of positive and negative tweets for various countries is calculated., The tweets' positive and negative sentiments are analyzed using a BERT-based model. we propose a method based on the neighborhood average for modeling and clustering the time series of countries., Then. The clustering results provide valuable insight into public opinion regarding this conflict. and South American nations toward the conflict., Asian, Scandinavian, and most Western European countries versus the shared views of Eastern European, the United Kingdom, Canada, we can mention the similar thoughts of users from the United States, Among other things.", "title": "Russia-Ukraine war: Modeling and Clustering the Sentiments Trends of Various Countries.", "time": "2023-01-02T11:32:47Z", "link": "http://arxiv.org/abs/2301.00604v1", "id": "2301.00604v1_level3"}
{"author": "Hamed Vahdat-Nejad, Mohammad Ghasem Akbari, Fatemeh Salmani, Faezeh Azizi, Hamid-Reza Nili-Sani", "abstract": "Importantly, With twitter's growth and popularity, a huge number of views are shared by users on various topics, making this platform a valuable information source on various political, social, and economic issues. Importantly, This paper investigates english tweets on the russia-ukraine war to analyze trends reflecting users' opinions and sentiments regarding the conflict. Importantly, The tweets' positive and negative sentiments are analyzed using a bert-based model, and the time series associated with the frequency of positive and negative tweets for various countries is calculated. Importantly, Then, we propose a method based on the neighborhood average for modeling and clustering the time series of countries. Importantly, The clustering results provide valuable insight into public opinion regarding this conflict. Importantly, Among other things, we can mention the similar thoughts of users from the united states, canada, the united kingdom, and most western european countries versus the shared views of eastern european, scandinavian, asian, and south american nations toward the conflict.", "title": "Importantly, Russia-ukraine war: modeling and clustering the sentiments trends of various countries.", "time": "2023-01-02T11:32:47Z", "link": "http://arxiv.org/abs/2301.00604v1", "id": "2301.00604v1_level4"}
{"author": "Hamed Vahdat-Nejad, Mohammad Ghasem Akbari, Fatemeh Salmani, Faezeh Azizi, Hamid-Reza Nili-Sani", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-02T11:32:47Z", "link": "http://arxiv.org/abs/2301.00604v1", "id": "2301.00604v1_level5"}
{"author": "Tahereh Firoozi, Hamid Mohammadi, Mark J. Gierl", "abstract": "Research on automated essay scoring has become increasing important because it serves as a approach for evaluating students' written-responses at scale. Scalable approachs for scoring written responses are needed as students migrate to online learning environments resulting in the need to evaluate large numbers of written-response assessments. The purpose of this study is to describe and evaluate three active learning approachs than can be used to minimize the number of essays that must be scored by human raters while still providing the data needed to train a modern automated essay scoring system. The three active learning approachs are the uncertainty-based, the topological-based, and the hybrid approach. These three approachs were used to select essays included as part of the Automated Student Assessment Prize competition that were then classified using a scoring framework that was training with the bidirectional encoder representations from transformer language framework. All three active learning approachs produced strong results, with the topological-based approach producing the most efficient classification. Growth rate accuracy was also evaluated. The active learning approachs produced different levels of efficiency under different sample size allocations but, overall, all three approachs were highly efficient and produced classifications that were similar to one another.", "title": "Using Active Learning Methods to Strategically Select Essays for Automated Scoring.", "time": "2023-04-13T23:17:58Z", "link": "http://arxiv.org/abs/2301.00628v2", "id": "2301.00628v2_level1"}
{"author": "Tahereh Firoozi, Hamid Mohammadi, Mark J. Gierl", "abstract": "Research on automated essay scoring has become increasing important because it serves as a method for evaluating students' written-responses at scale. Scalable methods for scoring written responses are needed as students migrate to online learning environments resulting in the need to evaluate large numbers of written-response assessments. The purpose of this study is to describe and evaluate three active learning methods than can be used to minimize the number of essays that must be scored by human raters while still providing the data needed to train a modern automated essay scoring system. The three active learning methods are the uncertainty-based, the topological-based, and the hybrid method. These three methods were used to select essays included as part of the Automated Student Assessment Prize competition that were then classified using a scoring model that was training with the bidirectional encoder representations from transformer language model. All three active learning methods produced strong results, with the topological-based method producing the most efficient classification. Growth rate accuracy was also evaluated. The active learning methods produced different levels of efficiency under different sample size allocations but, overall, all three methods were highly efficient and produced classifications that were similar to one another.", "title": "Using Active Learning Methods to Strategically Select Essays for Automated Scoring.", "time": "2023-04-13T23:17:58Z", "link": "http://arxiv.org/abs/2301.00628v2", "id": "2301.00628v2_level2"}
{"author": "Tahereh Firoozi, Hamid Mohammadi, Mark J. Gierl", "abstract": "Research on automated essay scoring has become increasing important because it serves as a method for evaluating students' written-responses at scale. Scalable methods for scoring written responses are needed as students migrate to online learning environments resulting in the need to evaluate large numbers of written-response assessments. The purpose of this study is to describe and evaluate three active learning methods than can be used to minimize the number of essays that must be scored by human raters while still providing the data needed to train a modern automated essay scoring system. and the hybrid method., the topological-based, The three active learning methods are the uncertainty-based. These three methods were used to select essays included as part of the Automated Student Assessment Prize competition that were then classified using a scoring model that was training with the bidirectional encoder representations from transformer language model. with the topological-based method producing the most efficient classification., All three active learning methods produced strong results. Growth rate accuracy was also evaluated. all three methods were highly efficient and produced classifications that were similar to one another., overall, The active learning methods produced different levels of efficiency under different sample size allocations but.", "title": "Using Active Learning Methods to Strategically Select Essays for Automated Scoring.", "time": "2023-04-13T23:17:58Z", "link": "http://arxiv.org/abs/2301.00628v2", "id": "2301.00628v2_level3"}
{"author": "Tahereh Firoozi, Hamid Mohammadi, Mark J. Gierl", "abstract": "Importantly, Research on automated essay scoring has become increasing important because it serves as a method for evaluating students' written-responses at scale. scalable methods for scoring written responses are needed as students migrate to online learning environments resulting in the need to evaluate large numbers of written-response assessments. Importantly, The purpose of this study is to describe and evaluate three active learning methods than can be used to minimize the number of essays that must be scored by human raters while still providing the data needed to train a modern automated essay scoring system. Importantly, The three active learning methods are the uncertainty-based, the topological-based, and the hybrid method. Importantly, These three methods were used to select essays included as part of the automated student assessment prize competition that were then classified using a scoring model that was training with the bidirectional encoder representations from transformer language model. Importantly, All three active learning methods produced strong results, with the topological-based method producing the most efficient classification. Importantly, Growth rate accuracy was also evaluated. Importantly, The active learning methods produced different levels of efficiency under different sample size allocations but, overall, all three methods were highly efficient and produced classifications that were similar to one another.", "title": "Importantly, Using active learning methods to strategically select essays for automated scoring.", "time": "2023-04-13T23:17:58Z", "link": "http://arxiv.org/abs/2301.00628v2", "id": "2301.00628v2_level4"}
{"author": "Tahereh Firoozi, Hamid Mohammadi, Mark J. Gierl", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-04-13T23:17:58Z", "link": "http://arxiv.org/abs/2301.00628v2", "id": "2301.00628v2_level5"}
{"author": "Zhongtao Chen, Chenghu Mi, Siwei Duo, Jingfei He, Yatong Zhou", "abstract": "Text clustering and topic extraction are two important tasks in text mining. Usually, these two tasks are performed separately. For topic extraction to facilitate clustering, we can first project texts into a topic space and then perform a clustering algorithm to obtain clusters. To promote topic extraction by clustering, we can first obtain clusters with a clustering algorithm and then extract cluster-specific topics. However, this naive strategy ignores the fact that text clustering and topic extraction are strongly correlated and follow a chicken-and-egg relationship. Performing them separately fails to make them mutually benefit each other to achieve the best overall performance. In this paper, we propose an unsupervised text clustering and topic extraction framework (ClusTop) which integrates text clustering and topic extraction into a unified framework and can achieve high-quality clustering result and extract topics from each cluster simultaneously. Our framework includes four components: enhanced language framework training, dimensionality reduction, clustering and topic extraction, where the enhanced language framework can be viewed as a bridge between clustering and topic extraction. On one hand, it provides text embeddings with a strong cluster structure which facilitates effective text clustering; on the other hand, it pays high attention on the topic related words for topic extraction because of its self-attention architecture. Moreover, the training of enhanced language framework is unsupervised. Experiments on two datasets demonstrate the effectiveness of our framework and provide benchmarks for different framework combinations in this framework.", "title": "ClusTop: An unsupervised and integrated text clustering and topic extraction framework.", "time": "2023-01-03T03:26:26Z", "link": "http://arxiv.org/abs/2301.00818v1", "id": "2301.00818v1_level1"}
{"author": "Zhongtao Chen, Chenghu Mi, Siwei Duo, Jingfei He, Yatong Zhou", "abstract": "Text clustering and topic extraction are two important tasks in text mining. Usually, these two tasks are performed separately. For topic extraction to facilitate clustering, we can first project texts into a topic space and then perform a clustering algorithm to obtain clusters. To promote topic extraction by clustering, we can first obtain clusters with a clustering algorithm and then extract cluster-specific topics. However, this naive strategy ignores the fact that text clustering and topic extraction are strongly correlated and follow a chicken-and-egg relationship. Performing them separately fails to make them mutually benefit each other to achieve the best overall performance. In this paper, we propose an unsupervised text clustering and topic extraction framework (ClusTop) which integrates text clustering and topic extraction into a unified framework and can achieve high-quality clustering result and extract topics from each cluster simultaneously. Our framework includes four components: enhanced language model training, dimensionality reduction, clustering and topic extraction, where the enhanced language model can be viewed as a bridge between clustering and topic extraction. On one hand, it provides text embeddings with a strong cluster structure which facilitates effective text clustering; on the other hand, it pays high attention on the topic related words for topic extraction because of its self-attention architecture. Moreover, the training of enhanced language model is unsupervised. Experiments on two datasets indicate the effectiveness of our framework and provide benchmarks for different model combinations in this framework.", "title": "ClusTop: An unsupervised and integrated text clustering and topic extraction framework.", "time": "2023-01-03T03:26:26Z", "link": "http://arxiv.org/abs/2301.00818v1", "id": "2301.00818v1_level2"}
{"author": "Zhongtao Chen, Chenghu Mi, Siwei Duo, Jingfei He, Yatong Zhou", "abstract": "these two tasks are performed separately., Text clustering and topic extraction are two important tasks in text mining. Usually. we can first project texts into a topic space and then perform a clustering algorithm to obtain clusters., For topic extraction to facilitate clustering. we can first obtain clusters with a clustering algorithm and then extract cluster-specific topics., To promote topic extraction by clustering. this naive strategy ignores the fact that text clustering and topic extraction are strongly correlated and follow a chicken-and-egg relationship., However. Performing them separately fails to make them mutually benefit each other to achieve the best overall performance. we propose an unsupervised text clustering and topic extraction framework (ClusTop) which integrates text clustering and topic extraction into a unified framework and can achieve high-quality clustering result and extract topics from each cluster simultaneously., In this paper. where the enhanced language model can be viewed as a bridge between clustering and topic extraction., dimensionality reduction, clustering and topic extraction, Our framework includes four components: enhanced language model training. it pays high attention on the topic related words for topic extraction because of its self-attention architecture., it provides text embeddings with a strong cluster structure which facilitates effective text clustering; on the other hand, On one hand. the training of enhanced language model is unsupervised., Moreover. Experiments on two datasets demonstrate the effectiveness of our framework and provide benchmarks for different model combinations in this framework.", "title": "ClusTop: An unsupervised and integrated text clustering and topic extraction framework.", "time": "2023-01-03T03:26:26Z", "link": "http://arxiv.org/abs/2301.00818v1", "id": "2301.00818v1_level3"}
{"author": "Zhongtao Chen, Chenghu Mi, Siwei Duo, Jingfei He, Yatong Zhou", "abstract": "Importantly, Text clustering and topic extraction are two important tasks in text mining. usually, these two tasks are performed separately. Importantly, For topic extraction to facilitate clustering, we can first project texts into a topic space and then perform a clustering algorithm to obtain clusters. Importantly, To promote topic extraction by clustering, we can first obtain clusters with a clustering algorithm and then extract cluster-specific topics. Importantly, However, this naive strategy ignores the fact that text clustering and topic extraction are strongly correlated and follow a chicken-and-egg relationship. Importantly, Performing them separately fails to make them mutually benefit each other to achieve the best overall performance. Importantly, In this paper, we propose an unsupervised text clustering and topic extraction framework (clustop) which integrates text clustering and topic extraction into a unified framework and can achieve high-quality clustering result and extract topics from each cluster simultaneously. Importantly, Our framework includes four components: enhanced language model training, dimensionality reduction, clustering and topic extraction, where the enhanced language model can be viewed as a bridge between clustering and topic extraction. Importantly, On one hand, it provides text embeddings with a strong cluster structure which facilitates effective text clustering; on the other hand, it pays high attention on the topic related words for topic extraction because of its self-attention architecture. Importantly, Moreover, the training of enhanced language model is unsupervised. Importantly, Experiments on two datasets demonstrate the effectiveness of our framework and provide benchmarks for different model combinations in this framework.", "title": "Importantly, Clustop: an unsupervised and integrated text clustering and topic extraction framework.", "time": "2023-01-03T03:26:26Z", "link": "http://arxiv.org/abs/2301.00818v1", "id": "2301.00818v1_level4"}
{"author": "Zhongtao Chen, Chenghu Mi, Siwei Duo, Jingfei He, Yatong Zhou", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-03T03:26:26Z", "link": "http://arxiv.org/abs/2301.00818v1", "id": "2301.00818v1_level5"}
{"author": "Xiuying Chen, Mingzhe Li, Shen Gao, Zhangming Chan, Dongyan Zhao, Xin Gao, Xiangliang Zhang, Rui Yan", "abstract": "Nowadays, time-stamped web documents related to a general news query floods spread throughout the Internet, and timeline summarization targets concisely summarizing the evolution trajectory of events along the timeline. Unlike traditional document summarization, timeline summarization needs to framework the time series information of the input events and summarize important events in chronological order. To tackle this challenge, in this paper, we propose a Unified Timeline Summarizer (UTS) that can generate abstractive and extractive timeline summaries in time order. Concretely, in the encoder part, we propose a graph-based event encoder that relates multiple events according to their content dependency and learns a global representation of each event. In the decoder part, to ensure the chronological order of the abstractive summary, we propose to extract the feature of event-level attention in its generation process with sequential information remained and use it to simulate the evolutionary attention of the ground truth summary. The event-level attention can also be used to assist in extracting summary, where the extracted summary also comes in time sequence. We augment the previous Chinese large-scale timeline summarization dataset and collect a new English timeline dataset. Extensive experiments conducted on these datasets and on the out-of-domain Timeline 17 dataset show that UTS achieves state-of-the-art performance in terms of both automatic and human evaluations.", "title": "Follow the Timeline!. Generating Abstractive and Extractive Timeline Summary in Chronological Order.", "time": "2023-01-02T20:29:40Z", "link": "http://arxiv.org/abs/2301.00867v1", "id": "2301.00867v1_level1"}
{"author": "Xiuying Chen, Mingzhe Li, Shen Gao, Zhangming Chan, Dongyan Zhao, Xin Gao, Xiangliang Zhang, Rui Yan", "abstract": "Nowadays, time-stamped web documents related to a general news query floods spread throughout the Internet, and timeline summarization targets concisely summarizing the evolution trajectory of events along the timeline. Unlike traditional document summarization, timeline summarization needs to model the time series information of the input events and summarize important events in chronological order. To tackle this challenge, in this paper, we propose a Unified Timeline Summarizer (UTS) that can generate abstractive and extractive timeline summaries in time order. Concretely, in the encoder part, we propose a graph-based event encoder that relates multiple events according to their content dependency and learns a global representation of each event. In the decoder part, to ensure the chronological order of the abstractive summary, we propose to extract the feature of event-level attention in its generation process with sequential information remained and use it to simulate the evolutionary attention of the ground truth summary. The event-level attention can also be used to assist in extracting summary, where the extracted summary also comes in time sequence. We augment the previous Chinese large-scale timeline summarization dataset and collect a new English timeline dataset. Extensive experiments conducted on these datasets and on the out-of-domain Timeline 17 dataset show that UTS achieves state-of-the-art performance in terms of both automatic and human evaluations.", "title": "Follow the Timeline!. Generating Abstractive and Extractive Timeline Summary in Chronological Order.", "time": "2023-01-02T20:29:40Z", "link": "http://arxiv.org/abs/2301.00867v1", "id": "2301.00867v1_level2"}
{"author": "Xiuying Chen, Mingzhe Li, Shen Gao, Zhangming Chan, Dongyan Zhao, Xin Gao, Xiangliang Zhang, Rui Yan", "abstract": "and timeline summarization targets concisely summarizing the evolution trajectory of events along the timeline., time-stamped web documents related to a general news query floods spread throughout the Internet, Nowadays. timeline summarization needs to model the time series information of the input events and summarize important events in chronological order., Unlike traditional document summarization. we propose a Unified Timeline Summarizer (UTS) that can generate abstractive and extractive timeline summaries in time order., in this paper, To tackle this challenge. we propose a graph-based event encoder that relates multiple events according to their content dependency and learns a global representation of each event., in the encoder part, Concretely. we propose to extract the feature of event-level attention in its generation process with sequential information remained and use it to simulate the evolutionary attention of the ground truth summary., to ensure the chronological order of the abstractive summary, In the decoder part. where the extracted summary also comes in time sequence., The event-level attention can also be used to assist in extracting summary. We augment the previous Chinese large-scale timeline summarization dataset and collect a new English timeline dataset. Extensive experiments conducted on these datasets and on the out-of-domain Timeline 17 dataset show that UTS achieves state-of-the-art performance in terms of both automatic and human evaluations.", "title": "Follow the Timeline!. Generating Abstractive and Extractive Timeline Summary in Chronological Order.", "time": "2023-01-02T20:29:40Z", "link": "http://arxiv.org/abs/2301.00867v1", "id": "2301.00867v1_level3"}
{"author": "Xiuying Chen, Mingzhe Li, Shen Gao, Zhangming Chan, Dongyan Zhao, Xin Gao, Xiangliang Zhang, Rui Yan", "abstract": "Importantly, Nowadays, time-stamped web documents related to a general news query floods spread throughout the internet, and timeline summarization targets concisely summarizing the evolution trajectory of events along the timeline. Importantly, Unlike traditional document summarization, timeline summarization needs to model the time series information of the input events and summarize important events in chronological order. Importantly, To tackle this challenge, in this paper, we propose a unified timeline summarizer (uts) that can generate abstractive and extractive timeline summaries in time order. Importantly, Concretely, in the encoder part, we propose a graph-based event encoder that relates multiple events according to their content dependency and learns a global representation of each event. Importantly, In the decoder part, to ensure the chronological order of the abstractive summary, we propose to extract the feature of event-level attention in its generation process with sequential information remained and use it to simulate the evolutionary attention of the ground truth summary. Importantly, The event-level attention can also be used to assist in extracting summary, where the extracted summary also comes in time sequence. Importantly, We augment the previous chinese large-scale timeline summarization dataset and collect a new english timeline dataset. extensive experiments conducted on these datasets and on the out-of-domain timeline 17 dataset show that uts achieves state-of-the-art performance in terms of both automatic and human evaluations.", "title": "Importantly, Follow the timeline!. Importantly, Generating abstractive and extractive timeline summary in chronological order.", "time": "2023-01-02T20:29:40Z", "link": "http://arxiv.org/abs/2301.00867v1", "id": "2301.00867v1_level4"}
{"author": "Xiuying Chen, Mingzhe Li, Shen Gao, Zhangming Chan, Dongyan Zhao, Xin Gao, Xiangliang Zhang, Rui Yan", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-02T20:29:40Z", "link": "http://arxiv.org/abs/2301.00867v1", "id": "2301.00867v1_level5"}
{"author": "Steven H. Wang, Antoine Scardigli, Leonard Tang, Wei Chen, Dimitry Levkin, Anya Chen, Spencer Ball, Thomas Woodside, Oliver Zhang, Dan Hendrycks", "abstract": "Reading comprehension of legal text can be a particularly challenging task due to the length and complexity of legal clauses and a shortage of expert-annotated datasets. To address this challenge, we introduce the Merger Agreement Understanding Dataset (MAUD), an expert-annotated reading comprehension dataset based on the American Bar Association's 2021 Public Target Deal Points Study, with over 39,000 examples and over 47,000 total annotations. Our fine-tuned Transformer baselines show promising results, with frameworks performing well above random on most questions. However, on a large subset of questions, there is still room for significant improvement. As the only expert-annotated merger agreement dataset, MAUD is valuable as a benchmark for both the legal profession and the NLP community.", "title": "MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding.", "time": "2023-11-24T14:24:01Z", "link": "http://arxiv.org/abs/2301.00876v3", "id": "2301.00876v3_level1"}
{"author": "Steven H. Wang, Antoine Scardigli, Leonard Tang, Wei Chen, Dimitry Levkin, Anya Chen, Spencer Ball, Thomas Woodside, Oliver Zhang, Dan Hendrycks", "abstract": "Reading comprehension of legal text can be a particularly challenging task due to the length and complexity of legal clauses and a shortage of expert-annotated datasets. To address this challenge, we introduce the Merger Agreement Understanding Dataset (MAUD), an expert-annotated reading comprehension dataset based on the American Bar Association's 2021 Public Target Deal Points Study, with over 39,000 examples and over 47,000 total annotations. Our fine-tuned Transformer baselines show promising results, with models performing well above random on most questions. However, on a large subset of questions, there is still room for significant improvement. As the only expert-annotated merger agreement dataset, MAUD is valuable as a benchmark for both the legal profession and the NLP community.", "title": "MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding.", "time": "2023-11-24T14:24:01Z", "link": "http://arxiv.org/abs/2301.00876v3", "id": "2301.00876v3_level2"}
{"author": "Steven H. Wang, Antoine Scardigli, Leonard Tang, Wei Chen, Dimitry Levkin, Anya Chen, Spencer Ball, Thomas Woodside, Oliver Zhang, Dan Hendrycks", "abstract": "Reading comprehension of legal text can be a particularly challenging task due to the length and complexity of legal clauses and a shortage of expert-annotated datasets. with over 39,000 examples and over 47,000 total annotations., an expert-annotated reading comprehension dataset based on the American Bar Association's 2021 Public Target Deal Points Study, we introduce the Merger Agreement Understanding Dataset (MAUD), To address this challenge. with models performing well above random on most questions., Our fine-tuned Transformer baselines show promising results. there is still room for significant improvement., on a large subset of questions, However. MAUD is valuable as a benchmark for both the legal profession and the NLP community., As the only expert-annotated merger agreement dataset.", "title": "MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement Understanding.", "time": "2023-11-24T14:24:01Z", "link": "http://arxiv.org/abs/2301.00876v3", "id": "2301.00876v3_level3"}
{"author": "Steven H. Wang, Antoine Scardigli, Leonard Tang, Wei Chen, Dimitry Levkin, Anya Chen, Spencer Ball, Thomas Woodside, Oliver Zhang, Dan Hendrycks", "abstract": "Importantly, Reading comprehension of legal text can be a particularly challenging task due to the length and complexity of legal clauses and a shortage of expert-annotated datasets. Importantly, To address this challenge, we introduce the merger agreement understanding dataset (maud), an expert-annotated reading comprehension dataset based on the american bar association's 2021 public target deal points study, with over 39,000 examples and over 47,000 total annotations. Importantly, Our fine-tuned transformer baselines show promising results, with models performing well above random on most questions. Importantly, However, on a large subset of questions, there is still room for significant improvement. Importantly, As the only expert-annotated merger agreement dataset, maud is valuable as a benchmark for both the legal profession and the nlp community.", "title": "Importantly, Maud: an expert-annotated legal nlp dataset for merger agreement understanding.", "time": "2023-11-24T14:24:01Z", "link": "http://arxiv.org/abs/2301.00876v3", "id": "2301.00876v3_level4"}
{"author": "Steven H. Wang, Antoine Scardigli, Leonard Tang, Wei Chen, Dimitry Levkin, Anya Chen, Spencer Ball, Thomas Woodside, Oliver Zhang, Dan Hendrycks", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-11-24T14:24:01Z", "link": "http://arxiv.org/abs/2301.00876v3", "id": "2301.00876v3_level5"}
{"author": "Samiran Gode, Supreeth Bare, Bhiksha Raj, Hyungon Yoo", "abstract": "Our paper aims to analyze political polarization in US political system using Language Models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates views on the economy, healthcare, education and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a Language framework based approach that helps analyze how polarized a candidate is. Our data is divided into 2 parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, etc. We further split this data into 4 phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization we begin by showing results from some classical language frameworks in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background.", "title": "Understanding Political Polarisation using Language Models: A dataset and approach.", "time": "2023-01-02T22:15:04Z", "link": "http://arxiv.org/abs/2301.00891v1", "id": "2301.00891v1_level1"}
{"author": "Samiran Gode, Supreeth Bare, Bhiksha Raj, Hyungon Yoo", "abstract": "Our paper aims to analyze political polarization in US political system using Language Models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates views on the economy, healthcare, education and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is. Our data is divided into 2 parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, etc. We further split this data into 4 phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background.", "title": "Understanding Political Polarisation using Language Models: A dataset and method.", "time": "2023-01-02T22:15:04Z", "link": "http://arxiv.org/abs/2301.00891v1", "id": "2301.00891v1_level2"}
{"author": "Samiran Gode, Supreeth Bare, Bhiksha Raj, Hyungon Yoo", "abstract": "and thereby help candidates make an informed decision., Our paper aims to analyze political polarization in US political system using Language Models. education and other social issues., healthcare, The availability of this information will help voters understand their candidates views on the economy. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is. etc., alma mater, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, background information and political information about a candidate, Our data is divided into 2 parts. to help understand if and how the polarization amongst candidates changes., We further split this data into 4 phases chronologically. This data has been cleaned to remove biases. To understand the polarization we begin by showing results from some classical language models in Word2Vec and Doc2Vec. to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background., a transformer based encoder, And then use more powerful techniques like the Longformer.", "title": "Understanding Political Polarisation using Language Models: A dataset and method.", "time": "2023-01-02T22:15:04Z", "link": "http://arxiv.org/abs/2301.00891v1", "id": "2301.00891v1_level3"}
{"author": "Samiran Gode, Supreeth Bare, Bhiksha Raj, Hyungon Yoo", "abstract": "Importantly, Our paper aims to analyze political polarization in us political system using language models, and thereby help candidates make an informed decision. Importantly, The availability of this information will help voters understand their candidates views on the economy, healthcare, education and other social issues. Importantly, Our main contributions are a dataset extracted from wikipedia that spans the past 120 years and a language model based method that helps analyze how polarized a candidate is. Importantly, Our data is divided into 2 parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, etc. Importantly, We further split this data into 4 phases chronologically, to help understand if and how the polarization amongst candidates changes. Importantly, This data has been cleaned to remove biases. Importantly, To understand the polarization we begin by showing results from some classical language models in word2vec and doc2vec. Importantly, And then use more powerful techniques like the longformer, a transformer based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background.", "title": "Importantly, Understanding political polarisation using language models: a dataset and method.", "time": "2023-01-02T22:15:04Z", "link": "http://arxiv.org/abs/2301.00891v1", "id": "2301.00891v1_level4"}
{"author": "Samiran Gode, Supreeth Bare, Bhiksha Raj, Hyungon Yoo", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-02T22:15:04Z", "link": "http://arxiv.org/abs/2301.00891v1", "id": "2301.00891v1_level5"}
{"author": "Mingzhe Li, Xiuying Chen, Weiheng Liao, Yang Song, Tao Zhang, Dongyan Zhao, Rui Yan", "abstract": "Interview has been regarded as one of the most crucial step for recruitment. To fully prepare for the interview with the recruiters, job seekers usually practice with mock interviews between each other. However, such a mock interview with peers is generally far away from the real interview experience: the mock interviewers are not guaranteed to be professional and are not likely to behave like a real interviewer. Due to the rapid growth of online recruitment in recent years, recruiters tend to have online interviews, which makes it possible to collect real interview data from real interviewers. In this paper, we propose a novel application named EZInterviewer, which aims to learn from the online interview data and provides mock interview services to the job seekers. The task is challenging in two ways: (1) the interview data are now available but still of low-resource; (2) to generate meaningful and relevant interview dialogs requires thorough understanding of both resumes and job descriptions. To address the low-resource challenge, EZInterviewer is trained on a very small set of interview dialogs. The key idea is to reduce the number of parameters that rely on interview dialogs by disentangling the knowledge selector and dialog generator so that most parameters can be trained with ungrounded dialogs as well as the resume data that are not low-resource. Evaluation results on a real-world job interview dialog dataset indicate that we achieve promising results to generate mock interviews. With the help of EZInterviewer, we hope to make mock interview practice become easier for job seekers.", "title": "EZInterviewer: To Improve Job Interview Performance with Mock Interview Generator.", "time": "2023-01-03T07:00:30Z", "link": "http://arxiv.org/abs/2301.00972v1", "id": "2301.00972v1_level1"}
{"author": "Mingzhe Li, Xiuying Chen, Weiheng Liao, Yang Song, Tao Zhang, Dongyan Zhao, Rui Yan", "abstract": "Interview has been regarded as one of the most crucial step for recruitment. To fully prepare for the interview with the recruiters, job seekers usually practice with mock interviews between each other. However, such a mock interview with peers is generally far away from the real interview experience: the mock interviewers are not guaranteed to be professional and are not likely to behave like a real interviewer. Due to the rapid growth of online recruitment in recent years, recruiters tend to have online interviews, which makes it possible to collect real interview data from real interviewers. In this paper, we propose a novel application named EZInterviewer, which aims to learn from the online interview data and provides mock interview services to the job seekers. The task is challenging in two ways: (1) the interview data are now available but still of low-resource; (2) to generate meaningful and relevant interview dialogs requires thorough understanding of both resumes and job descriptions. To address the low-resource challenge, EZInterviewer is trained on a very small set of interview dialogs. The key idea is to reduce the number of parameters that rely on interview dialogs by disentangling the knowledge selector and dialog generator so that most parameters can be trained with ungrounded dialogs as well as the resume data that are not low-resource. Evaluation results on a real-world job interview dialog dataset indicate that we achieve promising results to generate mock interviews. With the help of EZInterviewer, we hope to make mock interview practice become easier for job seekers.", "title": "EZInterviewer: To Improve Job Interview Performance with Mock Interview Generator.", "time": "2023-01-03T07:00:30Z", "link": "http://arxiv.org/abs/2301.00972v1", "id": "2301.00972v1_level2"}
{"author": "Mingzhe Li, Xiuying Chen, Weiheng Liao, Yang Song, Tao Zhang, Dongyan Zhao, Rui Yan", "abstract": "job seekers usually practice with mock interviews between each other., Interview has been regarded as one of the most crucial step for recruitment. To fully prepare for the interview with the recruiters. such a mock interview with peers is generally far away from the real interview experience: the mock interviewers are not guaranteed to be professional and are not likely to behave like a real interviewer., However. which makes it possible to collect real interview data from real interviewers., recruiters tend to have online interviews, Due to the rapid growth of online recruitment in recent years. which aims to learn from the online interview data and provides mock interview services to the job seekers., we propose a novel application named EZInterviewer, In this paper. The task is challenging in two ways: (1) the interview data are now available but still of low-resource; (2) to generate meaningful and relevant interview dialogs requires thorough understanding of both resumes and job descriptions. EZInterviewer is trained on a very small set of interview dialogs., To address the low-resource challenge. The key idea is to reduce the number of parameters that rely on interview dialogs by disentangling the knowledge selector and dialog generator so that most parameters can be trained with ungrounded dialogs as well as the resume data that are not low-resource. Evaluation results on a real-world job interview dialog dataset indicate that we achieve promising results to generate mock interviews. we hope to make mock interview practice become easier for job seekers., With the help of EZInterviewer.", "title": "EZInterviewer: To Improve Job Interview Performance with Mock Interview Generator.", "time": "2023-01-03T07:00:30Z", "link": "http://arxiv.org/abs/2301.00972v1", "id": "2301.00972v1_level3"}
{"author": "Mingzhe Li, Xiuying Chen, Weiheng Liao, Yang Song, Tao Zhang, Dongyan Zhao, Rui Yan", "abstract": "Importantly, Interview has been regarded as one of the most crucial step for recruitment. to fully prepare for the interview with the recruiters, job seekers usually practice with mock interviews between each other. Importantly, However, such a mock interview with peers is generally far away from the real interview experience: the mock interviewers are not guaranteed to be professional and are not likely to behave like a real interviewer. Importantly, Due to the rapid growth of online recruitment in recent years, recruiters tend to have online interviews, which makes it possible to collect real interview data from real interviewers. Importantly, In this paper, we propose a novel application named ezinterviewer, which aims to learn from the online interview data and provides mock interview services to the job seekers. Importantly, The task is challenging in two ways: (1) the interview data are now available but still of low-resource; (2) to generate meaningful and relevant interview dialogs requires thorough understanding of both resumes and job descriptions. Importantly, To address the low-resource challenge, ezinterviewer is trained on a very small set of interview dialogs. Importantly, The key idea is to reduce the number of parameters that rely on interview dialogs by disentangling the knowledge selector and dialog generator so that most parameters can be trained with ungrounded dialogs as well as the resume data that are not low-resource. evaluation results on a real-world job interview dialog dataset indicate that we achieve promising results to generate mock interviews. Importantly, With the help of ezinterviewer, we hope to make mock interview practice become easier for job seekers.", "title": "Importantly, Ezinterviewer: to improve job interview performance with mock interview generator.", "time": "2023-01-03T07:00:30Z", "link": "http://arxiv.org/abs/2301.00972v1", "id": "2301.00972v1_level4"}
{"author": "Mingzhe Li, Xiuying Chen, Weiheng Liao, Yang Song, Tao Zhang, Dongyan Zhao, Rui Yan", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-03T07:00:30Z", "link": "http://arxiv.org/abs/2301.00972v1", "id": "2301.00972v1_level5"}
{"author": "Longxu Dou, Yan Gao, Xuqi Liu, Mingyang Pan, Dingzirui Wang, Wanxiang Che, Dechen Zhan, Min-Yen Kan, Jian-Guang Lou", "abstract": "In this paper, we study the problem of knowledge-intensive text-to-SQL, in which domain knowledge is necessary to parse expert questions into SQL queries over domain-specific tables. We formalize this scenario by building a new Chinese benchmark KnowSQL consisting of domain-specific questions covering various domains. We then address this problem by presenting formulaic knowledge, rather than by annotating additional data examples. More concretely, we construct a formulaic knowledge bank as a domain knowledge base and propose a framework (ReGrouP) to leverage this formulaic knowledge during parsing. Experiments using ReGrouP demonstrate a significant 28.2% improvement overall on KnowSQL.", "title": "Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge.", "time": "2023-01-03T12:37:47Z", "link": "http://arxiv.org/abs/2301.01067v1", "id": "2301.01067v1_level1"}
{"author": "Longxu Dou, Yan Gao, Xuqi Liu, Mingyang Pan, Dingzirui Wang, Wanxiang Che, Dechen Zhan, Min-Yen Kan, Jian-Guang Lou", "abstract": "In this paper, we study the problem of knowledge-intensive text-to-SQL, in which domain knowledge is necessary to parse expert questions into SQL queries over domain-specific tables. We formalize this scenario by building a new Chinese benchmark KnowSQL consisting of domain-specific questions covering various domains. We then address this problem by presenting formulaic knowledge, rather than by annotating additional data examples. More concretely, we construct a formulaic knowledge bank as a domain knowledge base and propose a framework (ReGrouP) to leverage this formulaic knowledge during parsing. Experiments using ReGrouP indicate a significant 28.2% improvement overall on KnowSQL.", "title": "Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge.", "time": "2023-01-03T12:37:47Z", "link": "http://arxiv.org/abs/2301.01067v1", "id": "2301.01067v1_level2"}
{"author": "Longxu Dou, Yan Gao, Xuqi Liu, Mingyang Pan, Dingzirui Wang, Wanxiang Che, Dechen Zhan, Min-Yen Kan, Jian-Guang Lou", "abstract": "in which domain knowledge is necessary to parse expert questions into SQL queries over domain-specific tables., we study the problem of knowledge-intensive text-to-SQL, In this paper. We formalize this scenario by building a new Chinese benchmark KnowSQL consisting of domain-specific questions covering various domains. rather than by annotating additional data examples., We then address this problem by presenting formulaic knowledge. More concretely, we construct a formulaic knowledge bank as a domain knowledge base and propose a framework (ReGrouP) to leverage this formulaic knowledge during parsing. Experiments using ReGrouP demonstrate a significant 28.2% improvement overall on KnowSQL.", "title": "Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic Knowledge.", "time": "2023-01-03T12:37:47Z", "link": "http://arxiv.org/abs/2301.01067v1", "id": "2301.01067v1_level3"}
{"author": "Longxu Dou, Yan Gao, Xuqi Liu, Mingyang Pan, Dingzirui Wang, Wanxiang Che, Dechen Zhan, Min-Yen Kan, Jian-Guang Lou", "abstract": "Importantly, In this paper, we study the problem of knowledge-intensive text-to-sql, in which domain knowledge is necessary to parse expert questions into sql queries over domain-specific tables. Importantly, We formalize this scenario by building a new chinese benchmark knowsql consisting of domain-specific questions covering various domains. Importantly, We then address this problem by presenting formulaic knowledge, rather than by annotating additional data examples. Importantly, More concretely, we construct a formulaic knowledge bank as a domain knowledge base and propose a framework (regroup) to leverage this formulaic knowledge during parsing. experiments using regroup demonstrate a significant 28.2% improvement overall on knowsql.", "title": "Importantly, Towards knowledge-intensive text-to-sql semantic parsing with formulaic knowledge.", "time": "2023-01-03T12:37:47Z", "link": "http://arxiv.org/abs/2301.01067v1", "id": "2301.01067v1_level4"}
{"author": "Longxu Dou, Yan Gao, Xuqi Liu, Mingyang Pan, Dingzirui Wang, Wanxiang Che, Dechen Zhan, Min-Yen Kan, Jian-Guang Lou", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-03T12:37:47Z", "link": "http://arxiv.org/abs/2301.01067v1", "id": "2301.01067v1_level5"}
{"author": "Mat Pikuliak, Marin imko", "abstract": "This position paper discusses the problem of multilingual evaluation. Using simple statistics, such as average language performance, might inject linguistic biases in favor of dominant language families into evaluation approachology. We argue that a qualitative analysis informed by comparative linguistics is needed for multilingual results to detect this kind of bias. We show in our case study that results in published works can indeed be linguistically biased and we demonstrate that visualization based on URIEL typological database can detect it.", "title": "Average Is Not Enough: Caveats of Multilingual Evaluation.", "time": "2023-01-03T18:23:42Z", "link": "http://arxiv.org/abs/2301.01269v1", "id": "2301.01269v1_level1"}
{"author": "Mat Pikuliak, Marin imko", "abstract": "This position paper discusses the problem of multilingual evaluation. Using simple statistics, such as average language performance, might inject linguistic biases in favor of dominant language families into evaluation methodology. We argue that a qualitative analysis informed by comparative linguistics is needed for multilingual results to detect this kind of bias. We show in our case study that results in published works can indeed be linguistically biased and we indicate that visualization based on URIEL typological database can detect it.", "title": "Average Is Not Enough: Caveats of Multilingual Evaluation.", "time": "2023-01-03T18:23:42Z", "link": "http://arxiv.org/abs/2301.01269v1", "id": "2301.01269v1_level2"}
{"author": "Mat Pikuliak, Marin imko", "abstract": "This position paper discusses the problem of multilingual evaluation. might inject linguistic biases in favor of dominant language families into evaluation methodology., such as average language performance, Using simple statistics. We argue that a qualitative analysis informed by comparative linguistics is needed for multilingual results to detect this kind of bias. We show in our case study that results in published works can indeed be linguistically biased and we demonstrate that visualization based on URIEL typological database can detect it.", "title": "Average Is Not Enough: Caveats of Multilingual Evaluation.", "time": "2023-01-03T18:23:42Z", "link": "http://arxiv.org/abs/2301.01269v1", "id": "2301.01269v1_level3"}
{"author": "Mat Pikuliak, Marin imko", "abstract": "Importantly, This position paper discusses the problem of multilingual evaluation. Importantly, Using simple statistics, such as average language performance, might inject linguistic biases in favor of dominant language families into evaluation methodology. Importantly, We argue that a qualitative analysis informed by comparative linguistics is needed for multilingual results to detect this kind of bias. Importantly, We show in our case study that results in published works can indeed be linguistically biased and we demonstrate that visualization based on uriel typological database can detect it.", "title": "Importantly, Average is not enough: caveats of multilingual evaluation.", "time": "2023-01-03T18:23:42Z", "link": "http://arxiv.org/abs/2301.01269v1", "id": "2301.01269v1_level4"}
{"author": "Mat Pikuliak, Marin imko", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-03T18:23:42Z", "link": "http://arxiv.org/abs/2301.01269v1", "id": "2301.01269v1_level5"}
{"author": "Xiaoying Mou, Haiming Chen", "abstract": "Extended regular expressions with counting and interleaving are widely used in practice. However the related theoretical studies for this kind of expressions currently cannot meet the need of practical work. This paper develops syntax definitions for extended deterministic expressions and their subclasses, hope to completely solve the long-standing problem that there are no syntax definitions for this kind of expressions, which has become an important reason for restricting the use of extended expressions.", "title": "Grammar construction approachs for extended deterministic expressions.", "time": "2023-02-02T05:22:04Z", "link": "http://arxiv.org/abs/2301.01621v3", "id": "2301.01621v3_level1"}
{"author": "Xiaoying Mou, Haiming Chen", "abstract": "Extended regular expressions with counting and interleaving are widely used in practice. However the related theoretical studies for this kind of expressions currently cannot meet the need of practical work. This paper develops syntax definitions for extended deterministic expressions and their subclasses, hope to completely solve the long-standing problem that there are no syntax definitions for this kind of expressions, which has become an important reason for restricting the use of extended expressions.", "title": "Grammar construction methods for extended deterministic expressions.", "time": "2023-02-02T05:22:04Z", "link": "http://arxiv.org/abs/2301.01621v3", "id": "2301.01621v3_level2"}
{"author": "Xiaoying Mou, Haiming Chen", "abstract": "Extended regular expressions with counting and interleaving are widely used in practice. However the related theoretical studies for this kind of expressions currently cannot meet the need of practical work. which has become an important reason for restricting the use of extended expressions., hope to completely solve the long-standing problem that there are no syntax definitions for this kind of expressions, This paper develops syntax definitions for extended deterministic expressions and their subclasses.", "title": "Grammar construction methods for extended deterministic expressions.", "time": "2023-02-02T05:22:04Z", "link": "http://arxiv.org/abs/2301.01621v3", "id": "2301.01621v3_level3"}
{"author": "Xiaoying Mou, Haiming Chen", "abstract": "Importantly, Extended regular expressions with counting and interleaving are widely used in practice. Importantly, However the related theoretical studies for this kind of expressions currently cannot meet the need of practical work. Importantly, This paper develops syntax definitions for extended deterministic expressions and their subclasses, hope to completely solve the long-standing problem that there are no syntax definitions for this kind of expressions, which has become an important reason for restricting the use of extended expressions.", "title": "Importantly, Grammar construction methods for extended deterministic expressions.", "time": "2023-02-02T05:22:04Z", "link": "http://arxiv.org/abs/2301.01621v3", "id": "2301.01621v3_level4"}
{"author": "Xiaoying Mou, Haiming Chen", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-02-02T05:22:04Z", "link": "http://arxiv.org/abs/2301.01621v3", "id": "2301.01621v3_level5"}
{"author": "Dennis Aumiller, Michael Gertz", "abstract": "Previous state-of-the-art frameworks for lexical simplification consist of complex pipelines with several components, each of which requires deep technical knowledge and fine-tuned interaction to achieve its full potential. As an alternative, we describe a frustratingly simple pipeline based on prompted GPT-3 responses, beating competing approaches by a wide margin in settings with few training instances. Our best-performing submission to the English language track of the TSAR-2022 shared task consists of an ``ensemble'' of six different prompt templates with varying context levels. As a late-breaking result, we further detail a language transfer technique that allows simplification in languages other than English. Applied to the Spanish and Portuguese subset, we achieve state-of-the-art results with only minor modification to the original prompts. Aside from detailing the implementation and setup, we spend the remainder of this work discussing the particularities of prompting and implications for future work. Code for the experiments is available online at https://github.com/dennlinger/TSAR-2022-Shared-Task.", "title": "UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?.", "time": "2023-01-05T15:22:05Z", "link": "http://arxiv.org/abs/2301.01764v2", "id": "2301.01764v2_level1"}
{"author": "Dennis Aumiller, Michael Gertz", "abstract": "Previous state-of-the-art models for lexical simplification consist of complex pipelines with several components, each of which requires deep technical knowledge and fine-tuned interaction to achieve its full potential. As an alternative, we describe a frustratingly simple pipeline based on prompted GPT-3 responses, beating competing approaches by a wide margin in settings with few training instances. Our best-performing submission to the English language track of the TSAR-2022 shared task consists of an ``ensemble'' of six different prompt templates with varying context levels. As a late-breaking result, we further detail a language transfer technique that allows simplification in languages other than English. Applied to the Spanish and Portuguese subset, we achieve state-of-the-art results with only minor modification to the original prompts. Aside from detailing the implementation and setup, we spend the remainder of this work discussing the particularities of prompting and implications for future work. Code for the experiments is available online at https://github.com/dennlinger/TSAR-2022-Shared-Task.", "title": "UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?.", "time": "2023-01-05T15:22:05Z", "link": "http://arxiv.org/abs/2301.01764v2", "id": "2301.01764v2_level2"}
{"author": "Dennis Aumiller, Michael Gertz", "abstract": "beating competing approaches by a wide margin in settings with few training instances., we describe a frustratingly simple pipeline based on prompted GPT-3 responses, each of which requires deep technical knowledge and fine-tuned interaction to achieve its full potential. As an alternative, Previous state-of-the-art models for lexical simplification consist of complex pipelines with several components. Our best-performing submission to the English language track of the TSAR-2022 shared task consists of an ``ensemble'' of six different prompt templates with varying context levels. we further detail a language transfer technique that allows simplification in languages other than English., As a late-breaking result. we achieve state-of-the-art results with only minor modification to the original prompts., Applied to the Spanish and Portuguese subset. we spend the remainder of this work discussing the particularities of prompting and implications for future work., Aside from detailing the implementation and setup. Code for the experiments is available online at https://github.com/dennlinger/TSAR-2022-Shared-Task.", "title": "UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical Simplification?.", "time": "2023-01-05T15:22:05Z", "link": "http://arxiv.org/abs/2301.01764v2", "id": "2301.01764v2_level3"}
{"author": "Dennis Aumiller, Michael Gertz", "abstract": "Importantly, Previous state-of-the-art models for lexical simplification consist of complex pipelines with several components, each of which requires deep technical knowledge and fine-tuned interaction to achieve its full potential. as an alternative, we describe a frustratingly simple pipeline based on prompted gpt-3 responses, beating competing approaches by a wide margin in settings with few training instances. Importantly, Our best-performing submission to the english language track of the tsar-2022 shared task consists of an ``ensemble'' of six different prompt templates with varying context levels. Importantly, As a late-breaking result, we further detail a language transfer technique that allows simplification in languages other than english. Importantly, Applied to the spanish and portuguese subset, we achieve state-of-the-art results with only minor modification to the original prompts. Importantly, Aside from detailing the implementation and setup, we spend the remainder of this work discussing the particularities of prompting and implications for future work. Importantly, Code for the experiments is available online at https://github.com/dennlinger/tsar-2022-shared-task.", "title": "Importantly, Unihd at tsar-2022 shared task: is compute all we need for lexical simplification?.", "time": "2023-01-05T15:22:05Z", "link": "http://arxiv.org/abs/2301.01764v2", "id": "2301.01764v2_level4"}
{"author": "Dennis Aumiller, Michael Gertz", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-05T15:22:05Z", "link": "http://arxiv.org/abs/2301.01764v2", "id": "2301.01764v2_level5"}
{"author": "A. Seza Doruz, Sunayana Sitaram, Barbara E. Bullock, Almeida Jacqueline Toribio", "abstract": "The analysis of data in which multiple languages are represented has gained popularity among computational linguists in recent years. So far, much of this research focuses mainly on the improvement of computational approachs and largely ignores linguistic and social aspects of C-S discussed across a wide range of languages within the long-established literature in linguistics. To fill this gap, we offer a survey of code-switching (C-S) covering the literature in linguistics with a reflection on the key issues in language technologies. From the linguistic perspective, we provide an overview of structural and functional patterns of C-S focusing on the literature from European and Indian contexts as highly multilingual areas. From the language technologies perspective, we discuss how massive language frameworks fail to represent diverse C-S types due to lack of appropriate training data, lack of robust evaluation benchmarks for C-S (across multilingual situations and types of C-S) and lack of end-to-end systems that cover sociolinguistic aspects of C-S as well. Our survey will be a step towards an outcome of mutual benefit for computational scientists and linguists with a shared interest in multilingualism and C-S.", "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies.", "time": "2023-01-05T09:08:04Z", "link": "http://arxiv.org/abs/2301.01967v1", "id": "2301.01967v1_level1"}
{"author": "A. Seza Doruz, Sunayana Sitaram, Barbara E. Bullock, Almeida Jacqueline Toribio", "abstract": "The analysis of data in which multiple languages are represented has gained popularity among computational linguists in recent years. So far, much of this research focuses mainly on the improvement of computational methods and largely ignores linguistic and social aspects of C-S discussed across a wide range of languages within the long-established literature in linguistics. To fill this gap, we offer a survey of code-switching (C-S) covering the literature in linguistics with a reflection on the key issues in language technologies. From the linguistic perspective, we provide an overview of structural and functional patterns of C-S focusing on the literature from European and Indian contexts as highly multilingual areas. From the language technologies perspective, we discuss how massive language models fail to represent diverse C-S types due to lack of appropriate training data, lack of robust evaluation benchmarks for C-S (across multilingual situations and types of C-S) and lack of end-to-end systems that cover sociolinguistic aspects of C-S as well. Our survey will be a step towards an outcome of mutual benefit for computational scientists and linguists with a shared interest in multilingualism and C-S.", "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies.", "time": "2023-01-05T09:08:04Z", "link": "http://arxiv.org/abs/2301.01967v1", "id": "2301.01967v1_level2"}
{"author": "A. Seza Doruz, Sunayana Sitaram, Barbara E. Bullock, Almeida Jacqueline Toribio", "abstract": "The analysis of data in which multiple languages are represented has gained popularity among computational linguists in recent years. much of this research focuses mainly on the improvement of computational methods and largely ignores linguistic and social aspects of C-S discussed across a wide range of languages within the long-established literature in linguistics., So far. we offer a survey of code-switching (C-S) covering the literature in linguistics with a reflection on the key issues in language technologies., To fill this gap. we provide an overview of structural and functional patterns of C-S focusing on the literature from European and Indian contexts as highly multilingual areas., From the linguistic perspective. lack of robust evaluation benchmarks for C-S (across multilingual situations and types of C-S) and lack of end-to-end systems that cover sociolinguistic aspects of C-S as well., we discuss how massive language models fail to represent diverse C-S types due to lack of appropriate training data, From the language technologies perspective. Our survey will be a step towards an outcome of mutual benefit for computational scientists and linguists with a shared interest in multilingualism and C-S.", "title": "A Survey of Code-switching: Linguistic and Social Perspectives for Language Technologies.", "time": "2023-01-05T09:08:04Z", "link": "http://arxiv.org/abs/2301.01967v1", "id": "2301.01967v1_level3"}
{"author": "A. Seza Doruz, Sunayana Sitaram, Barbara E. Bullock, Almeida Jacqueline Toribio", "abstract": "Importantly, The analysis of data in which multiple languages are represented has gained popularity among computational linguists in recent years. Importantly, So far, much of this research focuses mainly on the improvement of computational methods and largely ignores linguistic and social aspects of c-s discussed across a wide range of languages within the long-established literature in linguistics. Importantly, To fill this gap, we offer a survey of code-switching (c-s) covering the literature in linguistics with a reflection on the key issues in language technologies. Importantly, From the linguistic perspective, we provide an overview of structural and functional patterns of c-s focusing on the literature from european and indian contexts as highly multilingual areas. Importantly, From the language technologies perspective, we discuss how massive language models fail to represent diverse c-s types due to lack of appropriate training data, lack of robust evaluation benchmarks for c-s (across multilingual situations and types of c-s) and lack of end-to-end systems that cover sociolinguistic aspects of c-s as well. Importantly, Our survey will be a step towards an outcome of mutual benefit for computational scientists and linguists with a shared interest in multilingualism and c-s.", "title": "Importantly, A survey of code-switching: linguistic and social perspectives for language technologies.", "time": "2023-01-05T09:08:04Z", "link": "http://arxiv.org/abs/2301.01967v1", "id": "2301.01967v1_level4"}
{"author": "A. Seza Doruz, Sunayana Sitaram, Barbara E. Bullock, Almeida Jacqueline Toribio", "abstract": "This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications. This work explores a novel perspective within the field, emphasizing its broader implications.", "title": "This work explores a novel perspective within the field, emphasizing its broader implications.", "time": "2023-01-05T09:08:04Z", "link": "http://arxiv.org/abs/2301.01967v1", "id": "2301.01967v1_level5"}
