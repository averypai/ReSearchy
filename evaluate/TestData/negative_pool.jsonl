{"author": "Xiaopeng Jiang, Shuai Zhao, Guy Jacobson, Rittwik Jana, Wen-Ling Hsu, Manoop Talasila, Syed Anwar Aftab, Yi Chen, Cristian Borcea", "abstract": "Fine-grained location prediction on smart phones can be used to improve\napp/system performance. Application scenarios include video quality adaptation\nas a function of the 5G network quality at predicted user locations, and\naugmented reality apps that speed up content rendering based on predicted user\nlocations. Such use cases require prediction error in the same range as the GPS\nerror, and no existing works on location prediction can achieve this level of\naccuracy. We present a system for fine-grained location prediction (FGLP) of\nmobile users, based on GPS traces collected on the phones. FGLP has two\ncomponents: a federated learning framework and a prediction model. The\nframework runs on the phones of the users and also on a server that coordinates\nlearning from all users in the system. FGLP represents the user location data\nas relative points in an abstract 2D space, which enables learning across\ndifferent physical spaces. The model merges Bidirectional Long Short-Term\nMemory (BiLSTM) and Convolutional Neural Networks (CNN), where BiLSTM learns\nthe speed and direction of the mobile users, and CNN learns information such as\nuser movement preferences. FGLP uses federated learning to protect user privacy\nand reduce bandwidth consumption. Our experimental results, using a dataset\nwith over 600,000 users, demonstrate that FGLP outperforms baseline models in\nterms of prediction accuracy. We also demonstrate that FGLP works well in\nconjunction with transfer learning, which enables model reusability. Finally,\nbenchmark results on several types of Android phones demonstrate FGLP's\nfeasibility in real life.", "time": "2021-06-13T02:09:29Z", "link": "http://arxiv.org/abs/2106.08946v1", "id": "2106.08946v1", "title": "FGLP: A Federated Fine-Grained Location Prediction System for Mobile\n  Users"}
{"author": "Xiaopeng Jiang, Han Hu, Vijaya Datta Mayyuri, An Chen, Devu M. Shila, Adriaan Larmuseau, Ruoming Jin, Cristian Borcea, NhatHai Phan", "abstract": "This article presents the design, implementation, and evaluation of FLSys, a\nmobile-cloud federated learning (FL) system, which can be a key component for\nan open ecosystem of FL models and apps. FLSys is designed to work on smart\nphones with mobile sensing data. It balances model performance with resource\nconsumption, tolerates communication failures, and achieves scalability. In\nFLSys, different DL models with different FL aggregation methods can be trained\nand accessed concurrently by different apps. Furthermore, FLSys provides\nadvanced privacy preserving mechanisms and a common API for third-party app\ndevelopers to access FL models. FLSys adopts a modular design and is\nimplemented in Android and AWS cloud. We co-designed FLSys with a human\nactivity recognition (HAR) model. HAR sensing data was collected in the wild\nfrom 100+ college students during a 4-month period. We implemented HAR-Wild, a\nCNN model tailored to mobile devices, with a data augmentation mechanism to\nmitigate the problem of non-Independent and Identically Distributed data. A\nsentiment analysis model is also used to demonstrate that FLSys effectively\nsupports concurrent models. This article reports our experience and lessons\nlearned from conducting extensive experiments using simulations, Android/Linux\nemulations, and Android phones that demonstrate FLSys achieves good model\nutility and practical system performance.", "time": "2023-03-11T00:04:58Z", "link": "http://arxiv.org/abs/2111.09445v3", "id": "2111.09445v3", "title": "FLSys: Toward an Open Ecosystem for Federated Learning Mobile Apps"}
{"author": "Ji Liu, Zhihua Wu, Dianhai Yu, Yanjun Ma, Danlei Feng, Minxu Zhang, Xinxuan Wu, Xuefeng Yao, Dejing Dou", "abstract": "Deep neural networks (DNNs) exploit many layers and a large number of\nparameters to achieve excellent performance. The training process of DNN models\ngenerally handles large-scale input data with many sparse features, which\nincurs high Input/Output (IO) cost, while some layers are compute-intensive.\nThe training process generally exploits distributed computing resources to\nreduce training time. In addition, heterogeneous computing resources, e.g.,\nCPUs, GPUs of multiple types, are available for the distributed training\nprocess. Thus, the scheduling of multiple layers to diverse computing resources\nis critical for the training process. To efficiently train a DNN model using\nthe heterogeneous computing resources, we propose a distributed framework,\ni.e., Paddle-Heterogeneous Parameter Server (Paddle-HeterPS), composed of a\ndistributed architecture and a Reinforcement Learning (RL)-based scheduling\nmethod. The advantages of Paddle-HeterPS are three-fold compared with existing\nframeworks. First, Paddle-HeterPS enables efficient training process of diverse\nworkloads with heterogeneous computing resources. Second, Paddle-HeterPS\nexploits an RL-based method to efficiently schedule the workload of each layer\nto appropriate computing resources to minimize the cost while satisfying\nthroughput constraints. Third, Paddle-HeterPS manages data storage and data\ncommunication among distributed computing resources. We carry out extensive\nexperiments to show that Paddle-HeterPS significantly outperforms\nstate-of-the-art approaches in terms of throughput (14.5 times higher) and\nmonetary cost (312.3% smaller). The codes of the framework are publicly\navailable at: https://github.com/PaddlePaddle/Paddle.", "time": "2023-06-07T13:33:11Z", "link": "http://arxiv.org/abs/2111.10635v4", "id": "2111.10635v4", "title": "HeterPS: Distributed Deep Learning With Reinforcement Learning Based\n  Scheduling in Heterogeneous Environments"}
{"author": "Anam Tahir, Bastian Alt, Amr Rizk, Heinz Koeppl", "abstract": "Load balancing arises as a fundamental problem, underlying the dimensioning\nand operation of many computing and communication systems, such as job routing\nin data center clusters, multipath communication, Big Data and queueing\nsystems. In essence, the decision-making agent maps each arriving job to one of\nthe possibly heterogeneous servers while aiming at an optimization goal such as\nload balancing, low average delay or low loss rate. One main difficulty in\nfinding optimal load balancing policies here is that the agent only partially\nobserves the impact of its decisions, e.g., through the delayed\nacknowledgements of the served jobs. In this paper, we provide a partially\nobservable (PO) model that captures the load balancing decisions in parallel\nbuffered systems under limited information of delayed acknowledgements. We\npresent a simulation model for this PO system to find a load balancing policy\nin real-time using a scalable Monte Carlo tree search algorithm. We numerically\nshow that the resulting policy outperforms other limited information load\nbalancing strategies such as variants of Join-the-Most-Observations and has\ncomparable performance to full information strategies like:\nJoin-the-Shortest-Queue, Join-the-Shortest-Queue(d) and\nShortest-Expected-Delay. Finally, we show that our approach can optimise the\nreal-time parallel processing by using network data provided by Kaggle.", "time": "2022-10-11T14:31:26Z", "link": "http://arxiv.org/abs/2109.08548v2", "id": "2109.08548v2", "title": "Load Balancing in Compute Clusters with Delayed Feedback"}
{"author": "Chendi Zhou, Ji Liu, Juncheng Jia, Jingbo Zhou, Yang Zhou, Huaiyu Dai, Dejing Dou", "abstract": "Recent years have witnessed a large amount of decentralized data in multiple\n(edge) devices of end-users, while the aggregation of the decentralized data\nremains difficult for machine learning jobs due to laws or regulations.\nFederated Learning (FL) emerges as an effective approach to handling\ndecentralized data without sharing the sensitive raw data, while\ncollaboratively training global machine learning models. The servers in FL need\nto select (and schedule) devices during the training process. However, the\nscheduling of devices for multiple jobs with FL remains a critical and open\nproblem. In this paper, we propose a novel multi-job FL framework to enable the\nparallel training process of multiple jobs. The framework consists of a system\nmodel and two scheduling methods. In the system model, we propose a parallel\ntraining process of multiple jobs, and construct a cost model based on the\ntraining time and the data fairness of various devices during the training\nprocess of diverse jobs. We propose a reinforcement learning-based method and a\nBayesian optimization-based method to schedule devices for multiple jobs while\nminimizing the cost. We conduct extensive experimentation with multiple jobs\nand datasets. The experimental results show that our proposed approaches\nsignificantly outperform baseline approaches in terms of training time (up to\n8.67 times faster) and accuracy (up to 44.6% higher).", "time": "2021-12-15T11:40:35Z", "link": "http://arxiv.org/abs/2112.05928v2", "id": "2112.05928v2", "title": "Efficient Device Scheduling with Multi-Job Federated Learning"}
{"author": "Venkatesh Venkataramanan, Sridevi Kaza, Anuradha M. Annaswamy", "abstract": "With increasing penetration of Distributed Energy Resources (DERs) in grid\nedge including renewable generation, flexible loads, and storage, accurate\nprediction of distributed generation and consumption at the consumer level\nbecomes important. However, DER prediction based on the transmission of\ncustomer level data, either repeatedly or in large amounts, is not feasible due\nto privacy concerns. In this paper, a distributed machine learning approach,\nFederated Learning, is proposed to carry out DER forecasting using a network of\nIoT nodes, each of which transmits a model of the consumption and generation\npatterns without revealing consumer data. We consider a simulation study which\nincludes 1000 DERs, and show that our method leads to an accurate prediction of\npreserve consumer privacy, while still leading to an accurate forecast. We also\nevaluate grid-specific performance metrics such as load swings and load\ncurtailment and show that our FL algorithm leads to satisfactory performance.\nSimulations are also performed on the Pecan street dataset to demonstrate the\nvalidity of the proposed approach on real data.", "time": "2021-07-07T14:25:43Z", "link": "http://arxiv.org/abs/2107.03248v1", "id": "2107.03248v1", "title": "DER Forecast using Privacy Preserving Federated Learning"}
{"author": "Vipin K. Kukkala, Sooryaa V. Thiruloga, Sudeep Pasricha", "abstract": "Modern vehicles can be thought of as complex distributed embedded systems\nthat run a variety of automotive applications with real-time constraints.\nRecent advances in the automotive industry towards greater autonomy are driving\nvehicles to be increasingly connected with various external systems (e.g.,\nroadside beacons, other vehicles), which makes emerging vehicles highly\nvulnerable to cyber-attacks. Additionally, the increased complexity of\nautomotive applications and the in-vehicle networks results in poor attack\nvisibility, which makes detecting such attacks particularly challenging in\nautomotive systems. In this work, we present a novel anomaly detection\nframework called LATTE to detect cyber-attacks in Controller Area Network (CAN)\nbased networks within automotive platforms. Our proposed LATTE framework uses a\nstacked Long Short Term Memory (LSTM) predictor network with novel attention\nmechanisms to learn the normal operating behavior at design time. Subsequently,\na novel detection scheme (also trained at design time) is used to detect\nvarious cyber-attacks (as anomalies) at runtime. We evaluate our proposed LATTE\nframework under different automotive attack scenarios and present a detailed\ncomparison with the best-known prior works in this area, to demonstrate the\npotential of our approach.", "time": "2021-07-12T16:32:47Z", "link": "http://arxiv.org/abs/2107.05561v1", "id": "2107.05561v1", "title": "LATTE: LSTM Self-Attention based Anomaly Detection in Embedded\n  Automotive Platforms"}
{"author": "Ye Yuan, Jun Liu, Dou Jin, Zuogong Yue, Ruijuan Chen, Maolin Wang, Chuan Sun, Lei Xu, Feng Hua, Xin He, Xinlei Yi, Tao Yang, Hai-Tao Zhang, Shaochun Sui, Han Ding", "abstract": "Traditional machine learning relies on a centralized data pipeline, i.e.,\ndata are provided to a central server for model training. In many applications,\nhowever, data are inherently fragmented. Such a decentralized nature of these\ndatabases presents the biggest challenge for collaboration: sending all\ndecentralized datasets to a central server raises serious privacy concerns.\nAlthough there has been a joint effort in tackling such a critical issue by\nproposing privacy-preserving machine learning frameworks, such as federated\nlearning, most state-of-the-art frameworks are built still in a centralized\nway, in which a central client is needed for collecting and distributing model\ninformation (instead of data itself) from every other client, leading to high\ncommunication pressure and high vulnerability when there exists a failure at or\nattack on the central client. Here we propose a principled decentralized\nfederated learning algorithm (DeceFL), which does not require a central client\nand relies only on local information transmission between clients and their\nneighbors, representing a fully decentralized learning framework. It has been\nfurther proven that every client reaches the global minimum with zero\nperformance gap and achieves the same convergence rate $O(1/T)$ (where $T$ is\nthe number of iterations in gradient descent) as centralized federated learning\nwhen the loss function is smooth and strongly convex. Finally, the proposed\nalgorithm has been applied to a number of applications to illustrate its\neffectiveness for both convex and nonconvex loss functions, demonstrating its\napplicability to a wide range of real-world medical and industrial\napplications.", "time": "2021-10-30T02:19:08Z", "link": "http://arxiv.org/abs/2107.07171v2", "id": "2107.07171v2", "title": "DeceFL: A Principled Decentralized Federated Learning Framework"}
{"author": "Soumyadip Ghosh, Bernardo Aquino, Vijay Gupta", "abstract": "Communication in parallel systems imposes significant overhead which often\nturns out to be a bottleneck in parallel machine learning. To relieve some of\nthis overhead, in this paper, we present EventGraD - an algorithm with\nevent-triggered communication for stochastic gradient descent in parallel\nmachine learning. The main idea of this algorithm is to modify the requirement\nof communication at every iteration in standard implementations of stochastic\ngradient descent in parallel machine learning to communicating only when\nnecessary at certain iterations. We provide theoretical analysis of convergence\nof our proposed algorithm. We also implement the proposed algorithm for\ndata-parallel training of a popular residual neural network used for training\nthe CIFAR-10 dataset and show that EventGraD can reduce the communication load\nby up to 60% while retaining the same level of accuracy. In addition, EventGraD\ncan be combined with other approaches such as Top-K sparsification to decrease\ncommunication further while maintaining accuracy.", "time": "2021-12-08T23:35:24Z", "link": "http://arxiv.org/abs/2103.07454v2", "id": "2103.07454v2", "title": "EventGraD: Event-Triggered Communication in Parallel Machine Learning"}
{"author": "Haoyu Ren, Darko Anicic, Thomas Runkler", "abstract": "Tiny machine learning (TinyML) is a fast-growing research area committed to\ndemocratizing deep learning for all-pervasive microcontrollers (MCUs).\nChallenged by the constraints on power, memory, and computation, TinyML has\nachieved significant advancement in the last few years. However, the current\nTinyML solutions are based on batch/offline settings and support only the\nneural network's inference on MCUs. The neural network is first trained using a\nlarge amount of pre-collected data on a powerful machine and then flashed to\nMCUs. This results in a static model, hard to adapt to new data, and impossible\nto adjust for different scenarios, which impedes the flexibility of the\nInternet of Things (IoT). To address these problems, we propose a novel system\ncalled TinyOL (TinyML with Online-Learning), which enables incremental\non-device training on streaming data. TinyOL is based on the concept of online\nlearning and is suitable for constrained IoT devices. We experiment TinyOL\nunder supervised and unsupervised setups using an autoencoder neural network.\nFinally, we report the performance of the proposed solution and show its\neffectiveness and feasibility.", "time": "2021-04-10T23:06:28Z", "link": "http://arxiv.org/abs/2103.08295v3", "id": "2103.08295v3", "title": "TinyOL: TinyML with Online-Learning on Microcontrollers"}
{"author": "Mohammed Jouhari, Abdulla Al-Ali, Emna Baccour, Amr Mohamed, Aiman Erbad, Mohsen Guizani, Mounir Hamdi", "abstract": "Unmanned Aerial Vehicles (UAVs) have attracted great interest in the last few\nyears owing to their ability to cover large areas and access difficult and\nhazardous target zones, which is not the case of traditional systems relying on\ndirect observations obtained from fixed cameras and sensors. Furthermore,\nthanks to the advancements in computer vision and machine learning, UAVs are\nbeing adopted for a broad range of solutions and applications. However, Deep\nNeural Networks (DNNs) are progressing toward deeper and complex models that\nprevent them from being executed on-board. In this paper, we propose a DNN\ndistribution methodology within UAVs to enable data classification in\nresource-constrained devices and avoid extra delays introduced by the\nserver-based solutions due to data communication over air-to-ground links. The\nproposed method is formulated as an optimization problem that aims to minimize\nthe latency between data collection and decision-making while considering the\nmobility model and the resource constraints of the UAVs as part of the\nair-to-air communication. We also introduce the mobility prediction to adapt\nour system to the dynamics of UAVs and the network variation. The simulation\nconducted to evaluate the performance and benchmark the proposed methods,\nnamely Optimal UAV-based Layer Distribution (OULD) and OULD with Mobility\nPrediction (OULD-MP), were run in an HPC cluster. The obtained results show\nthat our optimization solution outperforms the existing and heuristic-based\napproaches.", "time": "2021-05-23T20:19:43Z", "link": "http://arxiv.org/abs/2105.11013v1", "id": "2105.11013v1", "title": "Distributed CNN Inference on Resource-Constrained UAVs for Surveillance\n  Systems: Design and Optimization"}
{"author": "Giacomo Lanciano, Filippo Galli, Tommaso Cucinotta, Davide Bacciu, Andrea Passarella", "abstract": "Cloud auto-scaling mechanisms are typically based on reactive automation\nrules that scale a cluster whenever some metric, e.g., the average CPU usage\namong instances, exceeds a predefined threshold. Tuning these rules becomes\nparticularly cumbersome when scaling-up a cluster involves non-negligible times\nto bootstrap new instances, as it happens frequently in production cloud\nservices.\n  To deal with this problem, we propose an architecture for auto-scaling cloud\nservices based on the status in which the system is expected to evolve in the\nnear future. Our approach leverages on time-series forecasting techniques, like\nthose based on machine learning and artificial neural networks, to predict the\nfuture dynamics of key metrics, e.g., resource consumption metrics, and apply a\nthreshold-based scaling policy on them. The result is a predictive automation\npolicy that is able, for instance, to automatically anticipate peaks in the\nload of a cloud application and trigger ahead of time appropriate scaling\nactions to accommodate the expected increase in traffic.\n  We prototyped our approach as an open-source OpenStack component, which\nrelies on, and extends, the monitoring capabilities offered by Monasca,\nresulting in the addition of predictive metrics that can be leveraged by\norchestration components like Heat or Senlin. We show experimental results\nusing a recurrent neural network and a multi-layer perceptron as predictor,\nwhich are compared with a simple linear regression and a traditional\nnon-predictive auto-scaling policy. However, the proposed framework allows for\nthe easy customization of the prediction policy as needed.", "time": "2021-11-03T11:02:08Z", "link": "http://arxiv.org/abs/2111.02133v1", "id": "2111.02133v1", "title": "Predictive Auto-scaling with OpenStack Monasca"}
{"author": "Aritra Mitra, Rayana Jaafar, George J. Pappas, Hamed Hassani", "abstract": "We consider a standard federated learning (FL) architecture where a group of\nclients periodically coordinate with a central server to train a statistical\nmodel. We develop a general algorithmic framework called FedLin to tackle some\nof the key challenges intrinsic to FL, namely objective heterogeneity, systems\nheterogeneity, and infrequent and imprecise communication. Our framework is\nmotivated by the observation that under these challenges, various existing FL\nalgorithms suffer from a fundamental speed-accuracy conflict: they either\nguarantee linear convergence but to an incorrect point, or convergence to the\nglobal minimum but at a sub-linear rate, i.e., fast convergence comes at the\nexpense of accuracy. In contrast, when the clients' local loss functions are\nsmooth and strongly convex, we show that FedLin guarantees linear convergence\nto the global minimum, despite arbitrary objective and systems heterogeneity.\nWe then establish matching upper and lower bounds on the convergence rate of\nFedLin that highlight the effects of intermittent communication. Finally, we\nshow that FedLin preserves linear convergence rates under aggressive gradient\nsparsification, and quantify the effect of the compression level on the\nconvergence rate. Our work is the first to provide tight linear convergence\nrate guarantees, and constitutes the first comprehensive analysis of gradient\nsparsification in FL.", "time": "2021-08-30T18:31:11Z", "link": "http://arxiv.org/abs/2102.07053v2", "id": "2102.07053v2", "title": "Linear Convergence in Federated Learning: Tackling Client Heterogeneity\n  and Sparse Gradients"}
{"author": "Hrishikesh Dutta, Subir Biswas", "abstract": "This paper proposes a distributed Reinforcement Learning (RL) based framework\nthat can be used for synthesizing MAC layer wireless protocols in IoT networks\nwith low-complexity wireless transceivers. The proposed framework does not rely\non complex hardware capabilities such as carrier sensing and its associated\nalgorithmic complexities that are often not supported in wireless transceivers\nof low-cost and low-energy IoT devices. In this framework, the access protocols\nare first formulated as Markov Decision Processes (MDP) and then solved using\nRL. A distributed and multi-Agent RL framework is used as the basis for\nprotocol synthesis. Distributed behavior makes the nodes independently learn\noptimal transmission strategies without having to rely on full network level\ninformation and direct knowledge of behavior of other nodes. The nodes learn to\nminimize packet collisions such that optimal throughput can be attained and\nmaintained for loading conditions that are higher than what the known benchmark\nprotocols (such as ALOHA) for IoT devices without complex transceivers. In\naddition, the nodes are observed to be able to learn to act optimally in the\npresence of heterogeneous loading and network topological conditions. Finally,\nthe proposed learning approach allows the wireless bandwidth to be fairly\ndistributed among network nodes in a way that is not dependent on such\nheterogeneities. Via simulation experiments, the paper demonstrates the\nperformance of the learning paradigm and its abilities to make nodes adapt\ntheir optimal transmission strategies on the fly in response to various network\ndynamics.", "time": "2021-04-29T17:57:43Z", "link": "http://arxiv.org/abs/2104.14549v1", "id": "2104.14549v1", "title": "Medium Access using Distributed Reinforcement Learning for IoTs with\n  Low-Complexity Wireless Transceivers"}
{"author": "Yan Pei, Keshav Pingali", "abstract": "Many applications in important problem domains such as machine learning and\ncomputer vision are streaming applications that take a sequence of inputs over\ntime. It is challenging to find knob settings that optimize the run-time\nperformance of such applications because the optimal knob settings are usually\nfunctions of inputs, computing platforms, time as well as user's requirements,\nwhich can be very diverse.\n  Most prior works address this problem by offline profiling followed by\ntraining models for control. However, profiling-based approaches incur large\noverhead before execution; it is also difficult to redeploy them in other\nrun-time configurations.\n  In this paper, we propose Sonic, a sampling-based online controller for\nlong-running streaming applications that does not require profiling ahead of\ntime. Within each phase of a streaming application's execution, Sonic utilizes\nthe beginning portion to sample the knob space strategically and aims to pick\nthe optimal knob setting for the rest of the phase, given a user-specified\nconstrained optimization problem. A hybrid approach of machine learning\nregressions and Bayesian optimization are used for better overall sampling\nchoices.\n  Sonic is implemented independent of application, device, input, performance\nobjective and constraints. We evaluate Sonic on traditional parallel benchmarks\nas well as on deep learning inference benchmarks across multiple platforms. Our\nexperiments show that when using Sonic to control knob settings, application\nrun-time performance is only 5.3% less than if optimal knob settings were used,\ndemonstrating that Sonic is able to find near-optimal knob settings under\ndiverse run-time configurations without prior knowledge quickly.", "time": "2021-08-15T18:51:02Z", "link": "http://arxiv.org/abs/2108.10701v1", "id": "2108.10701v1", "title": "Sonic: A Sampling-based Online Controller for Streaming Applications"}
{"author": "Hamid Reza Feyzmahdavian, Mikael Johansson", "abstract": "We introduce novel convergence results for asynchronous iterations that\nappear in the analysis of parallel and distributed optimization algorithms. The\nresults are simple to apply and give explicit estimates for how the degree of\nasynchrony impacts the convergence rates of the iterates. Our results shorten,\nstreamline and strengthen existing convergence proofs for several asynchronous\noptimization methods and allow us to establish convergence guarantees for\npopular algorithms that were thus far lacking a complete theoretical\nunderstanding. Specifically, we use our results to derive better iteration\ncomplexity bounds for proximal incremental aggregated gradient methods, to\nobtain tighter guarantees depending on the average rather than maximum delay\nfor the asynchronous stochastic gradient descent method, to provide less\nconservative analyses of the speedup conditions for asynchronous\nblock-coordinate implementations of Krasnoselskii-Mann iterations, and to\nquantify the convergence rates for totally asynchronous iterations under\nvarious assumptions on communication delays and update rates.", "time": "2023-04-03T17:53:20Z", "link": "http://arxiv.org/abs/2109.04522v2", "id": "2109.04522v2", "title": "Asynchronous Iterations in Optimization: New Sequence Results and\n  Sharper Algorithmic Guarantees"}
{"author": "Mohammad Taha Toghani, CÃ©sar A. Uribe", "abstract": "We propose a new decentralized average consensus algorithm with compressed\ncommunication that scales linearly with the network size n. We prove that the\nproposed method converges to the average of the initial values held locally by\nthe agents of a network when agents are allowed to communicate with compressed\nmessages. The proposed algorithm works for a broad class of compression\noperators (possibly biased), where agents interact over arbitrary static,\nundirected, and connected networks. We further present numerical experiments\nthat confirm our theoretical results and illustrate the scalability and\ncommunication efficiency of our algorithm.", "time": "2021-09-14T22:26:06Z", "link": "http://arxiv.org/abs/2109.06996v1", "id": "2109.06996v1", "title": "Scalable Average Consensus with Compressed Communications"}
{"author": "Sergio Valcarcel Macua, Ian Davies, Aleksi Tukiainen, Enrique Munoz de Cote", "abstract": "We propose a fully distributed actor-critic architecture, named Diff-DAC,\nwith application to multitask reinforcement learning (MRL). During the learning\nprocess, agents communicate their value and policy parameters to their\nneighbours, diffusing the information across a network of agents with no need\nfor a central station. Each agent can only access data from its local task, but\naims to learn a common policy that performs well for the whole set of tasks.\nThe architecture is scalable, since the computational and communication cost\nper agent depends on the number of neighbours rather than the overall number of\nagents. We derive Diff-DAC from duality theory and provide novel insights into\nthe actor-critic framework, showing that it is actually an instance of the dual\nascent method. We prove almost sure convergence of Diff-DAC to a common policy\nunder general assumptions that hold even for deep-neural network\napproximations. For more restrictive assumptions, we also prove that this\ncommon policy is a stationary point of an approximation of the original\nproblem. Numerical results on multitask extensions of common continuous control\nbenchmarks demonstrate that Diff-DAC stabilises learning and has a regularising\neffect that induces higher performance and better generalisation properties\nthan previous architectures.", "time": "2021-10-23T21:57:43Z", "link": "http://arxiv.org/abs/2110.12306v1", "id": "2110.12306v1", "title": "Fully Distributed Actor-Critic Architecture for Multitask Deep\n  Reinforcement Learning"}
{"author": "Samaa Gazzaz, Vishal Chakraborty, Faisal Nawab", "abstract": "Emerging edge applications require both a fast response latency and complex\nprocessing. This is infeasible without expensive hardware that can process\ncomplex operations -- such as object detection -- within a short time. Many\napproach this problem by addressing the complexity of the models -- via model\ncompression, pruning and quantization -- or compressing the input. In this\npaper, we propose a different perspective when addressing the performance\nchallenges. Croesus is a multi-stage approach to edge-cloud systems that\nprovides the ability to find the balance between accuracy and performance.\nCroesus consists of two stages (that can be generalized to multiple stages): an\ninitial and a final stage. The initial stage performs the computation in\nreal-time using approximate/best-effort computation at the edge. The final\nstage performs the full computation at the cloud, and uses the results to\ncorrect any errors made at the initial stage. In this paper, we demonstrate the\nimplications of such an approach on a video analytics use-case and show how\nmulti-stage processing yields a better balance between accuracy and\nperformance. Moreover, we study the safety of multi-stage transactions via two\nproposals: multi-stage serializability (MS-SR) and multi-stage invariant\nconfluence with Apologies (MS-IA).", "time": "2021-12-31T21:38:05Z", "link": "http://arxiv.org/abs/2201.00063v1", "id": "2201.00063v1", "title": "Croesus: Multi-Stage Processing and Transactions for Video-Analytics in\n  Edge-Cloud Systems"}
{"author": "Yao Fu, Yipeng Zhou, Di Wu, Shui Yu, Yonggang Wen, Chao Li", "abstract": "In spite that Federated Learning (FL) is well known for its privacy\nprotection when training machine learning models among distributed clients\ncollaboratively, recent studies have pointed out that the naive FL is\nsusceptible to gradient leakage attacks. In the meanwhile, Differential Privacy\n(DP) emerges as a promising countermeasure to defend against gradient leakage\nattacks. However, the adoption of DP by clients in FL may significantly\njeopardize the model accuracy. It is still an open problem to understand the\npracticality of DP from a theoretic perspective. In this paper, we make the\nfirst attempt to understand the practicality of DP in FL through tuning the\nnumber of conducted iterations. Based on the FedAvg algorithm, we formally\nderive the convergence rate with DP noises in FL. Then, we theoretically\nderive: 1) the conditions for the DP based FedAvg to converge as the number of\nglobal iterations (GI) approaches infinity; 2) the method to set the number of\nlocal iterations (LI) to minimize the negative influence of DP noises. By\nfurther substituting the Laplace and Gaussian mechanisms into the derived\nconvergence rate respectively, we show that: 3) The DP based FedAvg with the\nLaplace mechanism cannot converge, but the divergence rate can be effectively\nprohibited by setting the number of LIs with our method; 4) The learning error\nof the DP based FedAvg with the Gaussian mechanism can converge to a constant\nnumber finally if we use a fixed number of LIs per GI. To verify our\ntheoretical findings, we conduct extensive experiments using two real-world\ndatasets. The results not only validate our analysis results, but also provide\nuseful guidelines on how to optimize model accuracy when incorporating DP into\nFL", "time": "2021-01-11T19:43:12Z", "link": "http://arxiv.org/abs/2101.04163v1", "id": "2101.04163v1", "title": "On the Practicality of Differential Privacy in Federated Learning by\n  Tuning Iteration Times"}
{"author": "Manish Shetty, Chetan Bansal, Sumit Kumar, Nikitha Rao, Nachiappan Nagappan", "abstract": "The move from boxed products to services and the widespread adoption of cloud\ncomputing has had a huge impact on the software development life cycle and\nDevOps processes. Particularly, incident management has become critical for\ndeveloping and operating large-scale services. Prior work on incident\nmanagement has heavily focused on the challenges with incident triaging and\nde-duplication. In this work, we address the fundamental problem of structured\nknowledge extraction from service incidents. We have built SoftNER, a framework\nfor mining Knowledge Graphs from incident reports. First, we build a novel\nmulti-task learning based BiLSTM-CRF model which leverages not just the\nsemantic context but also the data-types for extracting factual information in\nthe form of named entities. Next, we present an approach to mine relations\nbetween the named entities for automatically constructing knowledge graphs. We\nhave deployed SoftNER at Microsoft, a major cloud service provider and have\nevaluated it on more than 2 months of cloud incidents. We show that the\nunsupervised machine learning pipeline has a high precision of 0.96. Our\nmulti-task learning based deep learning model also outperforms the\nstate-of-the-art NER models. Lastly, using the knowledge extracted by SoftNER,\nwe are able to build accurate models for applications such as incident triaging\nand recommending entities based on their relevance to incident titles.", "time": "2021-06-23T08:35:29Z", "link": "http://arxiv.org/abs/2101.05961v2", "id": "2101.05961v2", "title": "SoftNER: Mining Knowledge Graphs From Cloud Incidents"}
{"author": "Duc Thien Nguyen, Shiau Hoong Lim, Laura Wynter, Desmond Cai", "abstract": "Federated learning brings potential benefits of faster learning, better\nsolutions, and a greater propensity to transfer when heterogeneous data from\ndifferent parties increases diversity. However, because federated learning\ntasks tend to be large and complex, and training times non-negligible, it is\nimportant for the aggregation algorithm to be robust to non-IID data and\ncorrupted parties. This robustness relies on the ability to identify, and\nappropriately weight, incompatible parties. Recent work assumes that a\n\\textit{reference dataset} is available through which to perform the\nidentification. We consider settings where no such reference dataset is\navailable; rather, the quality and suitability of the parties needs to be\n\\textit{inferred}. We do so by bringing ideas from crowdsourced predictions and\ncollaborative filtering, where one must infer an unknown ground truth given\nproposals from participants with unknown quality. We propose novel federated\nlearning aggregation algorithms based on Bayesian inference that adapt to the\nquality of the parties. Empirically, we show that the algorithms outperform\nstandard and robust aggregation in federated learning on both synthetic and\nreal data.", "time": "2021-01-15T15:30:06Z", "link": "http://arxiv.org/abs/2101.06171v1", "id": "2101.06171v1", "title": "Probabilistic Inference for Learning from Untrusted Sources"}
{"author": "Shangming Cai, Dongsheng Wang, Haixia Wang, Yongqiang Lyu, Guangquan Xu, Xi Zheng, Athanasios V. Vasilakos", "abstract": "To reduce uploading bandwidth and address privacy concerns, deep learning at\nthe network edge has been an emerging topic. Typically, edge devices\ncollaboratively train a shared model using real-time generated data through the\nParameter Server framework. Although all the edge devices can share the\ncomputing workloads, the distributed training processes over edge networks are\nstill time-consuming due to the parameters and gradients transmission\nprocedures between parameter servers and edge devices. Focusing on accelerating\ndistributed Convolutional Neural Networks (CNNs) training at the network edge,\nwe present DynaComm, a novel scheduler that dynamically decomposes each\ntransmission procedure into several segments to achieve optimal layer-wise\ncommunications and computations overlapping during run-time. Through\nexperiments, we verify that DynaComm manages to achieve optimal layer-wise\nscheduling for all cases compared to competing strategies while the model\naccuracy remains untouched.", "time": "2021-10-08T03:39:19Z", "link": "http://arxiv.org/abs/2101.07968v2", "id": "2101.07968v2", "title": "DynaComm: Accelerating Distributed CNN Training between Edges and Clouds\n  through Dynamic Communication Scheduling"}
{"author": "Liangxi Liu, Xi Jiang, Feng Zheng, Hong Chen, Guo-Jun Qi, Heng Huang, Ling Shao", "abstract": "Federated learning (FL) allows multiple clients to collaboratively learn a\nglobally shared model through cycles of model aggregation and local model\ntraining, without the need to share data. Most existing FL methods train local\nmodels separately on different clients, and then simply average their\nparameters to obtain a centralized model on the server side. However, these\napproaches generally suffer from large aggregation errors and severe local\nforgetting, which are particularly bad in heterogeneous data settings. To\ntackle these issues, in this paper, we propose a novel FL framework that uses\nonline Laplace approximation to approximate posteriors on both the client and\nserver side. On the server side, a multivariate Gaussian product mechanism is\nemployed to construct and maximize a global posterior, largely reducing the\naggregation errors induced by large discrepancies between local models. On the\nclient side, a prior loss that uses the global posterior probabilistic\nparameters delivered from the server is designed to guide the local training.\nBinding such learning constraints from other clients enables our method to\nmitigate local forgetting. Finally, we achieve state-of-the-art results on\nseveral benchmarks, clearly demonstrating the advantages of the proposed\nmethod.", "time": "2023-12-02T07:13:00Z", "link": "http://arxiv.org/abs/2102.01936v3", "id": "2102.01936v3", "title": "A Bayesian Federated Learning Framework with Online Laplace\n  Approximation"}
{"author": "Albin Cassirer, Gabriel Barth-Maron, Eugene Brevdo, Sabela Ramos, Toby Boyd, Thibault Sottiaux, Manuel Kroiss", "abstract": "A central component of training in Reinforcement Learning (RL) is Experience:\nthe data used for training. The mechanisms used to generate and consume this\ndata have an important effect on the performance of RL algorithms.\n  In this paper, we introduce Reverb: an efficient, extensible, and easy to use\nsystem designed specifically for experience replay in RL. Reverb is designed to\nwork efficiently in distributed configurations with up to thousands of\nconcurrent clients.\n  The flexible API provides users with the tools to easily and accurately\nconfigure the replay buffer. It includes strategies for selecting and removing\nelements from the buffer, as well as options for controlling the ratio between\nsampled and inserted elements. This paper presents the core design of Reverb,\ngives examples of how it can be applied, and provides empirical results of\nReverb's performance characteristics.", "time": "2021-02-09T10:03:17Z", "link": "http://arxiv.org/abs/2102.04736v1", "id": "2102.04736v1", "title": "Reverb: A Framework For Experience Replay"}
{"author": "Sai Aparna Aketi, Amandeep Singh, Jan Rabaey", "abstract": "Current deep learning (DL) systems rely on a centralized computing paradigm\nwhich limits the amount of available training data, increases system latency,\nand adds privacy and security constraints. On-device learning, enabled by\ndecentralized and distributed training of DL models over peer-to-peer\nwirelessly connected edge devices, not only alleviate the above limitations but\nalso enable next-gen applications that need DL models to continuously interact\nand learn from their environment. However, this necessitates the development of\nnovel training algorithms that train DL models over time-varying and directed\npeer-to-peer graph structures while minimizing the amount of communication\nbetween the devices and also being resilient to non-IID data distributions. In\nthis work we propose, Sparse-Push, a communication efficient decentralized\ndistributed training algorithm that supports training over peer-to-peer,\ndirected, and time-varying graph topologies. The proposed algorithm enables\n466x reduction in communication with only 1% degradation in performance when\ntraining various DL models such as ResNet-20 and VGG11 over the CIFAR-10\ndataset. Further, we demonstrate how communication compression can lead to\nsignificant performance degradation in-case of non-IID datasets, and propose\nSkew-Compensated Sparse Push algorithm that recovers this performance drop\nwhile maintaining similar levels of communication compression.", "time": "2021-02-12T02:05:24Z", "link": "http://arxiv.org/abs/2102.05715v2", "id": "2102.05715v2", "title": "Sparse-Push: Communication- & Energy-Efficient Decentralized Distributed\n  Learning over Directed & Time-Varying Graphs with non-IID Datasets"}
{"author": "Yuping Fan, Zhiling Lan, Taylor Childers, Paul Rich, William Allcock, Michael E. Papka", "abstract": "Cluster scheduler is crucial in high-performance computing (HPC). It\ndetermines when and which user jobs should be allocated to available system\nresources. Existing cluster scheduling heuristics are developed by human\nexperts based on their experience with specific HPC systems and workloads.\nHowever, the increasing complexity of computing systems and the highly dynamic\nnature of application workloads have placed tremendous burden on manually\ndesigned and tuned scheduling heuristics. More aggressive optimization and\nautomation are needed for cluster scheduling in HPC. In this work, we present\nan automated HPC scheduling agent named DRAS (Deep Reinforcement Agent for\nScheduling) by leveraging deep reinforcement learning. DRAS is built on a\nnovel, hierarchical neural network incorporating special HPC scheduling\nfeatures such as resource reservation and backfilling. A unique training\nstrategy is presented to enable DRAS to rapidly learn the target environment.\nOnce being provided a specific scheduling objective given by system manager,\nDRAS automatically learns to improve its policy through interaction with the\nscheduling environment and dynamically adjusts its policy as workload changes.\nThe experiments with different production workloads demonstrate that DRAS\noutperforms the existing heuristic and optimization approaches by up to 45%.", "time": "2021-04-19T22:31:44Z", "link": "http://arxiv.org/abs/2102.06243v2", "id": "2102.06243v2", "title": "Deep Reinforcement Agent for Scheduling in HPC"}
{"author": "Cameron R. Wolfe, Jingkang Yang, Arindam Chowdhury, Chen Dun, Artun Bayer, Santiago Segarra, Anastasios Kyrillidis", "abstract": "The graph convolutional network (GCN) is a go-to solution for machine\nlearning on graphs, but its training is notoriously difficult to scale both in\nterms of graph size and the number of model parameters. Although some work has\nexplored training on large-scale graphs (e.g., GraphSAGE, ClusterGCN, etc.), we\npioneer efficient training of large-scale GCN models (i.e., ultra-wide,\noverparameterized models) with the proposal of a novel, distributed training\nframework. Our proposed training methodology, called GIST, disjointly\npartitions the parameters of a GCN model into several, smaller sub-GCNs that\nare trained independently and in parallel. In addition to being compatible with\nall GCN architectures and existing sampling techniques for efficient GCN\ntraining, GIST i) improves model performance, ii) scales to training on\narbitrarily large graphs, iii) decreases wall-clock training time, and iv)\nenables the training of markedly overparameterized GCN models. Remarkably, with\nGIST, we train an astonishgly-wide 32,768-dimensional GraphSAGE model, which\nexceeds the capacity of a single GPU by a factor of 8x, to SOTA performance on\nthe Amazon2M dataset.", "time": "2022-03-14T14:12:18Z", "link": "http://arxiv.org/abs/2102.10424v4", "id": "2102.10424v4", "title": "GIST: Distributed Training for Large-Scale Graph Convolutional Networks"}
{"author": "Claire Birnie, Haithem Jarraya, Fredrik Hansteen", "abstract": "Deep learning applications are drastically progressing in seismic processing\nand interpretation tasks. However, the majority of approaches subsample data\nvolumes and restrict model sizes to minimise computational requirements.\nSubsampling the data risks losing vital spatio-temporal information which could\naid training whilst restricting model sizes can impact model performance, or in\nsome extreme cases, renders more complicated tasks such as segmentation\nimpossible. This paper illustrates how to tackle the two main issues of\ntraining of large neural networks: memory limitations and impracticably large\ntraining times. Typically, training data is preloaded into memory prior to\ntraining, a particular challenge for seismic applications where data is\ntypically four times larger than that used for standard image processing tasks\n(float32 vs. uint8). Using a microseismic use case, we illustrate how over\n750GB of data can be used to train a model by using a data generator approach\nwhich only stores in memory the data required for that training batch.\nFurthermore, efficient training over large models is illustrated through the\ntraining of a 7-layer UNet with input data dimensions of 4096X4096. Through a\nbatch-splitting distributed training approach, training times are reduced by a\nfactor of four. The combination of data generators and distributed training\nremoves any necessity of data 1 subsampling or restriction of neural network\nsizes, offering the opportunity of utilisation of larger networks,\nhigher-resolution input data or moving from 2D to 3D problem spaces.", "time": "2021-02-25T17:06:00Z", "link": "http://arxiv.org/abs/2102.13003v1", "id": "2102.13003v1", "title": "An introduction to distributed training of deep neural networks for\n  segmentation tasks with large seismic datasets"}
{"author": "Dezhong Yao, Wanning Pan, Yutong Dai, Yao Wan, Xiaofeng Ding, Hai Jin, Zheng Xu, Lichao Sun", "abstract": "Federated learning enables multiple clients to collaboratively learn a global\nmodel by periodically aggregating the clients' models without transferring the\nlocal data. However, due to the heterogeneity of the system and data, many\napproaches suffer from the \"client-drift\" issue that could significantly slow\ndown the convergence of the global model training. As clients perform local\nupdates on heterogeneous data through heterogeneous systems, their local models\ndrift apart. To tackle this issue, one intuitive idea is to guide the local\nmodel training by the global teachers, i.e., past global models, where each\nclient learns the global knowledge from past global models via adaptive\nknowledge distillation techniques. Coming from these insights, we propose a\nnovel approach for heterogeneous federated learning, namely FedGKD, which fuses\nthe knowledge from historical global models for local training to alleviate the\n\"client-drift\" issue. In this paper, we evaluate FedGKD with extensive\nexperiments on various CV/NLP datasets (i.e., CIFAR-10/100, Tiny-ImageNet, AG\nNews, SST5) and different heterogeneous settings. The proposed method is\nguaranteed to converge under common assumptions, and achieves superior\nempirical accuracy in fewer communication runs than five state-of-the-art\nmethods.", "time": "2021-09-13T02:13:18Z", "link": "http://arxiv.org/abs/2107.00051v2", "id": "2107.00051v2", "title": "Local-Global Knowledge Distillation in Heterogeneous Federated Learning\n  with Non-IID Data"}
{"author": "Tehrim Yoon, Sumin Shin, Sung Ju Hwang, Eunho Yang", "abstract": "Federated learning (FL) allows edge devices to collectively learn a model\nwithout directly sharing data within each device, thus preserving privacy and\neliminating the need to store data globally. While there are promising results\nunder the assumption of independent and identically distributed (iid) local\ndata, current state-of-the-art algorithms suffer from performance degradation\nas the heterogeneity of local data across clients increases. To resolve this\nissue, we propose a simple framework, Mean Augmented Federated Learning (MAFL),\nwhere clients send and receive averaged local data, subject to the privacy\nrequirements of target applications. Under our framework, we propose a new\naugmentation algorithm, named FedMix, which is inspired by a phenomenal yet\nsimple data augmentation method, Mixup, but does not require local raw data to\nbe directly shared among devices. Our method shows greatly improved performance\nin the standard benchmark datasets of FL, under highly non-iid federated\nsettings, compared to conventional algorithms.", "time": "2021-07-01T06:14:51Z", "link": "http://arxiv.org/abs/2107.00233v1", "id": "2107.00233v1", "title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning"}
{"author": "Mahardhika Pratama, Choiru Za'in, Edwin Lughofer, Eric Pardede, Dwi A. P. Rahayu", "abstract": "The large-scale data stream problem refers to high-speed information flow\nwhich cannot be processed in scalable manner under a traditional computing\nplatform. This problem also imposes expensive labelling cost making the\ndeployment of fully supervised algorithms unfeasible. On the other hand, the\nproblem of semi-supervised large-scale data streams is little explored in the\nliterature because most works are designed in the traditional single-node\ncomputing environments while also being fully supervised approaches. This paper\noffers Weakly Supervised Scalable Teacher Forcing Network (WeScatterNet) to\ncope with the scarcity of labelled samples and the large-scale data streams\nsimultaneously. WeScatterNet is crafted under distributed computing platform of\nApache Spark with a data-free model fusion strategy for model compression after\nparallel computing stage. It features an open network structure to address the\nglobal and local drift problems while integrating a data augmentation,\nannotation and auto-correction ($DA^3$) method for handling partially labelled\ndata streams. The performance of WeScatterNet is numerically evaluated in the\nsix large-scale data stream problems with only $25\\%$ label proportions. It\nshows highly competitive performance even if compared with fully supervised\nlearners with $100\\%$ label proportions.", "time": "2021-06-26T03:37:40Z", "link": "http://arxiv.org/abs/2107.02943v1", "id": "2107.02943v1", "title": "Scalable Teacher Forcing Network for Semi-Supervised Large Scale Data\n  Streams"}
{"author": "Farnaz Tahmasebian, Jian Lou, Li Xiong", "abstract": "Federated learning is a prominent framework that enables clients (e.g.,\nmobile devices or organizations) to train a collaboratively global model under\na central server's orchestration while keeping local training datasets'\nprivacy. However, the aggregation step in federated learning is vulnerable to\nadversarial attacks as the central server cannot manage clients' behavior.\nTherefore, the global model's performance and convergence of the training\nprocess will be affected under such attacks.To mitigate this vulnerability\nissue, we propose a novel robust aggregation algorithm inspired by the truth\ninference methods in crowdsourcing via incorporating the worker's reliability\ninto aggregation. We evaluate our solution on three real-world datasets with a\nvariety of machine learning models. Experimental results show that our solution\nensures robust federated learning and is resilient to various types of attacks,\nincluding noisy data attacks, Byzantine attacks, and label flipping attacks.", "time": "2021-07-18T09:34:57Z", "link": "http://arxiv.org/abs/2107.08402v1", "id": "2107.08402v1", "title": "RobustFed: A Truth Inference Approach for Robust Federated Learning"}
{"author": "Yann Fraboni, Richard Vidal, Laetitia Kameni, Marco Lorenzi", "abstract": "While client sampling is a central operation of current state-of-the-art\nfederated learning (FL) approaches, the impact of this procedure on the\nconvergence and speed of FL remains under-investigated. In this work, we\nprovide a general theoretical framework to quantify the impact of a client\nsampling scheme and of the clients heterogeneity on the federated optimization.\nFirst, we provide a unified theoretical ground for previously reported sampling\nschemes experimental results on the relationship between FL convergence and the\nvariance of the aggregation weights. Second, we prove for the first time that\nthe quality of FL convergence is also impacted by the resulting covariance\nbetween aggregation weights. Our theory is general, and is here applied to\nMultinomial Distribution (MD) and Uniform sampling, two default unbiased client\nsampling schemes of FL, and demonstrated through a series of experiments in\nnon-iid and unbalanced scenarios. Our results suggest that MD sampling should\nbe used as default sampling scheme, due to the resilience to the changes in\ndata ratio during the learning process, while Uniform sampling is superior only\nin the special case when clients have the same amount of data.", "time": "2022-06-14T18:59:45Z", "link": "http://arxiv.org/abs/2107.12211v4", "id": "2107.12211v4", "title": "A General Theory for Client Sampling in Federated Learning"}
{"author": "Sung Kuk Shyn, Donghee Kim, Kwangsu Kim", "abstract": "Client contribution evaluation, also known as data valuation, is a crucial\napproach in federated learning(FL) for client selection and incentive\nallocation. However, due to restrictions of accessibility of raw data, only\nlimited information such as local weights and local data size of each client is\nopen for quantifying the client contribution. Using data size from available\ninformation, we introduce an empirical evaluation method called Federated\nClient Contribution Evaluation through Accuracy Approximation(FedCCEA). This\nmethod builds the Accuracy Approximation Model(AAM), which estimates a\nsimulated test accuracy using inputs of sampled data size and extracts the\nclients' data quality and data size to measure client contribution. FedCCEA\nstrengthens some advantages: (1) enablement of data size selection to the\nclients, (2) feasible evaluation time regardless of the number of clients, and\n(3) precise estimation in non-IID settings. We demonstrate the superiority of\nFedCCEA compared to previous methods through several experiments: client\ncontribution distribution, client removal, and robustness test to partial\nparticipation.", "time": "2021-06-04T07:42:56Z", "link": "http://arxiv.org/abs/2106.02310v1", "id": "2106.02310v1", "title": "FedCCEA : A Practical Approach of Client Contribution Evaluation for\n  Federated Learning"}
{"author": "Joel Lamy-Poirier", "abstract": "The advent of the transformer has sparked a quick growth in the size of\nlanguage models, far outpacing hardware improvements. (Dense) transformers are\nexpected to reach the trillion-parameter scale in the near future, for which\ntraining requires thousands or even tens of thousands of GPUs. We investigate\nthe challenges of training at this scale and beyond on commercially available\nhardware. In particular, we analyse the shortest possible training time for\ndifferent configurations of distributed training, leveraging empirical scaling\nlaws for language models to estimate the optimal (critical) batch size.\nContrary to popular belief, we find no evidence for a memory wall, and instead\nargue that the real limitation -- other than the cost -- lies in the training\nduration.\n  In addition to this analysis, we introduce two new methods, \\textit{layered\ngradient accumulation} and \\textit{modular pipeline parallelism}, which\ntogether cut the shortest training time by half. The methods also reduce data\nmovement, lowering the network requirement to a point where a fast InfiniBand\nconnection is not necessary. This increased network efficiency also improve on\nthe methods introduced with the ZeRO optimizer, reducing the memory usage to a\ntiny fraction of the available GPU memory.", "time": "2021-06-04T19:21:49Z", "link": "http://arxiv.org/abs/2106.02679v1", "id": "2106.02679v1", "title": "Layered gradient accumulation and modular pipeline parallelism: fast and\n  efficient training of large language models"}
{"author": "Mikhail Khodak, Renbo Tu, Tian Li, Liam Li, Maria-Florina Balcan, Virginia Smith, Ameet Talwalkar", "abstract": "Tuning hyperparameters is a crucial but arduous part of the machine learning\npipeline. Hyperparameter optimization is even more challenging in federated\nlearning, where models are learned over a distributed network of heterogeneous\ndevices; here, the need to keep data on device and perform local training makes\nit difficult to efficiently train and evaluate configurations. In this work, we\ninvestigate the problem of federated hyperparameter tuning. We first identify\nkey challenges and show how standard approaches may be adapted to form\nbaselines for the federated setting. Then, by making a novel connection to the\nneural architecture search technique of weight-sharing, we introduce a new\nmethod, FedEx, to accelerate federated hyperparameter tuning that is applicable\nto widely-used federated optimization methods such as FedAvg and recent\nvariants. Theoretically, we show that a FedEx variant correctly tunes the\non-device learning rate in the setting of online convex optimization across\ndevices. Empirically, we show that FedEx can outperform natural baselines for\nfederated hyperparameter tuning by several percentage points on the\nShakespeare, FEMNIST, and CIFAR-10 benchmarks, obtaining higher accuracy using\nthe same training budget.", "time": "2021-11-04T14:45:27Z", "link": "http://arxiv.org/abs/2106.04502v2", "id": "2106.04502v2", "title": "Federated Hyperparameter Tuning: Challenges, Baselines, and Connections\n  to Weight-Sharing"}
{"author": "Fan Yang, Gabriel Barth-Maron, Piotr StaÅczyk, Matthew Hoffman, Siqi Liu, Manuel Kroiss, Aedan Pope, Alban Rrustemi", "abstract": "A major driver behind the success of modern machine learning algorithms has\nbeen their ability to process ever-larger amounts of data. As a result, the use\nof distributed systems in both research and production has become increasingly\nprevalent as a means to scale to this growing data. At the same time, however,\ndistributing the learning process can drastically complicate the implementation\nof even simple algorithms. This is especially problematic as many machine\nlearning practitioners are not well-versed in the design of distributed\nsystems, let alone those that have complicated communication topologies. In\nthis work we introduce Launchpad, a programming model that simplifies the\nprocess of defining and launching distributed systems that is specifically\ntailored towards a machine learning audience. We describe our framework, its\ndesign philosophy and implementation, and give a number of examples of common\nlearning algorithms whose designs are greatly simplified by this approach.", "time": "2021-06-07T17:02:10Z", "link": "http://arxiv.org/abs/2106.04516v1", "id": "2106.04516v1", "title": "Launchpad: A Programming Model for Distributed Machine Learning Research"}
{"author": "Kahou Tam, Li Li, Bo Han, Chengzhong Xu, Huazhu Fu", "abstract": "Federated learning (FL) collaboratively trains a shared global model\ndepending on multiple local clients, while keeping the training data\ndecentralized in order to preserve data privacy. However, standard FL methods\nignore the noisy client issue, which may harm the overall performance of the\nshared model. We first investigate critical issue caused by noisy clients in FL\nand quantify the negative impact of the noisy clients in terms of the\nrepresentations learned by different layers. We have the following two key\nobservations: (1) the noisy clients can severely impact the convergence and\nperformance of the global model in FL, and (2) the noisy clients can induce\ngreater bias in the deeper layers than the former layers of the global model.\nBased on the above observations, we propose Fed-NCL, a framework that conducts\nrobust federated learning with noisy clients. Specifically, Fed-NCL first\nidentifies the noisy clients through well estimating the data quality and model\ndivergence. Then robust layer-wise aggregation is proposed to adaptively\naggregate the local models of each client to deal with the data heterogeneity\ncaused by the noisy clients. We further perform the label correction on the\nnoisy clients to improve the generalization of the global model. Experimental\nresults on various datasets demonstrate that our algorithm boosts the\nperformances of different state-of-the-art systems with noisy clients. Our code\nis available on https://github.com/TKH666/Fed-NCL", "time": "2022-12-01T01:34:25Z", "link": "http://arxiv.org/abs/2106.13239v4", "id": "2106.13239v4", "title": "Federated Noisy Client Learning"}
{"author": "Han Xie, Jing Ma, Li Xiong, Carl Yang", "abstract": "Federated learning has emerged as an important paradigm for training machine\nlearning models in different domains. For graph-level tasks such as graph\nclassification, graphs can also be regarded as a special type of data samples,\nwhich can be collected and stored in separate local systems. Similar to other\ndomains, multiple local systems, each holding a small set of graphs, may\nbenefit from collaboratively training a powerful graph mining model, such as\nthe popular graph neural networks (GNNs). To provide more motivation towards\nsuch endeavors, we analyze real-world graphs from different domains to confirm\nthat they indeed share certain graph properties that are statistically\nsignificant compared with random graphs. However, we also find that different\nsets of graphs, even from the same domain or same dataset, are non-IID\nregarding both graph structures and node features. To handle this, we propose a\ngraph clustered federated learning (GCFL) framework that dynamically finds\nclusters of local systems based on the gradients of GNNs, and theoretically\njustify that such clusters can reduce the structure and feature heterogeneity\namong graphs owned by the local systems. Moreover, we observe the gradients of\nGNNs to be rather fluctuating in GCFL which impedes high-quality clustering,\nand design a gradient sequence-based clustering mechanism based on dynamic time\nwarping (GCFL+). Extensive experimental results and in-depth analysis\ndemonstrate the effectiveness of our proposed frameworks.", "time": "2021-11-08T03:06:13Z", "link": "http://arxiv.org/abs/2106.13423v5", "id": "2106.13423v5", "title": "Federated Graph Classification over Non-IID Graphs"}
{"author": "Jamal Toutouh, Erik Hemberg, Una-May O'Reilly", "abstract": "Generative adversary networks (GANs) suffer from training pathologies such as\ninstability and mode collapse, which mainly arise from a lack of diversity in\ntheir adversarial interactions. Co-evolutionary GAN (CoE-GAN) training\nalgorithms have shown to be resilient to these pathologies. This article\nintroduces Mustangs, a spatially distributed CoE-GAN, which fosters diversity\nby using different loss functions during the training. Experimental analysis on\nMNIST and CelebA demonstrated that Mustangs trains statistically more accurate\ngenerators.", "time": "2021-06-25T12:40:36Z", "link": "http://arxiv.org/abs/2106.13590v1", "id": "2106.13590v1", "title": "Fostering Diversity in Spatial Evolutionary Generative Adversarial\n  Networks"}
{"author": "Alysa Ziying Tan, Han Yu, Lizhen Cui, Qiang Yang", "abstract": "In parallel with the rapid adoption of Artificial Intelligence (AI) empowered\nby advances in AI research, there have been growing awareness and concerns of\ndata privacy. Recent significant developments in the data regulation landscape\nhave prompted a seismic shift in interest towards privacy-preserving AI. This\nhas contributed to the popularity of Federated Learning (FL), the leading\nparadigm for the training of machine learning models on data silos in a\nprivacy-preserving manner. In this survey, we explore the domain of\nPersonalized FL (PFL) to address the fundamental challenges of FL on\nheterogeneous data, a universal characteristic inherent in all real-world\ndatasets. We analyze the key motivations for PFL and present a unique taxonomy\nof PFL techniques categorized according to the key challenges and\npersonalization strategies in PFL. We highlight their key ideas, challenges and\nopportunities and envision promising future trajectories of research towards\nnew PFL architectural design, realistic PFL benchmarking, and trustworthy PFL\napproaches.", "time": "2022-03-17T13:05:28Z", "link": "http://arxiv.org/abs/2103.00710v3", "id": "2103.00710v3", "title": "Towards Personalized Federated Learning"}
{"author": "Bingyan Liu, Yao Guo, Xiangqun Chen", "abstract": "Federated learning (FL) has become a prevalent distributed machine learning\nparadigm with improved privacy. After learning, the resulting federated model\nshould be further personalized to each different client. While several methods\nhave been proposed to achieve personalization, they are typically limited to a\nsingle local device, which may incur bias or overfitting since data in a single\ndevice is extremely limited. In this paper, we attempt to realize\npersonalization beyond a single client. The motivation is that during FL, there\nmay exist many clients with similar data distribution, and thus the\npersonalization performance could be significantly boosted if these similar\nclients can cooperate with each other. Inspired by this, this paper introduces\na new concept called federated adaptation, targeting at adapting the trained\nmodel in a federated manner to achieve better personalization results. However,\nthe key challenge for federated adaptation is that we could not outsource any\nraw data from the client during adaptation, due to privacy concerns. In this\npaper, we propose PFA, a framework to accomplish Privacy-preserving Federated\nAdaptation. PFA leverages the sparsity property of neural networks to generate\nprivacy-preserving representations and uses them to efficiently identify\nclients with similar data distributions. Based on the grouping results, PFA\nconducts an FL process in a group-wise way on the federated model to accomplish\nthe adaptation. For evaluation, we manually construct several practical FL\ndatasets based on public datasets in order to simulate both the class-imbalance\nand background-difference conditions. Extensive experiments on these datasets\nand popular model architectures demonstrate the effectiveness of PFA,\noutperforming other state-of-the-art methods by a large margin while ensuring\nuser privacy. We will release our code at: https://github.com/lebyni/PFA.", "time": "2021-03-08T16:11:55Z", "link": "http://arxiv.org/abs/2103.01548v2", "id": "2103.01548v2", "title": "PFA: Privacy-preserving Federated Adaptation for Effective Model\n  Personalization"}
{"author": "Yuncheng Wu, Tien Tuan Anh Dinh, Guoyu Hu, Meihui Zhang, Yeow Meng Chee, Beng Chin Ooi", "abstract": "Machine learning (ML) is an important part of modern data science\napplications. Data scientists today have to manage the end-to-end ML life cycle\nthat includes both model training and model serving, the latter of which is\nessential, as it makes their works available to end-users. Systems of model\nserving require high performance, low cost, and ease of management. Cloud\nproviders are already offering model serving choices, including managed\nservices and self-rented servers. Recently, serverless computing, whose\nadvantages include high elasticity and a fine-grained cost model, brings\nanother option for model serving. Our goal in this paper is to examine the\nviability of serverless as a mainstream model serving platform. To this end, we\nfirst conduct a comprehensive evaluation of the performance and cost of\nserverless against other model serving systems on Amazon Web Service and Google\nCloud Platform. We find that serverless outperforms many cloud-based\nalternatives. Further, there are settings under which it even achieves better\nperformance than GPU-based systems. Next, we present the design space of\nserverless model serving, which comprises multiple dimensions, including cloud\nplatforms, serving runtimes, and other function-specific parameters. For each\ndimension, we analyze the impact of different choices and provide suggestions\nfor data scientists to better utilize serverless model serving. Finally, we\ndiscuss challenges and opportunities in building a more practical serverless\nmodel serving system.", "time": "2022-03-01T15:53:39Z", "link": "http://arxiv.org/abs/2103.02958v3", "id": "2103.02958v3", "title": "Serverless Data Science -- Are We There Yet? A Case Study of Model\n  Serving"}
{"author": "Runhua Xu, Nathalie Baracaldo, Yi Zhou, Ali Anwar, James Joshi, Heiko Ludwig", "abstract": "Federated learning (FL) has been proposed to allow collaborative training of\nmachine learning (ML) models among multiple parties where each party can keep\nits data private. In this paradigm, only model updates, such as model weights\nor gradients, are shared. Many existing approaches have focused on horizontal\nFL, where each party has the entire feature set and labels in the training data\nset. However, many real scenarios follow a vertically-partitioned FL setup,\nwhere a complete feature set is formed only when all the datasets from the\nparties are combined, and the labels are only available to a single party.\nPrivacy-preserving vertical FL is challenging because complete sets of labels\nand features are not owned by one entity. Existing approaches for vertical FL\nrequire multiple peer-to-peer communications among parties, leading to lengthy\ntraining times, and are restricted to (approximated) linear models and just two\nparties. To close this gap, we propose FedV, a framework for secure gradient\ncomputation in vertical settings for several widely used ML models such as\nlinear models, logistic regression, and support vector machines. FedV removes\nthe need for peer-to-peer communication among parties by using functional\nencryption schemes; this allows FedV to achieve faster training times. It also\nworks for larger and changing sets of parties. We empirically demonstrate the\napplicability for multiple types of ML models and show a reduction of 10%-70%\nof training time and 80% to 90% in data transfer with respect to the\nstate-of-the-art approaches.", "time": "2021-06-16T20:31:33Z", "link": "http://arxiv.org/abs/2103.03918v2", "id": "2103.03918v2", "title": "FedV: Privacy-Preserving Federated Learning over Vertically Partitioned\n  Data"}
{"author": "Lusine Abrahamyan, Yiming Chen, Giannis Bekoulis, Nikos Deligiannis", "abstract": "Training deep neural networks on large datasets containing high-dimensional\ndata requires a large amount of computation. A solution to this problem is\ndata-parallel distributed training, where a model is replicated into several\ncomputational nodes that have access to different chunks of the data. This\napproach, however, entails high communication rates and latency because of the\ncomputed gradients that need to be shared among nodes at every iteration. The\nproblem becomes more pronounced in the case that there is wireless\ncommunication between the nodes (i.e. due to the limited network bandwidth). To\naddress this problem, various compression methods have been proposed including\nsparsification, quantization, and entropy encoding of the gradients. Existing\nmethods leverage the intra-node information redundancy, that is, they compress\ngradients at each node independently. In contrast, we advocate that the\ngradients across the nodes are correlated and propose methods to leverage this\ninter-node redundancy to improve compression efficiency. Depending on the node\ncommunication protocol (parameter server or ring-allreduce), we propose two\ninstances of the LGC approach that we coin Learned Gradient Compression (LGC).\nOur methods exploit an autoencoder (i.e. trained during the first stages of the\ndistributed training) to capture the common information that exists in the\ngradients of the distributed nodes. We have tested our LGC methods on the image\nclassification and semantic segmentation tasks using different convolutional\nneural networks (ResNet50, ResNet101, PSPNet) and multiple datasets (ImageNet,\nCifar10, CamVid). The ResNet101 model trained for image classification on\nCifar10 achieved an accuracy of 93.57%, which is lower than the baseline\ndistributed training with uncompressed gradients only by 0.18%.", "time": "2021-03-17T05:55:33Z", "link": "http://arxiv.org/abs/2103.08870v2", "id": "2103.08870v2", "title": "Learned Gradient Compression for Distributed Deep Learning"}
{"author": "Medha Atre, Birendra Jha, Ashwini Rao", "abstract": "Use of Deep Learning (DL) in commercial applications such as image\nclassification, sentiment analysis and speech recognition is increasing. When\ntraining DL models with large number of parameters and/or large datasets, cost\nand speed of training can become prohibitive. Distributed DL training solutions\nthat split a training job into subtasks and execute them over multiple nodes\ncan decrease training time. However, the cost of current solutions, built\npredominantly for cluster computing systems, can still be an issue. In contrast\nto cluster computing systems, Volunteer Computing (VC) systems can lower the\ncost of computing, but applications running on VC systems have to handle fault\ntolerance, variable network latency and heterogeneity of compute nodes, and the\ncurrent solutions are not designed to do so. We design a distributed solution\nthat can run DL training on a VC system by using a data parallel approach. We\nimplement a novel asynchronous SGD scheme called VC-ASGD suited for VC systems.\nIn contrast to traditional VC systems that lower cost by using untrustworthy\nvolunteer devices, we lower cost by leveraging preemptible computing instances\non commercial cloud platforms. By using preemptible instances that require\napplications to be fault tolerant, we lower cost by 70-90% and improve data\nsecurity.", "time": "2021-05-27T06:41:45Z", "link": "http://arxiv.org/abs/2103.08894v3", "id": "2103.08894v3", "title": "Distributed Deep Learning Using Volunteer Computing-Like Paradigm"}
{"author": "Vahideh Hayyolalam, Moayad Aloqaily, Oznur Ozkasap, Mohsen Guizani", "abstract": "The demand for real-time, affordable, and efficient smart healthcare services\nis increasing exponentially due to the technological revolution and burst of\npopulation. To meet the increasing demands on this critical infrastructure,\nthere is a need for intelligent methods to cope with the existing obstacles in\nthis area. In this regard, edge computing technology can reduce latency and\nenergy consumption by moving processes closer to the data sources in comparison\nto the traditional centralized cloud and IoT-based healthcare systems. In\naddition, by bringing automated insights into the smart healthcare systems,\nartificial intelligence (AI) provides the possibility of detecting and\npredicting high-risk diseases in advance, decreasing medical costs for\npatients, and offering efficient treatments. The objective of this article is\nto highlight the benefits of the adoption of edge intelligent technology, along\nwith AI in smart healthcare systems. Moreover, a novel smart healthcare model\nis proposed to boost the utilization of AI and edge technology in smart\nhealthcare systems. Additionally, the paper discusses issues and research\ndirections arising when integrating these different technologies together.", "time": "2021-03-22T19:35:06Z", "link": "http://arxiv.org/abs/2103.12144v1", "id": "2103.12144v1", "title": "Edge Intelligence for Empowering IoT-based Healthcare Systems"}
{"author": "Akhil Mathur, Daniel J. Beutel, Pedro Porto Buarque de GusmÃ£o, Javier Fernandez-Marques, Taner Topal, Xinchi Qiu, Titouan Parcollet, Yan Gao, Nicholas D. Lane", "abstract": "Federated Learning (FL) allows edge devices to collaboratively learn a shared\nprediction model while keeping their training data on the device, thereby\ndecoupling the ability to do machine learning from the need to store data in\nthe cloud. Despite the algorithmic advancements in FL, the support for\non-device training of FL algorithms on edge devices remains poor. In this\npaper, we present an exploration of on-device FL on various smartphones and\nembedded devices using the Flower framework. We also evaluate the system costs\nof on-device FL and discuss how this quantification could be used to design\nmore efficient FL algorithms.", "time": "2021-04-07T10:42:14Z", "link": "http://arxiv.org/abs/2104.03042v1", "id": "2104.03042v1", "title": "On-device Federated Learning with Flower"}
{"author": "Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie Amy Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnakumar Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr Lapukhov, Maxim Naumov, Ajit Mathews, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, Vijay Rao", "abstract": "Deep learning recommendation models (DLRMs) are used across many\nbusiness-critical services at Facebook and are the single largest AI\napplication in terms of infrastructure demand in its data-centers. In this\npaper we discuss the SW/HW co-designed solution for high-performance\ndistributed training of large-scale DLRMs. We introduce a high-performance\nscalable software stack based on PyTorch and pair it with the new evolution of\nZion platform, namely ZionEX. We demonstrate the capability to train very large\nDLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup\nin terms of time to solution over previous systems. We achieve this by (i)\ndesigning the ZionEX platform with dedicated scale-out network, provisioned\nwith high bandwidth, optimal topology and efficient transport (ii) implementing\nan optimized PyTorch-based training stack supporting both model and data\nparallelism (iii) developing sharding algorithms capable of hierarchical\npartitioning of the embedding tables along row, column dimensions and load\nbalancing them across multiple workers; (iv) adding high-performance core\noperators while retaining flexibility to support optimizers with fully\ndeterministic updates (v) leveraging reduced precision communications,\nmulti-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we\ndevelop and briefly comment on distributed data ingestion and other supporting\nservices that are required for the robust and efficient end-to-end training in\nproduction environments.", "time": "2023-02-27T00:21:53Z", "link": "http://arxiv.org/abs/2104.05158v7", "id": "2104.05158v7", "title": "Software-Hardware Co-design for Fast and Scalable Training of Deep\n  Learning Recommendation Models"}
{"author": "Chaoyang He, Keshav Balasubramanian, Emir Ceyani, Carl Yang, Han Xie, Lichao Sun, Lifang He, Liangwei Yang, Philip S. Yu, Yu Rong, Peilin Zhao, Junzhou Huang, Murali Annavaram, Salman Avestimehr", "abstract": "Graph Neural Network (GNN) research is rapidly growing thanks to the capacity\nof GNNs in learning distributed representations from graph-structured data.\nHowever, centralizing a massive amount of real-world graph data for GNN\ntraining is prohibitive due to privacy concerns, regulation restrictions, and\ncommercial competitions. Federated learning (FL), a trending distributed\nlearning paradigm, provides possibilities to solve this challenge while\npreserving data privacy. Despite recent advances in vision and language\ndomains, there is no suitable platform for the FL of GNNs. To this end, we\nintroduce FedGraphNN, an open FL benchmark system that can facilitate research\non federated GNNs. FedGraphNN is built on a unified formulation of graph FL and\ncontains a wide range of datasets from different domains, popular GNN models,\nand FL algorithms, with secure and efficient system support. Particularly for\nthe datasets, we collect, preprocess, and partition 36 datasets from 7 domains,\nincluding both publicly available ones and specifically obtained ones such as\nhERG and Tencent. Our empirical analysis showcases the utility of our benchmark\nsystem, while exposing significant challenges in graph FL: federated GNNs\nperform worse in most datasets with a non-IID split than centralized GNNs; the\nGNN model that attains the best result in the centralized setting may not\nmaintain its advantage in the FL setting. These results imply that more\nresearch efforts are needed to unravel the mystery behind federated GNNs.\nMoreover, our system performance analysis demonstrates that the FedGraphNN\nsystem is computationally efficient and secure to large-scale graphs datasets.\nWe maintain the source code at https://github.com/FedML-AI/FedGraphNN.", "time": "2021-09-08T00:22:55Z", "link": "http://arxiv.org/abs/2104.07145v2", "id": "2104.07145v2", "title": "FedGraphNN: A Federated Learning System and Benchmark for Graph Neural\n  Networks"}
{"author": "AurÃ©lien Bellet, Anne-Marie Kermarrec, Erick Lavoie", "abstract": "The convergence speed of machine learning models trained with Federated\nLearning is significantly affected by heterogeneous data partitions, even more\nso in a fully decentralized setting without a central server. In this paper, we\nshow that the impact of label distribution skew, an important type of data\nheterogeneity, can be significantly reduced by carefully designing the\nunderlying communication topology. We present D-Cliques, a novel topology that\nreduces gradient bias by grouping nodes in sparsely interconnected cliques such\nthat the label distribution in a clique is representative of the global label\ndistribution. We also show how to adapt the updates of decentralized SGD to\nobtain unbiased gradients and implement an effective momentum with D-Cliques.\nOur extensive empirical evaluation on MNIST and CIFAR10 demonstrates that our\napproach provides similar convergence speed as a fully-connected topology,\nwhich provides the best convergence in a data heterogeneous setting, with a\nsignificant reduction in the number of edges and messages. In a 1000-node\ntopology, D-Cliques require 98% less edges and 96% less total messages, with\nfurther possible gains using a small-world topology across cliques.", "time": "2021-11-04T11:48:52Z", "link": "http://arxiv.org/abs/2104.07365v4", "id": "2104.07365v4", "title": "D-Cliques: Compensating for Data Heterogeneity with Topology in\n  Decentralized Federated Learning"}
{"author": "Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He", "abstract": "In the last three years, the largest dense deep learning models have grown\nover 1000x to reach hundreds of billions of parameters, while the GPU memory\nhas only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has\nbeen supported primarily though system innovations that allow large models to\nfit in the aggregate GPU memory of multiple GPUs. However, we are getting close\nto the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion\nparameter model for training, and such clusters are simply out of reach for\nmost data scientists. In addition, training models at that scale requires\ncomplex combinations of parallelism techniques that puts a big burden on the\ndata scientists to refactor their model.\n  In this paper we present ZeRO-Infinity, a novel heterogeneous system\ntechnology that leverages GPU, CPU, and NVMe memory to allow for unprecedented\nmodel scale on limited resources without requiring model code refactoring. At\nthe same time it achieves excellent training throughput and scalability,\nunencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models\nwith tens and even hundreds of trillions of parameters for training on current\ngeneration GPU clusters. It can be used to fine-tune trillion parameter models\non a single NVIDIA DGX-2 node, making large models more accessible. In terms of\ntraining throughput and scalability, it sustains over 25 petaflops on 512\nNVIDIA V100 GPUs(40% of peak), while also demonstrating super linear\nscalability. An open source implementation of ZeRO-Infinity is available\nthrough DeepSpeed, a deep learning optimization library that makes distributed\ntraining easy, efficient, and effective.", "time": "2021-04-16T02:22:12Z", "link": "http://arxiv.org/abs/2104.07857v1", "id": "2104.07857v1", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep\n  Learning"}
{"author": "Narendra Chaudhary, Sanchit Misra, Dhiraj Kalamkar, Alexander Heinecke, Evangelos Georganas, Barukh Ziv, Menachem Adelman, Bharat Kaul", "abstract": "Convolutional neural networks (CNNs) have found many applications in tasks\ninvolving two-dimensional (2D) data, such as image classification and image\nprocessing. Therefore, 2D convolution layers have been heavily optimized on\nCPUs and GPUs. However, in many applications - for example genomics and speech\nrecognition, the data can be one-dimensional (1D). Such applications can\nbenefit from optimized 1D convolution layers. In this work, we introduce our\nefficient implementation of a generic 1D convolution layer covering a wide\nrange of parameters. It is optimized for x86 CPU architectures, in particular,\nfor architectures containing Intel AVX-512 and AVX-512 BFloat16 instructions.\nWe use the LIBXSMM library's batch-reduce General Matrix Multiplication\n(BRGEMM) kernel for FP32 and BFloat16 precision. We demonstrate that our\nimplementation can achieve up to 80% efficiency on Intel Xeon Cascade Lake and\nCooper Lake CPUs. Additionally, we show the generalization capability of our\nBRGEMM based approach by achieving high efficiency across a range of\nparameters. We consistently achieve higher efficiency than the 1D convolution\nlayer with Intel oneDNN library backend for varying input tensor widths, filter\nwidths, number of channels, filters, and dilation parameters. Finally, we\ndemonstrate the performance of our optimized 1D convolution layer by utilizing\nit in the end-to-end neural network training with real genomics datasets and\nachieve up to 6.86x speedup over the oneDNN library-based implementation on\nCascade Lake CPUs. We also demonstrate the scaling with 16 sockets of\nCascade/Cooper Lake CPUs and achieve significant speedup over eight V100 GPUs\nusing a similar power envelop. In the end-to-end training, we get a speedup of\n1.41x on Cascade Lake with FP32, 1.57x on Cooper Lake with FP32, and 2.27x on\nCooper Lake with BFloat16 over eight V100 GPUs with FP32.", "time": "2021-04-16T09:54:30Z", "link": "http://arxiv.org/abs/2104.08002v1", "id": "2104.08002v1", "title": "Efficient and Generic 1D Dilated Convolution Layer for Deep Learning"}
{"author": "Huifeng Guo, Wei Guo, Yong Gao, Ruiming Tang, Xiuqiang He, Wenzhi Liu", "abstract": "Because of the superior feature representation ability of deep learning,\nvarious deep Click-Through Rate (CTR) models are deployed in the commercial\nsystems by industrial companies. To achieve better performance, it is necessary\nto train the deep CTR models on huge volume of training data efficiently, which\nmakes speeding up the training process an essential problem. Different from the\nmodels with dense training data, the training data for CTR models is usually\nhigh-dimensional and sparse. To transform the high-dimensional sparse input\ninto low-dimensional dense real-value vectors, almost all deep CTR models adopt\nthe embedding layer, which easily reaches hundreds of GB or even TB. Since a\nsingle GPU cannot afford to accommodate all the embedding parameters, when\nperforming distributed training, it is not reasonable to conduct the\ndata-parallelism only. Therefore, existing distributed training platforms for\nrecommendation adopt model-parallelism. Specifically, they use CPU (Host)\nmemory of servers to maintain and update the embedding parameters and utilize\nGPU worker to conduct forward and backward computations. Unfortunately, these\nplatforms suffer from two bottlenecks: (1) the latency of pull \\& push\noperations between Host and GPU; (2) parameters update and synchronization in\nthe CPU servers. To address such bottlenecks, in this paper, we propose the\nScaleFreeCTR: a MixCache-based distributed training system for CTR models.\nSpecifically, in SFCTR, we also store huge embedding table in CPU but utilize\nGPU instead of CPU to conduct embedding synchronization efficiently. To reduce\nthe latency of data transfer between both GPU-Host and GPU-GPU, the MixCache\nmechanism and Virtual Sparse Id operation are proposed. Comprehensive\nexperiments and ablation studies are conducted to demonstrate the effectiveness\nand efficiency of SFCTR.", "time": "2021-05-11T14:11:46Z", "link": "http://arxiv.org/abs/2104.08542v2", "id": "2104.08542v2", "title": "ScaleFreeCTR: MixCache-based Distributed Training System for CTR Models\n  with Huge Embedding Table"}
{"author": "Jiehan Zhou, Shouhua Zhang, Qinghua Lu, Wenbin Dai, Min Chen, Xin Liu, Susanna Pirttikangas, Yang Shi, Weishan Zhang, Enrique Herrera-Viedma", "abstract": "Federated learning (FL) brings collaborative intelligence into industries\nwithout centralized training data to accelerate the process of Industry 4.0 on\nthe edge computing level. FL solves the dilemma in which enterprises wish to\nmake the use of data intelligence with security concerns. To accelerate\nindustrial Internet of things with the further leverage of FL, existing\nachievements on FL are developed from three aspects: 1) define terminologies\nand elaborate a general framework of FL for accommodating various scenarios; 2)\ndiscuss the state-of-the-art of FL on fundamental researches including data\npartitioning, privacy preservation, model optimization, local model\ntransportation, personalization, motivation mechanism, platform & tools, and\nbenchmark; 3) discuss the impacts of FL from the economic perspective. To\nattract more attention from industrial academia and practice, a FL-transformed\nmanufacturing paradigm is presented, and future research directions of FL are\ngiven and possible immediate applications in Industry 4.0 domain are also\nproposed.", "time": "2021-04-21T12:40:11Z", "link": "http://arxiv.org/abs/2104.10501v1", "id": "2104.10501v1", "title": "A Survey on Federated Learning and its Applications for Accelerating\n  Industrial Internet of Things"}
{"author": "Gunduz Vehbi Demirci, Hakan Ferhatosmanoglu", "abstract": "The state-of-the-art deep neural networks (DNNs) have significant\ncomputational and data management requirements. The size of both training data\nand models continue to increase. Sparsification and pruning methods are shown\nto be effective in removing a large fraction of connections in DNNs. The\nresulting sparse networks present unique challenges to further improve the\ncomputational efficiency of training and inference in deep learning. Both the\nfeedforward (inference) and backpropagation steps in stochastic gradient\ndescent (SGD) algorithm for training sparse DNNs involve consecutive sparse\nmatrix-vector multiplications (SpMVs). We first introduce a distributed-memory\nparallel SpMV-based solution for the SGD algorithm to improve its scalability.\nThe parallelization approach is based on row-wise partitioning of weight\nmatrices that represent neuron connections between consecutive layers. We then\npropose a novel hypergraph model for partitioning weight matrices to reduce the\ntotal communication volume and ensure computational load-balance among\nprocessors. Experiments performed on sparse DNNs demonstrate that the proposed\nsolution is highly efficient and scalable. By utilizing the proposed matrix\npartitioning scheme, the performance of our solution is further improved\nsignificantly.", "time": "2021-04-23T20:05:52Z", "link": "http://arxiv.org/abs/2104.11805v1", "id": "2104.11805v1", "title": "Partitioning sparse deep neural networks for scalable training and\n  inference"}
{"author": "Tegg Taekyong Sung, Bo Ryu", "abstract": "Deep Reinforcement Learning (DRL) underlies in a simulated environment and\noptimizes objective goals. By extending the conventional interaction scheme,\nthis paper proffers gym-ds3, a scalable and reproducible open environment\ntailored for a high-fidelity Domain-Specific System-on-Chip (DSSoC)\napplication. The simulation corroborates to schedule hierarchical jobs onto\nheterogeneous System-on-Chip (SoC) processors and bridges the system to\nreinforcement learning research. We systematically analyze the representative\nSoC simulator and discuss the primary challenging aspects that the system (1)\ncontinuously generates indefinite jobs at a rapid injection rate, (2) optimizes\ncomplex objectives, and (3) operates in steady-state scheduling. We provide\nexemplary snippets and experimentally demonstrate the run-time performances on\ndifferent schedulers that successfully mimic results achieved from the standard\nDS3 framework and real-world embedded systems.", "time": "2021-04-27T13:46:57Z", "link": "http://arxiv.org/abs/2104.13187v1", "id": "2104.13187v1", "title": "A Scalable and Reproducible System-on-Chip Simulation for Reinforcement\n  Learning"}
{"author": "Yunkai Wei, Zixian An, Supeng Leng, Kun Yang", "abstract": "The sixth generation (6G) systems are generally recognized to be established\non ubiquitous Artificial Intelligence (AI) and distributed ledger such as\nblockchain. However, the AI training demands tremendous computing resource,\nwhich is limited in most 6G devices. Meanwhile, miners in Proof-of-Work (PoW)\nbased blockchains devote massive computing power to block mining, and are\nwidely criticized for the waste of computation. To address this dilemma, we\npropose an Evolved-Proof-of-Work (E-PoW) consensus that can integrate the\nmatrix computations, which are widely existed in AI training, into the process\nof brute-force searches in the block mining. Consequently, E-PoW can connect AI\nlearning and block mining via the multiply used common computing resource.\nExperimental results show that E-PoW can salvage by up to 80 percent computing\npower from pure block mining for parallel AI training in 6G systems.", "time": "2021-04-29T03:19:52Z", "link": "http://arxiv.org/abs/2104.14088v1", "id": "2104.14088v1", "title": "Connecting AI Learning and Blockchain Mining in 6G Systems"}
{"author": "Ji Liu, Jizhou Huang, Yang Zhou, Xuhong Li, Shilei Ji, Haoyi Xiong, Dejing Dou", "abstract": "In recent years, data and computing resources are typically distributed in\nthe devices of end users, various regions or organizations. Because of laws or\nregulations, the distributed data and computing resources cannot be directly\nshared among different regions or organizations for machine learning tasks.\nFederated learning emerges as an efficient approach to exploit distributed data\nand computing resources, so as to collaboratively train machine learning\nmodels, while obeying the laws and regulations and ensuring data security and\ndata privacy. In this paper, we provide a comprehensive survey of existing\nworks for federated learning. We propose a functional architecture of federated\nlearning systems and a taxonomy of related techniques. Furthermore, we present\nthe distributed training, data communication, and security of FL systems.\nFinally, we analyze their limitations and propose future research directions.", "time": "2022-03-25T02:15:50Z", "link": "http://arxiv.org/abs/2104.14362v4", "id": "2104.14362v4", "title": "From Distributed Machine Learning to Federated Learning: A Survey"}
{"author": "Saeed Khorram, Xiao Fu, Mohamad H. Danesh, Zhongang Qi, Li Fuxin", "abstract": "In this paper, we propose Stochastic Block-ADMM as an approach to train deep\nneural networks in batch and online settings. Our method works by splitting\nneural networks into an arbitrary number of blocks and utilizes auxiliary\nvariables to connect these blocks while optimizing with stochastic gradient\ndescent. This allows training deep networks with non-differentiable constraints\nwhere conventional backpropagation is not applicable. An application of this is\nsupervised feature disentangling, where our proposed DeepFacto inserts a\nnon-negative matrix factorization (NMF) layer into the network. Since\nbackpropagation only needs to be performed within each block, our approach\nalleviates vanishing gradients and provides potentials for parallelization. We\nprove the convergence of our proposed method and justify its capabilities\nthrough experiments in supervised and weakly-supervised settings.", "time": "2021-05-01T19:56:13Z", "link": "http://arxiv.org/abs/2105.00339v1", "id": "2105.00339v1", "title": "Stochastic Block-ADMM for Training Deep Networks"}
{"author": "Weiming Zhuang, Xin Gan, Yonggang Wen, Shuai Zhang", "abstract": "Academia and industry have developed several platforms to support the popular\nprivacy-preserving distributed learning method -- Federated Learning (FL).\nHowever, these platforms are complex to use and require a deep understanding of\nFL, which imposes high barriers to entry for beginners, limits the productivity\nof researchers, and compromises deployment efficiency. In this paper, we\npropose the first low-code FL platform, EasyFL, to enable users with various\nlevels of expertise to experiment and prototype FL applications with little\ncoding. We achieve this goal while ensuring great flexibility and extensibility\nfor customization by unifying simple API design, modular design, and granular\ntraining flow abstraction. With only a few lines of code, EasyFL empowers them\nwith many out-of-the-box functionalities to accelerate experimentation and\ndeployment. These practical functionalities are heterogeneity simulation,\ncomprehensive tracking, distributed training optimization, and seamless\ndeployment. They are proposed based on challenges identified in the proposed FL\nlife cycle. Compared with other platforms, EasyFL not only requires just three\nlines of code (at least 10x lesser) to build a vanilla FL application but also\nincurs lower training overhead. Besides, our evaluations demonstrate that\nEasyFL expedites distributed training by 1.5x. It also improves the efficiency\nof deployment. We believe that EasyFL will increase the productivity of\nresearchers and democratize FL to wider audiences.", "time": "2022-01-20T02:21:46Z", "link": "http://arxiv.org/abs/2105.07603v3", "id": "2105.07603v3", "title": "EasyFL: A Low-code Federated Learning Platform For Dummies"}
{"author": "Weiming Zhuang, Xin Gan, Yonggang Wen, Xuesen Zhang, Shuai Zhang, Shuai Yi", "abstract": "Unsupervised domain adaptation has been widely adopted to generalize models\nfor unlabeled data in a target domain, given labeled data in a source domain,\nwhose data distributions differ from the target domain. However, existing works\nare inapplicable to face recognition under privacy constraints because they\nrequire sharing sensitive face images between two domains. To address this\nproblem, we propose a novel unsupervised federated face recognition approach\n(FedFR). FedFR improves the performance in the target domain by iteratively\naggregating knowledge from the source domain through federated learning. It\nprotects data privacy by transferring models instead of raw data between\ndomains. Besides, we propose a new domain constraint loss (DCL) to regularize\nsource domain training. DCL suppresses the data volume dominance of the source\ndomain. We also enhance a hierarchical clustering algorithm to predict pseudo\nlabels for the unlabeled target domain accurately. To this end, FedFR forms an\nend-to-end training pipeline: (1) pre-train in the source domain; (2) predict\npseudo labels by clustering in the target domain; (3) conduct\ndomain-constrained federated learning across two domains. Extensive experiments\nand analysis on two newly constructed benchmarks demonstrate the effectiveness\nof FedFR. It outperforms the baseline and classic methods in the target domain\nby over 4% on the more realistic benchmark. We believe that FedFR will shed\nlight on applying federated learning to more computer vision tasks under\nprivacy constraints.", "time": "2021-05-17T04:24:25Z", "link": "http://arxiv.org/abs/2105.07606v1", "id": "2105.07606v1", "title": "Towards Unsupervised Domain Adaptation for Deep Face Recognition under\n  Privacy Constraints via Federated Learning"}
{"author": "Wei Shao, Arian Prabowo, Sichen Zhao, Piotr Koniusz, Flora D. Salim", "abstract": "To model and forecast flight delays accurately, it is crucial to harness\nvarious vehicle trajectory and contextual sensor data on airport tarmac areas.\nThese heterogeneous sensor data, if modelled correctly, can be used to generate\na situational awareness map. Existing techniques apply traditional supervised\nlearning methods onto historical data, contextual features and route\ninformation among different airports to predict flight delay are inaccurate and\nonly predict arrival delay but not departure delay, which is essential to\nairlines. In this paper, we propose a vision-based solution to achieve a high\nforecasting accuracy, applicable to the airport. Our solution leverages a\nsnapshot of the airport situational awareness map, which contains various\ntrajectories of aircraft and contextual features such as weather and airline\nschedules. We propose an end-to-end deep learning architecture, TrajCNN, which\ncaptures both the spatial and temporal information from the situational\nawareness map. Additionally, we reveal that the situational awareness map of\nthe airport has a vital impact on estimating flight departure delay. Our\nproposed framework obtained a good result (around 18 minutes error) for\npredicting flight departure delay at Los Angeles International Airport.", "time": "2021-05-19T07:38:57Z", "link": "http://arxiv.org/abs/2105.08969v1", "id": "2105.08969v1", "title": "Predicting Flight Delay with Spatio-Temporal Trajectory Convolutional\n  Network and Airport Situational Awareness Map"}
{"author": "Milad Khademi Nori, Sangseok Yun, Il-Min Kim", "abstract": "Federated Learning (FL) has recently received a lot of attention for\nlarge-scale privacy-preserving machine learning. However, high communication\noverheads due to frequent gradient transmissions decelerate FL. To mitigate the\ncommunication overheads, two main techniques have been studied: (i) local\nupdate of weights characterizing the trade-off between communication and\ncomputation and (ii) gradient compression characterizing the trade-off between\ncommunication and precision. To the best of our knowledge, studying and\nbalancing those two trade-offs jointly and dynamically while considering their\nimpacts on convergence has remained unresolved even though it promises\nsignificantly faster FL. In this paper, we first formulate our problem to\nminimize learning error with respect to two variables: local update\ncoefficients and sparsity budgets of gradient compression who characterize\ntrade-offs between communication and computation/precision, respectively. We\nthen derive an upper bound of the learning error in a given wall-clock time\nconsidering the interdependency between the two variables. Based on this\ntheoretical analysis, we propose an enhanced FL scheme, namely Fast FL (FFL),\nthat jointly and dynamically adjusts the two variables to minimize the learning\nerror. We demonstrate that FFL consistently achieves higher accuracies faster\nthan similar schemes existing in the literature.", "time": "2021-05-23T21:55:14Z", "link": "http://arxiv.org/abs/2105.11028v1", "id": "2105.11028v1", "title": "Fast Federated Learning by Balancing Communication Trade-Offs"}
{"author": "Fan Lai, Yinwei Dai, Sanjay S. Singapuram, Jiachen Liu, Xiangfeng Zhu, Harsha V. Madhyastha, Mosharaf Chowdhury", "abstract": "We present FedScale, a federated learning (FL) benchmarking suite with\nrealistic datasets and a scalable runtime to enable reproducible FL research.\nFedScale datasets encompass a wide range of critical FL tasks, ranging from\nimage classification and object detection to language modeling and speech\nrecognition. Each dataset comes with a unified evaluation protocol using\nreal-world data splits and evaluation metrics. To reproduce realistic FL\nbehavior, FedScale contains a scalable and extensible runtime. It provides\nhigh-level APIs to implement FL algorithms, deploy them at scale across diverse\nhardware and software backends, and evaluate them at scale, all with minimal\ndeveloper efforts. We combine the two to perform systematic benchmarking\nexperiments and highlight potential opportunities for heterogeneity-aware\nco-optimizations in FL. FedScale is open-source and actively maintained by\ncontributors from different institutions at http://fedscale.ai. We welcome\nfeedback and contributions from the community.", "time": "2022-06-18T02:22:23Z", "link": "http://arxiv.org/abs/2105.11367v5", "id": "2105.11367v5", "title": "FedScale: Benchmarking Model and System Performance of Federated\n  Learning at Scale"}
{"author": "Jinyang Liu, Sheng Di, Kai Zhao, Sian Jin, Dingwen Tao, Xin Liang, Zizhong Chen, Franck Cappello", "abstract": "Error-bounded lossy compression is becoming an indispensable technique for\nthe success of today's scientific projects with vast volumes of data produced\nduring simulations or instrument data acquisitions. Not only can it\nsignificantly reduce data size, but it also can control the compression errors\nbased on user-specified error bounds. Autoencoder (AE) models have been widely\nused in image compression, but few AE-based compression approaches support\nerror-bounding features, which are highly required by scientific applications.\nTo address this issue, we explore using convolutional autoencoders to improve\nerror-bounded lossy compression for scientific data, with the following three\nkey contributions. (1) We provide an in-depth investigation of the\ncharacteristics of various autoencoder models and develop an error-bounded\nautoencoder-based framework in terms of the SZ model. (2) We optimize the\ncompression quality for the main stages in our designed AE-based error-bounded\ncompression framework, fine-tuning the block sizes and latent sizes and also\noptimizing the compression efficiency of latent vectors. (3) We evaluate our\nproposed solution using five real-world scientific datasets and compare them\nwith six other related works. Experiments show that our solution exhibits a\nvery competitive compression quality among all the compressors in our tests. In\nabsolute terms, it can obtain a much better compression quality (100% ~ 800%\nimprovement in compression ratio with the same data distortion) compared with\nSZ2.1 and ZFP in cases with a high compression ratio.", "time": "2023-10-21T22:26:08Z", "link": "http://arxiv.org/abs/2105.11730v7", "id": "2105.11730v7", "title": "Exploring Autoencoder-based Error-bounded Compression for Scientific\n  Data"}
{"author": "Boxiang Wang, Qifan Xu, Zhengda Bian, Yang You", "abstract": "Together with the improvements in state-of-the-art accuracies of various\ntasks, deep learning models are getting significantly larger. However, it is\nextremely difficult to implement these large models because limited GPU memory\nmakes it impossible to fit large models into a single GPU or even a GPU server.\nBesides, it is highly necessary to reduce the training time for large models.\nPrevious methods like Megatron-LM implemented a 1-Dimensional distributed\nmethod to use GPUs to speed up the training. However, these methods have a high\ncommunication overhead and a low scaling efficiency on large-scale clusters. To\nsolve these problems, we propose Tesseract, a highly scalable tensor\nparallelism with a novel design. It increases efficiency by reducing\ncommunication overhead and lowers the memory required for each GPU. By\nintroducing the novel dimension into tensor parallelism, Tesseract greatly\nincreases the memory capacity of tensor parallelism. Concretely, this new\ndimension furthermore increases the degree of tensor parallelism. Compared to\nprevious 1-D and 2-D methods, Tesseract manages to reduce the communication\ncost on each layer, resulting in speedups of 1.38x and 1.53x respectively with\nstrong scaling. In weak scaling experiments, Tesseract achieves a maximum of\n4.0/1.7 times inference speedup and 3.4/1.7 times throughput improvement\ncompared to 1-D/2-D methods, respectively. By introducing Tesseract, we offer a\nmore efficient and scalable way to implement large deep learning models with\nlimited GPU resources.", "time": "2022-09-01T04:49:37Z", "link": "http://arxiv.org/abs/2105.14500v2", "id": "2105.14500v2", "title": "Tesseract: Parallelize the Tensor Parallelism Efficiently"}
{"author": "Anis Elgabli, Chaouki Ben Issaid, Amrit S. Bedi, Mehdi Bennis, Vaneet Aggarwal", "abstract": "In this paper, we propose an energy-efficient federated meta-learning\nframework. The objective is to enable learning a meta-model that can be\nfine-tuned to a new task with a few number of samples in a distributed setting\nand at low computation and communication energy consumption. We assume that\neach task is owned by a separate agent, so a limited number of tasks is used to\ntrain a meta-model. Assuming each task was trained offline on the agent's local\ndata, we propose a lightweight algorithm that starts from the local models of\nall agents, and in a backward manner using projected stochastic gradient ascent\n(P-SGA) finds a meta-model. The proposed method avoids complex computations\nsuch as computing hessian, double looping, and matrix inversion, while\nachieving high performance at significantly less energy consumption compared to\nthe state-of-the-art methods such as MAML and iMAML on conducted experiments\nfor sinusoid regression and image classification tasks.", "time": "2021-05-31T08:15:44Z", "link": "http://arxiv.org/abs/2105.14772v1", "id": "2105.14772v1", "title": "Energy-Efficient and Federated Meta-Learning via Projected Stochastic\n  Gradient Ascent"}
{"author": "Kai Yue, Richeng Jin, Chau-Wai Wong, Huaiyu Dai", "abstract": "Federated learning can enable remote workers to collaboratively train a\nshared machine learning model while allowing training data to be kept locally.\nIn the use case of wireless mobile devices, the communication overhead is a\ncritical bottleneck due to limited power and bandwidth. Prior work has utilized\nvarious data compression tools such as quantization and sparsification to\nreduce the overhead. In this paper, we propose a predictive coding based\ncompression scheme for federated learning. The scheme has shared prediction\nfunctions among all devices and allows each worker to transmit a compressed\nresidual vector derived from the reference. In each communication round, we\nselect the predictor and quantizer based on the rate-distortion cost, and\nfurther reduce the redundancy with entropy coding. Extensive simulations reveal\nthat the communication cost can be reduced up to 99% with even better learning\nperformance when compared with other baseline methods.", "time": "2022-01-09T01:20:06Z", "link": "http://arxiv.org/abs/2108.00918v2", "id": "2108.00918v2", "title": "Communication-Efficient Federated Learning via Predictive Coding"}
{"author": "Jessica Shi, Laxman Dhulipala, David Eisenstat, Jakub ÅÄcki, Vahab Mirrokni", "abstract": "Graph clustering and community detection are central problems in modern data\nmining. The increasing need for analyzing billion-scale data calls for faster\nand more scalable algorithms for these problems. There are certain trade-offs\nbetween the quality and speed of such clustering algorithms. In this paper, we\ndesign scalable algorithms that achieve high quality when evaluated based on\nground truth. We develop a generalized sequential and shared-memory parallel\nframework based on the LambdaCC objective (introduced by Veldt et al.), which\nencompasses modularity and correlation clustering. Our framework consists of\nhighly-optimized implementations that scale to large data sets of billions of\nedges and that obtain high-quality clusters compared to ground-truth data, on\nboth unweighted and weighted graphs. Our empirical evaluation shows that this\nframework improves the state-of-the-art trade-offs between speed and quality of\nscalable community detection. For example, on a 30-core machine with two-way\nhyper-threading, our implementations achieve orders of magnitude speedups over\nother correlation clustering baselines, and up to 28.44x speedups over our own\nsequential baselines while maintaining or improving quality.", "time": "2021-07-27T04:33:37Z", "link": "http://arxiv.org/abs/2108.01731v1", "id": "2108.01731v1", "title": "Scalable Community Detection via Parallel Correlation Clustering"}
{"author": "Siddharth Samsi, Matthew L Weiss, David Bestor, Baolin Li, Michael Jones, Albert Reuther, Daniel Edelman, William Arcand, Chansup Byun, John Holodnack, Matthew Hubbell, Jeremy Kepner, Anna Klein, Joseph McDonald, Adam Michaleas, Peter Michaleas, Lauren Milechin, Julia Mullen, Charles Yee, Benjamin Price, Andrew Prout, Antonio Rosa, Allan Vanterpool, Lindsey McEvoy, Anson Cheng, Devesh Tiwari, Vijay Gadepally", "abstract": "Artificial intelligence (AI) and Machine learning (ML) workloads are an\nincreasingly larger share of the compute workloads in traditional\nHigh-Performance Computing (HPC) centers and commercial cloud systems. This has\nled to changes in deployment approaches of HPC clusters and the commercial\ncloud, as well as a new focus on approaches to optimized resource usage,\nallocations and deployment of new AI frame- works, and capabilities such as\nJupyter notebooks to enable rapid prototyping and deployment. With these\nchanges, there is a need to better understand cluster/datacenter operations\nwith the goal of developing improved scheduling policies, identifying\ninefficiencies in resource utilization, energy/power consumption, failure\nprediction, and identifying policy violations. In this paper we introduce the\nMIT Supercloud Dataset which aims to foster innovative AI/ML approaches to the\nanalysis of large scale HPC and datacenter/cloud operations. We provide\ndetailed monitoring logs from the MIT Supercloud system, which include CPU and\nGPU usage by jobs, memory usage, file system logs, and physical monitoring\ndata. This paper discusses the details of the dataset, collection methodology,\ndata availability, and discusses potential challenge problems being developed\nusing this data. Datasets and future challenge announcements will be available\nvia https://dcc.mit.edu.", "time": "2021-08-04T13:06:17Z", "link": "http://arxiv.org/abs/2108.02037v1", "id": "2108.02037v1", "title": "The MIT Supercloud Dataset"}
{"author": "Zhengda Bian, Shenggui Li, Wei Wang, Yang You", "abstract": "Efficient GPU resource scheduling is essential to maximize resource\nutilization and save training costs for the increasing amount of deep learning\nworkloads in shared GPU clusters. Existing GPU schedulers largely rely on\nstatic policies to leverage the performance characteristics of deep learning\njobs. However, they can hardly reach optimal efficiency due to the lack of\nelasticity. To address the problem, we propose ONES, an ONline Evolutionary\nScheduler for elastic batch size orchestration. ONES automatically manages the\nelasticity of each job based on the training batch size, so as to maximize GPU\nutilization and improve scheduling efficiency. It determines the batch size for\neach job through an online evolutionary search that can continuously optimize\nthe scheduling decisions. We evaluate the effectiveness of ONES with 64 GPUs on\nTACC's Longhorn supercomputers. The results show that ONES can outperform the\nprior deep learning schedulers with a significantly shorter average job\ncompletion time.", "time": "2021-08-08T14:20:05Z", "link": "http://arxiv.org/abs/2108.03645v1", "id": "2108.03645v1", "title": "Online Evolutionary Batch Size Orchestration for Scheduling Deep\n  Learning Workloads in GPU Clusters"}
{"author": "Mao V. Ngo, Tie Luo, Tony Q. S. Quek", "abstract": "The advances in deep neural networks (DNN) have significantly enhanced\nreal-time detection of anomalous data in IoT applications. However, the\ncomplexity-accuracy-delay dilemma persists: complex DNN models offer higher\naccuracy, but typical IoT devices can barely afford the computation load, and\nthe remedy of offloading the load to the cloud incurs long delay. In this\npaper, we address this challenge by proposing an adaptive anomaly detection\nscheme with hierarchical edge computing (HEC). Specifically, we first construct\nmultiple anomaly detection DNN models with increasing complexity, and associate\neach of them to a corresponding HEC layer. Then, we design an adaptive model\nselection scheme that is formulated as a contextual-bandit problem and solved\nby using a reinforcement learning policy network. We also incorporate a\nparallelism policy training method to accelerate the training process by taking\nadvantage of distributed models. We build an HEC testbed using real IoT\ndevices, implement and evaluate our contextual-bandit approach with both\nunivariate and multivariate IoT datasets. In comparison with both baseline and\nstate-of-the-art schemes, our adaptive approach strikes the best accuracy-delay\ntradeoff on the univariate dataset, and achieves the best accuracy and F1-score\non the multivariate dataset with only negligibly longer delay than the best\n(but inflexible) scheme.", "time": "2021-08-09T08:45:47Z", "link": "http://arxiv.org/abs/2108.03872v1", "id": "2108.03872v1", "title": "Adaptive Anomaly Detection for Internet of Things in Hierarchical Edge\n  Computing: A Contextual-Bandit Approach"}
{"author": "Daniel Rosendo, Alexandru Costan, Gabriel Antoniu, Matthieu Simonin, Jean-Christophe Lombardo, Alexis Joly, Patrick Valduriez", "abstract": "In more and more application areas, we are witnessing the emergence of\ncomplex workflows that combine computing, analytics and learning. They often\nrequire a hybrid execution infrastructure with IoT devices interconnected to\ncloud/HPC systems (aka Computing Continuum). Such workflows are subject to\ncomplex constraints and requirements in terms of performance, resource usage,\nenergy consumption and financial costs. This makes it challenging to optimize\ntheir configuration and deployment. We propose a methodology to support the\noptimization of real-life applications on the Edge-to-Cloud Continuum. We\nimplement it as an extension of E2Clab, a previously proposed framework\nsupporting the complete experimental cycle across the Edge-to-Cloud Continuum.\nOur approach relies on a rigorous analysis of possible configurations in a\ncontrolled testbed environment to understand their behaviour and related\nperformance trade-offs. We illustrate our methodology by optimizing Pl@ntNet, a\nworld-wide plant identification application. Our methodology can be generalized\nto other applications in the Edge-to-Cloud Continuum.", "time": "2021-08-04T07:35:14Z", "link": "http://arxiv.org/abs/2108.04033v1", "id": "2108.04033v1", "title": "Reproducible Performance Optimization of Complex Applications on the\n  Edge-to-Cloud Continuum"}
{"author": "Hyejun Jeong, Joonyong Hwang, Tai Myung Chung", "abstract": "Federated Learning is a distributed machine learning framework designed for\ndata privacy preservation i.e., local data remain private throughout the entire\ntraining and testing procedure. Federated Learning is gaining popularity\nbecause it allows one to use machine learning techniques while preserving\nprivacy. However, it inherits the vulnerabilities and susceptibilities raised\nin deep learning techniques. For instance, Federated Learning is particularly\nvulnerable to data poisoning attacks that may deteriorate its performance and\nintegrity due to its distributed nature and inaccessibility to the raw data. In\naddition, it is extremely difficult to correctly identify malicious clients due\nto the non-Independently and/or Identically Distributed (non-IID) data. The\nreal-world data can be complex and diverse, making them hardly distinguishable\nfrom the malicious data without direct access to the raw data. Prior research\nhas focused on detecting malicious clients while treating only the clients\nhaving IID data as benign. In this study, we propose a method that detects and\nclassifies anomalous clients from benign clients when benign ones have non-IID\ndata. Our proposed method leverages feature dimension reduction, dynamic\nclustering, and cosine similarity-based clipping. The experimental results\nvalidates that our proposed method not only classifies the malicious clients\nbut also alleviates their negative influences from the entire procedure. Our\nfindings may be used in future studies to effectively eliminate anomalous\nclients when building a model with diverse data.", "time": "2022-12-01T08:22:00Z", "link": "http://arxiv.org/abs/2108.04551v4", "id": "2108.04551v4", "title": "ABC-FL: Anomalous and Benign client Classification in Federated Learning"}
{"author": "Srikanth Chandar, Pravin Chandran, Raghavendra Bhat, Avinash Chakravarthi", "abstract": "Federated Learning (FL) solves many of this decade's concerns regarding data\nprivacy and computation challenges. FL ensures no data leaves its source as the\nmodel is trained at where the data resides. However, FL comes with its own set\nof challenges. The communication of model weight updates in this distributed\nenvironment comes with significant network bandwidth costs. In this context, we\npropose a mechanism of compressing the weight updates using Autoencoders (AE),\nwhich learn the data features of the weight updates and subsequently perform\ncompression. The encoder is set up on each of the nodes where the training is\nperformed while the decoder is set up on the node where the weights are\naggregated. This setup achieves compression through the encoder and recreates\nthe weights at the end of every communication round using the decoder. This\npaper shows that the dynamic and orthogonal AE based weight compression\ntechnique could serve as an advantageous alternative (or an add-on) in a large\nscale FL, as it not only achieves compression ratios ranging from 500x to 1720x\nand beyond, but can also be modified based on the accuracy requirements,\ncomputational capacity, and other requirements of the given FL setup.", "time": "2021-08-12T11:27:11Z", "link": "http://arxiv.org/abs/2108.05670v1", "id": "2108.05670v1", "title": "Communication Optimization in Large Scale Federated Learning using\n  Autoencoder Compressed Weight Updates"}
{"author": "Weiming Zhuang, Xin Gan, Yonggang Wen, Shuai Zhang, Shuai Yi", "abstract": "Unsupervised representation learning has achieved outstanding performances\nusing centralized data available on the Internet. However, the increasing\nawareness of privacy protection limits sharing of decentralized unlabeled image\ndata that grows explosively in multiple parties (e.g., mobile phones and\ncameras). As such, a natural problem is how to leverage these data to learn\nvisual representations for downstream tasks while preserving data privacy. To\naddress this problem, we propose a novel federated unsupervised learning\nframework, FedU. In this framework, each party trains models from unlabeled\ndata independently using contrastive learning with an online network and a\ntarget network. Then, a central server aggregates trained models and updates\nclients' models with the aggregated model. It preserves data privacy as each\nparty only has access to its raw data. Decentralized data among multiple\nparties are normally non-independent and identically distributed (non-IID),\nleading to performance degradation. To tackle this challenge, we propose two\nsimple but effective methods: 1) We design the communication protocol to upload\nonly the encoders of online networks for server aggregation and update them\nwith the aggregated encoder; 2) We introduce a new module to dynamically decide\nhow to update predictors based on the divergence caused by non-IID. The\npredictor is the other component of the online network. Extensive experiments\nand ablations demonstrate the effectiveness and significance of FedU. It\noutperforms training with only one party by over 5% and other methods by over\n14% in linear and semi-supervised evaluation on non-IID data.", "time": "2021-08-14T08:34:11Z", "link": "http://arxiv.org/abs/2108.06492v1", "id": "2108.06492v1", "title": "Collaborative Unsupervised Visual Representation Learning from\n  Decentralized Data"}
{"author": "Weiming Zhuang, Yonggang Wen, Shuai Zhang", "abstract": "Person re-identification (ReID) aims to re-identify a person from\nnon-overlapping camera views. Since person ReID data contains sensitive\npersonal information, researchers have adopted federated learning, an emerging\ndistributed training method, to mitigate the privacy leakage risks. However,\nexisting studies rely on data labels that are laborious and time-consuming to\nobtain. We present FedUReID, a federated unsupervised person ReID system to\nlearn person ReID models without any labels while preserving privacy. FedUReID\nenables in-situ model training on edges with unlabeled data. A cloud server\naggregates models from edges instead of centralizing raw data to preserve data\nprivacy. Moreover, to tackle the problem that edges vary in data volumes and\ndistributions, we personalize training in edges with joint optimization of\ncloud and edge. Specifically, we propose personalized epoch to reassign\ncomputation throughout training, personalized clustering to iteratively predict\nsuitable labels for unlabeled data, and personalized update to adapt the server\naggregated model to each edge. Extensive experiments on eight person ReID\ndatasets demonstrate that FedUReID not only achieves higher accuracy but also\nreduces computation cost by 29%. Our FedUReID system with the joint\noptimization will shed light on implementing federated learning to more\nmultimedia tasks without data labels.", "time": "2021-08-14T08:35:55Z", "link": "http://arxiv.org/abs/2108.06493v1", "id": "2108.06493v1", "title": "Joint Optimization in Edge-Cloud Continuum for Federated Unsupervised\n  Person Re-identification"}
{"author": "Cengguang Zhang, Junxue Zhang, Di Chai, Kai Chen", "abstract": "Vertical federated learning (VFL) leverages various privacy-preserving\nalgorithms, e.g., homomorphic encryption or secret sharing based SecureBoost,\nto ensure data privacy. However, these algorithms all require a semi-honest\nsecure definition, which raises concerns in real-world applications. In this\npaper, we present Aegis, a trusted, automatic, and accurate verification\nframework to verify the security of VFL jobs. Aegis is separated from local\nparties to ensure the security of the framework. Furthermore, it automatically\nadapts to evolving VFL algorithms by defining the VFL job as a finite state\nmachine to uniformly verify different algorithms and reproduce the entire job\nto provide more accurate verification. We implement and evaluate Aegis with\ndifferent threat models on financial and medical datasets. Evaluation results\nshow that: 1) Aegis can detect 95% threat models, and 2) it provides\nfine-grained verification results within 84% of the total VFL job time.", "time": "2021-08-23T02:50:09Z", "link": "http://arxiv.org/abs/2108.06958v2", "id": "2108.06958v2", "title": "Aegis: A Trusted, Automatic and Accurate Verification Framework for\n  Vertical Federated Learning"}
{"author": "Tharindu B. Adikari, Stark C. Draper", "abstract": "An increasing bottleneck in decentralized optimization is communication.\nBigger models and growing datasets mean that decentralization of computation is\nimportant and that the amount of information exchanged is quickly growing.\nWhile compression techniques have been introduced to cope with the latter, none\nhas considered leveraging the temporal correlations that exist in consecutive\nvector updates. An important example is distributed momentum-SGD where temporal\ncorrelation is enhanced by the low-pass-filtering effect of applying momentum.\nIn this paper we design and analyze compression methods that exploit temporal\ncorrelation in systems both with and without error-feedback. Experiments with\nthe ImageNet dataset demonstrate that our proposed methods offer significant\nreduction in the rate of communication at only a negligible increase in\ncomputation complexity. We further analyze the convergence of SGD when\ncompression is applied with error-feedback. In the literature, convergence\nguarantees are developed only for compressors that provide error-bounds\npoint-wise, i.e., for each input to the compressor. In contrast, many important\ncodes (e.g. rate-distortion codes) provide error-bounds only in expectation and\nthus provide a more general guarantee. In this paper we prove the convergence\nof SGD under an expected error assumption by establishing a bound for the\nminimum gradient norm.", "time": "2021-08-17T18:04:06Z", "link": "http://arxiv.org/abs/2108.07827v1", "id": "2108.07827v1", "title": "Compressing gradients by exploiting temporal correlation in momentum-SGD"}
{"author": "Junyu Luo, Jianlei Yang, Xucheng Ye, Xin Guo, Weisheng Zhao", "abstract": "Federated learning aims to protect users' privacy while performing data\nanalysis from different participants. However, it is challenging to guarantee\nthe training efficiency on heterogeneous systems due to the various\ncomputational capabilities and communication bottlenecks. In this work, we\npropose FedSkel to enable computation-efficient and communication-efficient\nfederated learning on edge devices by only updating the model's essential\nparts, named skeleton networks. FedSkel is evaluated on real edge devices with\nimbalanced datasets. Experimental results show that it could achieve up to\n5.52$\\times$ speedups for CONV layers' back-propagation, 1.82$\\times$ speedups\nfor the whole training process, and reduce 64.8% communication cost, with\nnegligible accuracy loss.", "time": "2021-08-20T09:21:58Z", "link": "http://arxiv.org/abs/2108.09081v1", "id": "2108.09081v1", "title": "FedSkel: Efficient Federated Learning on Heterogeneous Systems with\n  Skeleton Gradients Update"}
{"author": "Guodong Long, Tao Shen, Yue Tan, Leah Gerrard, Allison Clarke, Jing Jiang", "abstract": "Privacy protection is an ethical issue with broad concern in Artificial\nIntelligence (AI). Federated learning is a new machine learning paradigm to\nlearn a shared model across users or organisations without direct access to the\ndata. It has great potential to be the next-general AI model training framework\nthat offers privacy protection and therefore has broad implications for the\nfuture of digital health and healthcare informatics. Implementing an open\ninnovation framework in the healthcare industry, namely open health, is to\nenhance innovation and creative capability of health-related organisations by\nbuilding a next-generation collaborative framework with partner organisations\nand the research community. In particular, this game-changing collaborative\nframework offers knowledge sharing from diverse data with a privacy-preserving.\nThis chapter will discuss how federated learning can enable the development of\nan open health ecosystem with the support of AI. Existing challenges and\nsolutions for federated learning will be discussed.", "time": "2021-08-24T14:08:55Z", "link": "http://arxiv.org/abs/2108.10761v1", "id": "2108.10761v1", "title": "Federated Learning for Privacy-Preserving Open Innovation Future on\n  Digital Health"}
{"author": "Sasindu Wijeratne, Sandaruwan Jayaweera, Mahesh Dananjaya, Ajith Pasqual", "abstract": "Convolutional Neural Networks (CNNs) are widely used in deep learning\napplications, e.g. visual systems, robotics etc. However, existing software\nsolutions are not efficient. Therefore, many hardware accelerators have been\nproposed optimizing performance, power and resource utilization of the\nimplementation. Amongst existing solutions, Field Programmable Gate Array\n(FPGA) based architecture provides better cost-energy-performance trade-offs as\nwell as scalability and minimizing development time. In this paper, we present\na model-independent reconfigurable co-processing architecture to accelerate\nCNNs. Our architecture consists of parallel Multiply and Accumulate (MAC) units\nwith caching techniques and interconnection networks to exploit maximum data\nparallelism. In contrast to existing solutions, we introduce limited precision\n32 bit Q-format fixed point quantization for arithmetic representations and\noperations. As a result, our architecture achieved significant reduction in\nresource utilization with competitive accuracy. Furthermore, we developed an\nassembly-type microinstructions to access the co-processing fabric to manage\nlayer-wise parallelism, thereby making re-use of limited resources. Finally, we\nhave tested our architecture up to 9x9 kernel size on Xilinx Virtex 7 FPGA,\nachieving a throughput of up to 226.2 GOp/S for 3x3 kernel size.", "time": "2021-08-21T09:50:54Z", "link": "http://arxiv.org/abs/2109.03040v1", "id": "2109.03040v1", "title": "Reconfigurable co-processor architecture with limited numerical\n  precision to accelerate deep convolutional neural networks"}
{"author": "Muhammad Fahad Saleem", "abstract": "Machine learning algorithms have enabled computers to predict things by\nlearning from previous data. The data storage and processing power are\nincreasing rapidly, thus increasing machine learning and Artificial\nintelligence applications. Much of the work is done to improve the accuracy of\nthe models built in the past, with little research done to determine the\ncomputational costs of machine learning acquisitions. In this paper, I will\nproceed with this later research work and will make a performance comparison of\nmulti-threaded machine learning clustering algorithms. I will be working on\nLinear Regression, Random Forest, and K-Nearest Neighbors to determine the\nperformance characteristics of the algorithms as well as the computation costs\nto the obtained results. I will be benchmarking system hardware performance by\nrunning these multi-threaded algorithms to train and test the models on a\ndataset to note the differences in performance matrices of the algorithms. In\nthe end, I will state the best performing algorithms concerning the performance\nefficiency of these algorithms on my system.", "time": "2021-09-11T13:26:58Z", "link": "http://arxiv.org/abs/2109.05276v1", "id": "2109.05276v1", "title": "Benchmarking Processor Performance by Multi-Threaded Machine Learning\n  Algorithms"}
{"author": "Jiayi Xu, Hanqi Guo, Han-Wei Shen, Mukund Raj, Skylar W. Wurster, Tom Peterka", "abstract": "We explore an online reinforcement learning (RL) paradigm to dynamically\noptimize parallel particle tracing performance in distributed-memory systems.\nOur method combines three novel components: (1) a work donation algorithm, (2)\na high-order workload estimation model, and (3) a communication cost model.\nFirst, we design an RL-based work donation algorithm. Our algorithm monitors\nworkloads of processes and creates RL agents to donate data blocks and\nparticles from high-workload processes to low-workload processes to minimize\nprogram execution time. The agents learn the donation strategy on the fly based\non reward and cost functions designed to consider processes' workload changes\nand data transfer costs of donation actions. Second, we propose a workload\nestimation model, helping RL agents estimate the workload distribution of\nprocesses in future computations. Third, we design a communication cost model\nthat considers both block and particle data exchange costs, helping RL agents\nmake effective decisions with minimized communication costs. We demonstrate\nthat our algorithm adapts to different flow behaviors in large-scale fluid\ndynamics, ocean, and weather simulation data. Our algorithm improves parallel\nparticle tracing performance in terms of parallel efficiency, load balance, and\ncosts of I/O and communication for evaluations with up to 16,384 processors.", "time": "2022-02-01T04:15:45Z", "link": "http://arxiv.org/abs/2109.05679v2", "id": "2109.05679v2", "title": "Reinforcement Learning for Load-balanced Parallel Particle Tracing"}
{"author": "Derssie Mebratu, Niranjan Hasabnis, Pietro Mercati, Gaurit Sharma, Shamima Najnin", "abstract": "Modern deep learning (DL) applications are built using DL libraries and\nframeworks such as TensorFlow and PyTorch. These frameworks have complex\nparameters and tuning them to obtain good training and inference performance is\nchallenging for typical users, such as DL developers and data scientists.\nManual tuning requires deep knowledge of the user-controllable parameters of DL\nframeworks as well as the underlying hardware. It is a slow and tedious\nprocess, and it typically delivers sub-optimal solutions.\n  In this paper, we treat the problem of tuning parameters of DL frameworks to\nimprove training and inference performance as a black-box optimization problem.\nWe then investigate applicability and effectiveness of Bayesian optimization\n(BO), genetic algorithm (GA), and Nelder-Mead simplex (NMS) to tune the\nparameters of TensorFlow's CPU backend. While prior work has already\ninvestigated the use of Nelder-Mead simplex for a similar problem, it does not\nprovide insights into the applicability of other more popular algorithms.\nTowards that end, we provide a systematic comparative analysis of all three\nalgorithms in tuning TensorFlow's CPU backend on a variety of DL models. Our\nfindings reveal that Bayesian optimization performs the best on the majority of\nmodels. There are, however, cases where it does not deliver the best results.", "time": "2021-09-13T19:10:23Z", "link": "http://arxiv.org/abs/2109.06266v1", "id": "2109.06266v1", "title": "Automatic Tuning of Tensorflow's CPU Backend using Gradient-Free\n  Optimization Algorithms"}
{"author": "Haya Elayan, Moayad Aloqaily, Fakhri Karray, Mohsen Guizani", "abstract": "Pandemics and natural disasters over the years have changed the behavior of\npeople, which has had a tremendous impact on all life aspects. With the\ntechnologies available in each era, governments, organizations, and companies\nhave used these technologies to track, control, and influence the behavior of\nindividuals for a benefit. Nowadays, the use of the Internet of Things (IoT),\ncloud computing, and artificial intelligence (AI) have made it easier to track\nand change the behavior of users through changing IoT behavior. This article\nintroduces and discusses the concept of the Internet of Behavior (IoB) and its\nintegration with Explainable AI (XAI) techniques to provide trusted and evident\nexperience in the process of changing IoT behavior to ultimately improving\nusers' behavior. Therefore, a system based on IoB and XAI has been proposed in\na use case scenario of electrical power consumption that aims to influence user\nconsuming behavior to reduce power consumption and cost. The scenario results\nshowed a decrease of 522.2 kW of active power when compared to original\nconsumption over a 200-hours period. It also showed a total power cost saving\nof 95.04 Euro for the same period. Moreover, decreasing the global active power\nwill reduce the power intensity through the positive correlation.", "time": "2022-05-10T21:01:33Z", "link": "http://arxiv.org/abs/2109.07239v2", "id": "2109.07239v2", "title": "Internet of Behavior (IoB) and Explainable AI Systems for Influencing\n  IoT Behavior"}
{"author": "Haizhou Du, Xiaojie Feng, Qiao Xiang, Haoyu Liu", "abstract": "A fundamental issue for federated learning (FL) is how to achieve optimal\nmodel performance under highly dynamic communication environments. This issue\ncan be alleviated by the fact that modern edge devices usually can connect to\nthe edge FL server via multiple communication channels (e.g., 4G, LTE and 5G).\nHowever, having an edge device send copies of local models to the FL server\nalong multiple channels is redundant, time-consuming, and would waste resources\n(e.g., bandwidth, battery life and monetary cost). In this paper, motivated by\nthe layered coding techniques in video streaming, we propose a novel FL\nframework called layered gradient compression (LGC). Specifically, in LGC,\nlocal gradients from a device is coded into several layers and each layer is\nsent to the FL server along a different channel. The FL server aggregates the\nreceived layers of local gradients from devices to update the global model, and\nsends the result back to the devices. We prove the convergence of LGC, and\nformally define the problem of resource-efficient federated learning with LGC.\nWe then propose a learning based algorithm for each device to dynamically\nadjust its local computation (i.e., the number of local stochastic descent) and\ncommunication decisions (i.e.,the compression level of different layers and the\nlayer to channel mapping) in each iteration. Results from extensive experiments\nshow that using our algorithm, LGC significantly reduces the training time,\nimproves the resource utilization, while achieving a similar accuracy, compared\nwith well-known FL mechanisms.", "time": "2021-09-18T03:30:06Z", "link": "http://arxiv.org/abs/2109.08819v1", "id": "2109.08819v1", "title": "Toward Efficient Federated Learning in Multi-Channeled Mobile Edge\n  Network with Layerd Gradient Compression"}
{"author": "Eugenio Lomurno, Alberto Archetti, Lorenzo Cazzella, Stefano Samele, Leonardo Di Perna, Matteo Matteucci", "abstract": "Privacy regulation laws, such as GDPR, impose transparency and security as\ndesign pillars for data processing algorithms. In this context, federated\nlearning is one of the most influential frameworks for privacy-preserving\ndistributed machine learning, achieving astounding results in many natural\nlanguage processing and computer vision tasks. Several federated learning\nframeworks employ differential privacy to prevent private data leakage to\nunauthorized parties and malicious attackers. Many studies, however, highlight\nthe vulnerabilities of standard federated learning to poisoning and inference,\nthus raising concerns about potential risks for sensitive data. To address this\nissue, we present SGDE, a generative data exchange protocol that improves user\nsecurity and machine learning performance in a cross-silo federation. The core\nof SGDE is to share data generators with strong differential privacy guarantees\ntrained on private data instead of communicating explicit gradient information.\nThese generators synthesize an arbitrarily large amount of data that retain the\ndistinctive features of private samples but differ substantially. In this work,\nSGDE is tested in a cross-silo federated network on images and tabular\ndatasets, exploiting beta-variational autoencoders as data generators. From the\nresults, the inclusion of SGDE turns out to improve task accuracy and fairness,\nas well as resilience to the most influential attacks on federated learning.", "time": "2022-09-07T14:00:57Z", "link": "http://arxiv.org/abs/2109.12062v3", "id": "2109.12062v3", "title": "SGDE: Secure Generative Data Exchange for Cross-Silo Federated Learning"}
{"author": "Manish Shetty, Chetan Bansal, Suman Nath, Sean Bowles, Henry Wang, Ozgur Arman, Siamak Ahari", "abstract": "Crash localization, an important step in debugging crashes, is challenging\nwhen dealing with an extremely large number of diverse applications and\nplatforms and underlying root causes. Large-scale error reporting systems,\ne.g., Windows Error Reporting (WER), commonly rely on manually developed rules\nand heuristics to localize blamed frames causing the crashes. As new\napplications and features are routinely introduced and existing applications\nare run under new environments, developing new rules and maintaining existing\nones become extremely challenging. We propose a data-driven solution to address\nthe problem. We start with the first large-scale empirical study of 362K\ncrashes and their blamed methods reported to WER by tens of thousands of\napplications running in the field. The analysis provides valuable insights on\nwhere and how the crashes happen and what methods to blame for the crashes.\nThese insights enable us to develop DeepAnalyze, a novel multi-task sequence\nlabeling approach for identifying blamed frames in stack traces. We evaluate\nour model with over a million real-world crashes from four popular Microsoft\napplications and show that DeepAnalyze, trained with crashes from one set of\napplications, not only accurately localizes crashes of the same applications,\nbut also bootstraps crash localization for other applications with zero to very\nlittle additional training data.", "time": "2021-12-03T02:55:49Z", "link": "http://arxiv.org/abs/2109.14326v2", "id": "2109.14326v2", "title": "DeepAnalyze: Learning to Localize Crashes at Scale"}
{"author": "Fei Dai, Yawen Chen, Haibo Zhang, Zhiyi Huang", "abstract": "Fully Connected Neural Network (FCNN) is a class of Artificial Neural\nNetworks widely used in computer science and engineering, whereas the training\nprocess can take a long time with large datasets in existing many-core systems.\nOptical Network-on-Chip (ONoC), an emerging chip-scale optical interconnection\ntechnology, has great potential to accelerate the training of FCNN with low\ntransmission delay, low power consumption, and high throughput. However,\nexisting methods based on Electrical Network-on-Chip (ENoC) cannot fit in ONoC\nbecause of the unique properties of ONoC. In this paper, we propose a\nfine-grained parallel computing model for accelerating FCNN training on ONoC\nand derive the optimal number of cores for each execution stage with the\nobjective of minimizing the total amount of time to complete one epoch of FCNN\ntraining. To allocate the optimal number of cores for each execution stage, we\npresent three mapping strategies and compare their advantages and disadvantages\nin terms of hotspot level, memory requirement, and state transitions.\nSimulation results show that the average prediction error for the optimal\nnumber of cores in NN benchmarks is within 2.3%. We further carry out extensive\nsimulations which demonstrate that FCNN training time can be reduced by 22.28%\nand 4.91% on average using our proposed scheme, compared with traditional\nparallel computing methods that either allocate a fixed number of cores or\nallocate as many cores as possible, respectively. Compared with ENoC,\nsimulation results show that under batch sizes of 64 and 128, on average ONoC\ncan achieve 21.02% and 12.95% on reducing training time with 47.85% and 39.27%\non saving energy, respectively.", "time": "2021-09-30T06:54:55Z", "link": "http://arxiv.org/abs/2109.14878v1", "id": "2109.14878v1", "title": "Accelerating Fully Connected Neural Network on Optical Network-on-Chip\n  (ONoC)"}
{"author": "Zhen Chen, Meilu Zhu, Chen Yang, Yixuan Yuan", "abstract": "Nowadays, deep learning methods with large-scale datasets can produce\nclinically useful models for computer-aided diagnosis. However, the privacy and\nethical concerns are increasingly critical, which make it difficult to collect\nlarge quantities of data from multiple institutions. Federated Learning (FL)\nprovides a promising decentralized solution to train model collaboratively by\nexchanging client models instead of private data. However, the server\naggregation of existing FL methods is observed to degrade the model performance\nin real-world medical FL setting, which is termed as retrogress. To address\nthis problem, we propose a personalized retrogress-resilient framework to\nproduce a superior personalized model for each client. Specifically, we devise\na Progressive Fourier Aggregation (PFA) at the server to achieve more stable\nand effective global knowledge gathering by integrating client models from\nlow-frequency to high-frequency gradually. Moreover, with an introduced deputy\nmodel to receive the aggregated server model, we design a Deputy-Enhanced\nTransfer (DET) strategy at the client and conduct three steps of\nRecover-Exchange-Sublimate to ameliorate the personalized local model by\ntransferring the global knowledge smoothly. Extensive experiments on real-world\ndermoscopic FL dataset prove that our personalized retrogress-resilient\nframework outperforms state-of-the-art FL methods, as well as the\ngeneralization on an out-of-distribution cohort. The code and dataset are\navailable at https://github.com/CityU-AIM-Group/PRR-FL.", "time": "2021-10-01T13:24:29Z", "link": "http://arxiv.org/abs/2110.00394v1", "id": "2110.00394v1", "title": "Personalized Retrogress-Resilient Framework for Real-World Medical\n  Federated Learning"}
{"author": "Taige Zhao, Xiangyu Song, Jianxin Li, Wei Luo, Imran Razzak", "abstract": "In recent years, Graph Convolutional Networks (GCNs) have achieved great\nsuccess in learning from graph-structured data. With the growing tendency of\ngraph nodes and edges, GCN training by single processor cannot meet the demand\nfor time and memory, which led to a boom into distributed GCN training\nframeworks research. However, existing distributed GCN training frameworks\nrequire enormous communication costs between processors since multitudes of\ndependent nodes and edges information need to be collected and transmitted for\nGCN training from other processors. To address this issue, we propose a Graph\nAugmentation based Distributed GCN framework(GAD). In particular, GAD has two\nmain components, GAD-Partition and GAD-Optimizer. We first propose a graph\naugmentation-based partition (GAD-Partition) that can divide original graph\ninto augmented subgraphs to reduce communication by selecting and storing as\nfew significant nodes of other processors as possible while guaranteeing the\naccuracy of the training. In addition, we further design a subgraph\nvariance-based importance calculation formula and propose a novel weighted\nglobal consensus method, collectively referred to as GAD-Optimizer. This\noptimizer adaptively reduces the importance of subgraphs with large variances\nfor the purpose of reducing the effect of extra variance introduced by\nGAD-Partition on distributed GCN training. Extensive experiments on four\nlarge-scale real-world datasets demonstrate that our framework significantly\nreduces the communication overhead (50%), improves the convergence speed (2X)\nof distributed GCN training, and slight gain in accuracy (0.45%) based on\nminimal redundancy compared to the state-of-the-art methods.", "time": "2021-10-06T18:01:47Z", "link": "http://arxiv.org/abs/2110.02987v1", "id": "2110.02987v1", "title": "Distributed Optimization of Graph Convolutional Network using Subgraph\n  Variance"}
{"author": "Kai Yue, Richeng Jin, Chau-Wai Wong, Huaiyu Dai", "abstract": "Federated learning allows collaborative workers to solve a machine learning\nproblem while preserving data privacy. Recent studies have tackled various\nchallenges in federated learning, but the joint optimization of communication\noverhead, learning reliability, and deployment efficiency is still an open\nproblem. To this end, we propose a new scheme named federated learning via\nplurality vote (FedVote). In each communication round of FedVote, workers\ntransmit binary or ternary weights to the server with low communication\noverhead. The model parameters are aggregated via weighted voting to enhance\nthe resilience against Byzantine attacks. When deployed for inference, the\nmodel with binary or ternary weights is resource-friendly to edge devices. We\nshow that our proposed method can reduce quantization error and converges\nfaster compared with the methods directly quantizing the model updates.", "time": "2022-12-10T03:36:41Z", "link": "http://arxiv.org/abs/2110.02998v3", "id": "2110.02998v3", "title": "Federated Learning via Plurality Vote"}
{"author": "Michael Kamp, Jonas Fischer, Jilles Vreeken", "abstract": "Federated learning allows multiple parties to collaboratively train a joint\nmodel without sharing local data. This enables applications of machine learning\nin settings of inherently distributed, undisclosable data such as in the\nmedical domain. In practice, joint training is usually achieved by aggregating\nlocal models, for which local training objectives have to be in expectation\nsimilar to the joint (global) objective. Often, however, local datasets are so\nsmall that local objectives differ greatly from the global objective, resulting\nin federated learning to fail. We propose a novel approach that intertwines\nmodel aggregations with permutations of local models. The permutations expose\neach local model to a daisy chain of local datasets resulting in more efficient\ntraining in data-sparse domains. This enables training on extremely small local\ndatasets, such as patient data across hospitals, while retaining the training\nefficiency and privacy benefits of federated learning.", "time": "2023-10-12T11:53:59Z", "link": "http://arxiv.org/abs/2110.03469v3", "id": "2110.03469v3", "title": "Federated Learning from Small Datasets"}
{"author": "Sannara Ek, FranÃ§ois Portet, Philippe Lalanda, German Vega", "abstract": "Pervasive computing promotes the installation of connected devices in our\nliving spaces in order to provide services. Two major developments have gained\nsignificant momentum recently: an advanced use of edge resources and the\nintegration of machine learning techniques for engineering applications. This\nevolution raises major challenges, in particular related to the appropriate\ndistribution of computing elements along an edge-to-cloud continuum. About\nthis, Federated Learning has been recently proposed for distributed model\ntraining in the edge. The principle of this approach is to aggregate models\nlearned on distributed clients in order to obtain a new, more general model.\nThe resulting model is then redistributed to clients for further training. To\ndate, the most popular federated learning algorithm uses coordinate-wise\naveraging of the model parameters for aggregation. However, it has been shown\nthat this method is not adapted in heterogeneous environments where data is not\nidentically and independently distributed (non-iid). This corresponds directly\nto some pervasive computing scenarios where heterogeneity of devices and users\nchallenges machine learning with the double objective of generalization and\npersonalization. In this paper, we propose a novel aggregation algorithm,\ntermed FedDist, which is able to modify its model architecture (here, deep\nneural network) by identifying dissimilarities between specific neurons amongst\nthe clients. This permits to account for clients' specificity without impairing\ngeneralization. Furthermore, we define a complete method to evaluate federated\nlearning in a realistic way taking generalization and personalization into\naccount.\n  Using this method, FedDist is extensively tested and compared with three\nstate-of-the-art federated learning algorithms on the pervasive domain of Human\nActivity Recognition with smartphones.", "time": "2021-10-19T19:43:28Z", "link": "http://arxiv.org/abs/2110.10223v1", "id": "2110.10223v1", "title": "A Federated Learning Aggregation Algorithm for Pervasive Computing:\n  Evaluation and Comparison"}
{"author": "Nawras Alnaasan, Arpan Jain, Aamir Shafi, Hari Subramoni, Dhabaleswar K Panda", "abstract": "Python has become a dominant programming language for emerging areas like\nMachine Learning (ML), Deep Learning (DL), and Data Science (DS). An attractive\nfeature of Python is that it provides easy-to-use programming interface while\nallowing library developers to enhance performance of their applications by\nharnessing the computing power offered by High Performance Computing (HPC)\nplatforms. Efficient communication is key to scaling applications on parallel\nsystems, which is typically enabled by the Message Passing Interface (MPI)\nstandard and compliant libraries on HPC hardware. mpi4py is a Python-based\ncommunication library that provides an MPI-like interface for Python\napplications allowing application developers to utilize parallel processing\nelements including GPUs. However, there is currently no benchmark suite to\nevaluate communication performance of mpi4py -- and Python MPI codes in general\n-- on modern HPC systems. In order to bridge this gap, we propose OMB-Py --\nPython extensions to the open-source OSU Micro-Benchmark (OMB) suite -- aimed\nto evaluate communication performance of MPI-based parallel applications in\nPython. To the best of our knowledge, OMB-Py is the first communication\nbenchmark suite for parallel Python applications. OMB-Py consists of a variety\nof point-to-point and collective communication benchmark tests that are\nimplemented for a range of popular Python libraries including NumPy, CuPy,\nNumba, and PyCUDA. Our evaluation reveals that mpi4py introduces a small\noverhead when compared to native MPI libraries. We plan to publicly release\nOMB-Py to benefit the Python HPC community.", "time": "2022-08-24T18:05:57Z", "link": "http://arxiv.org/abs/2110.10659v2", "id": "2110.10659v2", "title": "OMB-Py: Python Micro-Benchmarks for Evaluating Performance of MPI\n  Libraries on HPC Systems"}
{"author": "Mehrdad Kiamari, Bhaskar Krishnamachari", "abstract": "We consider the classical problem of scheduling task graphs corresponding to\ncomplex applications on distributed computing systems. A number of heuristics\nhave been previously proposed to optimize task scheduling with respect to\nmetrics such as makespan and throughput. However, they tend to be slow to run,\nparticularly for larger problem instances, limiting their applicability in more\ndynamic systems. Motivated by the goal of solving these problems more rapidly,\nwe propose, for the first time, a graph convolutional network-based scheduler\n(GCNScheduler). By carefully integrating an inter-task data dependency\nstructure with network settings into an input graph and feeding it to an\nappropriate GCN, the GCNScheduler can efficiently schedule tasks of complex\napplications for a given objective. We evaluate our scheme with baselines\nthrough simulations. We show that not only can our scheme quickly and\nefficiently learn from existing scheduling schemes, but also it can easily be\napplied to large-scale settings where current scheduling schemes fail to\nhandle. We show that it achieves better makespan than the classic HEFT\nalgorithm, and almost the same throughput as throughput-oriented HEFT\n(TP-HEFT), while providing several orders of magnitude faster scheduling times\nin both cases. For example, for makespan minimization, GCNScheduler schedules\n50-node task graphs in about 4 milliseconds while HEFT takes more than 1500\nseconds; and for throughput maximization, GCNScheduler schedules 100-node task\ngraphs in about 3.3 milliseconds, compared to about 6.9 seconds for TP-HEFT.", "time": "2021-10-22T01:54:10Z", "link": "http://arxiv.org/abs/2110.11552v1", "id": "2110.11552v1", "title": "GCNScheduler: Scheduling Distributed Computing Applications using Graph\n  Convolutional Networks"}
{"author": "Zhuotao Lian, Qinglin Yang, Qingkui Zeng, Chunhua Su", "abstract": "For data isolated islands and privacy issues, federated learning has been\nextensively invoking much interest since it allows clients to collaborate on\ntraining a global model using their local data without sharing any with a third\nparty. However, the existing federated learning frameworks always need\nsophisticated condition configurations (e.g., sophisticated driver\nconfiguration of standalone graphics card like NVIDIA, compile environment)\nthat bring much inconvenience for large-scale development and deployment. To\nfacilitate the deployment of federated learning and the implementation of\nrelated applications, we innovatively propose WebFed, a novel browser-based\nfederated learning framework that takes advantage of the browser's features\n(e.g., Cross-platform, JavaScript Programming Features) and enhances the\nprivacy protection via local differential privacy mechanism. Finally, We\nconduct experiments on heterogeneous devices to evaluate the performance of the\nproposed WebFed framework.", "time": "2021-10-22T08:18:41Z", "link": "http://arxiv.org/abs/2110.11646v1", "id": "2110.11646v1", "title": "WebFed: Cross-platform Federated Learning Framework Based on Web Browser\n  with Local Differential Privacy"}
{"author": "Lokesh Nagalapatti, Ramasuri Narayanam", "abstract": "The paradigm of Federated learning (FL) deals with multiple clients\nparticipating in collaborative training of a machine learning model under the\norchestration of a central server. In this setup, each client's data is private\nto itself and is not transferable to other clients or the server. Though FL\nparadigm has received significant interest recently from the research\ncommunity, the problem of selecting the relevant clients w.r.t. the central\nserver's learning objective is under-explored. We refer to these problems as\nFederated Relevant Client Selection (FRCS). Because the server doesn't have\nexplicit control over the nature of data possessed by each client, the problem\nof selecting relevant clients is significantly complex in FL settings. In this\npaper, we resolve important and related FRCS problems viz., selecting clients\nwith relevant data, detecting clients that possess data relevant to a\nparticular target label, and rectifying corrupted data samples of individual\nclients. We follow a principled approach to address the above FRCS problems and\ndevelop a new federated learning method using the Shapley value concept from\ncooperative game theory. Towards this end, we propose a cooperative game\ninvolving the gradients shared by the clients. Using this game, we compute\nShapley values of clients and then present Shapley value based Federated\nAveraging (S-FedAvg) algorithm that empowers the server to select relevant\nclients with high probability. S-FedAvg turns out to be critical in designing\nspecific algorithms to address the FRCS problems. We finally conduct a thorough\nempirical analysis on image classification and speech recognition tasks to show\nthe superior performance of S-FedAvg than the baselines in the context of\nsupervised federated learning settings.", "time": "2021-10-23T16:34:42Z", "link": "http://arxiv.org/abs/2110.12257v1", "id": "2110.12257v1", "title": "Game of Gradients: Mitigating Irrelevant Clients in Federated Learning"}
{"author": "Fanfei Meng, Lalita Jagadeesan, Marina Thottan", "abstract": "Microservice-based architectures enable different aspects of web applications\nto be created and updated independently, even after deployment. Associated\ntechnologies such as service mesh provide application-level fault resilience\nthrough attribute configurations that govern the behavior of request-response\nservice -- and the interactions among them -- in the presence of failures.\nWhile this provides tremendous flexibility, the configured values of these\nattributes -- and the relationships among them -- can significantly affect the\nperformance and fault resilience of the overall application. Furthermore, it is\nimpossible to determine the best and worst combinations of attribute values\nwith respect to fault resiliency via testing, due to the complexities of the\nunderlying distributed system and the many possible attribute value\ncombinations. In this paper, we present a model-based reinforcement learning\nworkflow towards service mesh fault resiliency. Our approach enables the\nprediction of the most significant fault resilience behaviors at a web\napplication-level, scratching from single service to aggregated multi-service\nmanagement with efficient agent collaborations.", "time": "2021-10-21T23:30:40Z", "link": "http://arxiv.org/abs/2110.13621v1", "id": "2110.13621v1", "title": "Model-based Reinforcement Learning for Service Mesh Fault Resiliency in\n  a Web Application-level"}
{"author": "Jingwei Sun, Ang Li, Louis DiValentin, Amin Hassanzadeh, Yiran Chen, Hai Li", "abstract": "Federated learning (FL) is a popular distributed learning framework that\ntrains a global model through iterative communications between a central server\nand edge devices. Recent works have demonstrated that FL is vulnerable to model\npoisoning attacks. Several server-based defense approaches (e.g. robust\naggregation), have been proposed to mitigate such attacks. However, we\nempirically show that under extremely strong attacks, these defensive methods\nfail to guarantee the robustness of FL. More importantly, we observe that as\nlong as the global model is polluted, the impact of attacks on the global model\nwill remain in subsequent rounds even if there are no subsequent attacks. In\nthis work, we propose a client-based defense, named White Blood Cell for\nFederated Learning (FL-WBC), which can mitigate model poisoning attacks that\nhave already polluted the global model. The key idea of FL-WBC is to identify\nthe parameter space where long-lasting attack effect on parameters resides and\nperturb that space during local training. Furthermore, we derive a certified\nrobustness guarantee against model poisoning attacks and a convergence\nguarantee to FedAvg after applying our FL-WBC. We conduct experiments on\nFasionMNIST and CIFAR10 to evaluate the defense against state-of-the-art model\npoisoning attacks. The results demonstrate that our method can effectively\nmitigate model poisoning attack impact on the global model within 5\ncommunication rounds with nearly no accuracy drop under both IID and Non-IID\nsettings. Our defense is also complementary to existing server-based robust\naggregation approaches and can further improve the robustness of FL under\nextremely strong attacks.", "time": "2021-10-26T17:13:35Z", "link": "http://arxiv.org/abs/2110.13864v1", "id": "2110.13864v1", "title": "FL-WBC: Enhancing Robustness against Model Poisoning Attacks in\n  Federated Learning from a Client Perspective"}
{"author": "Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Denny Zhou, Jure Leskovec, Dale Schuurmans", "abstract": "Knowledge graphs (KGs) capture knowledge in the form of head--relation--tail\ntriples and are a crucial component in many AI systems. There are two important\nreasoning tasks on KGs: (1) single-hop knowledge graph completion, which\ninvolves predicting individual links in the KG; and (2), multi-hop reasoning,\nwhere the goal is to predict which KG entities satisfy a given logical query.\nEmbedding-based methods solve both tasks by first computing an embedding for\neach entity and relation, then using them to form predictions. However,\nexisting scalable KG embedding frameworks only support single-hop knowledge\ngraph completion and cannot be applied to the more challenging multi-hop\nreasoning task. Here we present Scalable Multi-hOp REasoning (SMORE), the first\ngeneral framework for both single-hop and multi-hop reasoning in KGs. Using a\nsingle machine SMORE can perform multi-hop reasoning in Freebase KG (86M\nentities, 338M edges), which is 1,500x larger than previously considered KGs.\nThe key to SMORE's runtime performance is a novel bidirectional rejection\nsampling that achieves a square root reduction of the complexity of online\ntraining data generation. Furthermore, SMORE exploits asynchronous scheduling,\noverlapping CPU-based data sampling, GPU-based embedding computation, and\nfrequent CPU--GPU IO. SMORE increases throughput (i.e., training speed) over\nprior multi-hop KG frameworks by 2.2x with minimal GPU memory requirements (2GB\nfor training 400-dim embeddings on 86M-node Freebase) and achieves near linear\nspeed-up with the number of GPUs. Moreover, on the simpler single-hop knowledge\ngraph completion task SMORE achieves comparable or even better runtime\nperformance to state-of-the-art frameworks on both single GPU and multi-GPU\nsettings.", "time": "2021-11-01T22:50:51Z", "link": "http://arxiv.org/abs/2110.14890v2", "id": "2110.14890v2", "title": "SMORE: Knowledge Graph Completion and Multi-hop Reasoning in Massive\n  Knowledge Graphs"}
{"author": "Jinhui Yuan, Xinqi Li, Cheng Cheng, Juncheng Liu, Ran Guo, Shenghang Cai, Chi Yao, Fei Yang, Xiaodong Yi, Chuan Wu, Haoran Zhang, Jie Zhao", "abstract": "Deep learning frameworks such as TensorFlow and PyTorch provide a productive\ninterface for expressing and training a deep neural network (DNN) model on a\nsingle device or using data parallelism. Still, they may not be flexible or\nefficient enough in training emerging large models on distributed devices,\nwhich require more sophisticated parallelism beyond data parallelism. Plugins\nor wrappers have been developed to strengthen these frameworks for model or\npipeline parallelism, but they complicate the usage and implementation of\ndistributed deep learning. Aiming at a simple, neat redesign of distributed\ndeep learning frameworks for various parallelism paradigms, we present OneFlow,\na novel distributed training framework based on an SBP (split, broadcast and\npartial-value) abstraction and the actor model. SBP enables much easier\nprogramming of data parallelism and model parallelism than existing frameworks,\nand the actor model provides a succinct runtime mechanism to manage the complex\ndependencies imposed by resource constraints, data movement and computation in\ndistributed deep learning. We demonstrate the general applicability and\nefficiency of OneFlow for training various large DNN models with case studies\nand extensive experiments. The results show that OneFlow outperforms many\nwell-known customized libraries built on top of the state-of-the-art\nframeworks. The code of OneFlow is available at:\nhttps://github.com/Oneflow-Inc/oneflow.", "time": "2022-04-19T11:57:54Z", "link": "http://arxiv.org/abs/2110.15032v6", "id": "2110.15032v6", "title": "OneFlow: Redesign the Distributed Deep Learning Framework from Scratch"}
{"author": "Josep Lluis Berral, Oriol Aranda, Juan Luis Dominguez, Jordi Torres", "abstract": "Most research on novel techniques for 3D Medical Image Segmentation (MIS) is\ncurrently done using Deep Learning with GPU accelerators. The principal\nchallenge of such technique is that a single input can easily cope computing\nresources, and require prohibitive amounts of time to be processed.\nDistribution of deep learning and scalability over computing devices is an\nactual need for progressing on such research field. Conventional distribution\nof neural networks consist in data parallelism, where data is scattered over\nresources (e.g., GPUs) to parallelize the training of the model. However,\nexperiment parallelism is also an option, where different training processes\nare parallelized across resources. While the first option is much more common\non 3D image segmentation, the second provides a pipeline design with less\ndependence among parallelized processes, allowing overhead reduction and more\npotential scalability. In this work we present a design for distributed deep\nlearning training pipelines, focusing on multi-node and multi-GPU environments,\nwhere the two different distribution approaches are deployed and benchmarked.\nWe take as proof of concept the 3D U-Net architecture, using the MSD Brain\nTumor Segmentation dataset, a state-of-art problem in medical image\nsegmentation with high computing and space requirements. Using the BSC\nMareNostrum supercomputer as benchmarking environment, we use TensorFlow and\nRay as neural network training and experiment distribution platforms. We\nevaluate the experiment speed-up, showing the potential for scaling out on GPUs\nand nodes. Also comparing the different parallelism techniques, showing how\nexperiment distribution leverages better such resources through scaling.\nFinally, we provide the implementation of the design open to the community, and\nthe non-trivial steps and methodology for adapting and deploying a MIS case as\nthe here presented.", "time": "2021-10-29T16:11:25Z", "link": "http://arxiv.org/abs/2110.15884v1", "id": "2110.15884v1", "title": "Distributing Deep Learning Hyperparameter Tuning for 3D Medical Image\n  Segmentation"}
{"author": "Yuxin Shi, Han Yu, Cyril Leung", "abstract": "Recent advances in Federated Learning (FL) have brought large-scale\ncollaborative machine learning opportunities for massively distributed clients\nwith performance and data privacy guarantees. However, most current works focus\non the interest of the central controller in FL,and overlook the interests of\nthe FL clients. This may result in unfair treatment of clients that discourages\nthem from actively participating in the learning process and damages the\nsustainability of the FL ecosystem. Therefore, the topic of ensuring fairness\nin FL is attracting a great deal of research interest. In recent years, diverse\nFairness-Aware FL (FAFL) approaches have been proposed in an effort to achieve\nfairness in FL from different perspectives. However, there is no comprehensive\nsurvey that helps readers gain insight into this interdisciplinary field. This\npaper aims to provide such a survey. By examining the fundamental and\nsimplifying assumptions, as well as the notions of fairness adopted by existing\nliterature in this field, we propose a taxonomy of FAFL approaches covering\nmajor steps in FL, including client selection, optimization, contribution\nevaluation and incentive distribution. In addition, we discuss the main metrics\nfor experimentally evaluating the performance of FAFL approaches, and suggest\npromising future research directions towards FAFL.", "time": "2023-04-03T06:22:10Z", "link": "http://arxiv.org/abs/2111.01872v3", "id": "2111.01872v3", "title": "Towards Fairness-Aware Federated Learning"}
{"author": "Hang Qiu, Ioanna Vavelidou, Jian Li, Evgenya Pergament, Pete Warden, Sandeep Chinchali, Zain Asgar, Sachin Katti", "abstract": "Benefiting from expanding cloud infrastructure, deep neural networks (DNNs)\ntoday have increasingly high performance when trained in the cloud. Researchers\nspend months of effort competing for an extra few percentage points of model\naccuracy. However, when these models are actually deployed on edge devices in\npractice, very often, the performance can abruptly drop over 10% without\nobvious reasons. The key challenge is that there is not much visibility into ML\ninference execution on edge devices, and very little awareness of potential\nissues during the edge deployment process. We present ML-EXray, an end-to-end\nframework, which provides visibility into layer-level details of the ML\nexecution, and helps developers analyze and debug cloud-to-edge deployment\nissues. More often than not, the reason for sub-optimal edge performance does\nnot only lie in the model itself, but every operation throughout the data flow\nand the deployment process. Evaluations show that ML-EXray can effectively\ncatch deployment issues, such as pre-processing bugs, quantization issues,\nsuboptimal kernels, etc. Using ML-EXray, users need to write less than 15 lines\nof code to fully examine the edge deployment pipeline. Eradicating these\nissues, ML-EXray can correct model performance by up to 30%, pinpoint\nerror-prone layers, and guide users to optimize kernel execution latency by two\norders of magnitude. Code and APIs will be released as an open-source\nmulti-lingual instrumentation library and a Python deployment validation\nlibrary.", "time": "2021-11-08T19:29:44Z", "link": "http://arxiv.org/abs/2111.04779v1", "id": "2111.04779v1", "title": "ML-EXray: Visibility into ML Deployment on the Edge"}
{"author": "Daniel Nichols, Siddharth Singh, Shu-Huai Lin, Abhinav Bhatele", "abstract": "The field of deep learning has witnessed a remarkable shift towards extremely\ncompute- and memory-intensive neural networks. These newer larger models have\nenabled researchers to advance state-of-the-art tools across a variety of\nfields. This phenomenon has spurred the development of algorithms for\ndistributed training of neural networks over a larger number of hardware\naccelerators. In this paper, we discuss and compare current state-of-the-art\nframeworks for large scale distributed deep learning. First, we survey current\npractices in distributed learning and identify the different types of\nparallelism used. Then, we present empirical results comparing their\nperformance on large image and language training tasks. Additionally, we\naddress their statistical efficiency and memory consumption behavior. Based on\nour results, we discuss algorithmic and implementation portions of each\nframework which hinder performance.", "time": "2022-07-01T02:06:03Z", "link": "http://arxiv.org/abs/2111.04949v2", "id": "2111.04949v2", "title": "A Survey and Empirical Evaluation of Parallel Deep Learning Frameworks"}
{"author": "Can Karakus, Rahul Huilgol, Fei Wu, Anirudh Subramanian, Cade Daniel, Derya Cavdar, Teng Xu, Haohan Chen, Arash Rahnama, Luis Quintela", "abstract": "With deep learning models rapidly growing in size, systems-level solutions\nfor large-model training are required. We present Amazon SageMaker model\nparallelism, a software library that integrates with PyTorch, and enables easy\ntraining of large models using model parallelism and other memory-saving\nfeatures. In contrast to existing solutions, the implementation of the\nSageMaker library is much more generic and flexible, in that it can\nautomatically partition and run pipeline parallelism over arbitrary model\narchitectures with minimal code change, and also offers a general and\nextensible framework for tensor parallelism, which supports a wider range of\nuse cases, and is modular enough to be easily applied to new training scripts.\nThe library also preserves the native PyTorch user experience to a much larger\ndegree, supporting module re-use and dynamic graphs, while giving the user full\ncontrol over the details of the training step. We evaluate performance over\nGPT-3, RoBERTa, BERT, and neural collaborative filtering, and demonstrate\ncompetitive performance over existing solutions.", "time": "2021-11-10T22:30:21Z", "link": "http://arxiv.org/abs/2111.05972v1", "id": "2111.05972v1", "title": "Amazon SageMaker Model Parallelism: A General and Flexible Framework for\n  Large Model Training"}
{"author": "Yifan Gong, Geng Yuan, Zheng Zhan, Wei Niu, Zhengang Li, Pu Zhao, Yuxuan Cai, Sijia Liu, Bin Ren, Xue Lin, Xulong Tang, Yanzhi Wang", "abstract": "Weight pruning is an effective model compression technique to tackle the\nchallenges of achieving real-time deep neural network (DNN) inference on mobile\ndevices. However, prior pruning schemes have limited application scenarios due\nto accuracy degradation, difficulty in leveraging hardware acceleration, and/or\nrestriction on certain types of DNN layers. In this paper, we propose a\ngeneral, fine-grained structured pruning scheme and corresponding compiler\noptimizations that are applicable to any type of DNN layer while achieving high\naccuracy and hardware inference performance. With the flexibility of applying\ndifferent pruning schemes to different layers enabled by our compiler\noptimizations, we further probe into the new problem of determining the\nbest-suited pruning scheme considering the different acceleration and accuracy\nperformance of various pruning schemes. Two pruning scheme mapping methods, one\nis search-based and the other is rule-based, are proposed to automatically\nderive the best-suited pruning regularity and block size for each layer of any\ngiven DNN. Experimental results demonstrate that our pruning scheme mapping\nmethods, together with the general fine-grained structured pruning scheme,\noutperform the state-of-the-art DNN optimization framework with up to\n2.48$\\times$ and 1.73$\\times$ DNN inference acceleration on CIFAR-10 and\nImageNet dataset without accuracy loss.", "time": "2021-11-22T23:53:14Z", "link": "http://arxiv.org/abs/2111.11581v1", "id": "2111.11581v1", "title": "Automatic Mapping of the Best-Suited DNN Pruning Schemes for Real-Time\n  Mobile Acceleration"}
{"author": "Muhammad Akbar Husnoo, Adnan Anwar, Nasser Hosseinzadeh, Shama Naz Islam, Abdun Naser Mahmood, Robin Doss", "abstract": "With the proliferation of smart devices and revolutions in communications,\nelectrical distribution systems are gradually shifting from passive,\nmanually-operated and inflexible ones, to a massively interconnected\ncyber-physical smart grid to address the energy challenges of the future.\nHowever, the integration of several cutting-edge technologies has introduced\nseveral security and privacy vulnerabilities due to the large-scale complexity\nand resource limitations of deployments. Recent research trends have shown that\nFalse Data Injection (FDI) attacks are becoming one of the most malicious cyber\nthreats within the entire smart grid paradigm. Therefore, this paper presents a\ncomprehensive survey of the recent advances in FDI attacks within active\ndistribution systems and proposes a taxonomy to classify the FDI threats with\nrespect to smart grid targets. The related studies are contrasted and\nsummarized in terms of the attack methodologies and implications on the\nelectrical power distribution networks. Finally, we identify some research gaps\nand recommend a number of future research directions to guide and motivate\nprospective researchers.", "time": "2022-09-29T13:13:43Z", "link": "http://arxiv.org/abs/2111.14251v2", "id": "2111.14251v2", "title": "False Data Injection Threats in Active Distribution Systems: A\n  Comprehensive Survey"}
{"author": "Dezhong Yao, Wanning Pan, Michael J O'Neill, Yutong Dai, Yao Wan, Hai Jin, Lichao Sun", "abstract": "One underlying assumption of recent federated learning (FL) paradigms is that\nall local models usually share the same network architecture and size, which\nbecomes impractical for devices with different hardware resources. A scalable\nfederated learning framework should address the heterogeneity that clients have\ndifferent computing capacities and communication capabilities. To this end,\nthis paper proposes FedHM, a novel heterogeneous federated model compression\nframework, distributing the heterogeneous low-rank models to clients and then\naggregating them into a full-rank model. Our solution enables the training of\nheterogeneous models with varying computational complexities and aggregates\nthem into a single global model. Furthermore, FedHM significantly reduces the\ncommunication cost by using low-rank models. Extensive experimental results\ndemonstrate that FedHM is superior in the performance and robustness of models\nof different sizes, compared with state-of-the-art heterogeneous FL methods\nunder various FL settings. Additionally, the convergence guarantee of FL for\nheterogeneous devices is first theoretically analyzed.", "time": "2022-05-26T09:46:40Z", "link": "http://arxiv.org/abs/2111.14655v2", "id": "2111.14655v2", "title": "FedHM: Efficient Federated Learning for Heterogeneous Models via\n  Low-rank Factorization"}
{"author": "Ha Min Son, Moon Hyun Kim, Tai-Myoung Chung", "abstract": "Federated Learning is a widely adopted method to train neural networks over\ndistributed data. One main limitation is the performance degradation that\noccurs when data is heterogeneously distributed. While many works have\nattempted to address this problem, these methods under-perform because they are\nfounded on a limited understanding of neural networks. In this work, we verify\nthat only certain important layers in a neural network require regularization\nfor effective training. We additionally verify that Centered Kernel Alignment\n(CKA) most accurately calculates similarity between layers of neural networks\ntrained on different data. By applying CKA-based regularization to important\nlayers during training, we significantly improve performance in heterogeneous\nsettings. We present FedCKA: a simple framework that out-performs previous\nstate-of-the-art methods on various deep learning tasks while also improving\nefficiency and scalability.", "time": "2021-12-01T10:46:13Z", "link": "http://arxiv.org/abs/2112.00407v1", "id": "2112.00407v1", "title": "Compare Where It Matters: Using Layer-Wise Regularization To Improve\n  Federated Learning on Heterogeneous Data"}
{"author": "Zhe Qu, Rui Duan, Lixing Chen, Jie Xu, Zhuo Lu, Yao Liu", "abstract": "Federated Learning (FL) has been considered as an appealing framework to\ntackle data privacy issues of mobile devices compared to conventional Machine\nLearning (ML). Using Edge Servers (ESs) as intermediaries to perform model\naggregation in proximity can reduce the transmission overhead, and it enables\ngreat potentials in low-latency FL, where the hierarchical architecture of FL\n(HFL) has been attracted more attention. Designing a proper client selection\npolicy can significantly improve training performance, and it has been\nextensively used in FL studies. However, to the best of our knowledge, there\nare no studies focusing on HFL. In addition, client selection for HFL faces\nmore challenges than conventional FL, e.g., the time-varying connection of\nclient-ES pairs and the limited budget of the Network Operator (NO). In this\npaper, we investigate a client selection problem for HFL, where the NO learns\nthe number of successful participating clients to improve the training\nperformance (i.e., select as many clients in each round) as well as under the\nlimited budget on each ES. An online policy, called Context-aware Online Client\nSelection (COCS), is developed based on Contextual Combinatorial Multi-Armed\nBandit (CC-MAB). COCS observes the side-information (context) of local\ncomputing and transmission of client-ES pairs and makes client selection\ndecisions to maximize NO's utility given a limited budget. Theoretically, COCS\nachieves a sublinear regret compared to an Oracle policy on both strongly\nconvex and non-convex HFL. Simulation results also support the efficiency of\nthe proposed COCS policy on real-world datasets.", "time": "2021-12-03T16:15:21Z", "link": "http://arxiv.org/abs/2112.00925v2", "id": "2112.00925v2", "title": "Context-Aware Online Client Selection for Hierarchical Federated\n  Learning"}
{"author": "StefÃ¡n PÃ¡ll Sturluson, Samuel Trew, Luis MuÃ±oz-GonzÃ¡lez, Matei Grama, Jonathan Passerat-Palmbach, Daniel Rueckert, Amir Alansary", "abstract": "The robustness of federated learning (FL) is vital for the distributed\ntraining of an accurate global model that is shared among large number of\nclients. The collaborative learning framework by typically aggregating model\nupdates is vulnerable to model poisoning attacks from adversarial clients.\nSince the shared information between the global server and participants are\nonly limited to model parameters, it is challenging to detect bad model\nupdates. Moreover, real-world datasets are usually heterogeneous and not\nindependent and identically distributed (Non-IID) among participants, which\nmakes the design of such robust FL pipeline more difficult. In this work, we\npropose a novel robust aggregation method, Federated Robust Adaptive\nDistillation (FedRAD), to detect adversaries and robustly aggregate local\nmodels based on properties of the median statistic, and then performing an\nadapted version of ensemble Knowledge Distillation. We run extensive\nexperiments to evaluate the proposed method against recently published works.\nThe results show that FedRAD outperforms all other aggregators in the presence\nof adversaries, as well as in heterogeneous data distributions.", "time": "2021-12-02T16:50:57Z", "link": "http://arxiv.org/abs/2112.01405v1", "id": "2112.01405v1", "title": "FedRAD: Federated Robust Adaptive Distillation"}
{"author": "Yulong Ao, Zhihua Wu, Dianhai Yu, Weibao Gong, Zhiqing Kui, Minxu Zhang, Zilingfeng Ye, Liang Shen, Yanjun Ma, Tian Wu, Haifeng Wang, Wei Zeng, Chao Yang", "abstract": "Distributed training has become a pervasive and effective approach for\ntraining a large neural network (NN) model with processing massive data.\nHowever, it is very challenging to satisfy requirements from various NN models,\ndiverse computing resources, and their dynamic changes during a training job.\nIn this study, we design our distributed training framework in a systematic\nend-to-end view to provide the built-in adaptive ability for different\nscenarios, especially for industrial applications and production environments,\nby fully considering resource allocation, model partition, task placement, and\ndistributed execution. Based on the unified distributed graph and the unified\ncluster object, our adaptive framework is equipped with a global cost model and\na global planner, which can enable arbitrary parallelism, resource-aware\nplacement, multi-mode execution, fault-tolerant, and elastic distributed\ntraining. The experiments demonstrate that our framework can satisfy various\nrequirements from the diversity of applications and the heterogeneity of\nresources with highly competitive performance. The ERNIE language model with\n260 billion parameters is efficiently trained on thousands of AI processors\nwith 91.7% weak scalability. The throughput of the model from the recommender\nsystem by employing the heterogeneous pipeline asynchronous execution can be\nincreased up to 2.1 times and 3.3 times that of the GPU-only and CPU-only\ntraining respectively. Moreover, the fault-tolerant and elastic distributed\ntraining have been successfully applied to the online industrial applications,\nwhich give a reduction of 34.49% in the number of failed long-term training\njobs and an increase of 33.91% for the global scheduling efficiency in the\nproduction environment.", "time": "2021-12-06T03:00:32Z", "link": "http://arxiv.org/abs/2112.02752v1", "id": "2112.02752v1", "title": "End-to-end Adaptive Distributed Training on PaddlePaddle"}
{"author": "Xiao-Yang Liu, Zechu Li, Zhuoran Yang, Jiahao Zheng, Zhaoran Wang, Anwar Walid, Jian Guo, Michael I. Jordan", "abstract": "Deep reinforcement learning (DRL) has revolutionized learning and actuation\nin applications such as game playing and robotic control. The cost of data\ncollection, i.e., generating transitions from agent-environment interactions,\nremains a major challenge for wider DRL adoption in complex real-world\nproblems. Following a cloud-native paradigm to train DRL agents on a GPU cloud\nplatform is a promising solution. In this paper, we present a scalable and\nelastic library ElegantRL-podracer for cloud-native deep reinforcement\nlearning, which efficiently supports millions of GPU cores to carry out\nmassively parallel training at multiple levels. At a high-level,\nElegantRL-podracer employs a tournament-based ensemble scheme to orchestrate\nthe training process on hundreds or even thousands of GPUs, scheduling the\ninteractions between a leaderboard and a training pool with hundreds of pods.\nAt a low-level, each pod simulates agent-environment interactions in parallel\nby fully utilizing nearly 7,000 GPU CUDA cores in a single GPU. Our\nElegantRL-podracer library features high scalability, elasticity and\naccessibility by following the development principles of containerization,\nmicroservices and MLOps. Using an NVIDIA DGX SuperPOD cloud, we conduct\nextensive experiments on various tasks in locomotion and stock trading and show\nthat ElegantRL-podracer substantially outperforms RLlib. Our codes are\navailable on GitHub.", "time": "2022-04-04T01:58:52Z", "link": "http://arxiv.org/abs/2112.05923v2", "id": "2112.05923v2", "title": "ElegantRL-Podracer: Scalable and Elastic Library for Cloud-Native Deep\n  Reinforcement Learning"}
{"author": "Hunmin Lee, Yueyang Liu, Donghyun Kim, Yingshu Li", "abstract": "Non-IID dataset and heterogeneous environment of the local clients are\nregarded as a major issue in Federated Learning (FL), causing a downturn in the\nconvergence without achieving satisfactory performance. In this paper, we\npropose a novel Label-wise clustering algorithm that guarantees the\ntrainability among geographically dispersed heterogeneous local clients, by\nselecting only the local models trained with a dataset that approximates into\nuniformly distributed class labels, which is likely to obtain faster\nminimization of the loss and increment the accuracy among the FL network.\nThrough conducting experiments on the suggested six common non-IID scenarios,\nwe empirically show that the vanilla FL aggregation model is incapable of\ngaining robust convergence generating biased pre-trained local models and\ndrifting the local weights to mislead the trainability in the worst case.\nMoreover, we quantitatively estimate the expected performance of the local\nmodels before training, which offers a global server to select the optimal\nclients, saving additional computational costs. Ultimately, in order to gain\nresolution of the non-convergence in such non-IID situations, we design\nclustering algorithms based on local input class labels, accommodating the\ndiversity and assorting clients that could lead the overall system to attain\nthe swift convergence as global training continues. Our paper shows that\nproposed Label-wise clustering demonstrates prompt and robust convergence\ncompared to other FL algorithms when local training datasets are non-IID or\ncoexist with IID through multiple experiments.", "time": "2021-12-28T18:13:09Z", "link": "http://arxiv.org/abs/2112.14244v1", "id": "2112.14244v1", "title": "Robust Convergence in Federated Learning through Label-wise Clustering"}
{"author": "Erik Heiland, Peter Hillmann, Andreas Karcher", "abstract": "With increasing linkage within value chains, the IT systems of different\ncompanies are also being connected with each other. This enables the\nintegration of services within the movement of Industry 4.0 in order to improve\nthe quality and performance of the processes. Enterprise architecture models\nform the basis for this with a better buisness IT-alignment. However, the\nheterogeneity of the modeling frameworks and description languages makes a\nconcatenation considerably difficult, especially differences in syntax,\nsemantic and relations. Therefore, this paper presents a transformation engine\nto convert enterprise architecture models between several languages. We\ndeveloped the first generic translation approach that is free of specific\nmeta-modeling, which is flexible adaptable to arbitrary modeling languages. The\ntransformation process is defined by various pattern matching techniques using\na rule-based description language. It uses set theory and first-order logic for\nan intuitive description as a basis. The concept is practical evaluated using\nan example in the area of a large German IT-service provider. Anyhow, the\napproach is applicable between a wide range of enterprise architecture\nframeworks.", "time": "2021-08-15T11:10:42Z", "link": "http://arxiv.org/abs/2108.13169v1", "id": "2108.13169v1", "title": "Enterprise Architecture Model Transformation Engine"}
{"author": "Minas Chatzos, Terrence W. K. Mak, Pascal Van Hentenryck", "abstract": "This paper proposes a novel machine-learning approach for predicting AC-OPF\nsolutions that features a fast and scalable training. It is motivated by the\ntwo critical considerations: (1) the fact that topology optimization and the\nstochasticity induced by renewable energy sources may lead to fundamentally\ndifferent AC-OPF instances; and (2) the significant training time needed by\nexisting machine-learning approaches for predicting AC-OPF. The proposed\napproach is a 2-stage methodology that exploits a spatial decomposition of the\npower network that is viewed as a set of regions. The first stage learns to\npredict the flows and voltages on the buses and lines coupling the regions, and\nthe second stage trains, in parallel, the machine-learning models for each\nregion. Experimental results on the French transmission system (up to 6,700\nbuses and 9,000 lines) demonstrate the potential of the approach. Within a\nshort training time, the approach predicts AC-OPF solutions with very high\nfidelity and minor constraint violations, producing significant improvements\nover the state-of-the-art. The results also show that the predictions can seed\na load flow optimization to return a feasible solution within 0.03% of the\nAC-OPF objective, while reducing running times significantly.", "time": "2021-01-17T20:09:11Z", "link": "http://arxiv.org/abs/2101.06768v1", "id": "2101.06768v1", "title": "Spatial Network Decomposition for Fast and Scalable AC-OPF Learning"}
{"author": "Timothy Verstraeten, Pieter-Jan Daems, Eugenio Bargiacchi, Diederik M. Roijers, Pieter J. K. Libin, Jan Helsen", "abstract": "Wind farms are a crucial driver toward the generation of ecological and\nrenewable energy. Due to their rapid increase in capacity, contemporary wind\nfarms need to adhere to strict constraints on power output to ensure stability\nof the electricity grid. Specifically, a wind farm controller is required to\nmatch the farm's power production with a power demand imposed by the grid\noperator. This is a non-trivial optimization problem, as complex dependencies\nexist between the wind turbines. State-of-the-art wind farm control typically\nrelies on physics-based heuristics that fail to capture the full load spectrum\nthat defines a turbine's health status. When this is not taken into account,\nthe long-term viability of the farm's turbines is put at risk. Given the\ncomplex dependencies that determine a turbine's lifetime, learning a flexible\nand optimal control strategy requires a data-driven approach. However, as wind\nfarms are large-scale multi-agent systems, optimizing control strategies over\nthe full joint action space is intractable. We propose a new learning method\nfor wind farm control that leverages the sparse wind farm structure to\nfactorize the optimization problem. Using a Bayesian approach, based on\nmulti-agent Thompson sampling, we explore the factored joint action space for\nconfigurations that match the demand, while considering the lifetime of\nturbines. We apply our method to a grid-like wind farm layout, and evaluate\nconfigurations using a state-of-the-art wind flow simulator. Our results are\ncompetitive with a physics-based heuristic approach in terms of demand error,\nwhile, contrary to the heuristic, our method prolongs the lifetime of high-risk\nturbines.", "time": "2021-01-19T20:12:30Z", "link": "http://arxiv.org/abs/2101.07844v1", "id": "2101.07844v1", "title": "Scalable Optimization for Wind Farm Control using Coordination Graphs"}
{"author": "Wenlong Liao, Birgitte Bak-Jensen, Jayakrishnan Radhakrishna Pillai, Yuelong Wang, Yusen Wang", "abstract": "Deep neural networks have revolutionized many machine learning tasks in power\nsystems, ranging from pattern recognition to signal processing. The data in\nthese tasks is typically represented in Euclidean domains. Nevertheless, there\nis an increasing number of applications in power systems, where data are\ncollected from non-Euclidean domains and represented as graph-structured data\nwith high dimensional features and interdependency among nodes. The complexity\nof graph-structured data has brought significant challenges to the existing\ndeep neural networks defined in Euclidean domains. Recently, many publications\ngeneralizing deep neural networks for graph-structured data in power systems\nhave emerged. In this paper, a comprehensive overview of graph neural networks\n(GNNs) in power systems is proposed. Specifically, several classical paradigms\nof GNNs structures (e.g., graph convolutional networks) are summarized, and key\napplications in power systems, such as fault scenario application, time series\nprediction, power flow calculation, and data generation are reviewed in detail.\nFurthermore, main issues and some research trends about the applications of\nGNNs in power systems are discussed.", "time": "2021-06-12T11:38:48Z", "link": "http://arxiv.org/abs/2101.10025v2", "id": "2101.10025v2", "title": "A Review of Graph Neural Networks and Their Applications in Power\n  Systems"}
{"author": "Xin Chen, Guannan Qu, Yujie Tang, Steven Low, Na Li", "abstract": "With large-scale integration of renewable generation and distributed energy\nresources, modern power systems are confronted with new operational challenges,\nsuch as growing complexity, increasing uncertainty, and aggravating volatility.\nMeanwhile, more and more data are becoming available owing to the widespread\ndeployment of smart meters, smart sensors, and upgraded communication networks.\nAs a result, data-driven control techniques, especially reinforcement learning\n(RL), have attracted surging attention in recent years. This paper provides a\ncomprehensive review of various RL techniques and how they can be applied to\ndecision-making and control in power systems. In particular, we select three\nkey applications, i.e., frequency regulation, voltage control, and energy\nmanagement, as examples to illustrate RL-based models and solutions. We then\npresent the critical issues in the application of RL, i.e., safety, robustness,\nscalability, and data. Several potential future directions are discussed as\nwell.", "time": "2022-02-25T14:16:27Z", "link": "http://arxiv.org/abs/2102.01168v5", "id": "2102.01168v5", "title": "Reinforcement Learning for Selective Key Applications in Power Systems:\n  Recent Advances and Future Challenges"}
{"author": "Hrishikesh Dutta, Subir Biswas", "abstract": "This paper proposes a multi-agent reinforcement learning based medium access\nframework for wireless networks. The access problem is formulated as a Markov\nDecision Process (MDP), and solved using reinforcement learning with every\nnetwork node acting as a distributed learning agent. The solution components\nare developed step by step, starting from a single-node access scenario in\nwhich a node agent incrementally learns to control MAC layer packet loads for\nreining in self-collisions. The strategy is then scaled up for multi-node\nfully-connected scenarios by using more elaborate reward structures. It also\ndemonstrates preliminary feasibility for more general partially connected\ntopologies. It is shown that by learning to adjust MAC layer transmission\nprobabilities, the protocol is not only able to attain theoretical maximum\nthroughput at an optimal load, but unlike classical approaches, it can also\nretain that maximum throughput at higher loading conditions. Additionally, the\nmechanism is agnostic to heterogeneous loading while preserving that feature.\nIt is also shown that access priorities of the protocol across nodes can be\nparametrically adjusted. Finally, it is also shown that the online learning\nfeature of reinforcement learning is able to make the protocol adapt to\ntime-varying loading conditions.", "time": "2021-02-02T17:13:37Z", "link": "http://arxiv.org/abs/2102.01611v1", "id": "2102.01611v1", "title": "Towards Multi-agent Reinforcement Learning for Wireless Network Protocol\n  Synthesis"}
{"author": "Tengchan Zeng, Omid Semiari, Mingzhe Chen, Walid Saad, Mehdi Bennis", "abstract": "A new federated learning (FL) framework enabled by large-scale wireless\nconnectivity is proposed for designing the autonomous controller of connected\nand autonomous vehicles (CAVs). In this framework, the learning models used by\nthe controllers are collaboratively trained among a group of CAVs. To capture\nthe varying CAV participation in the FL training process and the diverse local\ndata quality among CAVs, a novel dynamic federated proximal (DFP) algorithm is\nproposed that accounts for the mobility of CAVs, the wireless fading channels,\nas well as the unbalanced and nonindependent and identically distributed data\nacross CAVs. A rigorous convergence analysis is performed for the proposed\nalgorithm to identify how fast the CAVs converge to using the optimal\nautonomous controller. In particular, the impacts of varying CAV participation\nin the FL process and diverse CAV data quality on the convergence of the\nproposed DFP algorithm are explicitly analyzed. Leveraging this analysis, an\nincentive mechanism based on contract theory is designed to improve the FL\nconvergence speed. Simulation results using real vehicular data traces show\nthat the proposed DFP-based controller can accurately track the target CAV\nspeed over time and under different traffic scenarios. Moreover, the results\nshow that the proposed DFP algorithm has a much faster convergence compared to\npopular FL algorithms such as federated averaging (FedAvg) and federated\nproximal (FedProx). The results also validate the feasibility of the\ncontract-theoretic incentive mechanism and show that the proposed mechanism can\nimprove the convergence speed of the DFP algorithm by 40% compared to the\nbaselines.", "time": "2022-06-16T02:23:18Z", "link": "http://arxiv.org/abs/2102.03401v2", "id": "2102.03401v2", "title": "Federated Learning on the Road: Autonomous Controller Design for\n  Connected and Autonomous Vehicles"}
{"author": "Baiyu Peng, Yao Mu, Jingliang Duan, Yang Guan, Shengbo Eben Li, Jianyu Chen", "abstract": "Safety is essential for reinforcement learning (RL) applied in real-world\ntasks like autonomous driving. Chance constraints which guarantee the\nsatisfaction of state constraints at a high probability are suitable to\nrepresent the requirements in real-world environment with uncertainty. Existing\nchance constrained RL methods like the penalty method and the Lagrangian method\neither exhibit periodic oscillations or cannot satisfy the constraints. In this\npaper, we address these shortcomings by proposing a separated\nproportional-integral Lagrangian (SPIL) algorithm. Taking a control\nperspective, we first interpret the penalty method and the Lagrangian method as\nproportional feedback and integral feedback control, respectively. Then, a\nproportional-integral Lagrangian method is proposed to steady learning process\nwhile improving safety. To prevent integral overshooting and reduce\nconservatism, we introduce the integral separation technique inspired by PID\ncontrol. Finally, an analytical gradient of the chance constraint is utilized\nfor model-based policy optimization. The effectiveness of SPIL is demonstrated\nby a narrow car-following task. Experiments indicate that compared with\nprevious methods, SPIL improves the performance while guaranteeing safety, with\na steady learning process.", "time": "2021-02-17T02:40:01Z", "link": "http://arxiv.org/abs/2102.08539v1", "id": "2102.08539v1", "title": "Separated Proportional-Integral Lagrangian for Chance Constrained\n  Reinforcement Learning"}
{"author": "Ross Drummond, Mathew C. Turner, Stephen R. Duncan", "abstract": "In the wake of the explosive growth in smartphones and cyberphysical systems,\nthere has been an accelerating shift in how data is generated away from\ncentralised data towards on-device generated data. In response, machine\nlearning algorithms are being adapted to run locally on board, potentially\nhardware limited, devices to improve user privacy, reduce latency and be more\nenergy efficient. However, our understanding of how these device orientated\nalgorithms behave and should be trained is still fairly limited. To address\nthis issue, a method to automatically synthesize reduced-order neural networks\n(having fewer neurons) approximating the input/output mapping of a larger one\nis introduced. The reduced-order neural network's weights and biases are\ngenerated from a convex semi-definite programme that minimises the worst-case\napproximation error with respect to the larger network. Worst-case bounds for\nthis approximation error are obtained and the approach can be applied to a wide\nvariety of neural networks architectures. What differentiates the proposed\napproach to existing methods for generating small neural networks, e.g.\npruning, is the inclusion of the worst-case approximation error directly within\nthe training cost function, which should add robustness. Numerical examples\nhighlight the potential of the proposed approach. The overriding goal of this\npaper is to generalise recent results in the robustness analysis of neural\nnetworks to a robust synthesis problem for their weights and biases.", "time": "2021-12-07T22:18:40Z", "link": "http://arxiv.org/abs/2102.09284v2", "id": "2102.09284v2", "title": "Reduced-Order Neural Network Synthesis with Robustness Guarantees"}
{"author": "Xianyuan Zhan, Haoran Xu, Yue Zhang, Xiangyu Zhu, Honglei Yin, Yu Zheng", "abstract": "Optimizing the combustion efficiency of a thermal power generating unit\n(TPGU) is a highly challenging and critical task in the energy industry. We\ndevelop a new data-driven AI system, namely DeepThermal, to optimize the\ncombustion control strategy for TPGUs. At its core, is a new model-based\noffline reinforcement learning (RL) framework, called MORE, which leverages\nhistorical operational data of a TGPU to solve a highly complex constrained\nMarkov decision process problem via purely offline training. In DeepThermal, we\nfirst learn a data-driven combustion process simulator from the offline\ndataset. The RL agent of MORE is then trained by combining real historical data\nas well as carefully filtered and processed simulation data through a novel\nrestrictive exploration scheme. DeepThermal has been successfully deployed in\nfour large coal-fired thermal power plants in China. Real-world experiments\nshow that DeepThermal effectively improves the combustion efficiency of TPGUs.\nWe also report the superior performance of MORE by comparing with the\nstate-of-the-art algorithms on the standard offline RL benchmarks.", "time": "2022-04-05T09:05:30Z", "link": "http://arxiv.org/abs/2102.11492v3", "id": "2102.11492v3", "title": "DeepThermal: Combustion Optimization for Thermal Power Generating Units\n  Using Offline Reinforcement Learning"}
{"author": "Yigit Tuncel, Ganapati Bhat, Jaehyun Park, Umit Ogras", "abstract": "Energy harvesting offers an attractive and promising mechanism to power\nlow-energy devices. However, it alone is insufficient to enable an\nenergy-neutral operation, which can eliminate tedious battery charging and\nreplacement requirements. Achieving an energy-neutral operation is challenging\nsince the uncertainties in harvested energy undermine the quality of service\nrequirements. To address this challenge, we present a runtime energy-allocation\nframework that optimizes the utility of the target device under energy\nconstraints using a rollout algorithm, which is a sequential approach to solve\ndynamic optimization problems. The proposed framework uses an efficient\niterative algorithm to compute initial energy allocations at the beginning of a\nday. The initial allocations are then corrected at every interval to compensate\nfor the deviations from the expected energy harvesting pattern. We evaluate\nthis framework using solar and motion energy harvesting modalities and American\nTime Use Survey data from 4772 different users. Compared to prior techniques,\nthe proposed framework achieves up to 35% higher utility even under\nenergy-limited scenarios. Moreover, measurements on a wearable device prototype\nshow that the proposed framework has 1000x smaller energy overhead than\niterative approaches with a negligible loss in utility.", "time": "2021-09-10T17:58:20Z", "link": "http://arxiv.org/abs/2102.13605v2", "id": "2102.13605v2", "title": "ECO: Enabling Energy-Neutral IoT Devices through Runtime Allocation of\n  Harvested Energy"}
{"author": "Sampo Kuutti, Saber Fallah, Richard Bowden", "abstract": "Imitation learning has been widely used to learn control policies for\nautonomous driving based on pre-recorded data. However, imitation learning\nbased policies have been shown to be susceptible to compounding errors when\nencountering states outside of the training distribution. Further, these agents\nhave been demonstrated to be easily exploitable by adversarial road users\naiming to create collisions. To overcome these shortcomings, we introduce\nAdversarial Mixture Density Networks (AMDN), which learns two distributions\nfrom separate datasets. The first is a distribution of safe actions learned\nfrom a dataset of naturalistic human driving. The second is a distribution\nrepresenting unsafe actions likely to lead to collision, learned from a dataset\nof collisions. During training, we leverage these two distributions to provide\nan additional loss based on the similarity of the two distributions. By\npenalising the safe action distribution based on its similarity to the unsafe\naction distribution when training on the collision dataset, a more robust and\nsafe control policy is obtained. We demonstrate the proposed AMDN approach in a\nvehicle following use-case, and evaluate under naturalistic and adversarial\ntesting environments. We show that despite its simplicity, AMDN provides\nsignificant benefits for the safety of the learned control policy, when\ncompared to pure imitation learning or standard mixture density network\napproaches.", "time": "2021-07-09T15:16:30Z", "link": "http://arxiv.org/abs/2107.04485v1", "id": "2107.04485v1", "title": "Adversarial Mixture Density Networks: Learning to Drive Safely from\n  Collision Data"}
{"author": "Mridul Agarwal, Qinbo Bai, Vaneet Aggarwal", "abstract": "We consider the problem of constrained Markov Decision Process (CMDP) where\nan agent interacts with a unichain Markov Decision Process. At every\ninteraction, the agent obtains a reward. Further, there are $K$ cost functions.\nThe agent aims to maximize the long-term average reward while simultaneously\nkeeping the $K$ long-term average costs lower than a certain threshold. In this\npaper, we propose CMDP-PSRL, a posterior sampling based algorithm using which\nthe agent can learn optimal policies to interact with the CMDP. Further, for\nMDP with $S$ states, $A$ actions, and diameter $D$, we prove that following\nCMDP-PSRL algorithm, the agent can bound the regret of not accumulating rewards\nfrom optimal policy by $\\Tilde{O}(poly(DSA)\\sqrt{T})$. Further, we show that\nthe violations for any of the $K$ constraints is also bounded by\n$\\Tilde{O}(poly(DSA)\\sqrt{T})$. To the best of our knowledge, this is the first\nwork which obtains a $\\Tilde{O}(\\sqrt{T})$ regret bounds for ergodic MDPs with\nlong-term average constraints.", "time": "2022-06-21T01:08:53Z", "link": "http://arxiv.org/abs/2106.06680v2", "id": "2106.06680v2", "title": "Markov Decision Processes with Long-Term Average Constraints"}
{"author": "IÃ±igo Martinez, Elisabeth Viles, IÃ±aki Cabrejas", "abstract": "A failure detection system is the first step towards predictive maintenance\nstrategies. A popular data-driven method to detect incipient failures and\nanomalies is the training of normal behaviour models by applying a machine\nlearning technique like feed-forward neural networks (FFNN) or extreme learning\nmachines (ELM). However, the performance of any of these modelling techniques\ncan be deteriorated by the unexpected rise of non-stationarities in the dynamic\nenvironment in which industrial assets operate. This unpredictable statistical\nchange in the measured variable is known as concept drift. In this article a\nwind turbine maintenance case is presented, where non-stationarities of various\nkinds can happen unexpectedly. Such concept drift events are desired to be\ndetected by means of statistical detectors and window-based approaches.\nHowever, in real complex systems, concept drifts are not as clear and evident\nas in artificially generated datasets. In order to evaluate the effectiveness\nof current drift detectors and also to design an appropriate novel technique\nfor this specific industrial application, it is essential to dispose beforehand\nof a characterization of the existent drifts. Under the lack of information in\nthis regard, a methodology for labelling concept drift events in the lifetime\nof wind turbines is proposed. This methodology will facilitate the creation of\na drift database that will serve both as a training ground for concept drift\ndetectors and as a valuable information to enhance the knowledge about\nmaintenance of complex systems.", "time": "2021-06-18T07:14:14Z", "link": "http://arxiv.org/abs/2106.09951v1", "id": "2106.09951v1", "title": "Labelling Drifts in a Fault Detection System for Wind Turbine\n  Maintenance"}
{"author": "Christoph Dann, Yishay Mansour, Mehryar Mohri, Ayush Sekhari, Karthik Sridharan", "abstract": "There have been many recent advances on provably efficient Reinforcement\nLearning (RL) in problems with rich observation spaces. However, all these\nworks share a strong realizability assumption about the optimal value function\nof the true MDP. Such realizability assumptions are often too strong to hold in\npractice. In this work, we consider the more realistic setting of agnostic RL\nwith rich observation spaces and a fixed class of policies $\\Pi$ that may not\ncontain any near-optimal policy. We provide an algorithm for this setting whose\nerror is bounded in terms of the rank $d$ of the underlying MDP. Specifically,\nour algorithm enjoys a sample complexity bound of $\\widetilde{O}\\left((H^{4d}\nK^{3d} \\log |\\Pi|)/\\epsilon^2\\right)$ where $H$ is the length of episodes, $K$\nis the number of actions and $\\epsilon>0$ is the desired sub-optimality. We\nalso provide a nearly matching lower bound for this agnostic setting that shows\nthat the exponential dependence on rank is unavoidable, without further\nassumptions.", "time": "2021-06-22T03:20:40Z", "link": "http://arxiv.org/abs/2106.11519v1", "id": "2106.11519v1", "title": "Agnostic Reinforcement Learning with Low-Rank MDPs and Rich Observations"}
{"author": "Ruidong Yan, Rui Jiang, Bin Jia, Jin Huang, Diange Yang", "abstract": "Deep deterministic policy gradient (DDPG)-based car-following strategy can\nbreak through the constraints of the differential equation model due to the\nability of exploration on complex environments. However, the car-following\nperformance of DDPG is usually degraded by unreasonable reward function design,\ninsufficient training, and low sampling efficiency. In order to solve this kind\nof problem, a hybrid car-following strategy based on DDPG and cooperative\nadaptive cruise control (CACC) is proposed. First, the car-following process is\nmodeled as the Markov decision process to calculate CACC and DDPG\nsimultaneously at each frame. Given a current state, two actions are obtained\nfrom CACC and DDPG, respectively. Then, an optimal action, corresponding to the\none offering a larger reward, is chosen as the output of the hybrid strategy.\nMeanwhile, a rule is designed to ensure that the change rate of acceleration is\nsmaller than the desired value. Therefore, the proposed strategy not only\nguarantees the basic performance of car-following through CACC but also makes\nfull use of the advantages of exploration on complex environments via DDPG.\nFinally, simulation results show that the car-following performance of the\nproposed strategy is improved compared with that of DDPG and CACC.", "time": "2022-01-11T04:40:18Z", "link": "http://arxiv.org/abs/2103.03796v2", "id": "2103.03796v2", "title": "Hybrid Car-Following Strategy based on Deep Deterministic Policy\n  Gradient and Cooperative Adaptive Cruise Control"}
{"author": "Wanjun Huang, Xiang Pan, Minghua Chen, Steven H. Low", "abstract": "AC optimal power flow (AC-OPF) problems need to be solved more frequently in\nthe future to maintain stable and economic power system operation. To tackle\nthis challenge, a deep neural network-based voltage-constrained approach\n(DeepOPF-V) is proposed to solve AC-OPF problems with high computational\nefficiency. Its unique design predicts voltages of all buses and then uses them\nto reconstruct the remaining variables without solving non-linear AC power flow\nequations. A fast post-processing process is developed to enforce the box\nconstraints. The effectiveness of DeepOPF-V is validated by simulations on IEEE\n118/300-bus systems and a 2000-bus test system. Compared with existing studies,\nDeepOPF-V achieves decent computation speedup up to four orders of magnitude\nand comparable performance in optimality gap and preserving the feasibility of\nthe solution.", "time": "2021-07-19T03:15:44Z", "link": "http://arxiv.org/abs/2103.11793v2", "id": "2103.11793v2", "title": "DeepOPF-V: Solving AC-OPF Problems Efficiently"}
{"author": "Aquib Mustafa, Majid Mazouchi, Subramanya Nageshrao, Hamidreza Modares", "abstract": "Reinforcement learning (RL) agents with pre-specified reward functions cannot\nprovide guaranteed safety across variety of circumstances that an uncertain\nsystem might encounter. To guarantee performance while assuring satisfaction of\nsafety constraints across variety of circumstances, an assured autonomous\ncontrol framework is presented in this paper by empowering RL algorithms with\nmetacognitive learning capabilities. More specifically, adapting the reward\nfunction parameters of the RL agent is performed in a metacognitive\ndecision-making layer to assure the feasibility of RL agent. That is, to assure\nthat the learned policy by the RL agent satisfies safety constraints specified\nby signal temporal logic while achieving as much performance as possible. The\nmetacognitive layer monitors any possible future safety violation under the\nactions of the RL agent and employs a higher-layer Bayesian RL algorithm to\nproactively adapt the reward function for the lower-layer RL agent. To minimize\nthe higher-layer Bayesian RL intervention, a fitness function is leveraged by\nthe metacognitive layer as a metric to evaluate success of the lower-layer RL\nagent in satisfaction of safety and liveness specifications, and the\nhigher-layer Bayesian RL intervenes only if there is a risk of lower-layer RL\nfailure. Finally, a simulation example is provided to validate the\neffectiveness of the proposed approach.", "time": "2021-04-17T19:01:31Z", "link": "http://arxiv.org/abs/2103.12558v2", "id": "2103.12558v2", "title": "Assured Learning-enabled Autonomy: A Metacognitive Reinforcement\n  Learning Framework"}
{"author": "Min Wang, Shanchen Pang, Tong Ding, Sibo Qiao, Xue Zhai, Shuo Wang, Neal N. Xiong, Zhengwen Huang", "abstract": "At present, SOILD-STATE Fermentation (SSF) is mainly controlled by artificial\nexperience, and the product quality and yield are not stable. Accurately\npredicting the quality and yield of SSF is of great significance for improving\nhuman food security and supply. In this paper, we propose an Intelligent\nUtility Prediction (IUP) scheme for SSF in 5G Industrial Internet of Things\n(IoT), including parameter collection and utility prediction of SSF process.\nThis IUP scheme is based on the environmental perception and intelligent\nlearning algorithms of the 5G Industrial IoT. We build a workflow model based\non rewritable petri net to verify the correctness of the system model function\nand process. In addition, we design a utility prediction model for SSF based on\nthe Generative Adversarial Networks (GAN) and Fully Connected Neural Network\n(FCNN). We design a GAN with constraint of mean square error (MSE-GAN) to solve\nthe problem of few-shot learning of SSF, and then combine with the FCNN to\nrealize the utility prediction (usually use the alcohol) of SSF. Based on the\nproduction of liquor in laboratory, the experiments show that the proposed\nmethod is more accurate than the other prediction methods in the utility\nprediction of SSF, and provide the basis for the numerical analysis of the\nproportion of preconfigured raw materials and the appropriate setting of cellar\ntemperature.", "time": "2021-03-28T07:36:48Z", "link": "http://arxiv.org/abs/2103.15073v1", "id": "2103.15073v1", "title": "IUP: An Intelligent Utility Prediction Scheme for Solid-State\n  Fermentation in 5G IoT"}
{"author": "Piotr Kicki, Krzysztof Åakomy, Ki Myung Brian Lee", "abstract": "The extended state observer (ESO) is an inherent element of robust\nobserver-based control systems that allows estimating the impact of disturbance\non system dynamics. Proper tuning of ESO parameters is necessary to ensure a\ngood quality of estimated quantities and impacts the overall performance of the\nrobust control structure. In this paper, we propose a neural network (NN) based\ntuning procedure that allows the user to prioritize between selected quality\ncriteria such as the control and observation errors and the specified features\nof the control signal. The designed NN provides an accurate assessment of the\ncontrol system performance and returns a set of ESO parameters that delivers a\nnear-optimal solution to the user-defined cost function. The proposed tuning\nprocedure, using an estimated state from the single closed-loop experiment\nproduces near-optimal ESO gains within seconds.", "time": "2021-03-30T06:17:22Z", "link": "http://arxiv.org/abs/2103.15516v2", "id": "2103.15516v2", "title": "Tuning of extended state observer with neural network-based control\n  performance assessment"}
{"author": "Baicen Xiao, Bhaskar Ramasubramanian, Radha Poovendran", "abstract": "Multi-agent reinforcement learning involves multiple agents interacting with\neach other and a shared environment to complete tasks. When rewards provided by\nthe environment are sparse, agents may not receive immediate feedback on the\nquality of actions that they take, thereby affecting learning of policies. In\nthis paper, we propose a method called Shaping Advice in deep Multi-agent\nreinforcement learning (SAM) to augment the reward signal from the environment\nwith an additional reward termed shaping advice. The shaping advice is given by\na difference of potential functions at consecutive time-steps. Each potential\nfunction is a function of observations and actions of the agents. The shaping\nadvice needs to be specified only once at the start of training, and can be\neasily provided by non-experts. We show through theoretical analyses and\nexperimental validation that shaping advice provided by SAM does not distract\nagents from completing tasks specified by the environment reward.\nTheoretically, we prove that convergence of policy gradients and value\nfunctions when using SAM implies convergence of these quantities in the absence\nof SAM. Experimentally, we evaluate SAM on three tasks in the multi-agent\nParticle World environment that have sparse rewards. We observe that using SAM\nresults in agents learning policies to complete tasks faster, and obtain higher\nrewards than: i) using sparse rewards alone; ii) a state-of-the-art reward\nredistribution method.", "time": "2021-03-29T20:33:50Z", "link": "http://arxiv.org/abs/2103.15941v1", "id": "2103.15941v1", "title": "Shaping Advice in Deep Multi-Agent Reinforcement Learning"}
{"author": "Ning Wang, Mohammed Abouheaf, Wail Gueaieb", "abstract": "A data-driven computational heuristic is proposed to control MIMO systems\nwithout prior knowledge of their dynamics. The heuristic is illustrated on a\ntwo-input two-output balance system. It integrates a self-adjusting nonlinear\nthreshold accepting heuristic with a neural network to compromise between the\ndesired transient and steady state characteristics of the system while\noptimizing a dynamic cost function. The heuristic decides on the control gains\nof multiple interacting PID control loops. The neural network is trained upon\noptimizing a weighted-derivative like objective cost function. The performance\nof the developed mechanism is compared with another controller that employs a\ncombined PID-Riccati approach. One of the salient features of the proposed\ncontrol schemes is that they do not require prior knowledge of the system\ndynamics. However, they depend on a known region of stability for the control\ngains to be used as a search space by the optimization algorithm. The control\nmechanism is validated using different optimization criteria which address\ndifferent design requirements.", "time": "2021-04-01T02:00:20Z", "link": "http://arxiv.org/abs/2104.00199v1", "id": "2104.00199v1", "title": "Data-Driven Optimized Tracking Control Heuristic for MIMO Structures: A\n  Balance System Case Study"}
{"author": "Anousheh Gholami, Nariman Torkzaban, John S. Baras", "abstract": "With the increasing scale, complexity, and heterogeneity of the next\ngeneration networked systems, seamless control, management, and security of\nsuch systems becomes increasingly challenging. Many diverse applications have\ndriven interest in networked systems, including large-scale distributed\nlearning, multi-agent optimization, 5G service provisioning, and network\nslicing, etc. In this paper, we propose trust as a measure to evaluate the\nstatus of network agents and improve the decision-making process. We interpret\ntrust as a relation among entities that participate in various protocols. Trust\nrelations are based on evidence created by the interactions of entities within\na protocol and may be a composite of multiple metrics such as availability,\nreliability, resilience, etc. depending on application context. We first\nelaborate on the importance of trust as a metric and then present a\nmathematical framework for trust computation and aggregation within a network.\nThen we show in practice, how trust can be integrated into network\ndecision-making processes by presenting two examples. In the first example, we\nshow how utilizing the trust evidence can improve the performance and the\nsecurity of Federated Learning. Second, we show how a 5G network resource\nprovisioning framework can be improved when augmented with a trust-aware\ndecision-making scheme. We verify the validity of our trust-based approach\nthrough simulations. Finally, we explain the challenges associated with\naggregating the trust evidence and briefly explain our ideas to tackle them.", "time": "2021-04-16T02:12:13Z", "link": "http://arxiv.org/abs/2104.07853v1", "id": "2104.07853v1", "title": "On the Importance of Trust in Next-Generation Networked CPS Systems: An\n  AI Perspective"}
{"author": "Mobin Zhao, Wangzhi Li, Yongjie Fu, Kangrui Ruan, Xuan Di", "abstract": "This paper develops a decentralized reinforcement learning (RL) scheme for\nmulti-intersection adaptive traffic signal control (TSC), called \"CVLight\",\nthat leverages data collected from connected vehicles (CVs). The state and\nreward design facilitates coordination among agents and considers travel delays\ncollected by CVs. A novel algorithm, Asymmetric Advantage Actor-critic\n(Asym-A2C), is proposed where both CV and non-CV information is used to train\nthe critic network, while only CV information is used to execute optimal signal\ntiming. Comprehensive experiments show the superiority of CVLight over\nstate-of-the-art algorithms under a 2-by-2 synthetic road network with various\ntraffic demand patterns and penetration rates. The learned policy is then\nvisualized to further demonstrate the advantage of Asym-A2C. A pre-train\ntechnique is applied to improve the scalability of CVLight, which significantly\nshortens the training time and shows the advantage in performance under a\n5-by-5 road network. A case study is performed on a 2-by-2 road network located\nin State College, Pennsylvania, USA, to further demonstrate the effectiveness\nof the proposed algorithm under real-world scenarios. Compared to other\nbaseline models, the trained CVLight agent can efficiently control multiple\nintersections solely based on CV data and achieve the best performance,\nespecially under low CV penetration rates.", "time": "2022-07-01T03:28:09Z", "link": "http://arxiv.org/abs/2104.10340v3", "id": "2104.10340v3", "title": "CVLight: Decentralized Learning for Adaptive Traffic Signal Control with\n  Connected Vehicles"}
{"author": "Alvaro Cabrejas-Egea, Raymond Zhang, Neil Walton", "abstract": "Recently, Intelligent Transportation Systems are leveraging the power of\nincreased sensory coverage and computing power to deliver data-intensive\nsolutions achieving higher levels of performance than traditional systems.\nWithin Traffic Signal Control (TSC), this has allowed the emergence of Machine\nLearning (ML) based systems. Among this group, Reinforcement Learning (RL)\napproaches have performed particularly well. Given the lack of industry\nstandards in ML for TSC, literature exploring RL often lacks comparison against\ncommercially available systems and straightforward formulations of how the\nagents operate. Here we attempt to bridge that gap. We propose three different\narchitectures for TSC RL agents and compare them against the currently used\ncommercial systems MOVA, SurTrac and Cyclic controllers and provide pseudo-code\nfor them. The agents use variations of Deep Q-Learning and Actor Critic, using\nstates and rewards based on queue lengths. Their performance is compared in\nacross different map scenarios with variable demand, assessing them in terms of\nthe global delay and average queue length. We find that the RL-based systems\ncan significantly and consistently achieve lower delays when compared with\nexisting commercial systems.", "time": "2021-04-30T11:40:14Z", "link": "http://arxiv.org/abs/2104.10455v2", "id": "2104.10455v2", "title": "Reinforcement Learning for Traffic Signal Control: Comparison with\n  Commercial Systems"}
{"author": "Alex Devonport, Adnane Saoud, Murat Arcak", "abstract": "Symbolic control techniques aim to satisfy complex logic specifications. A\ncritical step in these techniques is the construction of a symbolic (discrete)\nabstraction, a finite-state system whose behaviour mimics that of a given\ncontinuous-state system. The methods used to compute symbolic abstractions,\nhowever, require knowledge of an accurate closed-form model. To generalize them\nto systems with unknown dynamics, we present a new data-driven approach that\ndoes not require closed-form dynamics, instead relying only the ability to\nevaluate successors of each state under given inputs. To provide guarantees for\nthe learned abstraction, we use the Probably Approximately Correct (PAC)\nstatistical framework. We first introduce a PAC-style behavioural relationship\nand an appropriate refinement procedure. We then show how the symbolic\nabstraction can be constructed to satisfy this new behavioural relationship.\nMoreover, we provide PAC bounds that dictate the number of data required to\nguarantee a prescribed level of accuracy and confidence. Finally, we present an\nillustrative example.", "time": "2021-04-28T17:34:28Z", "link": "http://arxiv.org/abs/2104.13901v1", "id": "2104.13901v1", "title": "Symbolic Abstractions From Data: A PAC Learning Approach"}
{"author": "Masoud Fetanat, Michael Stevens, Christopher Hayward, Nigel H. Lovell", "abstract": "Left ventricular assist devices (LVADs) are mechanical pumps, which can be\nused to support heart failure (HF) patients as bridge to transplant and\ndestination therapy. To automatically adjust the LVAD speed, a physiological\ncontrol system needs to be designed to respond to variations of patient\nhemodynamics across a variety of clinical scenarios. These control systems\nrequire pressure feedback signals from the cardiovascular system. However,\nthere are no suitable long-term implantable sensors available. In this study, a\nnovel real-time deep convolutional neural network (CNN) for estimation of\npreload based on the LVAD flow was proposed. A new sensorless adaptive\nphysiological control system for an LVAD pump was developed using the full\ndynamic form of model free adaptive control (FFDL-MFAC) and the proposed\npreload estimator to maintain the patient conditions in safe physiological\nranges. The CNN model for preload estimation was trained and evaluated through\n10-fold cross validation on 100 different patient conditions and the proposed\nsensorless control system was assessed on a new testing set of 30 different\npatient conditions across six different patient scenarios. The proposed preload\nestimator was extremely accurate with a correlation coefficient of 0.97, root\nmean squared error of 0.84 mmHg, reproducibility coefficient of 1.56 mmHg,\ncoefficient of variation of 14.44 %, and bias of 0.29 mmHg for the testing\ndataset. The results also indicate that the proposed sensorless physiological\ncontroller works similarly to the preload-based physiological control system\nfor LVAD using measured preload to prevent ventricular suction and pulmonary\ncongestion. This study shows that the LVADs can respond appropriately to\nchanging patient states and physiological demands without the need for\nadditional pressure or flow measurements.", "time": "2021-04-30T05:12:09Z", "link": "http://arxiv.org/abs/2105.00875v1", "id": "2105.00875v1", "title": "A Sensorless Control System for an Implantable Heart Pump using a\n  Real-time Deep Convolutional Neural Network"}
{"author": "Giorgio Angelotti, Nicolas Drougard, Caroline Ponzoni Carvalho Chanel", "abstract": "In Offline Model Learning for Planning and in Offline Reinforcement Learning,\nthe limited data set hinders the estimate of the Value function of the relative\nMarkov Decision Process (MDP). Consequently, the performance of the obtained\npolicy in the real world is bounded and possibly risky, especially when the\ndeployment of a wrong policy can lead to catastrophic consequences. For this\nreason, several pathways are being followed with the scope of reducing the\nmodel error (or the distributional shift between the learned model and the true\none) and, more broadly, obtaining risk-aware solutions with respect to model\nuncertainty. But when it comes to the final application which baseline should a\npractitioner choose? In an offline context where computational time is not an\nissue and robustness is the priority we propose Exploitation vs Caution (EvC),\na paradigm that (1) elegantly incorporates model uncertainty abiding by the\nBayesian formalism, and (2) selects the policy that maximizes a risk-aware\nobjective over the Bayesian posterior between a fixed set of candidate policies\nprovided, for instance, by the current baselines. We validate EvC with\nstate-of-the-art approaches in different discrete, yet simple, environments\noffering a fair variety of MDP classes. In the tested scenarios EvC manages to\nselect robust policies and hence stands out as a useful tool for practitioners\nthat aim to apply offline planning and reinforcement learning solvers in the\nreal world.", "time": "2023-04-11T13:01:07Z", "link": "http://arxiv.org/abs/2105.13431v2", "id": "2105.13431v2", "title": "An Offline Risk-aware Policy Selection Method for Bayesian Markov\n  Decision Processes"}
{"author": "Vektor Dewanto, Marcus Gallagher", "abstract": "For continuing environments, reinforcement learning (RL) methods commonly\nmaximize the discounted reward criterion with discount factor close to 1 in\norder to approximate the average reward (the gain). However, such a criterion\nonly considers the long-run steady-state performance, ignoring the transient\nbehaviour in transient states. In this work, we develop a policy gradient\nmethod that optimizes the gain, then the bias (which indicates the transient\nperformance and is important to capably select from policies with equal gain).\nWe derive expressions that enable sampling for the gradient of the bias and its\npreconditioning Fisher matrix. We further devise an algorithm that solves the\ngain-then-bias (bi-level) optimization. Its key ingredient is an RL-specific\nlogarithmic barrier function. Experimental results provide insights into the\nfundamental mechanisms of our proposal.", "time": "2022-07-03T06:34:57Z", "link": "http://arxiv.org/abs/2105.13609v3", "id": "2105.13609v3", "title": "A nearly Blackwell-optimal policy gradient method"}
{"author": "Qinbo Bai, Mridul Agarwal, Vaneet Aggarwal", "abstract": "Many engineering problems have multiple objectives, and the overall aim is to\noptimize a non-linear function of these objectives. In this paper, we formulate\nthe problem of maximizing a non-linear concave function of multiple long-term\nobjectives. A policy-gradient based model-free algorithm is proposed for the\nproblem. To compute an estimate of the gradient, a biased estimator is\nproposed. The proposed algorithm is shown to achieve convergence to within an\n$\\epsilon$ of the global optima after sampling\n$\\mathcal{O}(\\frac{M^4\\sigma^2}{(1-\\gamma)^8\\epsilon^4})$ trajectories where\n$\\gamma$ is the discount factor and $M$ is the number of the agents, thus\nachieving the same dependence on $\\epsilon$ as the policy gradient algorithm\nfor the standard reinforcement learning.", "time": "2021-05-28T22:20:54Z", "link": "http://arxiv.org/abs/2105.14125v1", "id": "2105.14125v1", "title": "Joint Optimization of Multi-Objective Reinforcement Learning with Policy\n  Gradient Based Algorithm"}
{"author": "Mingming Liu", "abstract": "Recently, there has been an increasing interest in the roll-out of electric\nvehicles (EVs) in the global automotive market. Compared to conventional\ninternal combustion engine vehicles (ICEVs), EVs can not only help users reduce\nmonetary costs in their daily commuting, but also can effectively help mitigate\nthe increasing level of traffic emissions produced in cities. Among many\nothers, battery electric vehicles (BEVs) exclusively use chemical energy stored\nin their battery packs for propulsion. Hence, it becomes important to\nunderstand how much energy can be consumed by such vehicles in various traffic\nscenarios towards effective energy management. To address this challenge, we\npropose a novel framework in this paper by leveraging the federated learning\napproaches for modelling energy consumption for BEVs (Fed-BEV). More\nspecifically, a group of BEVs involved in the Fed-BEV framework can learn from\neach other to jointly enhance their energy consumption model. We present the\ndesign of the proposed system architecture and implementation details in a\nco-simulation environment. Finally, comparative studies and simulation results\nare discussed to illustrate the efficacy of our proposed framework for accurate\nenergy modelling of BEVs.", "time": "2021-08-05T01:56:09Z", "link": "http://arxiv.org/abs/2108.04036v1", "id": "2108.04036v1", "title": "Fed-BEV: A Federated Learning Framework for Modelling Energy Consumption\n  of Battery Electric Vehicles"}
{"author": "Gayathri Krishnamoorthy, Anamika Dubey", "abstract": "Reinforcement learning has been found useful in solving optimal power flow\n(OPF) problems in electric power distribution systems. However, the use of\nlargely model-free reinforcement learning algorithms that completely ignore the\nphysics-based modeling of the power grid compromises the optimizer performance\nand poses scalability challenges. This paper proposes a novel approach to\nsynergistically combine the physics-based models with learning-based algorithms\nusing imitation learning to solve distribution-level OPF problems.\nSpecifically, we propose imitation learning based improvements in deep\nreinforcement learning (DRL) methods to solve the OPF problem for a specific\ncase of battery storage dispatch in the power distribution systems. The\nproposed imitation learning algorithm uses the approximate optimal solutions\nobtained from a linearized model-based OPF solver to provide a good initial\npolicy for the DRL algorithms while improving the training efficiency. The\neffectiveness of the proposed approach is demonstrated using IEEE 34-bus and\n123-bus distribution feeders with numerous distribution-level battery storage\nsystems.", "time": "2021-09-02T14:48:25Z", "link": "http://arxiv.org/abs/2109.01659v1", "id": "2109.01659v1", "title": "Reinforcement Learning for Battery Energy Storage Dispatch augmented\n  with Model-based Optimizer"}
{"author": "Anna Winnicki, Joseph Lubars, Michael Livesay, R. Srikant", "abstract": "Function approximation is widely used in reinforcement learning to handle the\ncomputational difficulties associated with very large state spaces. However,\nfunction approximation introduces errors which may lead to instabilities when\nusing approximate dynamic programming techniques to obtain the optimal policy.\nTherefore, techniques such as lookahead for policy improvement and m-step\nrollout for policy evaluation are used in practice to improve the performance\nof approximate dynamic programming with function approximation. We\nquantitatively characterize, for the first time, the impact of lookahead and\nm-step rollout on the performance of approximate dynamic programming (DP) with\nfunction approximation: (i) without a sufficient combination of lookahead and\nm-step rollout, approximate DP may not converge, (ii) both lookahead and m-step\nrollout improve the convergence rate of approximate DP, and (iii) lookahead\nhelps mitigate the effect of function approximation and the discount factor on\nthe asymptotic performance of the algorithm. Our results are presented for two\napproximate DP methods: one which uses least-squares regression to perform\nfunction approximation and another which performs several steps of gradient\ndescent of the least-squares objective in each iteration.", "time": "2022-12-13T23:18:51Z", "link": "http://arxiv.org/abs/2109.13419v7", "id": "2109.13419v7", "title": "The Role of Lookahead and Approximate Policy Evaluation in Reinforcement\n  Learning with Linear Value Function Approximation"}
{"author": "Animesh Basak Chowdhury, Benjamin Tan, Ramesh Karri, Siddharth Garg", "abstract": "Logic synthesis is a challenging and widely-researched combinatorial\noptimization problem during integrated circuit (IC) design. It transforms a\nhigh-level description of hardware in a programming language like Verilog into\nan optimized digital circuit netlist, a network of interconnected Boolean logic\ngates, that implements the function. Spurred by the success of ML in solving\ncombinatorial and graph problems in other domains, there is growing interest in\nthe design of ML-guided logic synthesis tools. Yet, there are no standard\ndatasets or prototypical learning tasks defined for this problem domain. Here,\nwe describe OpenABC-D,a large-scale, labeled dataset produced by synthesizing\nopen source designs with a leading open-source logic synthesis tool and\nillustrate its use in developing, evaluating and benchmarking ML-guided logic\nsynthesis. OpenABC-D has intermediate and final outputs in the form of 870,000\nAnd-Inverter-Graphs (AIGs) produced from 1500 synthesis runs plus labels such\nas the optimized node counts, and de-lay. We define a generic learning problem\non this dataset and benchmark existing solutions for it. The codes related to\ndataset creation and benchmark models are available\nathttps://github.com/NYU-MLDA/OpenABC.git. The dataset generated is available\nathttps://archive.nyu.edu/handle/2451/63311", "time": "2021-10-21T17:19:19Z", "link": "http://arxiv.org/abs/2110.11292v1", "id": "2110.11292v1", "title": "OpenABC-D: A Large-Scale Dataset For Machine Learning Guided Integrated\n  Circuit Synthesis"}
{"author": "Vinay Hanumaiah, Sahika Genc", "abstract": "It is estimated that about 40%-50% of total electricity consumption in\ncommercial buildings can be attributed to Heating, Ventilation, and Air\nConditioning (HVAC) systems. Minimizing the energy cost while considering the\nthermal comfort of the occupants is very challenging due to unknown and complex\nrelationships between various HVAC controls and thermal dynamics inside a\nbuilding. To this end, we present a multi-agent, distributed deep reinforcement\nlearning (DRL) framework based on Energy Plus simulation environment for\noptimizing HVAC in commercial buildings. This framework learns the complex\nthermal dynamics in the building and takes advantage of the differential effect\nof cooling and heating systems in the building to reduce energy costs, while\nmaintaining the thermal comfort of the occupants. With adaptive penalty, the RL\nalgorithm can be prioritized for energy savings or maintaining thermal comfort.\nUsing DRL, we achieve more than 75\\% savings in energy consumption. The\ndistributed DRL framework can be scaled to multiple GPUs and CPUs of\nheterogeneous types.", "time": "2021-10-26T07:29:16Z", "link": "http://arxiv.org/abs/2110.13450v1", "id": "2110.13450v1", "title": "Distributed Multi-Agent Deep Reinforcement Learning Framework for\n  Whole-building HVAC Control"}
{"author": "Krishnan Raghavan, Vignesh Narayanan, Jagannathan Saraangapani", "abstract": "Learning to control complex systems using non-traditional feedback, e.g., in\nthe form of snapshot images, is an important task encountered in diverse\ndomains such as robotics, neuroscience, and biology (cellular systems). In this\npaper, we present a two neural-network (NN)-based feedback control framework to\ndesign control policies for systems that generate feedback in the form of\nimages. In particular, we develop a deep $Q$-network (DQN)-driven learning\ncontrol strategy to synthesize a sequence of control inputs from snapshot\nimages that encode the information pertaining to the current state and control\naction of the system. Further, to train the networks we employ a direct\nerror-driven learning (EDL) approach that utilizes a set of linear\ntransformations of the NN training error to update the NN weights in each\nlayer. We verify the efficacy of the proposed control strategy using numerical\nexamples.", "time": "2021-10-28T16:52:18Z", "link": "http://arxiv.org/abs/2110.15290v1", "id": "2110.15290v1", "title": "Learning to Control using Image Feedback"}
{"author": "Krishnan Raghavan, Vignesh Narayanan, Jagannathan Sarangapani", "abstract": "In this paper, we address two key challenges in deep reinforcement learning\nsetting, sample inefficiency and slow learning, with a dual NN-driven learning\napproach. In the proposed approach, we use two deep NNs with independent\ninitialization to robustly approximate the action-value function in the\npresence of image inputs. In particular, we develop a temporal difference (TD)\nerror-driven learning approach, where we introduce a set of linear\ntransformations of the TD error to directly update the parameters of each layer\nin the deep NN. We demonstrate theoretically that the cost minimized by the\nerror-driven learning (EDL) regime is an approximation of the empirical cost\nand the approximation error reduces as learning progresses, irrespective of the\nsize of the network. Using simulation analysis, we show that the proposed\nmethods enables faster learning and convergence and requires reduced buffer\nsize (thereby increasing the sample efficiency).", "time": "2021-10-28T17:12:41Z", "link": "http://arxiv.org/abs/2110.15305v1", "id": "2110.15305v1", "title": "Cooperative Deep $Q$-learning Framework for Environments Providing Image\n  Feedback"}
{"author": "Matthew S. Zhang, Murat A. Erdogdu, Animesh Garg", "abstract": "Policy gradient methods have been frequently applied to problems in control\nand reinforcement learning with great success, yet existing convergence\nanalysis still relies on non-intuitive, impractical and often opaque\nconditions. In particular, existing rates are achieved in limited settings,\nunder strict regularity conditions. In this work, we establish explicit\nconvergence rates of policy gradient methods, extending the convergence regime\nto weakly smooth policy classes with $L_2$ integrable gradient. We provide\nintuitive examples to illustrate the insight behind these new conditions.\nNotably, our analysis also shows that convergence rates are achievable for both\nthe standard policy gradient and the natural policy gradient algorithms under\nthese assumptions. Lastly we provide performance guarantees for the converged\npolicies.", "time": "2022-04-07T06:31:57Z", "link": "http://arxiv.org/abs/2111.00185v2", "id": "2111.00185v2", "title": "Convergence and Optimality of Policy Gradient Methods in Weakly Smooth\n  Settings"}
{"author": "Joaquin Delgado Fernandez, Sergio Potenciano Menci, Charles Lee, Gilbert Fridgen", "abstract": "With high levels of intermittent power generation and dynamic demand\npatterns, accurate forecasts for residential loads have become essential. Smart\nmeters can play an important role when making these forecasts as they provide\ndetailed load data. However, using smart meter data for load forecasting is\nchallenging due to data privacy requirements. This paper investigates how these\nrequirements can be addressed through a combination of federated learning and\nprivacy preserving techniques such as differential privacy and secure\naggregation. For our analysis, we employ a large set of residential load data\nand simulate how different federated learning models and privacy preserving\ntechniques affect performance and privacy. Our simulations reveal that\ncombining federated learning and privacy preserving techniques can secure both\nhigh forecasting accuracy and near-complete privacy. Specifically, we find that\nsuch combinations enable a high level of information sharing while ensuring\nprivacy of both the processed load data and forecasting models. Moreover, we\nidentify and discuss challenges of applying federated learning, differential\nprivacy and secure aggregation for residential short-term load forecasting.", "time": "2022-09-19T09:00:42Z", "link": "http://arxiv.org/abs/2111.09248v4", "id": "2111.09248v4", "title": "Privacy-preserving Federated Learning for Residential Short Term Load\n  Forecasting"}
{"author": "Henrik S. Steude, Alexander Windmann, Oliver Niggemann", "abstract": "Machine Learning (ML) has achieved great successes in recent decades, both in\nresearch and in practice. In Cyber-Physical Systems (CPS), ML can for example\nbe used to optimize systems, to detect anomalies or to identify root causes of\nsystem failures. However, existing algorithms suffer from two major drawbacks:\n(i) They are hard to interpret by human experts. (ii) Transferring results from\none systems to another (similar) system is often a challenge. Concept learning,\nor Representation Learning (RepL), is a solution to both of these drawbacks;\nmimicking the human solution approach to explain-ability and transfer-ability:\nBy learning general concepts such as physical quantities or system states, the\nmodel becomes interpretable by humans. Furthermore concepts on this abstract\nlevel can normally be applied to a wide range of different systems. Modern ML\nmethods are already widely used in CPS, but concept learning and transfer\nlearning are hardly used so far. In this paper, we provide an overview of the\ncurrent state of research regarding methods for learning physical concepts in\ntime series data, which is the primary form of sensor data of CPS. We also\nanalyze the most important methods from the current state of the art using the\nexample of a three-tank system. Based on these concrete implementations1, we\ndiscuss the advantages and disadvantages of the methods and show for which\npurpose and under which conditions they can be used.", "time": "2021-12-17T15:43:00Z", "link": "http://arxiv.org/abs/2111.14151v2", "id": "2111.14151v2", "title": "Learning Physical Concepts in Cyber-Physical Systems: A Case Study"}
{"author": "Mohammad Azangoo, Joonas Salmi, Iivo YrjÃ¶lÃ¤, Jonathan Bensky, Gerardo Santillan, Nikolaos Papakonstantinou, Seppo Sierla, Valeriy Vyatkin", "abstract": "Making an updated and as-built model plays an important role in the\nlife-cycle of a process plant. In particular, Digital Twin models must be\nprecise to guarantee the efficiency and reliability of the systems. Data-driven\nmodels can simulate the latest behavior of the sub-systems by considering\nuncertainties and life-cycle related changes. This paper presents a\nstep-by-step concept for hybrid Digital Twin models of process plants using an\nearly implemented prototype as an example. It will detail the steps for\nupdating the first-principles model and Digital Twin of a brownfield process\nsystem using data-driven models of the process equipment. The challenges for\ngeneration of an as-built hybrid Digital Twin will also be discussed. With the\nhelp of process history data to teach Machine Learning models, the implemented\nDigital Twin can be continually improved over time and this work in progress\ncan be further optimized.", "time": "2021-12-03T13:35:33Z", "link": "http://arxiv.org/abs/2112.01903v1", "id": "2112.01903v1", "title": "Hybrid Digital Twin for process industry using Apros simulation\n  environment"}
{"author": "Loris Di Natale, Bratislav Svetozarevic, Philipp Heer, Colin N. Jones", "abstract": "Due to their high energy intensity, buildings play a major role in the\ncurrent worldwide energy transition. Building models are ubiquitous since they\nare needed at each stage of the life of buildings, i.e. for design,\nretrofitting, and control operations. Classical white-box models, based on\nphysical equations, are bound to follow the laws of physics but the specific\ndesign of their underlying structure might hinder their expressiveness and\nhence their accuracy. On the other hand, black-box models are better suited to\ncapture nonlinear building dynamics and thus can often achieve better accuracy,\nbut they require a lot of data and might not follow the laws of physics, a\nproblem that is particularly common for neural network (NN) models. To counter\nthis known generalization issue, physics-informed NNs have recently been\nintroduced, where researchers introduce prior knowledge in the structure of NNs\nto ground them in known underlying physical laws and avoid classical NN\ngeneralization issues.\n  In this work, we present a novel physics-informed NN architecture, dubbed\nPhysically Consistent NN (PCNN), which only requires past operational data and\nno engineering overhead, including prior knowledge in a linear module running\nin parallel to a classical NN. We formally prove that such networks are\nphysically consistent - by design and even on unseen data - with respect to\ndifferent control inputs and temperatures outside and in neighboring zones. We\ndemonstrate their performance on a case study, where the PCNN attains an\naccuracy up to 40% better than a classical physics-based resistance-capacitance\nmodel on 3-day long prediction horizons. Furthermore, despite their constrained\nstructure, PCNNs attain similar performance to classical NNs on the validation\ndata, overfitting the training data less and retaining high expressiveness to\ntackle the generalization issue.", "time": "2022-07-11T08:14:42Z", "link": "http://arxiv.org/abs/2112.03212v3", "id": "2112.03212v3", "title": "Physically Consistent Neural Networks for building thermal modeling:\n  theory and analysis"}
{"author": "Kingsley Nweye, Bo Liu, Peter Stone, Zoltan Nagy", "abstract": "Building upon prior research that highlighted the need for standardizing\nenvironments for building control research, and inspired by recently introduced\nchallenges for real life reinforcement learning control, here we propose a\nnon-exhaustive set of nine real world challenges for reinforcement learning\ncontrol in grid-interactive buildings. We argue that research in this area\nshould be expressed in this framework in addition to providing a standardized\nenvironment for repeatability. Advanced controllers such as model predictive\ncontrol and reinforcement learning (RL) control have both advantages and\ndisadvantages that prevent them from being implemented in real world problems.\nComparisons between the two are rare, and often biased. By focusing on the\nchallenges, we can investigate the performance of the controllers under a\nvariety of situations and generate a fair comparison. As a demonstration, we\nimplement the offline learning challenge in CityLearn and study the impact of\ndifferent levels of domain knowledge and complexity of RL algorithms. We show\nthat the sequence of operations utilized in a rule based controller (RBC) used\nfor offline training affects the performance of the RL agents when evaluated on\na set of four energy flexibility metrics. Longer offline learning from an\noptimized RBC leads to improved performance in the long run. RL agents that\nlearn from a simplified RBC risk poorer performance as the offline learning\nperiod increases. We also observe no impact on performance from information\nsharing amongst agents. We call for a more interdisciplinary effort of the\nresearch community to address the real world challenges, and unlock the\npotential of grid-interactive building", "time": "2022-02-23T21:50:21Z", "link": "http://arxiv.org/abs/2112.06127v2", "id": "2112.06127v2", "title": "Real-world challenges for multi-agent reinforcement learning in\n  grid-interactive buildings"}
{"author": "Pegah Rokhforoz, Olga Fink", "abstract": "This paper proposes a safe reinforcement learning algorithm for generation\nbidding decisions and unit maintenance scheduling in a competitive electricity\nmarket environment. In this problem, each unit aims to find a bidding strategy\nthat maximizes its revenue while concurrently retaining its reliability by\nscheduling preventive maintenance. The maintenance scheduling provides some\nsafety constraints which should be satisfied at all times. Satisfying the\ncritical safety and reliability constraints while the generation units have an\nincomplete information of each others' bidding strategy is a challenging\nproblem. Bi-level optimization and reinforcement learning are state of the art\napproaches for solving this type of problems. However, neither bi-level\noptimization nor reinforcement learning can handle the challenges of incomplete\ninformation and critical safety constraints. To tackle these challenges, we\npropose the safe deep deterministic policy gradient reinforcement learning\nalgorithm which is based on a combination of reinforcement learning and a\npredicted safety filter. The case study demonstrates that the proposed approach\ncan achieve a higher profit compared to other state of the art methods while\nconcurrently satisfying the system safety constraints.", "time": "2021-12-20T11:45:21Z", "link": "http://arxiv.org/abs/2112.10459v1", "id": "2112.10459v1", "title": "Safe multi-agent deep reinforcement learning for joint bidding and\n  maintenance scheduling of generation units"}
{"author": "Jinhao Li, Ruichang Zhang, Hao Wang, Zhi Liu, Hongyang Lai, Yanru Zhang", "abstract": "Renewable energy resources (RERs) have been increasingly integrated into\nlarge-scale distributed power systems. Considering uncertainties and voltage\nfluctuation issues introduced by RERs, in this paper, we propose a deep\nreinforcement learning (DRL)-based strategy leveraging spatial-temporal (ST)\ngraphical information of power systems, to dynamically search for the optimal\noperation, i.e., optimal power flow (OPF), of power systems with a high uptake\nof RERs. Specifically, we formulate the OPF problem as a multi-objective\noptimization problem considering generation cost, voltage fluctuation, and\ntransmission loss, and employ deep deterministic policy gradient (DDPG) to\nlearn an optimal allocation strategy for OPF. Moreover, given that the nodes in\npower systems are self-correlated and interrelated in temporal and spatial\nviews, we develop a multi-grained attention-based spatial-temporal graph\nconvolution network (MG-ASTGCN) for extracting ST graphical correlations and\nfeatures, aiming to provide prior knowledge of power systems for its sequential\nDDPG algorithm to more effectively solve OPF. We validate our algorithm on\nmodified IEEE 33, 69, and 118-bus radial distribution systems and demonstrate\nthat our algorithm outperforms other benchmark algorithms. Our experimental\nresults also reveal that our MG-ASTGCN can significantly accelerate DDPG's\ntraining process and performance in solving OPF.", "time": "2022-08-05T06:54:18Z", "link": "http://arxiv.org/abs/2112.11461v3", "id": "2112.11461v3", "title": "Deep Reinforcement Learning for Optimal Power Flow with Renewables Using\n  Graph Information"}
{"author": "Syed Wali, Irfan Khan", "abstract": "The transformation of conventional power networks into smart grids with the\nheavy penetration level of renewable energy resources, particularly\ngrid-connected Photovoltaic (PV) systems, has increased the need for efficient\nfault identification systems. Malfunctioning any single component in\ngrid-connected PV systems may lead to grid instability and other serious\nconsequences, showing that a reliable fault identification system is the utmost\nrequirement for ensuring operational integrity. Therefore, this paper presents\na novel fault identification approach based on statistical signatures of PV\noperational states. These signatures are unique because each fault has a\ndifferent nature and distinctive impact on the electrical system. Thus, the\nRandom Forest Classifier trained on these extracted signatures showed 100%\naccuracy in identifying all types of faults. Furthermore, the performance\ncomparison of the proposed framework with other Machine Learning classifiers\ndepicts its credibility. Moreover, to elevate user trust in the predicted\noutcomes, SHAP (Shapley Additive Explanation) was utilized during the training\nphase to extract a complete model response (global explanation). This extracted\nglobal explanation can help in the assessment of predicted outcomes credibility\nby decoding each prediction in terms of features contribution. Hence, the\nproposed explainable signature-based fault identification technique is highly\ncredible and fulfills all the requirements of smart grids.", "time": "2021-12-25T15:11:18Z", "link": "http://arxiv.org/abs/2112.14842v1", "id": "2112.14842v1", "title": "Explainable Signature-based Machine Learning Approach for Identification\n  of Faults in Grid-Connected Photovoltaic Systems"}
{"author": "Liang Zhang, Shubin Xie, Jianming Deng", "abstract": "Reinforcement learning (RL) techniques for traffic signal control (TSC) have\ngained increasing popularity in recent years. However, most existing RL-based\nTSC methods tend to focus primarily on the RL model structure while neglecting\nthe significance of proper traffic state representation. Furthermore, some\nRL-based methods heavily rely on expert-designed traffic signal phase\ncompetition. In this paper, we present a novel approach to TSC that utilizes\nqueue length as an efficient state representation. We propose two new methods:\n(1) Max Queue-Length (M-QL), an optimization-based traditional method designed\nbased on the property of queue length; and (2) AttentionLight, an RL model that\nemploys the self-attention mechanism to capture the signal phase correlation\nwithout requiring human knowledge of phase relationships. Comprehensive\nexperiments on multiple real-world datasets demonstrate the effectiveness of\nour approach: (1) the M-QL method outperforms the latest RL-based methods; (2)\nAttentionLight achieves a new state-of-the-art performance; and (3) our results\nhighlight the significance of proper state representation, which is as crucial\nas neural network design in TSC methods. Our findings have important\nimplications for advancing the development of more effective and efficient TSC\nmethods. Our code is released on Github (https://github.\ncom/LiangZhang1996/AttentionLight).", "time": "2023-09-25T07:50:54Z", "link": "http://arxiv.org/abs/2201.00006v3", "id": "2201.00006v3", "title": "Leveraging Queue Length and Attention Mechanisms for Enhanced Traffic\n  Signal Control Optimization"}
{"author": "Erdem AkagÃ¼ndÃ¼z, Oguzhan Cifdaloz", "abstract": "In this paper, we investigate the parameter identification problem in\ndynamical systems through a deep learning approach. Focusing mainly on\nsecond-order, linear time-invariant dynamical systems, the topic of damping\nfactor identification is studied. By utilizing a six-layer deep neural network\nwith different recurrent cells, namely GRUs, LSTMs or BiLSTMs; and by feeding\ninput-output sequence pairs captured from a dynamical system simulator, we\nsearch for an effective deep recurrent architecture in order to resolve damping\nfactor identification problem. Our study results show that, although previously\nnot utilized for this task in the literature, bidirectional gated recurrent\ncells (BiLSTMs) provide better parameter identification results when compared\nto unidirectional gated recurrent memory cells such as GRUs and LSTM. Thus,\nindicating that an input-output sequence pair of finite length, collected from\na dynamical system and when observed anachronistically, may carry information\nin both time directions for prediction of a dynamical systems parameter.", "time": "2021-07-06T07:04:36Z", "link": "http://arxiv.org/abs/2107.02427v1", "id": "2107.02427v1", "title": "Dynamical System Parameter Identification using Deep Recurrent Cell\n  Networks"}
{"author": "Thanh Nguyen, Tung Luu, Trung Pham, Sanzhar Rakhimkul, Chang D. Yoo", "abstract": "Model agnostic meta-learning (MAML) is a popular state-of-the-art\nmeta-learning algorithm that provides good weight initialization of a model\ngiven a variety of learning tasks. The model initialized by provided weight can\nbe fine-tuned to an unseen task despite only using a small amount of samples\nand within a few adaptation steps. MAML is simple and versatile but requires\ncostly learning rate tuning and careful design of the task distribution which\naffects its scalability and generalization. This paper proposes a more robust\nMAML based on an adaptive learning scheme and a prioritization task buffer(PTB)\nreferred to as Robust MAML (RMAML) for improving scalability of training\nprocess and alleviating the problem of distribution mismatch. RMAML uses\ngradient-based hyper-parameter optimization to automatically find the optimal\nlearning rate and uses the PTB to gradually adjust train-ing task distribution\ntoward testing task distribution over the course of training. Experimental\nresults on meta reinforcement learning environments demonstrate a substantial\nperformance gain as well as being less sensitive to hyper-parameter choice and\nrobust to distribution mismatch.", "time": "2021-06-10T13:56:07Z", "link": "http://arxiv.org/abs/2103.08233v2", "id": "2103.08233v2", "title": "Robust MAML: Prioritization task buffer with adaptive learning process\n  for model-agnostic meta-learning"}
{"author": "Xiaojun Li, Jianwei Li, Ali Abdollahi, Trevor Jones", "abstract": "For electric vehicles (EV) and energy storage (ES) batteries, thermal runaway\nis a critical issue as it can lead to uncontrollable fires or even explosions.\nThermal anomaly detection can identify problematic battery packs that may\neventually undergo thermal runaway. However, there are common challenges like\ndata unavailability, environment and configuration variations, and battery\naging. We propose a data-driven method to detect battery thermal anomaly based\non comparing shape-similarity between thermal measurements. Based on their\nshapes, the measurements are continuously being grouped into different\nclusters. Anomaly is detected by monitoring deviations within the clusters.\nUnlike model-based or other data-driven methods, the proposed method is robust\nto data loss and requires minimal reference data for different pack\nconfigurations. As the initial experimental results show, the method not only\ncan be more accurate than the onboard BMS and but also can detect unforeseen\nanomalies at the early stage.", "time": "2021-05-19T23:56:30Z", "link": "http://arxiv.org/abs/2103.08796v2", "id": "2103.08796v2", "title": "Data-driven Thermal Anomaly Detection for Batteries using Unsupervised\n  Shape Clustering"}
{"author": "Ghezlane Halhoul Merabet, Mohamed Essaaidi, Mohamed Ben Haddou, Basheer Qolomany, Junaid Qadir, Muhammad Anan, Ala Al-Fuqaha, Mohamed Riduan Abid, Driss Benhaddou", "abstract": "Building operations represent a significant percentage of the total primary\nenergy consumed in most countries due to the proliferation of Heating,\nVentilation and Air-Conditioning (HVAC) installations in response to the\ngrowing demand for improved thermal comfort. Reducing the associated energy\nconsumption while maintaining comfortable conditions in buildings are\nconflicting objectives and represent a typical optimization problem that\nrequires intelligent system design. Over the last decade, different\nmethodologies based on the Artificial Intelligence (AI) techniques have been\ndeployed to find the sweet spot between energy use in HVAC systems and suitable\nindoor comfort levels to the occupants. This paper performs a comprehensive and\nan in-depth systematic review of AI-based techniques used for building control\nsystems by assessing the outputs of these techniques, and their implementations\nin the reviewed works, as well as investigating their abilities to improve the\nenergy-efficiency, while maintaining thermal comfort conditions. This enables a\nholistic view of (1) the complexities of delivering thermal comfort to users\ninside buildings in an energy-efficient way, and (2) the associated\nbibliographic material to assist researchers and experts in the field in\ntackling such a challenge. Among the 20 AI tools developed for both energy\nconsumption and comfort control, functions such as identification and\nrecognition patterns, optimization, predictive control. Based on the findings\nof this work, the application of AI technology in building control is a\npromising area of research and still an ongoing, i.e., the performance of\nAI-based control is not yet completely satisfactory. This is mainly due in part\nto the fact that these algorithms usually need a large amount of high-quality\nreal-world data, which is lacking in the building or, more precisely, the\nenergy sector.", "time": "2021-04-06T01:04:28Z", "link": "http://arxiv.org/abs/2104.02214v1", "id": "2104.02214v1", "title": "Intelligent Building Control Systems for Thermal Comfort and\n  Energy-Efficiency: A Systematic Review of Artificial Intelligence-Assisted\n  Techniques"}
{"author": "Brittany Davis Pierson, Justine Ventura, Matthew E. Taylor", "abstract": "Reinforcement learning has made great strides in recent years due to the\nsuccess of methods using deep neural networks. However, such neural networks\nact as a black box, obscuring the inner workings. While reinforcement learning\nhas the potential to solve unique problems, a lack of trust and understanding\nof reinforcement learning algorithms could prevent their widespread adoption.\nHere, we present a library that attaches a \"data scraper\" to deep reinforcement\nlearning agents, acting as an observer, and then show how the data collected by\nthe Atari Data Scraper can be used to understand and interpret deep\nreinforcement learning agents. The code for the Atari Data Scraper can be found\nhere: https://github.com/IRLL/Atari-Data-Scraper", "time": "2021-04-11T01:39:33Z", "link": "http://arxiv.org/abs/2104.04893v1", "id": "2104.04893v1", "title": "The Atari Data Scraper"}
{"author": "Haotian Liu, Wenchuan Wu", "abstract": "In Volt/Var control (VVC) of active distribution networks(ADNs), both slow\ntimescale discrete devices (STDDs) and fast timescale continuous devices\n(FTCDs) are involved. The STDDs such as on-load tap changers (OLTC) and FTCDs\nsuch as distributed generators should be coordinated in time sequence. Such VCC\nis formulated as a two-timescale optimization problem to jointly optimize FTCDs\nand STDDs in ADNs. Traditional optimization methods are heavily based on\naccurate models of the system, but sometimes impractical because of their\nunaffordable effort on modelling. In this paper, a novel bi-level off-policy\nreinforcement learning (RL) algorithm is proposed to solve this problem in a\nmodel-free manner. A Bi-level Markov decision process (BMDP) is defined to\ndescribe the two-timescale VVC problem and separate agents are set up for the\nslow and fast timescale sub-problems. For the fast timescale sub-problem, we\nadopt an off-policy RL method soft actor-critic with high sample efficiency.\nFor the slow one, we develop an off-policy multi-discrete soft actor-critic\n(MDSAC) algorithm to address the curse of dimensionality with various STDDs. To\nmitigate the non-stationary issue existing the two agents' learning processes,\nwe propose a multi-timescale off-policy correction (MTOPC) method by adopting\nimportance sampling technique. Comprehensive numerical studies not only\ndemonstrate that the proposed method can achieve stable and satisfactory\noptimization of both STDDs and FTCDs without any model information, but also\nsupport that the proposed method outperforms existing two-timescale VVC\nmethods.", "time": "2021-04-13T02:22:43Z", "link": "http://arxiv.org/abs/2104.05902v1", "id": "2104.05902v1", "title": "Bi-level Off-policy Reinforcement Learning for Volt/VAR Control\n  Involving Continuous and Discrete Devices"}
{"author": "Daniel Weber, Clemens GÃ¼hmann", "abstract": "The application of neural networks to non-linear dynamic system\nidentification tasks has a long history, which consists mostly of\nautoregressive approaches. Autoregression, the usage of the model outputs of\nprevious time steps, is a method of transferring a system state between time\nsteps, which is not necessary for modeling dynamic systems with modern neural\nnetwork structures, such as gated recurrent units (GRUs) and Temporal\nConvolutional Networks (TCNs). We compare the accuracy and execution\nperformance of autoregressive and non-autoregressive implementations of a GRU\nand TCN on the simulation task of three publicly available system\nidentification benchmarks. Our results show, that the non-autoregressive neural\nnetworks are significantly faster and at least as accurate as their\nautoregressive counterparts. Comparisons with other state-of-the-art black-box\nsystem identification methods show, that our implementation of the\nnon-autoregressive GRU is the best performing neural network-based system\nidentification method, and in the benchmarks without extrapolation, the best\nperforming black-box method.", "time": "2021-05-05T12:52:06Z", "link": "http://arxiv.org/abs/2105.02027v1", "id": "2105.02027v1", "title": "Non-Autoregressive vs Autoregressive Neural Networks for System\n  Identification"}
{"author": "Caleb M. Bowyer", "abstract": "The reinforcement learning problem of finding a control policy that minimizes\nthe minimum time objective for the Mountain Car environment is considered.\nParticularly, a class of parameterized nonlinear feedback policies is optimized\nover to reach the top of the highest mountain peak in minimum time. The\noptimization is carried out using quasi-Stochastic Gradient Descent (qSGD)\nmethods. In attempting to find the optimal minimum time policy, a new\nparameterized policy approach is considered that seeks to learn an optimal\npolicy parameter for different regions of the state space, rather than rely on\na single macroscopic policy parameter for the entire state space. This\npartitioned parameterized policy approach is shown to outperform the uniform\nparameterized policy approach and lead to greater generalization than prior\nmethods, where the Mountain Car became trapped in circular trajectories in the\nstate space.", "time": "2021-05-28T17:11:10Z", "link": "http://arxiv.org/abs/2105.13986v1", "id": "2105.13986v1", "title": "Improving Generalization in Mountain Car Through the Partitioned\n  Parameterized Policy Approach via Quasi-Stochastic Gradient Descent"}
{"author": "Xiaoqian Chen, Zhiqiang Gong, Xiaoyu Zhao, Weien Zhou, Wen Yao", "abstract": "Temperature field reconstruction of heat source systems (TFR-HSS) with\nlimited monitoring sensors occurred in thermal management plays an important\nrole in real time health detection system of electronic equipment in\nengineering. However, prior methods with common interpolations usually cannot\nprovide accurate reconstruction performance as required. In addition, there\nexists no public dataset for widely research of reconstruction methods to\nfurther boost the reconstruction performance and engineering applications. To\novercome this problem, this work develops a machine learning modelling\nbenchmark for TFR-HSS task. First, the TFR-HSS task is mathematically modelled\nfrom real-world engineering problem and four types of numerically modellings\nhave been constructed to transform the problem into discrete mapping forms.\nThen, this work proposes a set of machine learning modelling methods, including\nthe general machine learning methods and the deep learning methods, to advance\nthe state-of-the-art methods over temperature field reconstruction. More\nimportantly, this work develops a novel benchmark dataset, namely Temperature\nField Reconstruction Dataset (TFRD), to evaluate these machine learning\nmodelling methods for the TFR-HSS task. Finally, a performance analysis of\ntypical methods is given on TFRD, which can be served as the baseline results\non this benchmark.", "time": "2023-01-03T09:16:49Z", "link": "http://arxiv.org/abs/2108.08298v5", "id": "2108.08298v5", "title": "A Machine Learning Surrogate Modeling Benchmark for Temperature Field\n  Reconstruction of Heat-Source Systems"}
{"author": "Jianhua Jiang, Yangang Ren, Yang Guan, Shengbo Eben Li, Yuming Yin, Xiaoping Jin", "abstract": "Autonomous driving at intersections is one of the most complicated and\naccident-prone traffic scenarios, especially with mixed traffic participants\nsuch as vehicles, bicycles and pedestrians. The driving policy should make safe\ndecisions to handle the dynamic traffic conditions and meet the requirements of\non-board computation. However, most of the current researches focuses on\nsimplified intersections considering only the surrounding vehicles and\nidealized traffic lights. This paper improves the integrated decision and\ncontrol framework and develops a learning-based algorithm to deal with complex\nintersections with mixed traffic flows, which can not only take account of\nrealistic characteristics of traffic lights, but also learn a safe policy under\ndifferent safety constraints. We first consider different velocity models for\ngreen and red lights in the training process and use a finite state machine to\nhandle different modes of light transformation. Then we design different types\nof distance constraints for vehicles, traffic lights, pedestrians, bicycles\nrespectively and formulize the constrained optimal control problems (OCPs) to\nbe optimized. Finally, reinforcement learning (RL) with value and policy\nnetworks is adopted to solve the series of OCPs. In order to verify the safety\nand efficiency of the proposed method, we design a multi-lane intersection with\nthe existence of large-scale mixed traffic participants and set practical\ntraffic light phases. The simulation results indicate that the trained decision\nand control policy can well balance safety and tracking performance. Compared\nwith model predictive control (MPC), the computational time is three orders of\nmagnitude lower.", "time": "2021-08-30T07:55:32Z", "link": "http://arxiv.org/abs/2108.13038v1", "id": "2108.13038v1", "title": "Integrated Decision and Control at Multi-Lane Intersections with Mixed\n  Traffic Flow"}
{"author": "Marcus A. Pereira, Camilo A. Duarte, Ioannis Exarchos, Evangelos A. Theodorou", "abstract": "In this paper, we introduce a novel deep learning based solution to the\nPowered-Descent Guidance (PDG) problem, grounded in principles of nonlinear\nStochastic Optimal Control (SOC) and Feynman-Kac theory. Our algorithm solves\nthe PDG problem by framing it as an $\\mathcal{L}^1$ SOC problem for minimum\nfuel consumption. Additionally, it can handle practically useful control\nconstraints, nonlinear dynamics and enforces state constraints as\nsoft-constraints. This is achieved by building off of recent work on deep\nForward-Backward Stochastic Differential Equations (FBSDEs) and differentiable\nnon-convex optimization neural-network layers based on stochastic search. In\ncontrast to previous approaches, our algorithm does not require convexification\nof the constraints or linearization of the dynamics and is empirically shown to\nbe robust to stochastic disturbances and the initial position of the\nspacecraft. After training offline, our controller can be activated once the\nspacecraft is within a pre-specified radius of the landing zone and at a\npre-specified altitude i.e., the base of an inverted cone with the tip at the\nlanding zone. We demonstrate empirically that our controller can successfully\nand safely land all trajectories initialized at the base of this cone while\nminimizing fuel consumption.", "time": "2021-09-01T04:28:38Z", "link": "http://arxiv.org/abs/2109.00183v1", "id": "2109.00183v1", "title": "Deep $\\mathcal{L}^1$ Stochastic Optimal Control Policies for Planetary\n  Soft-landing"}
{"author": "Trier Mortlock, Deepan Muthirayan, Shih-Yuan Yu, Pramod P. Khargonekar, Mohammad A. Al Faruque", "abstract": "Future manufacturing requires complex systems that connect simulation\nplatforms and virtualization with physical data from industrial processes.\nDigital twins incorporate a physical twin, a digital twin, and the connection\nbetween the two. Benefits of using digital twins, especially in manufacturing,\nare abundant as they can increase efficiency across an entire manufacturing\nlife-cycle. The digital twin concept has become increasingly sophisticated and\ncapable over time, enabled by rises in many technologies. In this paper, we\ndetail the cognitive digital twin as the next stage of advancement of a digital\ntwin that will help realize the vision of Industry 4.0. Cognitive digital twins\nwill allow enterprises to creatively, effectively, and efficiently exploit\nimplicit knowledge drawn from the experience of existing manufacturing systems.\nThey also enable more autonomous decisions and control, while improving the\nperformance across the enterprise (at scale). This paper presents graph\nlearning as one potential pathway towards enabling cognitive functionalities in\nmanufacturing digital twins. A novel approach to realize cognitive digital\ntwins in the product design stage of manufacturing that utilizes graph learning\nis presented.", "time": "2021-09-17T16:34:33Z", "link": "http://arxiv.org/abs/2109.08632v1", "id": "2109.08632v1", "title": "Graph Learning for Cognitive Digital Twins in Manufacturing Systems"}
{"author": "Sai Krishna Sumanth Nakka, Behdad Chalaki, Andreas Malikopoulos", "abstract": "The steady increase in the number of vehicles operating on the highways\ncontinues to exacerbate congestion, accidents, energy consumption, and\ngreenhouse gas emissions. Emerging mobility systems, e.g., connected and\nautomated vehicles (CAVs), have the potential to directly address these issues\nand improve transportation network efficiency and safety. In this paper, we\nconsider a highway merging scenario and propose a framework for coordinating\nCAVs such that stop-and-go driving is eliminated. We use a decentralized form\nof the actor-critic approach to deep reinforcement learning$-$multi-agent deep\ndeterministic policy gradient. We demonstrate the coordination of CAVs through\nnumerical simulations and show that a smooth traffic flow is achieved by\neliminating stop-and-go driving. Videos and plots of the simulation results can\nbe found at this supplemental\n$\\href{https://sites.google.com/view/ud-ids-lab/MADRL}{\\text{site}}$.", "time": "2022-03-14T02:25:20Z", "link": "http://arxiv.org/abs/2109.11672v2", "id": "2109.11672v2", "title": "A Multi-Agent Deep Reinforcement Learning Coordination Framework for\n  Connected and Automated Vehicles at Merging Roadways"}
{"author": "Xian Yeow Lee, Soumik Sarkar, Yubo Wang", "abstract": "Volt-var control (VVC) is the problem of operating power distribution systems\nwithin healthy regimes by controlling actuators in power systems. Existing\nworks have mostly adopted the conventional routine of representing the power\nsystems (a graph with tree topology) as vectors to train deep reinforcement\nlearning (RL) policies. We propose a framework that combines RL with graph\nneural networks and study the benefits and limitations of graph-based policy in\nthe VVC setting. Our results show that graph-based policies converge to the\nsame rewards asymptotically however at a slower rate when compared to vector\nrepresentation counterpart. We conduct further analysis on the impact of both\nobservations and actions: on the observation end, we examine the robustness of\ngraph-based policy on two typical data acquisition errors in power systems,\nnamely sensor communication failure and measurement misalignment. On the action\nend, we show that actuators have various impacts on the system, thus using a\ngraph representation induced by power systems topology may not be the optimal\nchoice. In the end, we conduct a case study to demonstrate that the choice of\nreadout function architecture and graph augmentation can further improve\ntraining performance and robustness.", "time": "2022-06-20T19:57:54Z", "link": "http://arxiv.org/abs/2109.12073v2", "id": "2109.12073v2", "title": "A Graph Policy Network Approach for Volt-Var Control in Power\n  Distribution Systems"}
{"author": "Alexander Pan, Yongkyun Lee, Huan Zhang, Yize Chen, Yuanyuan Shi", "abstract": "Due to the proliferation of renewable energy and its intrinsic intermittency\nand stochasticity, current power systems face severe operational challenges.\nData-driven decision-making algorithms from reinforcement learning (RL) offer a\nsolution towards efficiently operating a clean energy system. Although RL\nalgorithms achieve promising performance compared to model-based control\nmodels, there has been limited investigation of RL robustness in\nsafety-critical physical systems. In this work, we first show that several\ncompetition-winning, state-of-the-art RL agents proposed for power system\ncontrol are vulnerable to adversarial attacks. Specifically, we use an\nadversary Markov Decision Process to learn an attack policy, and demonstrate\nthe potency of our attack by successfully attacking multiple winning agents\nfrom the Learning To Run a Power Network (L2RPN) challenge, under both\nwhite-box and black-box attack settings. We then propose to use adversarial\ntraining to increase the robustness of RL agent against attacks and avoid\ninfeasible operational decisions. To the best of our knowledge, our work is the\nfirst to highlight the fragility of grid control RL algorithms, and contribute\nan effective defense scheme towards improving their robustness and security.", "time": "2021-10-19T01:43:41Z", "link": "http://arxiv.org/abs/2110.08956v2", "id": "2110.08956v2", "title": "Improving Robustness of Reinforcement Learning for Power System Control\n  with Adversarial Training"}
{"author": "Bernardo Aquino, Arash Rahnama, Peter Seiler, Lizhen Lin, Vijay Gupta", "abstract": "Adversarial examples can easily degrade the classification performance in\nneural networks. Empirical methods for promoting robustness to such examples\nhave been proposed, but often lack both analytical insights and formal\nguarantees. Recently, some robustness certificates have appeared in the\nliterature based on system theoretic notions. This work proposes an incremental\ndissipativity-based robustness certificate for neural networks in the form of a\nlinear matrix inequality for each layer. We also propose an equivalent spectral\nnorm bound for this certificate which is scalable to neural networks with\nmultiple layers. We demonstrate the improved performance against adversarial\nattacks on a feed-forward neural network trained on MNIST and an Alexnet\ntrained using CIFAR-10.", "time": "2022-02-14T03:20:54Z", "link": "http://arxiv.org/abs/2111.12906v2", "id": "2111.12906v2", "title": "Robustness against Adversarial Attacks in Neural Networks using\n  Incremental Dissipativity"}
{"author": "Donghwan Lee, Do Wan Kim", "abstract": "The goal of this manuscript is to conduct a controltheoretic analysis of\nTemporal Difference (TD) learning algorithms. TD-learning serves as a\ncornerstone in the realm of reinforcement learning, offering a methodology for\napproximating the value function associated with a given policy in a Markov\nDecision Process. Despite several existing works that have contributed to the\ntheoretical understanding of TD-learning, it is only in recent years that\nresearchers have been able to establish concrete guarantees on its statistical\nefficiency. In this paper, we introduce a finite-time, control-theoretic\nframework for analyzing TD-learning, leveraging established concepts from the\nfield of linear systems control. Consequently, this paper provides additional\ninsights into the mechanics of TD learning and the broader landscape of\nreinforcement learning, all while employing straightforward analytical tools\nderived from control theory.", "time": "2023-09-08T18:37:43Z", "link": "http://arxiv.org/abs/2112.14417v6", "id": "2112.14417v6", "title": "Control Theoretic Analysis of Temporal Difference Learning"}
{"author": "Zhuoqing Song, Lei Shi, Shi Pu, Ming Yan", "abstract": "We consider the decentralized optimization problem, where a network of $n$\nagents aims to collaboratively minimize the average of their individual smooth\nand convex objective functions through peer-to-peer communication in a directed\ngraph. To tackle this problem, we propose two accelerated gradient tracking\nmethods, namely APD and APD-SC, for non-strongly convex and strongly convex\nobjective functions, respectively. We show that APD and APD-SC converge at the\nrates $O\\left(\\frac{1}{k^2}\\right)$ and $O\\left(\\left(1 -\nC\\sqrt{\\frac{\\mu}{L}}\\right)^k\\right)$, respectively, up to constant factors\ndepending only on the mixing matrix. APD and APD-SC are the first decentralized\nmethods over unbalanced directed graphs that achieve the same provable\nacceleration as centralized methods. Numerical experiments demonstrate the\neffectiveness of both methods.", "time": "2023-12-06T12:52:40Z", "link": "http://arxiv.org/abs/2107.12065v2", "id": "2107.12065v2", "title": "Provably Accelerated Decentralized Gradient Method Over Unbalanced\n  Directed Graphs"}
{"author": "Jiaqi Zhang, Keyou You, Lihua Xie", "abstract": "Information compression is essential to reduce communication cost in\ndistributed optimization over peer-to-peer networks. This paper proposes a\ncommunication-efficient linearly convergent distributed (COLD) algorithm to\nsolve strongly convex optimization problems. By compressing innovation vectors,\nwhich are the differences between decision vectors and their estimates, COLD is\nable to achieve linear convergence for a class of $\\delta$-contracted\ncompressors. We explicitly quantify how the compression affects the convergence\nrate and show that COLD matches the same rate of its uncompressed version. To\naccommodate a wider class of compressors that includes the binary quantizer, we\nfurther design a novel dynamical scaling mechanism and obtain the linearly\nconvergent Dyna-COLD. Importantly, our results strictly improve existing\nresults for the quantized consensus problem. Numerical experiments demonstrate\nthe advantages of both algorithms under different compressors.", "time": "2021-05-14T08:15:18Z", "link": "http://arxiv.org/abs/2105.06697v1", "id": "2105.06697v1", "title": "Innovation Compression for Communication-efficient Distributed\n  Optimization with Linear Convergence"}
{"author": "Bing Luo, Xiang Li, Shiqiang Wang, Jianwei Huang, Leandros Tassiulas", "abstract": "Federated learning (FL) is a distributed learning paradigm that enables a\nlarge number of mobile devices to collaboratively learn a model under the\ncoordination of a central server without sharing their raw data. Despite its\npractical efficiency and effectiveness, the iterative on-device learning\nprocess (e.g., local computations and global communications with the server)\nincurs a considerable cost in terms of learning time and energy consumption,\nwhich depends crucially on the number of selected clients and the number of\nlocal iterations in each training round. In this paper, we analyze how to\ndesign adaptive FL in mobile edge networks that optimally chooses these\nessential control variables to minimize the total cost while ensuring\nconvergence. We establish the analytical relationship between the total cost\nand the control variables with the convergence upper bound. To efficiently\nsolve the cost minimization problem, we develop a low-cost sampling-based\nalgorithm to learn the convergence related unknown parameters. We derive\nimportant solution properties that effectively identify the design principles\nfor different optimization metrics. Practically, we evaluate our theoretical\nresults both in a simulated environment and on a hardware prototype.\nExperimental evidence verifies our derived properties and demonstrates that our\nproposed solution achieves near-optimal performance for different optimization\nmetrics for various datasets and heterogeneous system and statistical settings.", "time": "2021-09-12T03:02:24Z", "link": "http://arxiv.org/abs/2109.05411v1", "id": "2109.05411v1", "title": "Cost-Effective Federated Learning in Mobile Edge Networks"}
{"author": "Francois Gauthier, Vinay Chakravarthi Gogineni, Stefan Werner, Yih-Fang Huang, Anthony Kuh", "abstract": "Many assumptions in the federated learning literature present a best-case\nscenario that can not be satisfied in most real-world applications. An\nasynchronous setting reflects the realistic environment in which federated\nlearning methods must be able to operate reliably. Besides varying amounts of\nnon-IID data at participants, the asynchronous setting models heterogeneous\nclient participation due to available computational power and battery\nconstraints and also accounts for delayed communications between clients and\nthe server. To reduce the communication overhead associated with asynchronous\nonline federated learning (ASO-Fed), we use the principles of\npartial-sharing-based communication. In this manner, we reduce the\ncommunication load of the participants and, therefore, render participation in\nthe learning task more accessible. We prove the convergence of the proposed\nASO-Fed and provide simulations to analyze its behavior further. The\nsimulations reveal that, in the asynchronous setting, it is possible to achieve\nthe same convergence as the federated stochastic gradient (Online-FedSGD) while\nreducing the communication tenfold.", "time": "2021-11-27T16:41:30Z", "link": "http://arxiv.org/abs/2111.13931v1", "id": "2111.13931v1", "title": "Resource-Aware Asynchronous Online Federated Learning for Nonlinear\n  Regression"}
{"author": "Dave Dice, Alex Kogan", "abstract": "The Transformer architecture revolutionized the field of natural language\nprocessing (NLP). Transformers-based models (e.g., BERT) power many important\nWeb services, such as search, translation, question-answering, etc. While\nenormous research attention is paid to the training of those models, relatively\nlittle efforts are made to improve their inference performance. This paper\ncomes to address this gap by presenting an empirical analysis of scalability\nand performance of inferencing a Transformer-based model on CPUs. Focusing on\nthe highly popular BERT model, we identify key components of the Transformer\narchitecture where the bulk of the computation happens, and propose three\noptimizations to speed them up. The optimizations are evaluated using the\ninference benchmark from HuggingFace, and are shown to achieve the speedup of\nup to x2.37. The considered optimizations do not require any changes to the\nimplementation of the models nor affect their accuracy.", "time": "2021-02-22T16:54:34Z", "link": "http://arxiv.org/abs/2102.06621v3", "id": "2102.06621v3", "title": "Optimizing Inference Performance of Transformers on CPUs"}
{"author": "PaweÅ Renc, Patryk Orzechowski, Aleksander Byrski, JarosÅaw WÄs, Jason H. Moore", "abstract": "Biclustering is a data mining technique which searches for local patterns in\nnumeric tabular data with main application in bioinformatics. This technique\nhas shown promise in multiple areas, including development of biomarkers for\ncancer, disease subtype identification, or gene-drug interactions among others.\nIn this paper we introduce EBIC.JL - an implementation of one of the most\naccurate biclustering algorithms in Julia, a modern highly parallelizable\nprogramming language for data science. We show that the new version maintains\ncomparable accuracy to its predecessor EBIC while converging faster for the\nmajority of the problems. We hope that this open source software in a\nhigh-level programming language will foster research in this promising field of\nbioinformatics and expedite development of new biclustering methods for big\ndata.", "time": "2021-05-03T22:30:38Z", "link": "http://arxiv.org/abs/2105.01196v1", "id": "2105.01196v1", "title": "EBIC.JL -- an Efficient Implementation of Evolutionary Biclustering\n  Algorithm in Julia"}
{"author": "Hang Chen, Syed Ali Asif, Jihong Park, Chien-Chung Shen, Mehdi Bennis", "abstract": "Federated learning (FL) is a promising distributed learning solution that\nonly exchanges model parameters without revealing raw data. However, the\ncentralized architecture of FL is vulnerable to the single point of failure. In\naddition, FL does not examine the legitimacy of local models, so even a small\nfraction of malicious devices can disrupt global training. To resolve these\nrobustness issues of FL, in this paper, we propose a blockchain-based\ndecentralized FL framework, termed VBFL, by exploiting two mechanisms in a\nblockchained architecture. First, we introduced a novel decentralized\nvalidation mechanism such that the legitimacy of local model updates is\nexamined by individual validators. Second, we designed a dedicated\nproof-of-stake consensus mechanism where stake is more frequently rewarded to\nhonest devices, which protects the legitimate local model updates by increasing\ntheir chances of dictating the blocks appended to the blockchain. Together,\nthese solutions promote more federation within legitimate devices, enabling\nrobust FL. Our emulation results of the MNIST classification corroborate that\nwith 15% of malicious devices, VBFL achieves 87% accuracy, which is 7.4x higher\nthan Vanilla FL.", "time": "2021-01-09T06:30:38Z", "link": "http://arxiv.org/abs/2101.03300v1", "id": "2101.03300v1", "title": "Robust Blockchained Federated Learning with Model Validation and\n  Proof-of-Stake Inspired Consensus"}
{"author": "Jean-Francois Rajotte, Sumit Mukherjee, Caleb Robinson, Anthony Ortiz, Christopher West, Juan Lavista Ferres, Raymond T Ng", "abstract": "We introduce FELICIA (FEderated LearnIng with a CentralIzed Adversary) a\ngenerative mechanism enabling collaborative learning. In particular, we show\nhow a data owner with limited and biased data could benefit from other data\nowners while keeping data from all the sources private. This is a common\nscenario in medical image analysis where privacy legislation prevents data from\nbeing shared outside local premises. FELICIA works for a large family of\nGenerative Adversarial Networks (GAN) architectures including vanilla and\nconditional GANs as demonstrated in this work. We show that by using the\nFELICIA mechanism, a data owner with limited image samples can generate\nhigh-quality synthetic images with high utility while neither data owners has\nto provide access to its data. The sharing happens solely through a central\ndiscriminator that has access limited to synthetic data. Here, utility is\ndefined as classification performance on a real test set. We demonstrate these\nbenefits on several realistic healthcare scenarions using benchmark image\ndatasets (MNIST, CIFAR-10) as well as on medical images for the task of skin\nlesion classification. With multiple experiments, we show that even in the\nworst cases, combining FELICIA with real data gracefully achieves performance\non par with real data while most results significantly improves the utility.", "time": "2021-08-29T01:20:38Z", "link": "http://arxiv.org/abs/2101.07235v2", "id": "2101.07235v2", "title": "Reducing bias and increasing utility by federated generative modeling of\n  medical images using a centralized adversary"}
{"author": "Dennis Bautembach, Iason Oikonomidis, Antonis Argyros", "abstract": "We present two novel optimizations that accelerate clock-based spiking neural\nnetwork (SNN) simulators. The first one targets spike timing dependent\nplasticity (STDP). It combines lazy- with event-driven plasticity and\nefficiently facilitates the computation of pre- and post-synaptic spikes using\nbitfields and integer intrinsics. It offers higher bandwidth than event-driven\nplasticity alone and achieves a 1.5x-2x speedup over our closest competitor.\nThe second optimization targets spike delivery. We partition our graph\nrepresentation in a way that bounds the number of neurons that need be updated\nat any given time which allows us to perform said update in shared memory\ninstead of global memory. This is 2x-2.5x faster than our closest competitor.\nBoth optimizations represent the final evolutionary stages of years of\niteration on STDP and spike delivery inside \"Spice\" (/spaIk/), our state of the\nart SNN simulator. The proposed optimizations are not exclusive to our graph\nrepresentation or pipeline but are applicable to a multitude of simulator\ndesigns. We evaluate our performance on three well-established models and\ncompare ourselves against three other state of the art simulators.", "time": "2021-08-23T22:57:54Z", "link": "http://arxiv.org/abs/2107.04092v2", "id": "2107.04092v2", "title": "Even Faster SNN Simulation with Lazy+Event-driven Plasticity and Shared\n  Atomics"}
{"author": "Samuel Yen-Chi Chen, Shinjae Yoo", "abstract": "Distributed training across several quantum computers could significantly\nimprove the training time and if we could share the learned model, not the\ndata, it could potentially improve the data privacy as the training would\nhappen where the data is located. However, to the best of our knowledge, no\nwork has been done in quantum machine learning (QML) in federation setting yet.\nIn this work, we present the federated training on hybrid quantum-classical\nmachine learning models although our framework could be generalized to pure\nquantum machine learning model. Specifically, we consider the quantum neural\nnetwork (QNN) coupled with classical pre-trained convolutional model. Our\ndistributed federated learning scheme demonstrated almost the same level of\ntrained model accuracies and yet significantly faster distributed training. It\ndemonstrates a promising future research direction for scaling and privacy\naspects.", "time": "2021-03-22T17:00:19Z", "link": "http://arxiv.org/abs/2103.12010v1", "id": "2103.12010v1", "title": "Federated Quantum Machine Learning"}
{"author": "Haoyu Ren, Darko Anicic, Thomas Runkler", "abstract": "Focusing on comprehensive networking, big data, and artificial intelligence,\nthe Industrial Internet-of-Things (IIoT) facilitates efficiency and robustness\nin factory operations. Various sensors and field devices play a central role,\nas they generate a vast amount of real-time data that can provide insights into\nmanufacturing. The synergy of complex event processing (CEP) and machine\nlearning (ML) has been developed actively in the last years in IIoT to identify\npatterns in heterogeneous data streams and fuse raw data into tangible facts.\nIn a traditional compute-centric paradigm, the raw field data are continuously\nsent to the cloud and processed centrally. As IIoT devices become increasingly\npervasive and ubiquitous, concerns are raised since transmitting such amount of\ndata is energy-intensive, vulnerable to be intercepted, and subjected to high\nlatency. The data-centric paradigm can essentially solve these problems by\nempowering IIoT to perform decentralized on-device ML and CEP, keeping data\nprimarily on edge devices and minimizing communications. However, this is no\nmean feat because most IIoT edge devices are designed to be computationally\nconstrained with low power consumption. This paper proposes a framework that\nexploits ML and CEP's synergy at the edge in distributed sensor networks. By\nleveraging tiny ML and micro CEP, we shift the computation from the cloud to\nthe power-constrained IIoT devices and allow users to adapt the on-device ML\nmodel and the CEP reasoning logic flexibly on the fly without requiring to\nreupload the whole program. Lastly, we evaluate the proposed solution and show\nits effectiveness and feasibility using an industrial use case of machine\nsafety monitoring.", "time": "2021-05-04T14:58:48Z", "link": "http://arxiv.org/abs/2105.03371v1", "id": "2105.03371v1", "title": "The Synergy of Complex Event Processing and Tiny Machine Learning in\n  Industrial IoT"}
{"author": "Jaehoon Koo, Prasanna Balaprakash, Michael Kruse, Xingfu Wu, Paul Hovland, Mary Hall", "abstract": "Polly is the LLVM project's polyhedral loop nest optimizer. Recently,\nuser-directed loop transformation pragmas were proposed based on LLVM/Clang and\nPolly. The search space exposed by the transformation pragmas is a tree,\nwherein each node represents a specific combination of loop transformations\nthat can be applied to the code resulting from the parent node's loop\ntransformations. We have developed a search algorithm based on Monte Carlo tree\nsearch (MCTS) to find the best combination of loop transformations. Our\nalgorithm consists of two phases: exploring loop transformations at different\ndepths of the tree to identify promising regions in the tree search space and\nexploiting those regions by performing a local search. Moreover, a restart\nmechanism is used to avoid the MCTS getting trapped in a local solution. The\nbest and worst solutions are transferred from the previous phases of the\nrestarts to leverage the search history. We compare our approach with random,\ngreedy, and breadth-first search methods on PolyBench kernels and ECP proxy\napplications. Experimental results show that our MCTS algorithm finds pragma\ncombinations with a speedup of 2.3x over Polly's heuristic optimizations on\naverage.", "time": "2021-05-10T21:57:39Z", "link": "http://arxiv.org/abs/2105.04555v1", "id": "2105.04555v1", "title": "Customized Monte Carlo Tree Search for LLVM/Polly's Composable Loop\n  Optimization Transformations"}
{"author": "Kaixin Zhang, Hongzhi Wang, Han Hu, Songling Zou, Jiye Qiu, Tongxin Li, Zhishun Wang", "abstract": "Recently, deep learning has been an area of intense research. However, as a\nkind of computing-intensive task, deep learning highly relies on the scale of\nGPU memory, which is usually prohibitive and scarce. Although some extensive\nworks have been proposed for dynamic GPU memory management, they are hard to\napply to systems with multiple dynamic workloads, such as in-database machine\nlearning systems.\n  In this paper, we demonstrated TENSILE, a method of managing GPU memory in\ntensor granularity to reduce the GPU memory peak, considering the multiple\ndynamic workloads. TENSILE tackled the cold-starting and across-iteration\nscheduling problem existing in previous works. We implemented TENSILE on a deep\nlearning framework built by ourselves and evaluated its performance. The\nexperiment results show that TENSILE can save more GPU memory with less extra\noverhead than prior works in single and multiple dynamic workloads scenarios.", "time": "2022-11-06T09:50:16Z", "link": "http://arxiv.org/abs/2105.13336v5", "id": "2105.13336v5", "title": "TENSILE: A Tensor granularity dynamic GPU memory scheduling method\n  toward multiple dynamic workloads system"}
{"author": "Siddharth Singh, Abhinav Bhatele", "abstract": "In the last few years, the memory requirements to train state-of-the-art\nneural networks have far exceeded the DRAM capacities of modern hardware\naccelerators. This has necessitated the development of efficient algorithms to\ntrain these neural networks in parallel on large-scale GPU-based clusters.\nSince computation is relatively inexpensive on modern GPUs, designing and\nimplementing extremely efficient communication in these parallel training\nalgorithms is critical for extracting the maximum performance. This paper\npresents AxoNN, a parallel deep learning framework that exploits asynchrony and\nmessage-driven execution to schedule neural network operations on each GPU,\nthereby reducing GPU idle time and maximizing hardware efficiency. By using the\nCPU memory as a scratch space for offloading data periodically during training,\nAxoNN is able to reduce GPU memory consumption by four times. This allows us to\nincrease the number of parameters per GPU by four times, thus reducing the\namount of communication and increasing performance by over 13%. When tested\nagainst large transformer models with 12-100 billion parameters on 48-384\nNVIDIA Tesla V100 GPUs, AxoNN achieves a per-GPU throughput of 49.4-54.78% of\ntheoretical peak and reduces the training time by 22-37 days (15-25% speedup)\nas compared to the state-of-the-art.", "time": "2023-05-14T04:38:38Z", "link": "http://arxiv.org/abs/2110.13005v5", "id": "2110.13005v5", "title": "AxoNN: An asynchronous, message-driven parallel framework for\n  extreme-scale deep learning"}
{"author": "Shenggui Li, Hongxin Liu, Zhengda Bian, Jiarui Fang, Haichen Huang, Yuliang Liu, Boxiang Wang, Yang You", "abstract": "The success of Transformer models has pushed the deep learning model scale to\nbillions of parameters. Due to the limited memory resource of a single GPU,\nHowever, the best practice for choosing the optimal parallel strategy is still\nlacking, since it requires domain expertise in both deep learning and parallel\ncomputing.\n  The Colossal-AI system addressed the above challenge by introducing a unified\ninterface to scale your sequential code of model training to distributed\nenvironments. It supports parallel training methods such as data, pipeline,\ntensor, and sequence parallelism, as well as heterogeneous training methods\nintegrated with zero redundancy optimizer. Compared to the baseline system,\nColossal-AI can achieve up to 2.76 times training speedup on large-scale\nmodels.", "time": "2023-10-05T04:09:09Z", "link": "http://arxiv.org/abs/2110.14883v3", "id": "2110.14883v3", "title": "Colossal-AI: A Unified Deep Learning System For Large-Scale Parallel\n  Training"}
{"author": "Yujia Wang, Lu Lin, Jinghui Chen", "abstract": "Due to the explosion in the size of the training datasets, distributed\nlearning has received growing interest in recent years. One of the major\nbottlenecks is the large communication cost between the central server and the\nlocal workers. While error feedback compression has been proven to be\nsuccessful in reducing communication costs with stochastic gradient descent\n(SGD), there are much fewer attempts in building communication-efficient\nadaptive gradient methods with provable guarantees, which are widely used in\ntraining large-scale machine learning models. In this paper, we propose a new\ncommunication-compressed AMSGrad for distributed nonconvex optimization\nproblem, which is provably efficient. Our proposed distributed learning\nframework features an effective gradient compression strategy and a worker-side\nmodel update design. We prove that the proposed communication-efficient\ndistributed adaptive gradient method converges to the first-order stationary\npoint with the same iteration complexity as uncompressed vanilla AMSGrad in the\nstochastic nonconvex optimization setting. Experiments on various benchmarks\nback up our theory.", "time": "2022-02-24T00:56:56Z", "link": "http://arxiv.org/abs/2111.00705v2", "id": "2111.00705v2", "title": "Communication-Compressed Adaptive Gradient Method for Distributed\n  Nonconvex Optimization"}
{"author": "Shuo Wan, Jiaxun Lu, Pingyi Fan, Yunfeng Shao, Chenghui Peng, Khaled B. Letaief", "abstract": "Federated learning (FL) has recently emerged as a transformative paradigm\nthat jointly train a model with distributed data sets in IoT while avoiding the\nneed for central data collection. Due to the limited observation range, such\ndata sets can only reflect local information, which limits the quality of\ntrained models. In practice, the global information and local observations\nwould require a joint consideration for learning to make a reasonable policy.\nHowever, in horizontal FL, the central agency only acts as a model aggregator\nwithout utilizing its global observation to further improve the model. This\ncould significantly degrade the performance in some missions such as traffic\nflow prediction in network systems, where the global information may enhance\nthe accuracy. Meanwhile, the global feature may not be directly transmitted to\nagents for data security. How to utilize the global observation residing in the\ncentral agency while protecting its safety thus rises up as an important\nproblem in FL. In this paper, we develop a vertical-horizontal federated\nlearning (VHFL) process, where the global feature is shared with the agents in\na procedure similar to that of vertical FL without any extra communication\nrounds. By considering the delay and packet loss, we will analyze VHFL\nconvergence and validate its performance by experiments. It is shown that the\nproposed VHFL could enhance the accuracy compared with horizontal FL while\nstill protecting the security of global data.", "time": "2021-12-10T09:35:19Z", "link": "http://arxiv.org/abs/2112.01039v3", "id": "2112.01039v3", "title": "How global observation works in Federated Learning: Integrating vertical\n  training into Horizontal Federated Learning"}
{"author": "Bing Luo, Wenli Xiao, Shiqiang Wang, Jianwei Huang, Leandros Tassiulas", "abstract": "Federated learning (FL) algorithms usually sample a fraction of clients in\neach round (partial participation) when the number of participants is large and\nthe server's communication bandwidth is limited. Recent works on the\nconvergence analysis of FL have focused on unbiased client sampling, e.g.,\nsampling uniformly at random, which suffers from slow wall-clock time for\nconvergence due to high degrees of system heterogeneity and statistical\nheterogeneity. This paper aims to design an adaptive client sampling algorithm\nthat tackles both system and statistical heterogeneity to minimize the\nwall-clock convergence time. We obtain a new tractable convergence bound for FL\nalgorithms with arbitrary client sampling probabilities. Based on the bound, we\nanalytically establish the relationship between the total learning time and\nsampling probabilities, which results in a non-convex optimization problem for\ntraining time minimization. We design an efficient algorithm for learning the\nunknown parameters in the convergence bound and develop a low-complexity\nalgorithm to approximately solve the non-convex problem. Experimental results\nfrom both hardware prototype and simulation demonstrate that our proposed\nsampling scheme significantly reduces the convergence time compared to several\nbaseline sampling schemes. Notably, our scheme in hardware prototype spends 73%\nless time than the uniform sampling baseline for reaching the same target loss.", "time": "2021-12-21T14:28:40Z", "link": "http://arxiv.org/abs/2112.11256v1", "id": "2112.11256v1", "title": "Tackling System and Statistical Heterogeneity for Federated Learning\n  with Adaptive Client Sampling"}
{"author": "Kaiqing Zhang, Xiangyuan Zhang, Bin Hu, Tamer BaÅar", "abstract": "Direct policy search serves as one of the workhorses in modern reinforcement\nlearning (RL), and its applications in continuous control tasks have recently\nattracted increasing attention. In this work, we investigate the convergence\ntheory of policy gradient (PG) methods for learning the linear risk-sensitive\nand robust controller. In particular, we develop PG methods that can be\nimplemented in a derivative-free fashion by sampling system trajectories, and\nestablish both global convergence and sample complexity results in the\nsolutions of two fundamental settings in risk-sensitive and robust control: the\nfinite-horizon linear exponential quadratic Gaussian, and the finite-horizon\nlinear-quadratic disturbance attenuation problems. As a by-product, our results\nalso provide the first sample complexity for the global convergence of PG\nmethods on solving zero-sum linear-quadratic dynamic games, a\nnonconvex-nonconcave minimax optimization problem that serves as a baseline\nsetting in multi-agent reinforcement learning (MARL) with continuous spaces.\nOne feature of our algorithms is that during the learning phase, a certain\nlevel of robustness/risk-sensitivity of the controller is preserved, which we\ntermed as the implicit regularization property, and is an essential requirement\nin safety-critical control systems.", "time": "2021-12-30T19:55:28Z", "link": "http://arxiv.org/abs/2101.01041v3", "id": "2101.01041v3", "title": "Derivative-Free Policy Optimization for Linear Risk-Sensitive and Robust\n  Control Design: Implicit Regularization and Sample Complexity"}
{"author": "Lili Zhu, Petros Spachos", "abstract": "Food quality and safety are of great concern to society since it is an\nessential guarantee not only for human health but also for social development,\nand stability. Ensuring food quality and safety is a complex process. All food\nprocessing stages should be considered, from cultivating, harvesting and\nstorage to preparation and consumption. Grading is one of the essential\nprocesses to control food quality. This paper proposed a mobile visual-based\nsystem to evaluate food grading. Specifically, the proposed system acquires\nimages of bananas when they are on moving conveyors. A two-layer image\nprocessing system based on machine learning is used to grade bananas, and these\ntwo layers are allocated on edge devices and cloud servers, respectively.\nSupport Vector Machine (SVM) is the first layer to classify bananas based on an\nextracted feature vector composed of color and texture features. Then, the a\nYou Only Look Once (YOLO) v3 model further locating the peel's defected area\nand determining if the inputs belong to the mid-ripened or well-ripened class.\nAccording to experimental results, the first layer's performance achieved an\naccuracy of 98.5% while the accuracy of the second layer is 85.7%, and the\noverall accuracy is 96.4%.", "time": "2021-01-05T09:01:06Z", "link": "http://arxiv.org/abs/2101.01418v1", "id": "2101.01418v1", "title": "Support Vector Machine and YOLO for a Mobile Food Grading System"}
{"author": "Christopher KÃ¶nig, Matteo Turchetta, John Lygeros, Alisa Rupenyan, Andreas Krause", "abstract": "Adaptive control approaches yield high-performance controllers when a precise\nsystem model or suitable parametrizations of the controller are available.\nExisting data-driven approaches for adaptive control mostly augment standard\nmodel-based methods with additional information about uncertainties in the\ndynamics or about disturbances. In this work, we propose a purely data-driven,\nmodel-free approach for adaptive control. Tuning low-level controllers based\nsolely on system data raises concerns on the underlying algorithm safety and\ncomputational performance. Thus, our approach builds on GoOSE, an algorithm for\nsafe and sample-efficient Bayesian optimization. We introduce several\ncomputational and algorithmic modifications in GoOSE that enable its practical\nuse on a rotational motion system. We numerically demonstrate for several types\nof disturbances that our approach is sample efficient, outperforms constrained\nBayesian optimization in terms of safety, and achieves the performance optima\ncomputed by grid evaluation. We further demonstrate the proposed adaptive\ncontrol approach experimentally on a rotational motion system.", "time": "2021-03-02T13:26:34Z", "link": "http://arxiv.org/abs/2101.07825v2", "id": "2101.07825v2", "title": "Safe and Efficient Model-free Adaptive Control via Bayesian Optimization"}
{"author": "Chao Yan, Xiaojia Xiang, Chang Wang, Zhen Lan", "abstract": "Developing the flocking behavior for a dynamic squad of fixed-wing UAVs is\nstill a challenge due to kinematic complexity and environmental uncertainty. In\nthis paper, we deal with the decentralized flocking and collision avoidance\nproblem through deep reinforcement learning (DRL). Specifically, we formulate a\ndecentralized DRL-based decision making framework from the perspective of every\nfollower, where a collision avoidance mechanism is integrated into the flocking\ncontroller. Then, we propose a novel reinforcement learning algorithm PS-CACER\nfor training a shared control policy for all the followers. Besides, we design\na plug-n-play embedding module based on convolutional neural networks and the\nattention mechanism. As a result, the variable-length system state can be\nencoded into a fixed-length embedding vector, which makes the learned DRL\npolicy independent with the number and the order of followers. Finally,\nnumerical simulation results demonstrate the effectiveness of the proposed\nmethod, and the learned policies can be directly transferred to semi-physical\nsimulation without any parameter finetuning.", "time": "2021-07-22T11:37:13Z", "link": "http://arxiv.org/abs/2101.08074v2", "id": "2101.08074v2", "title": "Flocking and Collision Avoidance for a Dynamic Squad of Fixed-Wing UAVs\n  Using Deep Reinforcement Learning"}
{"author": "Carter Blum, Hao Liu, Hui Xiong", "abstract": "Electric vehicles have been rapidly increasing in usage, but stations to\ncharge them have not always kept up with demand, so efficient routing of\nvehicles to stations is critical to operating at maximum efficiency. Deciding\nwhich stations to recommend drivers to is a complex problem with a multitude of\npossible recommendations, volatile usage patterns and temporally extended\nconsequences of recommendations. Reinforcement learning offers a powerful\nparadigm for solving sequential decision-making problems, but traditional\nmethods may struggle with sample efficiency due to the high number of possible\nactions. By developing a model that allows complex representations of actions,\nwe improve outcomes for users of our system by over 30% when compared to\nexisting baselines in a simulation. If implemented widely, these better\nrecommendations can globally save over 4 million person-hours of waiting and\ndriving each year.", "time": "2021-01-28T21:25:33Z", "link": "http://arxiv.org/abs/2102.00847v1", "id": "2102.00847v1", "title": "CoordiQ : Coordinated Q-learning for Electric Vehicle Charging\n  Recommendation"}
{"author": "Guyue Huang, Jingbo Hu, Yifan He, Jialong Liu, Mingyuan Ma, Zhaoyang Shen, Juejian Wu, Yuanfan Xu, Hengrui Zhang, Kai Zhong, Xuefei Ning, Yuzhe Ma, Haoyu Yang, Bei Yu, Huazhong Yang, Yu Wang", "abstract": "With the down-scaling of CMOS technology, the design complexity of very\nlarge-scale integrated (VLSI) is increasing. Although the application of\nmachine learning (ML) techniques in electronic design automation (EDA) can\ntrace its history back to the 90s, the recent breakthrough of ML and the\nincreasing complexity of EDA tasks have aroused more interests in incorporating\nML to solve EDA tasks. In this paper, we present a comprehensive review of\nexisting ML for EDA studies, organized following the EDA hierarchy.", "time": "2021-03-08T08:18:35Z", "link": "http://arxiv.org/abs/2102.03357v2", "id": "2102.03357v2", "title": "Machine Learning for Electronic Design Automation: A Survey"}
{"author": "Yuhang Zhang, Yao Mu, Yujie Yang, Yang Guan, Shengbo Eben Li, Qi Sun, Jianyu Chen", "abstract": "Reinforcement learning has shown great potential in developing high-level\nautonomous driving. However, for high-dimensional tasks, current RL methods\nsuffer from low data efficiency and oscillation in the training process. This\npaper proposes an algorithm called Learn to drive with Virtual Memory (LVM) to\novercome these problems. LVM compresses the high-dimensional information into\ncompact latent states and learns a latent dynamic model to summarize the\nagent's experience. Various imagined latent trajectories are generated as\nvirtual memory by the latent dynamic model. The policy is learned by\npropagating gradient through the learned latent model with the imagined latent\ntrajectories and thus leads to high data efficiency. Furthermore, a double\ncritic structure is designed to reduce the oscillation during the training\nprocess. The effectiveness of LVM is demonstrated by an image-input autonomous\ndriving task, in which LVM outperforms the existing method in terms of data\nefficiency, learning stability, and control performance.", "time": "2021-02-16T10:46:52Z", "link": "http://arxiv.org/abs/2102.08072v1", "id": "2102.08072v1", "title": "Steadily Learn to Drive with Virtual Memory"}
{"author": "Yunhan Huang, Quanyan Zhu", "abstract": "In this paper, we study Markov Decision Processes (MDPs) with self-triggered\nstrategies, where the idea of self-triggered control is extended to more\ngeneric MDP models. This extension broadens the application of self-triggering\npolicies to a broader range of systems. We study the co-design problems of the\ncontrol policy and the triggering policy to optimize two pre-specified cost\ncriteria. The first cost criterion is introduced by incorporating a\npre-specified update penalty into the traditional MDP cost criteria to reduce\nthe use of communication resources. Under this criteria, a novel dynamic\nprogramming (DP) equation called DP equation with optimized lookahead to\nproposed to solve for the self-triggering policy under this criteria. The\nsecond self-triggering policy is to maximize the triggering time while still\nguaranteeing a pre-specified level of sub-optimality. Theoretical underpinnings\nare established for the computation and implementation of both policies.\nThrough a gridworld numerical example, we illustrate the two policies'\neffectiveness in reducing sources consumption and demonstrate the trade-offs\nbetween resource consumption and system performance.", "time": "2021-02-17T04:41:44Z", "link": "http://arxiv.org/abs/2102.08571v1", "id": "2102.08571v1", "title": "Self-Triggered Markov Decision Processes"}
{"author": "Mohammad Fattahi Sani, Raimondo Ascione, Sanja Dogramadzi", "abstract": "Purpose: Recent developments in robotics and artificial intelligence (AI)\nhave led to significant advances in healthcare technologies enhancing\nrobot-assisted minimally invasive surgery (RAMIS) in some surgical specialties.\nHowever, current human-robot interfaces lack intuitive teleoperation and cannot\nmimic surgeon's hand/finger sensing and fine motion. These limitations make\ntele-operated robotic surgery not suitable for micro-surgery and difficult to\nlearn for established surgeons. We report a pilot study showing an intuitive\nway of recording and mapping surgeon's gross hand motion and the fine synergic\nmotion during cardiac micro-surgery as a way to enhance future intuitive\nteleoperation. Methods: We set to develop a prototype system able to train a\nDeep Neural Net-work (DNN) by mapping wrist, hand and surgical tool real-time\ndata acquisition(RTDA) inputs during mock-up heart micro-surgery procedures.\nThe trained network was used to estimate the tools poses from refined hand\njoint angles. Results: Based on surgeon's feedback during mock micro-surgery,\nthe developed wearable system with light-weight sensors for motion tracking did\nnot interfere with the surgery and instrument handling. The wearable motion\ntracking system used 15 finger-thumb-wrist joint angle sensors to generate\nmeaningful data-sets representing inputs of the DNN network with new hand joint\nangles added as necessary based on comparing the estimated tool poses against\nmeasured tool pose. The DNN architecture was optimized for the highest\nestimation accuracy and the ability to determine the tool pose with the least\nmean squared error. This novel approach showed that the surgical instrument's\npose, an essential requirement for teleoperation, can be accurately estimated\nfrom recorded surgeon's hand/finger movements with a mean squared error (MSE)\nless than 0.3%", "time": "2021-02-21T11:21:30Z", "link": "http://arxiv.org/abs/2102.10585v1", "id": "2102.10585v1", "title": "Mapping Surgeon's Hand/Finger Motion During Conventional Microsurgery to\n  Enhance Intuitive Surgical Robot Teleoperation"}
{"author": "Keidai Iiyama, Kento Tomita, Bhavi A. Jagatia, Tatsuwaki Nakagawa, Koki Ho", "abstract": "This research proposes a new integrated framework for identifying safe\nlanding locations and planning in-flight divert maneuvers. The state-of-the-art\nalgorithms for landing zone selection utilize local terrain features such as\nslopes and roughness to judge the safety and priority of the landing point.\nHowever, when there are additional chances of observation and diverting in the\nfuture, these algorithms are not able to evaluate the safety of the decision\nitself to target the selected landing point considering the overall descent\ntrajectory. In response to this challenge, we propose a reinforcement learning\nframework that optimizes a landing site selection strategy concurrently with a\nguidance and control strategy to the target landing site. The trained agent\ncould evaluate and select landing sites with explicit consideration of the\nterrain features, quality of future observations, and control to achieve a safe\nand efficient landing trajectory at a system-level. The proposed framework was\nable to achieve 94.8 $\\%$ of successful landing in highly challenging landing\nsites where over 80$\\%$ of the area around the initial target lading point is\nhazardous, by effectively updating the target landing site and feedback control\ngain during descent.", "time": "2021-02-24T17:53:10Z", "link": "http://arxiv.org/abs/2102.12432v1", "id": "2102.12432v1", "title": "Deep Reinforcement Learning for Safe Landing Site Selection with\n  Concurrent Consideration of Divert Maneuvers"}
{"author": "Hiroyasu Tsukamoto, Soon-Jo Chung", "abstract": "This paper presents Learning-based Autonomous Guidance with RObustness and\nStability guarantees (LAG-ROS), which provides machine learning-based nonlinear\nmotion planners with formal robustness and stability guarantees, by designing a\ndifferential Lyapunov function using contraction theory. LAG-ROS utilizes a\nneural network to model a robust tracking controller independently of a target\ntrajectory, for which we show that the Euclidean distance between the target\nand controlled trajectories is exponentially bounded linearly in the learning\nerror, even under the existence of bounded external disturbances. We also\npresent a convex optimization approach that minimizes the steady-state bound of\nthe tracking error to construct the robust control law for neural network\ntraining. In numerical simulations, it is demonstrated that the proposed method\nindeed possesses superior properties of robustness and nonlinear stability\nresulting from contraction theory, whilst retaining the computational\nefficiency of existing learning-based motion planners.", "time": "2021-10-02T00:21:43Z", "link": "http://arxiv.org/abs/2102.12668v4", "id": "2102.12668v4", "title": "Learning-based Robust Motion Planning with Guaranteed Stability: A\n  Contraction Theory Approach"}
{"author": "Baohe Zhang, Raghu Rajan, Luis Pineda, Nathan Lambert, AndrÃ© Biedenkapp, Kurtland Chua, Frank Hutter, Roberto Calandra", "abstract": "Model-based Reinforcement Learning (MBRL) is a promising framework for\nlearning control in a data-efficient manner. MBRL algorithms can be fairly\ncomplex due to the separate dynamics modeling and the subsequent planning\nalgorithm, and as a result, they often possess tens of hyperparameters and\narchitectural choices. For this reason, MBRL typically requires significant\nhuman expertise before it can be applied to new problems and domains. To\nalleviate this problem, we propose to use automatic hyperparameter optimization\n(HPO). We demonstrate that this problem can be tackled effectively with\nautomated HPO, which we demonstrate to yield significantly improved performance\ncompared to human experts. In addition, we show that tuning of several MBRL\nhyperparameters dynamically, i.e. during the training itself, further improves\nthe performance compared to using static hyperparameters which are kept fixed\nfor the whole training. Finally, our experiments provide valuable insights into\nthe effects of several hyperparameters, such as plan horizon or learning rate\nand their influence on the stability of training and resulting rewards.", "time": "2021-02-26T18:57:47Z", "link": "http://arxiv.org/abs/2102.13651v1", "id": "2102.13651v1", "title": "On the Importance of Hyperparameter Optimization for Model-based\n  Reinforcement Learning"}
{"author": "Vektor Dewanto, Marcus Gallagher", "abstract": "In reinforcement learning (RL), the goal is to obtain an optimal policy, for\nwhich the optimality criterion is fundamentally important. Two major optimality\ncriteria are average and discounted rewards. While the latter is more popular,\nit is problematic to apply in environments without an inherent notion of\ndiscounting. This motivates us to revisit a) the progression of optimality\ncriteria in dynamic programming, b) justification for and complication of an\nartificial discount factor, and c) benefits of directly maximizing the average\nreward criterion, which is discounting-free. Our contributions include a\nthorough examination of the relationship between average and discounted\nrewards, as well as a discussion of their pros and cons in RL. We emphasize\nthat average-reward RL methods possess the ingredient and mechanism for\napplying a family of discounting-free optimality criteria (Veinott, 1969) to\nRL.", "time": "2022-09-01T22:42:39Z", "link": "http://arxiv.org/abs/2107.01348v2", "id": "2107.01348v2", "title": "Examining average and discounted reward optimality criteria in\n  reinforcement learning"}
{"author": "Sampo Kuutti, Saber Fallah, Richard Bowden", "abstract": "Deep neural networks have demonstrated their capability to learn control\npolicies for a variety of tasks. However, these neural network-based policies\nhave been shown to be susceptible to exploitation by adversarial agents.\nTherefore, there is a need to develop techniques to learn control policies that\nare robust against adversaries. We introduce Adversarially Robust Control\n(ARC), which trains the protagonist policy and the adversarial policy\nend-to-end on the same loss. The aim of the protagonist is to maximise this\nloss, whilst the adversary is attempting to minimise it. We demonstrate the\nproposed ARC training in a highway driving scenario, where the protagonist\ncontrols the follower vehicle whilst the adversary controls the lead vehicle.\nBy training the protagonist against an ensemble of adversaries, it learns a\nsignificantly more robust control policy, which generalises to a variety of\nadversarial strategies. The approach is shown to reduce the amount of\ncollisions against new adversaries by up to 90.25%, compared to the original\npolicy. Moreover, by utilising an auxiliary distillation loss, we show that the\nfine-tuned control policy shows no drop in performance across its original\ntraining distribution.", "time": "2021-07-09T15:22:29Z", "link": "http://arxiv.org/abs/2107.04487v1", "id": "2107.04487v1", "title": "ARC: Adversarially Robust Control Policies for Autonomous Vehicles"}
{"author": "Hugo Vinicius Bitencourt, Frederico Gadelha GuimarÃ£es", "abstract": "In Internet of things (IoT), data is continuously recorded from different\ndata sources and devices can suffer faults in their embedded electronics, thus\nleading to a high-dimensional data sets and concept drift events. Therefore,\nmethods that are capable of high-dimensional non-stationary time series are of\ngreat value in IoT applications. Fuzzy Time Series (FTS) models stand out as\ndata-driven non-parametric models of easy implementation and high accuracy.\nUnfortunately, FTS encounters difficulties when dealing with data sets of many\nvariables and scenarios with concept drift. We present a new approach to handle\nhigh-dimensional non-stationary time series, by projecting the original\nhigh-dimensional data into a low dimensional embedding space and using FTS\napproach. Combining these techniques enables a better representation of the\ncomplex content of non-stationary multivariate time series and accurate\nforecasts. Our model is able to explain 98% of the variance and reach 11.52% of\nRMSE, 2.68% of MAE and 2.91% of MAPE.", "time": "2021-07-20T22:00:43Z", "link": "http://arxiv.org/abs/2107.09785v1", "id": "2107.09785v1", "title": "High-dimensional Multivariate Time Series Forecasting in IoT\n  Applications using Embedding Non-stationary Fuzzy Time Series"}
{"author": "Gangshan Jing, He Bai, Jemin George, Aranya Chakrabortty, Piyush K. Sharma", "abstract": "Recently introduced distributed zeroth-order optimization (ZOO) algorithms\nhave shown their utility in distributed reinforcement learning (RL).\nUnfortunately, in the gradient estimation process, almost all of them require\nrandom samples with the same dimension as the global variable and/or require\nevaluation of the global cost function, which may induce high estimation\nvariance for large-scale networks. In this paper, we propose a novel\ndistributed zeroth-order algorithm by leveraging the network structure inherent\nin the optimization objective, which allows each agent to estimate its local\ngradient by local cost evaluation independently, without use of any consensus\nprotocol. The proposed algorithm exhibits an asynchronous update scheme, and is\ndesigned for stochastic non-convex optimization with a possibly non-convex\nfeasible domain based on the block coordinate descent method. The algorithm is\nlater employed as a distributed model-free RL algorithm for distributed linear\nquadratic regulator design, where a learning graph is designed to describe the\nrequired interaction relationship among agents in distributed learning. We\nprovide an empirical validation of the proposed algorithm to benchmark its\nperformance on convergence rate and variance against a centralized ZOO\nalgorithm.", "time": "2024-05-03T03:56:09Z", "link": "http://arxiv.org/abs/2107.12416v4", "id": "2107.12416v4", "title": "Asynchronous Distributed Reinforcement Learning for LQR Control via\n  Zeroth-Order Block Coordinate Descent"}
{"author": "Quan Zhou, Jakub Marecek, Robert N. Shorten", "abstract": "It is well known that two-sided markets are unfair in a number of ways. For\ninstance, female workers at Uber earn less than their male colleagues per mile\ndriven. Similar observations have been made for other minority subgroups in\nother two-sided markets. Here, we suggest a novel market-clearing mechanism for\ntwo-sided markets, which promotes equalisation of the pay per hour worked\nacross multiple subgroups, as well as within each subgroup. In the process, we\nintroduce a novel notion of subgroup fairness (which we call Inter-fairness),\nwhich can be combined with other notions of fairness within each subgroup\n(called Intra-fairness), and the utility for the customers (Customer-Care) in\nthe objective of the market-clearing problem. While the novel non-linear terms\nin the objective complicate market clearing by making the problem non-convex,\nwe show that a certain non-convex augmented Lagrangian relaxation can be\napproximated to any precision in time polynomial in the number of market\nparticipants using semi-definite programming. This makes it possible to\nimplement the market-clearing mechanism efficiently. On the example of\ndriver-ride assignment in an Uber-like system, we demonstrate the efficacy and\nscalability of the approach, and trade-offs between Inter- and Intra-fairness.", "time": "2023-01-30T22:39:10Z", "link": "http://arxiv.org/abs/2106.02702v2", "id": "2106.02702v2", "title": "Subgroup Fairness in Two-Sided Markets"}
{"author": "Junyoung Park, Sanjar Bakhtiyar, Jinkyoo Park", "abstract": "We propose ScheduleNet, a RL-based real-time scheduler, that can solve\nvarious types of multi-agent scheduling problems. We formulate these problems\nas a semi-MDP with episodic reward (makespan) and learn ScheduleNet, a\ndecentralized decision-making policy that can effectively coordinate multiple\nagents to complete tasks. The decision making procedure of ScheduleNet\nincludes: (1) representing the state of a scheduling problem with the\nagent-task graph, (2) extracting node embeddings for agent and tasks nodes, the\nimportant relational information among agents and tasks, by employing the\ntype-aware graph attention (TGA), and (3) computing the assignment probability\nwith the computed node embeddings. We validate the effectiveness of ScheduleNet\nas a general learning-based scheduler for solving various types of multi-agent\nscheduling tasks, including multiple salesman traveling problem (mTSP) and job\nshop scheduling problem (JSP).", "time": "2021-06-06T07:08:58Z", "link": "http://arxiv.org/abs/2106.03051v1", "id": "2106.03051v1", "title": "ScheduleNet: Learn to solve multi-agent scheduling problems with\n  reinforcement learning"}
{"author": "Stefano Massaroli, Michael Poli, Stefano Peluchetti, Jinkyoo Park, Atsushi Yamashita, Hajime Asama", "abstract": "We systematically develop a learning-based treatment of stochastic optimal\ncontrol (SOC), relying on direct optimization of parametric control policies.\nWe propose a derivation of adjoint sensitivity results for stochastic\ndifferential equations through direct application of variational calculus.\nThen, given an objective function for a predetermined task specifying the\ndesiderata for the controller, we optimize their parameters via iterative\ngradient descent methods. In doing so, we extend the range of applicability of\nclassical SOC techniques, often requiring strict assumptions on the functional\nform of system and control. We verify the performance of the proposed approach\non a continuous-time, finite horizon portfolio optimization with proportional\ntransaction costs.", "time": "2021-06-07T16:43:07Z", "link": "http://arxiv.org/abs/2106.03780v1", "id": "2106.03780v1", "title": "Learning Stochastic Optimal Policies via Gradient Descent"}
{"author": "Alireza Ranjbar, Ngo Anh Vien, Hanna Ziesche, Joschka Boedecker, Gerhard Neumann", "abstract": "While classic control theory offers state of the art solutions in many\nproblem scenarios, it is often desired to improve beyond the structure of such\nsolutions and surpass their limitations. To this end, residual policy learning\n(RPL) offers a formulation to improve existing controllers with reinforcement\nlearning (RL) by learning an additive \"residual\" to the output of a given\ncontroller. However, the applicability of such an approach highly depends on\nthe structure of the controller. Often, internal feedback signals of the\ncontroller limit an RL algorithm to adequately change the policy and, hence,\nlearn the task. We propose a new formulation that addresses these limitations\nby also modifying the feedback signals to the controller with an RL policy and\nshow superior performance of our approach on a contact-rich peg-insertion task\nunder position and orientation uncertainty. In addition, we use a recent\nCartesian impedance control architecture as the control framework which can be\navailable to us as a black-box while assuming no knowledge about its\ninput/output structure, and show the difficulties of standard RPL. Furthermore,\nwe introduce an adaptive curriculum for the given task to gradually increase\nthe task difficulty in terms of position and orientation uncertainty. A video\nshowing the results can be found at https://youtu.be/SAZm_Krze7U .", "time": "2021-08-06T16:03:31Z", "link": "http://arxiv.org/abs/2106.04306v2", "id": "2106.04306v2", "title": "Residual Feedback Learning for Contact-Rich Manipulation Tasks with\n  Uncertainty"}
{"author": "Guanya Shi, Kamyar Azizzadenesheli, Michael O'Connell, Soon-Jo Chung, Yisong Yue", "abstract": "We present an online multi-task learning approach for adaptive nonlinear\ncontrol, which we call Online Meta-Adaptive Control (OMAC). The goal is to\ncontrol a nonlinear system subject to adversarial disturbance and unknown\n$\\textit{environment-dependent}$ nonlinear dynamics, under the assumption that\nthe environment-dependent dynamics can be well captured with some shared\nrepresentation. Our approach is motivated by robot control, where a robotic\nsystem encounters a sequence of new environmental conditions that it must\nquickly adapt to. A key emphasis is to integrate online representation learning\nwith established methods from control theory, in order to arrive at a unified\nframework that yields both control-theoretic and learning-theoretic guarantees.\nWe provide instantiations of our approach under varying conditions, leading to\nthe first non-asymptotic end-to-end convergence guarantee for multi-task\nnonlinear control. OMAC can also be integrated with deep representation\nlearning. Experiments show that OMAC significantly outperforms conventional\nadaptive control approaches which do not learn the shared representation, in\ninverted pendulum and 6-DoF drone control tasks under varying wind conditions.", "time": "2021-10-26T19:30:03Z", "link": "http://arxiv.org/abs/2106.06098v3", "id": "2106.06098v3", "title": "Meta-Adaptive Nonlinear Control: Theory and Algorithms"}
{"author": "Fu-Ming Guo, Austin Huang", "abstract": "Reducing computation cost, inference latency, and memory footprint of neural\nnetworks are frequently cited as research motivations for pruning and sparsity.\nHowever, operationalizing those benefits and understanding the end-to-end\neffect of algorithm design and regularization on the runtime execution is not\noften examined in depth.\n  Here we apply structured and unstructured pruning to attention weights of\ntransformer blocks of the BERT language model, while also expanding block\nsparse representation (BSR) operations in the TVM compiler. Integration of BSR\noperations enables the TVM runtime execution to leverage structured pattern\nsparsity induced by model regularization.\n  This integrated view of pruning algorithms enables us to study relationships\nbetween modeling decisions and their direct impact on sparsity-enhanced\nexecution. Our main findings are: 1) we validate that performance benefits of\nstructured sparsity block regularization must be enabled by the BSR\naugmentations to TVM, with 4x speedup relative to vanilla PyTorch and 2.2x\nspeedup relative to standard TVM compilation (without expanded BSR support). 2)\nfor BERT attention weights, the end-to-end optimal block sparsity shape in this\nCPU inference context is not a square block (as in \\cite{gray2017gpu}) but\nrather a linear 32x1 block 3) the relationship between performance and block\nsize / shape is is suggestive of how model regularization parameters interact\nwith task scheduler optimizations resulting in the observed end-to-end\nperformance.", "time": "2021-06-17T04:03:11Z", "link": "http://arxiv.org/abs/2106.08846v2", "id": "2106.08846v2", "title": "Algorithm to Compilation Co-design: An Integrated View of Neural Network\n  Sparsity"}
{"author": "Yoshihiro Okawa, Tomotake Sasaki, Hidenao Iwane", "abstract": "In reinforcement learning (RL) algorithms, exploratory control inputs are\nused during learning to acquire knowledge for decision making and control,\nwhile the true dynamics of a controlled object is unknown. However, this\nexploring property sometimes causes undesired situations by violating\nconstraints regarding the state of the controlled object. In this paper, we\npropose an automatic exploration process adjustment method for safe RL in\ncontinuous state and action spaces utilizing a linear nominal model of the\ncontrolled object. Specifically, our proposed method automatically selects\nwhether the exploratory input is used or not at each time depending on the\nstate and its predicted value as well as adjusts the variance-covariance matrix\nused in the Gaussian policy for exploration. We also show that our exploration\nprocess adjustment method theoretically guarantees the satisfaction of the\nconstraints with the pre-specified probability, that is, the satisfaction of a\njoint chance constraint at every time. Finally, we illustrate the validity and\nthe effectiveness of our method through numerical simulation.", "time": "2021-03-05T13:30:53Z", "link": "http://arxiv.org/abs/2103.03656v1", "id": "2103.03656v1", "title": "Automatic Exploration Process Adjustment for Safe Reinforcement Learning\n  with Joint Chance Constraint Satisfaction"}
{"author": "Martin Figura, Krishna Chaitanya Kosaraju, Vijay Gupta", "abstract": "Recently, many cooperative distributed multi-agent reinforcement learning\n(MARL) algorithms have been proposed in the literature. In this work, we study\nthe effect of adversarial attacks on a network that employs a consensus-based\nMARL algorithm. We show that an adversarial agent can persuade all the other\nagents in the network to implement policies that optimize an objective that it\ndesires. In this sense, the standard consensus-based MARL algorithms are\nfragile to attacks.", "time": "2021-03-11T21:44:18Z", "link": "http://arxiv.org/abs/2103.06967v1", "id": "2103.06967v1", "title": "Adversarial attacks in consensus-based multi-agent reinforcement\n  learning"}
{"author": "G. G. Samatas, S. S. Moumgiakmas, G. A. Papakostas", "abstract": "This paper highlights the trends in the field of predictive maintenance with\nthe use of machine learning. With the continuous development of the Fourth\nIndustrial Revolution, through IoT, the technologies that use artificial\nintelligence are evolving. As a result, industries have been using these\ntechnologies to optimize their production. Through scientific research\nconducted for this paper, conclusions were drawn about the trends in Predictive\nMaintenance applications with the use of machine learning bridging Artificial\nIntelligence and IoT. These trends are related to the types of industries in\nwhich Predictive Maintenance was applied, the models of artificial intelligence\nwere implemented, mainly of machine learning and the types of sensors that are\napplied through the IoT to the applications. Six sectors were presented and the\nproduction sector was dominant as it accounted for 54.54% of total\npublications. In terms of artificial intelligence models, the most prevalent\namong ten were the Artificial Neural Networks, Support Vector Machine and\nRandom Forest with 27.84%, 17.72% and 13.92% respectively. Finally, twelve\ncategories of sensors emerged, of which the most widely used were the sensors\nof temperature and vibration with percentages of 60.71% and 46.42%\ncorrespondingly.", "time": "2021-04-20T17:50:03Z", "link": "http://arxiv.org/abs/2103.11148v2", "id": "2103.11148v2", "title": "Predictive Maintenance -- Bridging Artificial Intelligence and IoT"}
{"author": "Artur Sokolovsky, Luca Arnaboldi, Jaume Bacardit, Thomas Gross", "abstract": "Financial markets are a source of non-stationary multidimensional time series\nwhich has been drawing attention for decades. Each financial instrument has its\nspecific changing-over-time properties, making its analysis a complex task.\nHence, improvement of understanding and development of more informative,\ngeneralisable market representations are essential for the successful operation\nin financial markets, including risk assessment, diversification, trading, and\norder execution. In this study, we propose a volume-price-based market\nrepresentation for making financial time series more suitable for machine\nlearning pipelines. We use a statistical approach for evaluating the\nrepresentation. Through the research questions, we investigate, i) whether the\nproposed representation allows the more efficient design of machine learning\nmodels; ii) whether the proposed representation leads to increased performance\nover the price levels market pattern; iii) whether the proposed representation\nperforms better on the liquid markets, and iv) whether SHAP feature\ninteractions are reliable to be used in the considered setting. Our analysis\nshows that the proposed volume-based method allows successful classification of\nthe financial time series patterns, and also leads to better classification\nperformance than the price levels-based method, excelling specifically on more\nliquid financial instruments. Finally, we propose an approach for obtaining\nfeature interactions directly from tree-based models and compare the outcomes\nto those of the SHAP method. This results in the significant similarity between\nthe two methods, hence we claim that SHAP feature interactions are reliable to\nbe used in the setting of financial markets.", "time": "2022-05-08T18:23:07Z", "link": "http://arxiv.org/abs/2103.12419v4", "id": "2103.12419v4", "title": "Volume-Centred Range Bars: Novel Interpretable Representation of\n  Financial Markets Designed for Machine Learning Applications"}
{"author": "Zhongyu Li, Xuxin Cheng, Xue Bin Peng, Pieter Abbeel, Sergey Levine, Glen Berseth, Koushil Sreenath", "abstract": "Developing robust walking controllers for bipedal robots is a challenging\nendeavor. Traditional model-based locomotion controllers require simplifying\nassumptions and careful modelling; any small errors can result in unstable\ncontrol. To address these challenges for bipedal locomotion, we present a\nmodel-free reinforcement learning framework for training robust locomotion\npolicies in simulation, which can then be transferred to a real bipedal Cassie\nrobot. To facilitate sim-to-real transfer, domain randomization is used to\nencourage the policies to learn behaviors that are robust across variations in\nsystem dynamics. The learned policies enable Cassie to perform a set of diverse\nand dynamic behaviors, while also being more robust than traditional\ncontrollers and prior learning-based methods that use residual control. We\ndemonstrate this on versatile walking behaviors such as tracking a target\nwalking velocity, walking height, and turning yaw.", "time": "2021-03-26T07:14:01Z", "link": "http://arxiv.org/abs/2103.14295v1", "id": "2103.14295v1", "title": "Reinforcement Learning for Robust Parameterized Locomotion Control of\n  Bipedal Robots"}
{"author": "Keenon Werling, Dalton Omens, Jeongseok Lee, Ioannis Exarchos, C. Karen Liu", "abstract": "We present a fast and feature-complete differentiable physics engine, Nimble\n(nimblephysics.org), that supports Lagrangian dynamics and hard contact\nconstraints for articulated rigid body simulation. Our differentiable physics\nengine offers a complete set of features that are typically only available in\nnon-differentiable physics simulators commonly used by robotics applications.\nWe solve contact constraints precisely using linear complementarity problems\n(LCPs). We present efficient and novel analytical gradients through the LCP\nformulation of inelastic contact that exploit the sparsity of the LCP solution.\nWe support complex contact geometry, and gradients approximating\ncontinuous-time elastic collision. We also introduce a novel method to compute\ncomplementarity-aware gradients that help downstream optimization tasks avoid\nstalling in saddle points. We show that an implementation of this combination\nin an existing physics engine (DART) is capable of a 87x single-core speedup\nover finite-differencing in computing analytical Jacobians for a single\ntimestep, while preserving all the expressiveness of original DART.", "time": "2021-06-22T23:36:54Z", "link": "http://arxiv.org/abs/2103.16021v3", "id": "2103.16021v3", "title": "Fast and Feature-Complete Differentiable Physics for Articulated Rigid\n  Bodies with Contact"}
{"author": "Mohammed Abouheaf, Wail Gueaieb, Md. Suruz Miah, Davide Spinello", "abstract": "Underactuated systems like sea vessels have degrees of motion that are\ninsufficiently matched by a set of independent actuation forces. In addition,\nthe underlying trajectory-tracking control problems grow in complexity in order\nto decide the optimal rudder and thrust control signals. This enforces several\ndifficult-to-solve constraints that are associated with the error dynamical\nequations using classical optimal tracking and adaptive control approaches. An\nonline machine learning mechanism based on integral reinforcement learning is\nproposed to find a solution for a class of nonlinear tracking problems with\npartial prior knowledge of the system dynamics. The actuation forces are\ndecided using innovative forms of temporal difference equations relevant to the\nvessel's surge and angular velocities. The solution is implemented using an\nonline value iteration process which is realized by employing means of the\nadaptive critics and gradient descent approaches. The adaptive learning\nmechanism exhibited well-functioning and interactive features in react to\ndifferent desired reference-tracking scenarios.", "time": "2021-04-01T01:41:49Z", "link": "http://arxiv.org/abs/2104.00190v1", "id": "2104.00190v1", "title": "Trajectory Tracking of Underactuated Sea Vessels With Uncertain\n  Dynamics: An Integral Reinforcement Learning Approach"}
{"author": "Bhaskar Ramasubramanian, Luyao Niu, Andrew Clark, Radha Poovendran", "abstract": "The inputs and preferences of human users are important considerations in\nsituations where these users interact with autonomous cyber or cyber-physical\nsystems. In these scenarios, one is often interested in aligning behaviors of\nthe system with the preferences of one or more human users. Cumulative prospect\ntheory (CPT) is a paradigm that has been empirically shown to model a tendency\nof humans to view gains and losses differently. In this paper, we consider a\nsetting where an autonomous agent has to learn behaviors in an unknown\nenvironment. In traditional reinforcement learning, these behaviors are learned\nthrough repeated interactions with the environment by optimizing an expected\nutility. In order to endow the agent with the ability to closely mimic the\nbehavior of human users, we optimize a CPT-based cost. We introduce the notion\nof the CPT-value of an action taken in a state, and establish the convergence\nof an iterative dynamic programming-based approach to estimate this quantity.\nWe develop two algorithms to enable agents to learn policies to optimize the\nCPT-vale, and evaluate these algorithms in environments where a target state\nhas to be reached while avoiding obstacles. We demonstrate that behaviors of\nthe agent learned using these algorithms are better aligned with that of a\nhuman user who might be placed in the same environment, and is significantly\nimproved over a baseline that optimizes an expected utility.", "time": "2021-03-29T20:35:25Z", "link": "http://arxiv.org/abs/2104.00540v1", "id": "2104.00540v1", "title": "Reinforcement Learning Beyond Expectation"}
{"author": "Joshua Gruenstein, Tao Chen, Neel Doshi, Pulkit Agrawal", "abstract": "A majority of microrobots are constructed using compliant materials that are\ndifficult to model analytically, limiting the utility of traditional\nmodel-based controllers. Challenges in data collection on microrobots and large\nerrors between simulated models and real robots make current model-based\nlearning and sim-to-real transfer methods difficult to apply. We propose a\nnovel framework residual model learning (RML) that leverages approximate models\nto substantially reduce the sample complexity associated with learning an\naccurate robot model. We show that using RML, we can learn a model of the\nHarvard Ambulatory MicroRobot (HAMR) using just 12 seconds of passively\ncollected interaction data. The learned model is accurate enough to be\nleveraged as \"proxy-simulator\" for learning walking and turning behaviors using\nmodel-free reinforcement learning algorithms. RML provides a general framework\nfor learning from extremely small amounts of interaction data, and our\nexperiments with HAMR clearly demonstrate that RML substantially outperforms\nexisting techniques.", "time": "2021-09-08T03:06:30Z", "link": "http://arxiv.org/abs/2104.00631v2", "id": "2104.00631v2", "title": "Residual Model Learning for Microrobot Control"}
{"author": "Lokesh Krishna, Utkarsh A. Mishra, Guillermo A. Castillo, Ayonga Hereid, Shishir Kolathaya", "abstract": "In this paper, with a view toward deployment of light-weight control\nframeworks for bipedal walking robots, we realize end-foot trajectories that\nare shaped by a single linear feedback policy. We learn this policy via a\nmodel-free and a gradient-free learning algorithm, Augmented Random Search\n(ARS), in the two robot platforms Rabbit and Digit. Our contributions are\ntwo-fold: a) By using torso and support plane orientation as inputs, we achieve\nrobust walking on slopes of up to 20 degrees in simulation. b) We demonstrate\nadditional behaviors like walking backwards, stepping-in-place, and recovery\nfrom external pushes of up to 120 N. The end result is a robust and a fast\nfeedback control law for bipedal walking on terrains with varying slopes.\nTowards the end, we also provide preliminary results of hardware transfer to\nDigit.", "time": "2021-08-09T15:49:51Z", "link": "http://arxiv.org/abs/2104.01662v2", "id": "2104.01662v2", "title": "Learning Linear Policies for Robust Bipedal Locomotion on Terrains with\n  Varying Slopes"}
{"author": "Osman Boyaci, Amarachi Umunnakwe, Abhijeet Sahu, Mohammad Rasoul Narimani, Muhammad Ismail, Katherine Davis, Erchin Serpedin", "abstract": "False data injection attacks (FDIAs) represent a major class of attacks that\naim to break the integrity of measurements by injecting false data into the\nsmart metering devices in power grids. To the best of authors' knowledge, no\nstudy has attempted to design a detector that automatically models the\nunderlying graph topology and spatially correlated measurement data of the\nsmart grids to better detect cyber attacks. The contributions of this paper to\ndetect and mitigate FDIAs are twofold. First, we present a generic, localized,\nand stealth (unobservable) attack generation methodology and publicly\naccessible datasets for researchers to develop and test their algorithms.\nSecond, we propose a Graph Neural Network (GNN) based, scalable and real-time\ndetector of FDIAs that efficiently combines model-driven and data-driven\napproaches by incorporating the inherent physical connections of modern AC\npower grids and exploiting the spatial correlations of the measurement. It is\nexperimentally verified by comparing the proposed GNN based detector with the\ncurrently available FDIA detectors in the literature that our algorithm\noutperforms the best available solutions by 3.14%, 4.25%, and 4.41% in F1 score\nfor standard IEEE testbeds with 14, 118, and 300 buses, respectively.", "time": "2021-10-10T22:51:49Z", "link": "http://arxiv.org/abs/2104.02012v2", "id": "2104.02012v2", "title": "Graph Neural Networks Based Detection of Stealth False Data Injection\n  Attacks in Smart Grids"}
{"author": "Prerit Terway, Kenza Hamidouche, Niraj K. Jha", "abstract": "Nonlinear system design is often a multi-objective optimization problem\ninvolving search for a design that satisfies a number of predefined\nconstraints. The design space is typically very large since it includes all\npossible system architectures with different combinations of components\ncomposing each architecture. In this article, we address nonlinear system\ndesign space exploration through a two-step approach encapsulated in a\nframework called Fast Design Space Exploration of Nonlinear Systems (ASSENT).\nIn the first step, we use a genetic algorithm to search for system\narchitectures that allow discrete choices for component values or else only\ncomponent values for a fixed architecture. This step yields a coarse design\nsince the system may or may not meet the target specifications. In the second\nstep, we use an inverse design to search over a continuous space and fine-tune\nthe component values with the goal of improving the value of the objective\nfunction. We use a neural network to model the system response. The neural\nnetwork is converted into a mixed-integer linear program for active learning to\nsample component values efficiently. We illustrate the efficacy of ASSENT on\nproblems ranging from nonlinear system design to design of electrical circuits.\nExperimental results show that ASSENT achieves the same or better value of the\nobjective function compared to various other optimization techniques for\nnonlinear system design by up to 54%. We improve sample efficiency by 6-10x\ncompared to reinforcement learning based synthesis of electrical circuits.", "time": "2021-04-08T19:35:15Z", "link": "http://arxiv.org/abs/2104.02464v2", "id": "2104.02464v2", "title": "Fast Design Space Exploration of Nonlinear Systems: Part II"}
{"author": "Erdal Kayacan, Erkan Kayacan, Herman Ramon, Okyay Kaynak, Wouter Saeys", "abstract": "Provision of some autonomous functions to an agricultural vehicle would\nlighten the job of the operator but in doing so, the accuracy should not be\nlost to still obtain an optimal yield. Autonomous navigation of an agricultural\nvehicle involves the control of different dynamic subsystems, such as the yaw\nangle dynamics and the longitudinal speed dynamics. In this study, a\nproportional-integral-derivative controller is used to control the longitudinal\nvelocity of the tractor. For the control of the yaw angle dynamics, a\nproportional-derivative controller works in parallel with a type-2 fuzzy neural\nnetwork. In such an arrangement, the former ensures the stability of the\nrelated subsystem, while the latter learns the system dynamics and becomes the\nleading controller. In this way, instead of modeling the interactions between\nthe subsystems prior to the design of model-based control, we develop a control\nalgorithm which learns the interactions online from the measured feedback\nerror. In addition to the control of the stated subsystems, a kinematic\ncontroller is needed to correct the errors in both the x- and the y- axis for\nthe trajectory tracking problem of the tractor. To demonstrate the real-time\nabilities of the proposed control scheme, an autonomous tractor is equipped\nwith the use of reasonably priced sensors and actuators. Experimental results\nshow the efficacy and efficiency of the proposed learning algorithm.", "time": "2021-04-09T00:46:23Z", "link": "http://arxiv.org/abs/2104.04123v1", "id": "2104.04123v1", "title": "Towards Agrobots: Trajectory Control of an Autonomous Tractor Using\n  Type-2 Fuzzy Logic Controllers"}
{"author": "Daniel Weber, Clemens GÃ¼hmann, Thomas Seel", "abstract": "Inertial-sensor-based attitude estimation is a crucial technology in various\napplications, from human motion tracking to autonomous aerial and ground\nvehicles. Application scenarios differ in characteristics of the performed\nmotion, presence of disturbances, and environmental conditions. Since\nstate-of-the-art attitude estimators do not generalize well over these\ncharacteristics, their parameters must be tuned for the individual motion\ncharacteristics and circumstances. We propose RIANN, a ready-to-use, neural\nnetwork-based, parameter-free, real-time-capable inertial attitude estimator,\nwhich generalizes well across different motion dynamics, environments, and\nsampling rates, without the need for application-specific adaptations. We\ngather six publicly available datasets of which we exploit two datasets for the\nmethod development and the training, and we use four datasets for evaluation of\nthe trained estimator in three different test scenarios with varying practical\nrelevance. Results show that RIANN outperforms state-of-the-art attitude\nestimation filters in the sense that it generalizes much better across a\nvariety of motions and conditions in different applications, with different\nsensor hardware and different sampling frequencies. This is true even if the\nfilters are tuned on each individual test dataset, whereas RIANN was trained on\ncompletely separate data and has never seen any of these test datasets. RIANN\ncan be applied directly without adaptations or training and is therefore\nexpected to enable plug-and-play solutions in numerous applications, especially\nwhen accuracy is crucial but no ground-truth data is available for tuning or\nwhen motion and disturbance characteristics are uncertain. We made RIANN\npublicly available.", "time": "2021-11-25T12:51:02Z", "link": "http://arxiv.org/abs/2104.07391v3", "id": "2104.07391v3", "title": "RIANN -- A Robust Neural Network Outperforms Attitude Estimation Filters"}
{"author": "Glenn Ceusters, RomÃ¡n CantÃº RodrÃ­guez, Alberte Bouso GarcÃ­a, RÃ¼diger Franke, Geert Deconinck, Lieve Helsen, Ann NowÃ©, Maarten Messagie, Luis Ramirez Camargo", "abstract": "Model-predictive-control (MPC) offers an optimal control technique to\nestablish and ensure that the total operation cost of multi-energy systems\nremains at a minimum while fulfilling all system constraints. However, this\nmethod presumes an adequate model of the underlying system dynamics, which is\nprone to modelling errors and is not necessarily adaptive. This has an\nassociated initial and ongoing project-specific engineering cost. In this\npaper, we present an on- and off-policy multi-objective reinforcement learning\n(RL) approach, that does not assume a model a priori, benchmarking this against\na linear MPC (LMPC - to reflect current practice, though non-linear MPC\nperforms better) - both derived from the general optimal control problem,\nhighlighting their differences and similarities. In a simple multi-energy\nsystem (MES) configuration case study, we show that a twin delayed deep\ndeterministic policy gradient (TD3) RL agent offers potential to match and\noutperform the perfect foresight LMPC benchmark (101.5%). This while the\nrealistic LMPC, i.e. imperfect predictions, only achieves 98%. While in a more\ncomplex MES system configuration, the RL agent's performance is generally lower\n(94.6%), yet still better than the realistic LMPC (88.9%). In both case\nstudies, the RL agents outperformed the realistic LMPC after a training period\nof 2 years using quarterly interactions with the environment. We conclude that\nreinforcement learning is a viable optimal control technique for multi-energy\nsystems given adequate constraint handling and pre-training, to avoid unsafe\ninteractions and long training periods, as is proposed in fundamental future\nwork.", "time": "2021-09-09T15:37:37Z", "link": "http://arxiv.org/abs/2104.09785v2", "id": "2104.09785v2", "title": "Model-predictive control and reinforcement learning in multi-energy\n  system case studies"}
{"author": "Osman Boyaci, Mohammad Rasoul Narimani, Katherine Davis, Muhammad Ismail, Thomas J Overbye, Erchin Serpedin", "abstract": "False data injection attacks (FDIA) are a main category of cyber-attacks\nthreatening the security of power systems. Contrary to the detection of these\nattacks, less attention has been paid to identifying the attacked units of the\ngrid. To this end, this work jointly studies detecting and localizing the\nstealth FDIA in power grids. Exploiting the inherent graph topology of power\nsystems as well as the spatial correlations of measurement data, this paper\nproposes an approach based on the graph neural network (GNN) to identify the\npresence and location of the FDIA. The proposed approach leverages the\nauto-regressive moving average (ARMA) type graph filters (GFs) which can better\nadapt to sharp changes in the spectral domain due to their rational type filter\ncomposition compared to the polynomial type GFs such as Chebyshev. To the best\nof our knowledge, this is the first work based on GNN that automatically\ndetects and localizes FDIA in power systems. Extensive simulations and\nvisualizations show that the proposed approach outperforms the available\nmethods in both detection and localization of FDIA for different IEEE test\nsystems. Thus, the targeted areas can be identified and preventive actions can\nbe taken before the attack impacts the grid.", "time": "2021-10-09T17:58:38Z", "link": "http://arxiv.org/abs/2104.11846v2", "id": "2104.11846v2", "title": "Joint Detection and Localization of Stealth False Data Injection Attacks\n  in Smart Grids using Graph Neural Networks"}
{"author": "Tao Li, Guanze Peng, Quanyan Zhu, Tamer Basar", "abstract": "Recent years have witnessed significant advances in technologies and services\nin modern network applications, including smart grid management, wireless\ncommunication, cybersecurity as well as multi-agent autonomous systems.\nConsidering the heterogeneous nature of networked entities, emerging network\napplications call for game-theoretic models and learning-based approaches in\norder to create distributed network intelligence that responds to uncertainties\nand disruptions in a dynamic or an adversarial environment. This paper\narticulates the confluence of networks, games and learning, which establishes a\ntheoretical underpinning for understanding multi-agent decision-making over\nnetworks. We provide an selective overview of game-theoretic learning\nalgorithms within the framework of stochastic approximation theory, and\nassociated applications in some representative contexts of modern network\nsystems, such as the next generation wireless communication networks, the smart\ngrid and distributed machine learning. In addition to existing research works\non game-theoretic learning over networks, we highlight several new angles and\nresearch endeavors on learning in games that are related to recent developments\nin artificial intelligence. Some of the new angles extrapolate from our own\nresearch interests. The overall objective of the paper is to provide the reader\na clear picture of the strengths and challenges of adopting game-theoretic\nlearning methods within the context of network systems, and further to identify\nfruitful future research directions on both theoretical and applied studies.", "time": "2023-08-26T16:29:18Z", "link": "http://arxiv.org/abs/2105.08158v2", "id": "2105.08158v2", "title": "The Confluence of Networks, Games and Learning"}
{"author": "Erotokritos Skordilis, Yi Hou, Charles Tripp, Matthew Moniot, Peter Graf, David Biagioni", "abstract": "Mobility on demand (MoD) systems show great promise in realizing flexible and\nefficient urban transportation. However, significant technical challenges arise\nfrom operational decision making associated with MoD vehicle dispatch and fleet\nrebalancing. For this reason, operators tend to employ simplified algorithms\nthat have been demonstrated to work well in a particular setting. To help\nbridge the gap between novel and existing methods, we propose a modular\nframework for fleet rebalancing based on model-free reinforcement learning (RL)\nthat can leverage an existing dispatch method to minimize system cost. In\nparticular, by treating dispatch as part of the environment dynamics, a\ncentralized agent can learn to intermittently direct the dispatcher to\nreposition free vehicles and mitigate against fleet imbalance. We formulate RL\nstate and action spaces as distributions over a grid partitioning of the\noperating area, making the framework scalable and avoiding the complexities\nassociated with multiagent RL. Numerical experiments, using real-world trip and\nnetwork data, demonstrate that this approach has several distinct advantages\nover baseline methods including: improved system cost; high degree of\nadaptability to the selected dispatch method; and the ability to perform\nscale-invariant transfer learning between problem instances with similar\nvehicle and request distributions.", "time": "2021-05-27T16:32:28Z", "link": "http://arxiv.org/abs/2105.13284v1", "id": "2105.13284v1", "title": "A Modular and Transferable Reinforcement Learning Framework for the\n  Fleet Rebalancing Problem"}
{"author": "Benjamin Maschler, Timo MÃ¼ller, Andreas LÃ¶cklin, Michael Weyrich", "abstract": "Reconfiguration demand is increasing due to frequent requirement changes for\nmanufacturing systems. Recent approaches aim at investigating feasible\nconfiguration alternatives from which they select the optimal one. This relies\non processes whose behavior is not reliant on e.g. the production sequence.\nHowever, when machine learning is used, components' behavior depends on the\nprocess' specifics, requiring additional concepts to successfully conduct\nreconfiguration management. Therefore, we propose the enhancement of the\ncomprehensive reconfiguration management with transfer learning. This provides\nthe ability to assess the machine learning dependent behavior of the different\nCPPS configurations with reduced effort and further assists the recommissioning\nof the chosen one. A real cyber-physical production system from the discrete\nmanufacturing domain is utilized to demonstrate the aforementioned proposal.", "time": "2021-05-31T06:50:58Z", "link": "http://arxiv.org/abs/2105.14730v1", "id": "2105.14730v1", "title": "Transfer Learning as an Enhancement for Reconfiguration Management of\n  Cyber-Physical Production Systems"}
{"author": "Shuang Dai, Fanlin Meng, Hongsheng Dai, Qian Wang, Xizhong Chen", "abstract": "The power system is undergoing rapid evolution with the roll-out of advanced\nmetering infrastructure and local energy applications (e.g. electric vehicles)\nas well as the increasing penetration of intermittent renewable energy at both\ntransmission and distribution level, which characterizes the peak load demand\nwith stronger randomness and less predictability and therefore poses a threat\nto the power grid security. Since storing large quantities of electricity to\nsatisfy load demand is neither economically nor environmentally friendly,\neffective peak demand management strategies and reliable peak load forecast\nmethods become essential for optimizing the power system operations. To this\nend, this paper provides a timely and comprehensive overview of peak load\ndemand forecast methods in the literature. To our best knowledge, this is the\nfirst comprehensive review on such topic. In this paper we first give a precise\nand unified problem definition of peak load demand forecast. Second, 139 papers\non peak load forecast methods were systematically reviewed where methods were\nclassified into different stages based on the timeline. Thirdly, a comparative\nanalysis of peak load forecast methods are summarized and different optimizing\nmethods to improve the forecast performance are discussed. The paper ends with\na comprehensive summary of the reviewed papers and a discussion of potential\nfuture research directions.", "time": "2021-08-03T10:04:33Z", "link": "http://arxiv.org/abs/2108.01393v1", "id": "2108.01393v1", "title": "Electrical peak demand forecasting- A review"}
{"author": "Yuping Luo, Tengyu Ma", "abstract": "Training-time safety violations have been a major concern when we deploy\nreinforcement learning algorithms in the real world. This paper explores the\npossibility of safe RL algorithms with zero training-time safety violations in\nthe challenging setting where we are only given a safe but trivial-reward\ninitial policy without any prior knowledge of the dynamics model and additional\noffline data. We propose an algorithm, Co-trained Barrier Certificate for Safe\nRL (CRABS), which iteratively learns barrier certificates, dynamics models, and\npolicies. The barrier certificates, learned via adversarial training, ensure\nthe policy's safety assuming calibrated learned dynamics model. We also add a\nregularization term to encourage larger certified regions to enable better\nexploration. Empirical simulations show that zero safety violations are already\nchallenging for a suite of simple environments with only 2-4 dimensional state\nspace, especially if high-reward policies have to visit regions near the safety\nboundary. Prior methods require hundreds of violations to achieve decent\nrewards on these tasks, whereas our proposed algorithms incur zero violations.", "time": "2022-03-11T04:38:21Z", "link": "http://arxiv.org/abs/2108.01846v2", "id": "2108.01846v2", "title": "Learning Barrier Certificates: Towards Safe Reinforcement Learning with\n  Zero Training-time Violations"}
{"author": "Mohammed Abouheaf, Wail Gueaieb, Frank Lewis", "abstract": "The control problem of the flexible wing aircraft is challenging due to the\nprevailing and high nonlinear deformations in the flexible wing system. This\nurged for new control mechanisms that are robust to the real-time variations in\nthe wing's aerodynamics. An online control mechanism based on a value iteration\nreinforcement learning process is developed for flexible wing aerial\nstructures. It employs a model-free control policy framework and a guaranteed\nconvergent adaptive learning architecture to solve the system's Bellman\noptimality equation. A Riccati equation is derived and shown to be equivalent\nto solving the underlying Bellman equation. The online reinforcement learning\nsolution is implemented using means of an adaptive-critic mechanism. The\ncontroller is proven to be asymptotically stable in the Lyapunov sense. It is\nassessed through computer simulations and its superior performance is\ndemonstrated on two scenarios under different operating conditions.", "time": "2021-08-05T06:10:37Z", "link": "http://arxiv.org/abs/2108.02393v1", "id": "2108.02393v1", "title": "Online Model-Free Reinforcement Learning for the Automatic Control of a\n  Flexible Wing Aircraft"}
{"author": "Hans Dermot Doran, Gianluca Ielpo, David Ganz, Michael Zapke", "abstract": "With edge-AI finding an increasing number of real-world applications,\nespecially in industry, the question of functionally safe applications using AI\nhas begun to be asked. In this body of work, we explore the issue of achieving\ndependable operation of neural networks. We discuss the issue of dependability\nin general implementation terms before examining lockstep solutions. We intuit\nthat it is not necessarily a given that two similar neural networks generate\nresults at precisely the same time and that synchronization between the\nplatforms will be required. We perform some preliminary measurements that may\nsupport this intuition and introduce some work in implementing lockstep neural\nnetwork engines.", "time": "2021-07-30T12:17:36Z", "link": "http://arxiv.org/abs/2108.02565v1", "id": "2108.02565v1", "title": "Dependable Neural Networks Through Redundancy, A Comparison of Redundant\n  Architectures"}
{"author": "Girish Joshi, Girish Chowdhary", "abstract": "In this paper, we present a Stochastic Deep Neural Network-based Model\nReference Adaptive Control. Building on our work \"Deep Model Reference Adaptive\nControl\", we extend the controller capability by using Bayesian deep neural\nnetworks (DNN) to represent uncertainties and model non-linearities. Stochastic\nDeep Model Reference Adaptive Control uses a Lyapunov-based method to adapt the\noutput-layer weights of the DNN model in real-time, while a data-driven\nsupervised learning algorithm is used to update the inner-layers parameters.\nThis asynchronous network update ensures boundedness and guaranteed tracking\nperformance with a learning-based real-time feedback controller. A Bayesian\napproach to DNN learning helped avoid over-fitting the data and provide\nconfidence intervals over the predictions. The controller's stochastic nature\nalso ensured \"Induced Persistency of excitation,\" leading to convergence of the\noverall system signal.", "time": "2021-08-04T14:05:09Z", "link": "http://arxiv.org/abs/2108.03120v1", "id": "2108.03120v1", "title": "Stochastic Deep Model Reference Adaptive Control"}
{"author": "Peide Cai, Hengli Wang, Yuxiang Sun, Ming Liu", "abstract": "Autonomous driving in multi-agent dynamic traffic scenarios is challenging:\nthe behaviors of road users are uncertain and are hard to model explicitly, and\nthe ego-vehicle should apply complicated negotiation skills with them, such as\nyielding, merging and taking turns, to achieve both safe and efficient driving\nin various settings. Traditional planning methods are largely rule-based and\nscale poorly in these complex dynamic scenarios, often leading to reactive or\neven overly conservative behaviors. Therefore, they require tedious human\nefforts to maintain workability. Recently, deep learning-based methods have\nshown promising results with better generalization capability but less hand\nengineering efforts. However, they are either implemented with supervised\nimitation learning (IL), which suffers from dataset bias and distribution\nmismatch issues, or are trained with deep reinforcement learning (DRL) but\nfocus on one specific traffic scenario. In this work, we propose DQ-GAT to\nachieve scalable and proactive autonomous driving, where graph attention-based\nnetworks are used to implicitly model interactions, and deep Q-learning is\nemployed to train the network end-to-end in an unsupervised manner. Extensive\nexperiments in a high-fidelity driving simulator show that our method achieves\nhigher success rates than previous learning-based methods and a traditional\nrule-based method, and better trades off safety and efficiency in both seen and\nunseen scenarios. Moreover, qualitative results on a trajectory dataset\nindicate that our learned policy can be transferred to the real world for\npractical applications with real-time speeds. Demonstration videos are\navailable at https://caipeide.github.io/dq-gat/.", "time": "2022-06-18T15:36:48Z", "link": "http://arxiv.org/abs/2108.05030v2", "id": "2108.05030v2", "title": "DQ-GAT: Towards Safe and Efficient Autonomous Driving with Deep\n  Q-Learning and Graph Attention Networks"}
{"author": "Somjit Nath, Mayank Baranwal, Harshad Khadilkar", "abstract": "Several real-world scenarios, such as remote control and sensing, are\ncomprised of action and observation delays. The presence of delays degrades the\nperformance of reinforcement learning (RL) algorithms, often to such an extent\nthat algorithms fail to learn anything substantial. This paper formally\ndescribes the notion of Markov Decision Processes (MDPs) with stochastic delays\nand shows that delayed MDPs can be transformed into equivalent standard MDPs\n(without delays) with significantly simplified cost structure. We employ this\nequivalence to derive a model-free Delay-Resolved RL framework and show that\neven a simple RL algorithm built upon this framework achieves near-optimal\nrewards in environments with stochastic delays in actions and observations. The\ndelay-resolved deep Q-network (DRDQN) algorithm is bench-marked on a variety of\nenvironments comprising of multi-step and stochastic delays and results in\nbetter performance, both in terms of achieving near-optimal rewards and\nminimizing the computational overhead thereof, with respect to the currently\nestablished algorithms.", "time": "2021-08-17T10:45:55Z", "link": "http://arxiv.org/abs/2108.07555v1", "id": "2108.07555v1", "title": "Revisiting State Augmentation methods for Reinforcement Learning with\n  Stochastic Delays"}
{"author": "Michael H. Lim, Andy Zeng, Brian Ichter, Maryam Bandari, Erwin Coumans, Claire Tomlin, Stefan Schaal, Aleksandra Faust", "abstract": "Enabling robots to solve multiple manipulation tasks has a wide range of\nindustrial applications. While learning-based approaches enjoy flexibility and\ngeneralizability, scaling these approaches to solve such compositional tasks\nremains a challenge. In this work, we aim to solve multi-task learning through\nthe lens of sequence-conditioning and weighted sampling. First, we propose a\nnew suite of benchmark specifically aimed at compositional tasks, MultiRavens,\nwhich allows defining custom task combinations through task modules that are\ninspired by industrial tasks and exemplify the difficulties in vision-based\nlearning and planning methods. Second, we propose a vision-based end-to-end\nsystem architecture, Sequence-Conditioned Transporter Networks, which augments\nGoal-Conditioned Transporter Networks with sequence-conditioning and weighted\nsampling and can efficiently learn to solve multi-task long horizon problems.\nOur analysis suggests that not only the new framework significantly improves\npick-and-place performance on novel 10 multi-task benchmark problems, but also\nthe multi-task learning with weighted sampling can vastly improve learning and\nagent performances on individual tasks.", "time": "2021-09-15T21:19:11Z", "link": "http://arxiv.org/abs/2109.07578v1", "id": "2109.07578v1", "title": "Multi-Task Learning with Sequence-Conditioned Transporter Networks"}
{"author": "Weison Lin, Tughrul Arslan", "abstract": "Edge AI accelerators have been emerging as a solution for near customers'\napplications in areas such as unmanned aerial vehicles (UAVs), image\nrecognition sensors, wearable devices, robotics, and remote sensing satellites.\nThese applications not only require meeting performance targets but also\nmeeting strict area and power constraints due to their portable mobility\nfeature and limited power sources. As a result, a column streaming-based\nconvolution engine has been proposed in this paper that includes column sets of\nprocessing elements design for flexibility in terms of the applicability for\ndifferent CNN algorithms in edge AI accelerators. Comparing to a commercialized\nCNN accelerator, the key results reveal that the column streaming-based\nconvolution engine requires similar execution cycles for processing a 227 x 227\nfeature map with avoiding zero-padding penalties.", "time": "2021-09-15T22:22:38Z", "link": "http://arxiv.org/abs/2109.07601v1", "id": "2109.07601v1", "title": "A Column Streaming-Based Convolution Engine and Mapping Algorithm for\n  CNN-based Edge AI accelerators"}
{"author": "Alexander Koenig, Zixi Liu, Lucas Janson, Robert Howe", "abstract": "A long-standing question in robot hand design is how accurate tactile sensing\nmust be. This paper uses simulated tactile signals and the reinforcement\nlearning (RL) framework to study the sensing needs in grasping systems. Our\nfirst experiment investigates the need for rich tactile sensing in the rewards\nof RL-based grasp refinement algorithms for multi-fingered robotic hands. We\nsystematically integrate different levels of tactile data into the rewards\nusing analytic grasp stability metrics. We find that combining information on\ncontact positions, normals, and forces in the reward yields the highest average\nsuccess rates of 95.4% for cuboids, 93.1% for cylinders, and 62.3% for spheres\nacross wrist position errors between 0 and 7 centimeters and rotational errors\nbetween 0 and 14 degrees. This contact-based reward outperforms a non-tactile\nbinary-reward baseline by 42.9%. Our follow-up experiment shows that when\ntraining with tactile-enabled rewards, the use of tactile information in the\ncontrol policy's state vector is drastically reducible at only a slight\nperformance decrease of at most 6.6% for no tactile sensing in the state. Since\npolicies do not require access to the reward signal at test time, our work\nimplies that models trained on tactile-enabled hands are deployable to robotic\nhands with a smaller sensor suite, potentially reducing cost dramatically.", "time": "2022-03-27T20:41:33Z", "link": "http://arxiv.org/abs/2109.11234v2", "id": "2109.11234v2", "title": "The Role of Tactile Sensing in Learning and Deploying Grasp Refinement\n  Algorithms"}
{"author": "Sridhar Dasaratha, Sai Akhil Puranam, Karmvir Singh Phogat, Sunil Reddy Tiyyagura, Nigel P. Duffy", "abstract": "We introduce DeepPSL a variant of probabilistic soft logic (PSL) to produce\nan end-to-end trainable system that integrates reasoning and perception. PSL\nrepresents first-order logic in terms of a convex graphical model -- hinge-loss\nMarkov random fields (HL-MRFs). PSL stands out among probabilistic logic\nframeworks due to its tractability having been applied to systems of more than\n1 billion ground rules. The key to our approach is to represent predicates in\nfirst-order logic using deep neural networks and then to approximately\nback-propagate through the HL-MRF and thus train every aspect of the\nfirst-order system being represented. We believe that this approach represents\nan interesting direction for the integration of deep learning and reasoning\ntechniques with applications to knowledge base learning, multi-task learning,\nand explainability. Evaluation on three different tasks demonstrates that\nDeepPSL significantly outperforms state-of-the-art neuro-symbolic methods on\nscalability while achieving comparable or better accuracy.", "time": "2023-02-04T10:30:09Z", "link": "http://arxiv.org/abs/2109.13662v4", "id": "2109.13662v4", "title": "DeepPSL: End-to-end perception and reasoning"}
{"author": "Jayanth Reddy Regatti, Aniket Anand Deshmukh, Frank Cheng, Young Hun Jung, Abhishek Gupta, Urun Dogan", "abstract": "Offline reinforcement learning is used to train policies in scenarios where\nreal-time access to the environment is expensive or impossible. As a natural\nconsequence of these harsh conditions, an agent may lack the resources to fully\nobserve the online environment before taking an action. We dub this situation\nthe resource-constrained setting. This leads to situations where the offline\ndataset (available for training) can contain fully processed features (using\npowerful language models, image models, complex sensors, etc.) which are not\navailable when actions are actually taken online. This disconnect leads to an\ninteresting and unexplored problem in offline RL: Is it possible to use a\nrichly processed offline dataset to train a policy which has access to fewer\nfeatures in the online environment? In this work, we introduce and formalize\nthis novel resource-constrained problem setting. We highlight the performance\ngap between policies trained using the full offline dataset and policies\ntrained using limited features. We address this performance gap with a policy\ntransfer algorithm which first trains a teacher agent using the offline dataset\nwhere features are fully available, and then transfers this knowledge to a\nstudent agent that only uses the resource-constrained features. To better\ncapture the challenge of this setting, we propose a data collection procedure:\nResource Constrained-Datasets for RL (RC-D4RL). We evaluate our transfer\nalgorithm on RC-D4RL and the popular D4RL benchmarks and observe consistent\nimprovement over the baseline (TD3+BC without transfer). The code for the\nexperiments is available at https://github.com/JayanthRR/RC-OfflineRL.", "time": "2021-12-07T23:46:15Z", "link": "http://arxiv.org/abs/2110.03165v2", "id": "2110.03165v2", "title": "Offline RL With Resource Constrained Online Deployment"}
{"author": "Marvin Alles, Elie Aljalbout", "abstract": "Robotic manipulators are widely used in modern manufacturing processes.\nHowever, their deployment in unstructured environments remains an open problem.\nTo deal with the variety, complexity, and uncertainty of real-world\nmanipulation tasks, it is essential to develop a flexible framework with\nreduced assumptions on the environment characteristics. In recent years,\nreinforcement learning (RL) has shown great results for single-arm robotic\nmanipulation. However, research focusing on dual-arm manipulation is still\nrare. From a classical control perspective, solving such tasks often involves\ncomplex modeling of interactions between two manipulators and the objects\nencountered in the tasks, as well as the two robots coupling at a control\nlevel. Instead, in this work, we explore the applicability of model-free RL to\ndual-arm assembly. As we aim to contribute towards an approach that is not\nlimited to dual-arm assembly, but dual-arm manipulation in general, we keep\nmodeling efforts at a minimum. Hence, to avoid modeling the interaction between\nthe two robots and the used assembly tools, we present a modular approach with\ntwo decentralized single-arm controllers which are coupled using a single\ncentralized learned policy. We reduce modeling effort to a minimum by using\nsparse rewards only. Our architecture enables successful assembly and simple\ntransfer from simulation to the real world. We demonstrate the effectiveness of\nthe framework on dual-arm peg-in-hole and analyze sample efficiency and success\nrates for different action spaces. Moreover, we compare results on different\nclearances and showcase disturbance recovery and robustness, when dealing with\nposition uncertainties. Finally we zero-shot transfer policies trained in\nsimulation to the real world and evaluate their performance.", "time": "2021-12-06T15:10:15Z", "link": "http://arxiv.org/abs/2110.04003v2", "id": "2110.04003v2", "title": "Learning to Centralize Dual-Arm Assembly"}
{"author": "Li Guo, Yonghong Peng, Rui Qin, Bingyu Liu", "abstract": "Iron ore feed load control is one of the most critical settings in a mineral\ngrinding process, directly impacting the quality of final products. The setting\nof the feed load is mainly determined by the characteristics of the ore\npellets. However, the characterisation of ore is challenging to acquire in many\nproduction environments, leading to poor feed load settings and inefficient\nproduction processes. This paper presents our work using deep learning models\nfor direct ore feed load estimation from ore pellet images. To address the\nchallenges caused by the large size of a full ore pellets image and the\nshortage of accurately annotated data, we treat the whole modelling process as\na weakly supervised learning problem. A two-stage model training algorithm and\ntwo neural network architectures are proposed. The experiment results show\ncompetitive model performance, and the trained models can be used for real-time\nfeed load estimation for grind process optimisation.", "time": "2021-10-06T11:24:47Z", "link": "http://arxiv.org/abs/2110.04063v1", "id": "2110.04063v1", "title": "A New Weakly Supervised Learning Approach for Real-time Iron Ore Feed\n  Load Estimation"}
{"author": "Yousef Emam, Gennaro Notomista, Paul Glotfelter, Zsolt Kira, Magnus Egerstedt", "abstract": "Reinforcement Learning (RL) has been shown to be effective in many scenarios.\nHowever, it typically requires the exploration of a sufficiently large number\nof state-action pairs, some of which may be unsafe. Consequently, its\napplication to safety-critical systems remains a challenge. An increasingly\ncommon approach to address safety involves the addition of a safety layer that\nprojects the RL actions onto a safe set of actions. In turn, a difficulty for\nsuch frameworks is how to effectively couple RL with the safety layer to\nimprove the learning performance. In this paper, we frame safety as a\ndifferentiable robust-control-barrier-function layer in a model-based RL\nframework. Moreover, we also propose an approach to modularly learn the\nunderlying reward-driven task, independent of safety constraints. We\ndemonstrate that this approach both ensures safety and effectively guides\nexploration during training in a range of experiments, including zero-shot\ntransfer when the reward is learned in a modular way.", "time": "2022-06-22T20:37:24Z", "link": "http://arxiv.org/abs/2110.05415v2", "id": "2110.05415v2", "title": "Safe Reinforcement Learning Using Robust Control Barrier Functions"}
{"author": "Dima Kilani, Baker Mohammad, Yasmin Halawani, Mohammed F. Tolba, Hani Saleh", "abstract": "This paper presents a novel cross-coupling capacitor processing unit (C3PU)\nthat supports analog-mixed signal in memory computing to perform\nmultiply-and-accumulate (MAC) operations. The C3PU consists of a capacitive\nunit, a CMOS transistor, and a voltage-to-time converter (VTC). The capacitive\nunit serves as a computational element that holds the multiplier operand and\nperforms multiplication once the multiplicand is applied at the terminal. The\nmultiplicand is the input voltage that is converted to a pulse width signal\nusing a low power VTC. The transistor transfers this multiplication where a\nvoltage level is generated. A demonstrator of 5x4 C3PU array that is capable of\nimplementing 4 MAC units is presented. The design has been verified using Monte\nCarlo simulation in 65 nm technology. The 5x4 C3PU consumed energy of 66.4\nfJ/MAC at 0.3 V voltage supply with an error of 5.7%. The proposed unit\nachieves lower energy and occupies a smaller area by 3.4x and 3.6x,\nrespectively, with similar error value when compared to a digital-based 8x4-bit\nfixed point MAC unit. The C3PU has been utilized through an iris fower\nclassification utilizing an artificial neural network which achieved a 90%\nclassification accuracy compared to ideal accuracy of 96.67% using MATLAB.", "time": "2021-10-11T08:08:12Z", "link": "http://arxiv.org/abs/2110.05947v1", "id": "2110.05947v1", "title": "C3PU: Cross-Coupling Capacitor Processing Unit Using Analog-Mixed Signal\n  In-Memory Computing for AI Inference"}
{"author": "Hsin-Hsuan Sung, Yuanchao Xu, Jiexiong Guan, Wei Niu, Shaoshan Liu, Bin Ren, Yanzhi Wang, Xipeng Shen", "abstract": "Autonomous driving is of great interest in both research and industry. The\nhigh cost has been one of the major roadblocks that slow down the development\nand adoption of autonomous driving in practice. This paper, for the first-time,\nshows that it is possible to run level-4 (i.e., fully autonomous driving)\nsoftware on a single off-the-shelf card (Jetson AGX Xavier) for less than $1k,\nan order of magnitude less than the state-of-the-art systems, while meeting all\nthe requirements of latency. The success comes from the resolution of some\nimportant issues shared by existing practices through a series of measures and\ninnovations. The study overturns the common perceptions of the computing\nresources required by level-4 autonomous driving, points out a promising path\nfor the industry to lower the cost, and suggests a number of research\nopportunities for rethinking the architecture, software design, and\noptimizations of autonomous driving.", "time": "2021-10-12T21:38:00Z", "link": "http://arxiv.org/abs/2110.06373v1", "id": "2110.06373v1", "title": "Enabling Level-4 Autonomous Driving on a Single $1k Off-the-Shelf Card"}
{"author": "Bingqing Chen, Jonathan Francis, Jean Oh, Eric Nyberg, Sylvia L. Herbert", "abstract": "Racing demands each vehicle to drive at its physical limits, when any safety\ninfraction could lead to catastrophic failure. In this work, we study the\nproblem of safe reinforcement learning (RL) for autonomous racing, using the\nvehicle's ego-camera view and speed as input. Given the nature of the task,\nautonomous agents need to be able to 1) identify and avoid unsafe scenarios\nunder the complex vehicle dynamics, and 2) make sub-second decision in a\nfast-changing environment. To satisfy these criteria, we propose to incorporate\nHamilton-Jacobi (HJ) reachability theory, a safety verification method for\ngeneral non-linear systems, into the constrained Markov decision process (CMDP)\nframework. HJ reachability not only provides a control-theoretic approach to\nlearn about safety, but also enables low-latency safety verification. Though HJ\nreachability is traditionally not scalable to high-dimensional systems, we\ndemonstrate that with neural approximation, the HJ safety value can be learned\ndirectly on vision context -- the highest-dimensional problem studied via the\nmethod, to-date. We evaluate our method on several benchmark tasks, including\nSafety Gym and Learn-to-Race (L2R), a recently-released high-fidelity\nautonomous racing environment. Our approach has significantly fewer constraint\nviolations in comparison to other constrained RL baselines in Safety Gym, and\nachieves the new state-of-the-art results on the L2R benchmark task. We provide\nadditional visualization of agent behavior at the following anonymized paper\nwebsite: https://sites.google.com/view/safeautonomousracing/home", "time": "2021-11-30T21:59:47Z", "link": "http://arxiv.org/abs/2110.07699v2", "id": "2110.07699v2", "title": "Safe Autonomous Racing via Approximate Reachability on Ego-vision"}
{"author": "Elie Aljalbout", "abstract": "Robot learning is a very promising topic for the future of automation and\nmachine intelligence. Future robots should be able to autonomously acquire\nskills, learn to represent their environment, and interact with it. While these\ntopics have been explored in simulation, real-world robot learning research\nseems to be still limited. This is due to the additional challenges encountered\nin the real-world, such as noisy sensors and actuators, safe exploration,\nnon-stationary dynamics, autonomous environment resetting as well as the cost\nof running experiments for long periods of time. Unless we develop scalable\nsolutions to these problems, learning complex tasks involving hand-eye\ncoordination and rich contacts will remain an untouched vision that is only\nfeasible in controlled lab environments. We propose dual-arm settings as\nplatforms for robot learning. Such settings enable safe data collection for\nacquiring manipulation skills as well as training perception modules in a\nrobot-supervised manner. They also ease the processes of resetting the\nenvironment. Furthermore, adversarial learning could potentially boost the\ngeneralization capability of robot learning methods by maximizing the\nexploration based on game-theoretic objectives while ensuring safety based on\ncollaborative task spaces. In this paper, we will discuss the potential\nbenefits of this setup as well as the challenges and research directions that\ncan be pursued.", "time": "2021-10-15T12:51:57Z", "link": "http://arxiv.org/abs/2110.08066v1", "id": "2110.08066v1", "title": "Dual-Arm Adversarial Robot Learning"}
{"author": "Gustavo Claudio Karl Couto, Eric Aislan Antonelo", "abstract": "Autonomous driving is a complex task, which has been tackled since the first\nself-driving car ALVINN in 1989, with a supervised learning approach, or\nbehavioral cloning (BC). In BC, a neural network is trained with state-action\npairs that constitute the training set made by an expert, i.e., a human driver.\nHowever, this type of imitation learning does not take into account the\ntemporal dependencies that might exist between actions taken in different\nmoments of a navigation trajectory. These type of tasks are better handled by\nreinforcement learning (RL) algorithms, which need to define a reward function.\nOn the other hand, more recent approaches to imitation learning, such as\nGenerative Adversarial Imitation Learning (GAIL), can train policies without\nexplicitly requiring to define a reward function, allowing an agent to learn by\ntrial and error directly on a training set of expert trajectories. In this\nwork, we propose two variations of GAIL for autonomous navigation of a vehicle\nin the realistic CARLA simulation environment for urban scenarios. Both of them\nuse the same network architecture, which process high dimensional image input\nfrom three frontal cameras, and other nine continuous inputs representing the\nvelocity, the next point from the sparse trajectory and a high-level driving\ncommand. We show that both of them are capable of imitating the expert\ntrajectory from start to end after training ends, but the GAIL loss function\nthat is augmented with BC outperforms the former in terms of convergence time\nand training stability.", "time": "2021-10-16T15:04:13Z", "link": "http://arxiv.org/abs/2110.08586v1", "id": "2110.08586v1", "title": "Generative Adversarial Imitation Learning for End-to-End Autonomous\n  Driving on Urban Environments"}
{"author": "Giulia Pedrielli, Tanmay Khandait, Surdeep Chotaliya, Quinn Thibeault, Hao Huang, Mauricio Castillo-Effen, Georgios Fainekos", "abstract": "Requirements driven search-based testing (also known as falsification) has\nproven to be a practical and effective method for discovering erroneous\nbehaviors in Cyber-Physical Systems. Despite the constant improvements on the\nperformance and applicability of falsification methods, they all share a common\ncharacteristic. Namely, they are best-effort methods which do not provide any\nguarantees on the absence of erroneous behaviors (falsifiers) when the testing\nbudget is exhausted. The absence of finite time guarantees is a major\nlimitation which prevents falsification methods from being utilized in\ncertification procedures. In this paper, we address the finite-time guarantees\nproblem by developing a new stochastic algorithm. Our proposed algorithm not\nonly estimates (bounds) the probability that falsifying behaviors exist, but\nalso it identifies the regions where these falsifying behaviors may occur. We\ndemonstrate the applicability of our approach on standard benchmark functions\nfrom the optimization literature and on the F16 benchmark problem.", "time": "2021-10-20T19:05:00Z", "link": "http://arxiv.org/abs/2110.10729v1", "id": "2110.10729v1", "title": "Part-X: A Family of Stochastic Algorithms for Search-Based Test\n  Generation with Probabilistic Guarantees"}
{"author": "Antoine Marot, Benjamin Donnot, Karim Chaouache, Adrian Kelly, Qiuhua Huang, Ramij-Raja Hossain, Jochen L. Cremer", "abstract": "Artificial agents are promising for real-time power network operations,\nparticularly, to compute remedial actions for congestion management. However,\ndue to high reliability requirements, purely autonomous agents will not be\ndeployed any time soon and operators will be in charge of taking action for the\nforeseeable future. Aiming at designing assistant for operators, we instead\nconsider humans in the loop and propose an original formulation. We first\nadvance an agent with the ability to send to the operator alarms ahead of time\nwhen the proposed actions are of low confidence. We further model the\noperator's available attention as a budget that decreases when alarms are sent.\nWe present the design and results of our competition \"Learning to run a power\nnetwork with trust\" in which we evaluate our formulation and benchmark the\nability of submitted agents to send relevant alarms while operating the network\nto their best.", "time": "2022-04-16T09:20:26Z", "link": "http://arxiv.org/abs/2110.12908v2", "id": "2110.12908v2", "title": "Learning to run a power network with trust"}
{"author": "Reilly Raab, Yang Liu", "abstract": "Realistically -- and equitably -- modeling the dynamics of group-level\ndisparities in machine learning remains an open problem. In particular, we\ndesire models that do not suppose inherent differences between artificial\ngroups of people -- but rather endogenize disparities by appeal to unequal\ninitial conditions of insular subpopulations. In this paper, agents each have a\nreal-valued feature $X$ (e.g., credit score) informed by a \"true\" binary label\n$Y$ representing qualification (e.g., for a loan). Each agent alternately (1)\nreceives a binary classification label $\\hat{Y}$ (e.g., loan approval) from a\nBayes-optimal machine learning classifier observing $X$ and (2) may update\ntheir qualification $Y$ by imitating successful strategies (e.g., seek a raise)\nwithin an isolated group $G$ of agents to which they belong. We consider the\ndisparity of qualification rates $\\Pr(Y=1)$ between different groups and how\nthis disparity changes subject to a sequence of Bayes-optimal classifiers\nrepeatedly retrained on the global population. We model the evolving\nqualification rates of each subpopulation (group) using the replicator\nequation, which derives from a class of imitation processes. We show that\ndifferences in qualification rates between subpopulations can persist\nindefinitely for a set of non-trivial equilibrium states due to uniformed\nclassifier deployments, even when groups are identical in all aspects except\ninitial qualification densities. We next simulate the effects of commonly\nproposed fairness interventions on this dynamical system along with a new\nfeedback control mechanism capable of permanently eliminating group-level\nqualification rate disparities. We conclude by discussing the limitations of\nour model and findings and by outlining potential future work.", "time": "2021-12-30T02:53:15Z", "link": "http://arxiv.org/abs/2111.01201v2", "id": "2111.01201v2", "title": "Unintended Selection: Persistent Qualification Rate Disparities and\n  Interventions"}
{"author": "Zhongxia Yan, Cathy Wu", "abstract": "We propose a model-free reinforcement learning method for controlling mixed\nautonomy traffic in simulated traffic networks with through-traffic-only\ntwo-way and four-way intersections. Our method utilizes multi-agent policy\ndecomposition which allows decentralized control based on local observations\nfor an arbitrary number of controlled vehicles. We demonstrate that, even\nwithout reward shaping, reinforcement learning learns to coordinate the\nvehicles to exhibit traffic signal-like behaviors, achieving near-optimal\nthroughput with 33-50% controlled vehicles. With the help of multi-task\nlearning and transfer learning, we show that this behavior generalizes across\ninflow rates and size of the traffic network. Our code, models, and videos of\nresults are available at\nhttps://github.com/ZhongxiaYan/mixed_autonomy_intersections.", "time": "2021-11-08T18:03:18Z", "link": "http://arxiv.org/abs/2111.04686v1", "id": "2111.04686v1", "title": "Reinforcement Learning for Mixed Autonomy Intersections"}
{"author": "David Biagioni, Xiangyu Zhang, Dylan Wald, Deepthi Vaidhynathan, Rohit Chintala, Jennifer King, Ahmed S. Zamzam", "abstract": "We present the PowerGridworld software package to provide users with a\nlightweight, modular, and customizable framework for creating\npower-systems-focused, multi-agent Gym environments that readily integrate with\nexisting training frameworks for reinforcement learning (RL). Although many\nframeworks exist for training multi-agent RL (MARL) policies, none can rapidly\nprototype and develop the environments themselves, especially in the context of\nheterogeneous (composite, multi-device) power systems where power flow\nsolutions are required to define grid-level variables and costs. PowerGridworld\nis an open-source software package that helps to fill this gap. To highlight\nPowerGridworld's key features, we present two case studies and demonstrate\nlearning MARL policies using both OpenAI's multi-agent deep deterministic\npolicy gradient (MADDPG) and RLLib's proximal policy optimization (PPO)\nalgorithms. In both cases, at least some subset of agents incorporates elements\nof the power flow solution at each time step as part of their reward (negative\ncost) structures.", "time": "2021-11-10T22:22:07Z", "link": "http://arxiv.org/abs/2111.05969v1", "id": "2111.05969v1", "title": "PowerGridworld: A Framework for Multi-Agent Reinforcement Learning in\n  Power Systems"}
{"author": "Marcel Menner, Karl Berntorp, Stefano Di Cairano", "abstract": "This paper proposes a method for calibrating control parameters. Examples of\nsuch control parameters are gains of PID controllers, weights of a cost\nfunction for optimal control, filter coefficients, the sliding surface of a\nsliding mode controller, or weights of a neural network. Hence, the proposed\nmethod can be applied to a wide range of controllers. The method uses a Kalman\nfilter that estimates control parameters, using data of closed-loop system\noperation. The control parameter calibration is driven by a training objective,\nwhich encompasses specifications on the performance of the dynamical system.\nThe performance-driven calibration method tunes the parameters online and\nrobustly, is computationally efficient, has low data storage requirements, and\nis easy to implement making it appealing for many real-time applications.\nSimulation results show that the method is able to learn control parameters\nquickly, is able to tune the parameters to compensate for disturbances, and is\nrobust to noise. A simulation study with the high-fidelity vehicle simulator\nCarSim shows that the method can calibrate controllers of a complex dynamical\nsystem online, which indicates its applicability to a real-world system. We\nalso verify the real-time feasibility on an embedded platform with\nautomotive-grade processors by implementing our method on a dSPACE\nMicroAutoBox-II rapid prototyping unit.", "time": "2023-03-08T19:45:11Z", "link": "http://arxiv.org/abs/2111.10832v3", "id": "2111.10832v3", "title": "Automated Controller Calibration by Kalman Filtering"}
{"author": "Daniel J. B. Harrold, Jun Cao, Zhong Fan", "abstract": "In this paper, multi-agent reinforcement learning is used to control a hybrid\nenergy storage system working collaboratively to reduce the energy costs of a\nmicrogrid through maximising the value of renewable energy and trading. The\nagents must learn to control three different types of energy storage system\nsuited for short, medium, and long-term storage under fluctuating demand,\ndynamic wholesale energy prices, and unpredictable renewable energy generation.\nTwo case studies are considered: the first looking at how the energy storage\nsystems can better integrate renewable energy generation under dynamic pricing,\nand the second with how those same agents can be used alongside an aggregator\nagent to sell energy to self-interested external microgrids looking to reduce\ntheir own energy bills. This work found that the centralised learning with\ndecentralised execution of the multi-agent deep deterministic policy gradient\nand its state-of-the-art variants allowed the multi-agent methods to perform\nsignificantly better than the control from a single global agent. It was also\nfound that using separate reward functions in the multi-agent approach\nperformed much better than using a single control agent. Being able to trade\nwith the other microgrids, rather than just selling back to the utility grid,\nalso was found to greatly increase the grid's savings.", "time": "2021-12-05T17:12:58Z", "link": "http://arxiv.org/abs/2111.10898v2", "id": "2111.10898v2", "title": "Renewable energy integration and microgrid energy trading using\n  multi-agent deep reinforcement learning"}
{"author": "Haitong Ma, Changliu Liu, Shengbo Eben Li, Sifa Zheng, Wenchao Sun, Jianyu Chen", "abstract": "In the trial-and-error mechanism of reinforcement learning (RL), a notorious\ncontradiction arises when we expect to learn a safe policy: how to learn a safe\npolicy without enough data and prior model about the dangerous region? Existing\nmethods mostly use the posterior penalty for dangerous actions, which means\nthat the agent is not penalized until experiencing danger. This fact causes\nthat the agent cannot learn a zero-violation policy even after convergence.\nOtherwise, it would not receive any penalty and lose the knowledge about\ndanger. In this paper, we propose the safe set actor-critic (SSAC) algorithm,\nwhich confines the policy update using safety-oriented energy functions, or the\nsafety indexes. The safety index is designed to increase rapidly for\npotentially dangerous actions, which allows us to locate the safe set on the\naction space, or the control safe set. Therefore, we can identify the dangerous\nactions prior to taking them, and further obtain a zero constraint-violation\npolicy after convergence.We claim that we can learn the energy function in a\nmodel-free manner similar to learning a value function. By using the energy\nfunction transition as the constraint objective, we formulate a constrained RL\nproblem. We prove that our Lagrangian-based solutions make sure that the\nlearned policy will converge to the constrained optimum under some assumptions.\nThe proposed algorithm is evaluated on both the complex simulation environments\nand a hardware-in-loop (HIL) experiment with a real controller from the\nautonomous vehicle. Experimental results suggest that the converged policy in\nall environments achieves zero constraint violation and comparable performance\nwith model-based baselines.", "time": "2021-11-25T07:24:30Z", "link": "http://arxiv.org/abs/2111.12953v1", "id": "2111.12953v1", "title": "Learn Zero-Constraint-Violation Policy in Model-Free Constrained\n  Reinforcement Learning"}
{"author": "Thomas Lew, Lucas Janson, Riccardo Bonalli, Marco Pavone", "abstract": "In this work, we analyze an efficient sampling-based algorithm for\ngeneral-purpose reachability analysis, which remains a notoriously challenging\nproblem with applications ranging from neural network verification to safety\nanalysis of dynamical systems. By sampling inputs, evaluating their images in\nthe true reachable set, and taking their $\\epsilon$-padded convex hull as a set\nestimator, this algorithm applies to general problem settings and is simple to\nimplement. Our main contribution is the derivation of asymptotic and\nfinite-sample accuracy guarantees using random set theory. This analysis\ninforms algorithmic design to obtain an $\\epsilon$-close reachable set\napproximation with high probability, provides insights into which reachability\nproblems are most challenging, and motivates safety-critical applications of\nthe technique. On a neural network verification task, we show that this\napproach is more accurate and significantly faster than prior work. Informed by\nour analysis, we also design a robust model predictive controller that we\ndemonstrate in hardware experiments.", "time": "2022-04-13T23:16:27Z", "link": "http://arxiv.org/abs/2112.05745v3", "id": "2112.05745v3", "title": "A Simple and Efficient Sampling-based Algorithm for General Reachability\n  Analysis"}
{"author": "Cameron Hickert, Sirui Li, Cathy Wu", "abstract": "Advances in autonomy offer the potential for dramatic positive outcomes in a\nnumber of domains, yet enabling their safe deployment remains an open problem.\nThis work's motivating question is: In safety-critical settings, can we avoid\nthe need to have one human supervise one machine at all times? The work\nformalizes this scalable supervision problem by considering remotely located\nhuman supervisors and investigating how autonomous agents can cooperate to\nachieve safety. This article focuses on the safety-critical context of\nautonomous vehicles (AVs) merging into traffic consisting of a mixture of AVs\nand human drivers. The analysis establishes high reliability upper bounds on\nhuman supervision requirements. It further shows that AV cooperation can\nimprove supervision reliability by orders of magnitude and counterintuitively\nrequires fewer supervisors (per AV) as more AVs are adopted. These analytical\nresults leverage queuing-theoretic analysis, order statistics, and a\nconservative, reachability-based approach. A key takeaway is the potential\nvalue of cooperation in enabling the deployment of autonomy at scale. While\nthis work focuses on AVs, the scalable supervision framework may be of\nindependent interest to a broader array of autonomous control challenges.", "time": "2023-05-10T20:18:12Z", "link": "http://arxiv.org/abs/2112.07569v2", "id": "2112.07569v2", "title": "Cooperation for Scalable Supervision of Autonomy in Mixed Traffic"}
{"author": "Sampada Deglurkar, Michael H. Lim, Johnathan Tucker, Zachary N. Sunberg, Aleksandra Faust, Claire J. Tomlin", "abstract": "The Partially Observable Markov Decision Process (POMDP) is a powerful\nframework for capturing decision-making problems that involve state and\ntransition uncertainty. However, most current POMDP planners cannot effectively\nhandle high-dimensional image observations prevalent in real world\napplications, and often require lengthy online training that requires\ninteraction with the environment. In this work, we propose Visual Tree Search\n(VTS), a compositional learning and planning procedure that combines generative\nmodels learned offline with online model-based POMDP planning. The deep\ngenerative observation models evaluate the likelihood of and predict future\nimage observations in a Monte Carlo tree search planner. We show that VTS is\nrobust to different types of image noises that were not present during training\nand can adapt to different reward structures without the need to re-train. This\nnew approach significantly and stably outperforms several baseline\nstate-of-the-art vision POMDP algorithms while using a fraction of the training\ntime.", "time": "2022-12-03T02:50:06Z", "link": "http://arxiv.org/abs/2112.09456v2", "id": "2112.09456v2", "title": "Compositional Learning-based Planning for Vision POMDPs"}
{"author": "Mohamad Kazem Shirani Faradonbeh, Mohamad Sadegh Shirani Faradonbeh", "abstract": "Linear dynamical systems are canonical models for learning-based control of\nplants with uncertain dynamics. The setting consists of a stochastic\ndifferential equation that captures the state evolution of the plant\nunderstudy, while the true dynamics matrices are unknown and need to be learned\nfrom the observed data of state trajectory. An important issue is to ensure\nthat the system is stabilized and destabilizing control actions due to model\nuncertainties are precluded as soon as possible. A reliable stabilization\nprocedure for this purpose that can effectively learn from unstable data to\nstabilize the system in a finite time is not currently available. In this work,\nwe propose a novel Bayesian learning algorithm that stabilizes unknown\ncontinuous-time stochastic linear systems. The presented algorithm is flexible\nand exposes effective stabilization performance after a remarkably short time\nperiod of interacting with the system.", "time": "2021-12-30T15:31:35Z", "link": "http://arxiv.org/abs/2112.15094v1", "id": "2112.15094v1", "title": "Bayesian Algorithms Learn to Stabilize Unknown Continuous-Time Systems"}
{"author": "Rutwik Shah, Bruno Astuto, Tyler Gleason, Will Fletcher, Justin Banaga, Kevin Sweetwood, Allen Ye, Rina Patel, Kevin McGill, Thomas Link, Jason Crane, Valentina Pedoia, Sharmila Majumdar", "abstract": "Radiologists today play a key role in making diagnostic decisions and\nlabeling images for training A.I. algorithms. Low inter-reader reliability\n(IRR) can be seen between experts when interpreting challenging cases. While\nteams-based decisions are known to outperform individual decisions,\ninter-personal biases often creep up in group interactions which limit\nnon-dominant participants from expressing true opinions. To overcome the dual\nproblems of low consensus and inter-personal bias, we explored a solution\nmodeled on biological swarms of bees. Two separate cohorts; three radiologists\nand five radiology residents collaborated on a digital swarm platform in real\ntime and in a blinded fashion, grading meniscal lesions on knee MR exams. These\nconsensus votes were benchmarked against clinical (arthroscopy) and\nradiological (senior-most radiologist) observations. The IRR of the consensus\nvotes was compared to the IRR of the majority and most confident votes of the\ntwo cohorts.The radiologist cohort saw an improvement of 23% in IRR of swarm\nvotes over majority vote. Similar improvement of 23% in IRR in 3-resident swarm\nvotes over majority vote, was observed. The 5-resident swarm had an even higher\nimprovement of 32% in IRR over majority vote. Swarm consensus votes also\nimproved specificity by up to 50%. The swarm consensus votes outperformed\nindividual and majority vote decisions in both the radiologists and resident\ncohorts. The 5-resident swarm had higher IRR than 3-resident swarm indicating\npositive effect of increased swarm size. The attending and resident swarms also\noutperformed predictions from a state-of-the-art A.I. algorithm. Utilizing a\ndigital swarm platform improved agreement and allows participants to express\njudgement free intent, resulting in superior clinical performance and robust\nA.I. training labels.", "time": "2021-09-06T23:59:10Z", "link": "http://arxiv.org/abs/2107.07341v2", "id": "2107.07341v2", "title": "Utilizing a digital swarm intelligence platform to improve consensus\n  among radiologists and exploring its applications"}
{"author": "Pawan Kumar", "abstract": "As a big data application, extreme multilabel classification has emerged as\nan important research topic with applications in ranking and recommendation of\nproducts and items. A scalable hybrid distributed and shared memory\nimplementation of extreme classification for large scale ranking and\nrecommendation is proposed. In particular, the implementation is a mix of\nmessage passing using MPI across nodes and using multithreading on the nodes\nusing OpenMP. The expression for communication latency and communication volume\nis derived. Parallelism using work-span model is derived for shared memory\narchitecture. This throws light on the expected scalability of similar extreme\nclassification methods. Experiments show that the implementation is relatively\nfaster to train and test on some large datasets. In some cases, model size is\nrelatively small.", "time": "2021-10-20T10:56:58Z", "link": "http://arxiv.org/abs/2112.10297v1", "id": "2112.10297v1", "title": "DXML: Distributed Extreme Multilabel Classification"}
{"author": "Alireza Masoumian, Shayan Kiyani, Mohammad Hossein Yassaee", "abstract": "The problem of Sequential Estimation under Multiple Resources (SEMR) is\ndefined in a federated setting. SEMR could be considered as the intersection of\nstatistical estimation and bandit theory. In this problem, an agent is\nconfronting with k resources to estimate a parameter $\\theta$. The agent should\ncontinuously learn the quality of the resources by wisely choosing them and at\nthe end, proposes an estimator based on the collected data. In this paper, we\nassume that the resources' distributions are Gaussian. The quality of the final\nestimator is evaluated by its mean squared error. Also, we restrict our class\nof estimators to unbiased estimators in order to define a meaningful notion of\nregret. The regret measures the performance of the agent by the variance of the\nfinal estimator in comparison to the optimal variance. We propose a lower bound\nto determine the fundamental limit of the setting even in the case that the\ndistributions are not Gaussian. Also, we offer an order-optimal algorithm to\nachieve this lower bound.", "time": "2021-09-29T20:19:09Z", "link": "http://arxiv.org/abs/2109.14703v1", "id": "2109.14703v1", "title": "Sequential Estimation under Multiple Resources: a Bandit Point of View"}
{"author": "Muhammad Shafique, Mahum Naseer, Theocharis Theocharides, Christos Kyrkou, Onur Mutlu, Lois Orosa, Jungwook Choi", "abstract": "Machine Learning (ML) techniques have been rapidly adopted by smart\nCyber-Physical Systems (CPS) and Internet-of-Things (IoT) due to their powerful\ndecision-making capabilities. However, they are vulnerable to various security\nand reliability threats, at both hardware and software levels, that compromise\ntheir accuracy. These threats get aggravated in emerging edge ML devices that\nhave stringent constraints in terms of resources (e.g., compute, memory,\npower/energy), and that therefore cannot employ costly security and reliability\nmeasures. Security, reliability, and vulnerability mitigation techniques span\nfrom network security measures to hardware protection, with an increased\ninterest towards formal verification of trained ML models.\n  This paper summarizes the prominent vulnerabilities of modern ML systems,\nhighlights successful defenses and mitigation techniques against these\nvulnerabilities, both at the cloud (i.e., during the ML training phase) and\nedge (i.e., during the ML inference stage), discusses the implications of a\nresource-constrained design on the reliability and security of the system,\nidentifies verification methodologies to ensure correct system behavior, and\ndescribes open research challenges for building secure and reliable ML systems\nat both the edge and the cloud.", "time": "2021-01-04T20:06:56Z", "link": "http://arxiv.org/abs/2101.02559v1", "id": "2101.02559v1", "title": "Robust Machine Learning Systems: Challenges, Current Trends,\n  Perspectives, and the Road Ahead"}
{"author": "Stefano Massaroli, Michael Poli, Federico Califano, Jinkyoo Park, Atsushi Yamashita, Hajime Asama", "abstract": "We introduce optimal energy shaping as an enhancement of classical\npassivity-based control methods. A promising feature of passivity theory,\nalongside stability, has traditionally been claimed to be intuitive performance\ntuning along the execution of a given task. However, a systematic approach to\nadjust performance within a passive control framework has yet to be developed,\nas each method relies on few and problem-specific practical insights. Here, we\ncast the classic energy-shaping control design process in an optimal control\nframework; once a task-dependent performance metric is defined, an optimal\nsolution is systematically obtained through an iterative procedure relying on\nneural networks and gradient-based optimization. The proposed method is\nvalidated on state-regulation tasks.", "time": "2021-01-14T10:25:58Z", "link": "http://arxiv.org/abs/2101.05537v1", "id": "2101.05537v1", "title": "Optimal Energy Shaping via Neural Approximators"}
{"author": "Pranav Ashok, Mathias Jackermeier, Jan KÅetÃ­nskÃ½, Christoph Weinhuber, Maximilian Weininger, Mayank Yadav", "abstract": "Recent advances have shown how decision trees are apt data structures for\nconcisely representing strategies (or controllers) satisfying various\nobjectives. Moreover, they also make the strategy more explainable. The recent\ntool dtControl had provided pipelines with tools supporting strategy synthesis\nfor hybrid systems, such as SCOTS and Uppaal Stratego. We present dtControl\n2.0, a new version with several fundamentally novel features. Most importantly,\nthe user can now provide domain knowledge to be exploited in the decision tree\nlearning process and can also interactively steer the process based on the\ndynamically provided information. To this end, we also provide a graphical user\ninterface. It allows for inspection and re-computation of parts of the result,\nsuggesting as well as receiving advice on predicates, and visual simulation of\nthe decision-making process. Besides, we interface model checkers of\nprobabilistic systems, namely Storm and PRISM and provide dedicated support for\ncategorical enumeration-type state variables. Consequently, the controllers are\nmore explainable and smaller.", "time": "2021-05-04T10:10:43Z", "link": "http://arxiv.org/abs/2101.07202v2", "id": "2101.07202v2", "title": "dtControl 2.0: Explainable Strategy Representation via Decision Tree\n  Learning Steered by Experts"}
{"author": "Thomas Chaffre, Julien Moras, Adrien Chan-Hon-Tong, Julien Marzat, Karl Sammut, Gilles Le Chenadec, Benoit Clement", "abstract": "Navigation problems under unknown varying conditions are among the most\nimportant and well-studied problems in the control field. Classic model-based\nadaptive control methods can be applied only when a convenient model of the\nplant or environment is provided. Recent model-free adaptive control methods\naim at removing this dependency by learning the physical characteristics of the\nplant and/or process directly from sensor feedback. Although there have been\nprior attempts at improving these techniques, it remains an open question as to\nwhether it is possible to cope with real-world uncertainties in a control\nsystem that is fully based on either paradigm. We propose a conceptually simple\nlearning-based approach composed of a full state feedback controller, tuned\nrobustly by a deep reinforcement learning framework based on the Soft\nActor-Critic algorithm. We compare it, in realistic simulations, to a\nmodel-free controller that uses the same deep reinforcement learning framework\nfor the control of a micro aerial vehicle under wind gust. The results indicate\nthe great potential of learning-based adaptive control methods in modern\ndynamical systems.", "time": "2021-07-06T01:05:45Z", "link": "http://arxiv.org/abs/2101.12501v2", "id": "2101.12501v2", "title": "Learning-based vs Model-free Adaptive Control of a MAV under Wind Gust"}
{"author": "Arash Mahyari", "abstract": "Despite advancements in deep reinforcement learning algorithms, developing an\neffective exploration strategy is still an open problem. Most existing\nexploration strategies either are based on simple heuristics, or require the\nmodel of the environment, or train additional deep neural networks to generate\nimagination-augmented paths. In this paper, a revolutionary algorithm, called\nPolicy Augmentation, is introduced. Policy Augmentation is based on a newly\ndeveloped inductive matrix completion method. The proposed algorithm augments\nthe values of unexplored state-action pairs, helping the agent take actions\nthat will result in high-value returns while the agent is in the early\nepisodes. Training deep reinforcement learning algorithms with high-value\nrollouts leads to the faster convergence of deep reinforcement learning\nalgorithms. Our experiments show the superior performance of Policy\nAugmentation. The code can be found at:\nhttps://github.com/arashmahyari/PolicyAugmentation.", "time": "2021-02-10T03:51:45Z", "link": "http://arxiv.org/abs/2102.05249v1", "id": "2102.05249v1", "title": "Policy Augmentation: An Exploration Strategy for Faster Convergence of\n  Deep Reinforcement Learning Algorithms"}
{"author": "Chao-Han Huck Yang, I-Te Danny Hung, Yi Ouyang, Pin-Yu Chen", "abstract": "Deep reinforcement learning (DRL) has demonstrated impressive performance in\nvarious gaming simulators and real-world applications. In practice, however, a\nDRL agent may receive faulty observation by abrupt interferences such as\nblack-out, frozen-screen, and adversarial perturbation. How to design a\nresilient DRL algorithm against these rare but mission-critical and\nsafety-crucial scenarios is an essential yet challenging task. In this paper,\nwe consider a deep q-network (DQN) framework training with an auxiliary task of\nobservational interferences such as artificial noises. Inspired by causal\ninference for observational interference, we propose a causal inference based\nDQN algorithm called causal inference Q-network (CIQ). We evaluate the\nperformance of CIQ in several benchmark DQN environments with different types\nof interferences as auxiliary labels. Our experimental results show that the\nproposed CIQ method could achieve higher performance and more resilience\nagainst observational interferences.", "time": "2022-01-25T06:39:24Z", "link": "http://arxiv.org/abs/2102.09677v3", "id": "2102.09677v3", "title": "Training a Resilient Q-Network against Observational Interference"}
{"author": "Shikhar Bahl, Abhinav Gupta, Deepak Pathak", "abstract": "We tackle the problem of generalization to unseen configurations for dynamic\ntasks in the real world while learning from high-dimensional image input. The\nfamily of nonlinear dynamical system-based methods have successfully\ndemonstrated dynamic robot behaviors but have difficulty in generalizing to\nunseen configurations as well as learning from image inputs. Recent works\napproach this issue by using deep network policies and reparameterize actions\nto embed the structure of dynamical systems but still struggle in domains with\ndiverse configurations of image goals, and hence, find it difficult to\ngeneralize. In this paper, we address this dichotomy by leveraging embedding\nthe structure of dynamical systems in a hierarchical deep policy learning\nframework, called Hierarchical Neural Dynamical Policies (H-NDPs). Instead of\nfitting deep dynamical systems to diverse data directly, H-NDPs form a\ncurriculum by learning local dynamical system-based policies on small regions\nin state-space and then distill them into a global dynamical system-based\npolicy that operates only from high-dimensional images. H-NDPs additionally\nprovide smooth trajectories, a strong safety benefit in the real world. We\nperform extensive experiments on dynamic tasks both in the real world (digit\nwriting, scooping, and pouring) and simulation (catching, throwing, picking).\nWe show that H-NDPs are easily integrated with both imitation as well as\nreinforcement learning setups and achieve state-of-the-art results. Video\nresults are at https://shikharbahl.github.io/hierarchical-ndps/", "time": "2021-07-12T17:59:58Z", "link": "http://arxiv.org/abs/2107.05627v1", "id": "2107.05627v1", "title": "Hierarchical Neural Dynamic Policies"}
{"author": "Amrit Singh Bedi, Anjaly Parayil, Junyu Zhang, Mengdi Wang, Alec Koppel", "abstract": "Reinforcement learning is a framework for interactive decision-making with\nincentives sequentially revealed across time without a system dynamics model.\nDue to its scaling to continuous spaces, we focus on policy search where one\niteratively improves a parameterized policy with stochastic policy gradient\n(PG) updates. In tabular Markov Decision Problems (MDPs), under persistent\nexploration and suitable parameterization, global optimality may be obtained.\nBy contrast, in continuous space, the non-convexity poses a pathological\nchallenge as evidenced by existing convergence results being mostly limited to\nstationarity or arbitrary local extrema. To close this gap, we step towards\npersistent exploration in continuous space through policy parameterizations\ndefined by distributions of heavier tails defined by tail-index parameter\nalpha, which increases the likelihood of jumping in state space. Doing so\ninvalidates smoothness conditions of the score function common to PG. Thus, we\nestablish how the convergence rate to stationarity depends on the policy's tail\nindex alpha, a Holder continuity parameter, integrability conditions, and an\nexploration tolerance parameter introduced here for the first time. Further, we\ncharacterize the dependence of the set of local maxima on the tail index\nthrough an exit and transition time analysis of a suitably defined Markov\nchain, identifying that policies associated with Levy Processes of a heavier\ntail converge to wider peaks. This phenomenon yields improved stability to\nperturbations in supervised learning, which we corroborate also manifests in\nimproved performance of policy search, especially when myopic and farsighted\nincentives are misaligned.", "time": "2023-01-02T22:08:42Z", "link": "http://arxiv.org/abs/2106.08414v2", "id": "2106.08414v2", "title": "On the Sample Complexity and Metastability of Heavy-tailed Policy Search\n  in Continuous Control"}
{"author": "Neel Gandhi, Shakti Mishra", "abstract": "Reinforcement Learning has applications in field of mechatronics, robotics,\nand other resource-constrained control system. Problem of resource allocation\nis primarily solved using traditional predefined techniques and modern deep\nlearning methods. The drawback of predefined and most deep learning methods for\nresource allocation is failing to meet the requirements in cases of uncertain\nsystem environment. We can approach problem of resource allocation in uncertain\nsystem environment alongside following certain criteria using deep\nreinforcement learning. Also, reinforcement learning has ability for adapting\nto new uncertain environment for prolonged period of time. The paper provides a\ndetailed comparative analysis on various deep reinforcement learning methods by\napplying different components to modify architecture of reinforcement learning\nwith use of noisy layers, prioritized replay, bagging, duelling networks, and\nother related combination to obtain improvement in terms of performance and\nreduction of computational cost. The paper identifies problem of resource\nallocation in uncertain environment could be effectively solved using Noisy\nBagging duelling double deep Q network achieving efficiency of 97.7% by\nmaximizing reward with significant exploration in given simulated environment\nfor resource allocation.", "time": "2021-06-17T13:13:34Z", "link": "http://arxiv.org/abs/2106.09461v1", "id": "2106.09461v1", "title": "Modelling resource allocation in uncertain system environment through\n  deep reinforcement learning"}
{"author": "Tamir Blum, Gabin Paillet, Watcharawut Masawat, Mickael Laine, Kazuya Yoshida", "abstract": "The visuomotor system of any animal is critical for its survival, and the\ndevelopment of a complex one within humans is large factor in our success as a\nspecies on Earth. This system is an essential part of our ability to adapt to\nour environment. We use this system continuously throughout the day, when\npicking something up, or walking around while avoiding bumping into objects.\nEquipping robots with such capabilities will help produce more intelligent\nlocomotion with the ability to more easily understand their surroundings and to\nmove safely. In particular, such capabilities are desirable for traversing the\nlunar surface, as it is full of hazardous obstacles, such as rocks. These\nobstacles need to be identified and avoided in real time. This paper seeks to\ndemonstrate the development of a visuomotor system within a robot for\nnavigation and obstacle avoidance, with complex rock shaped objects\nrepresenting hazards. Our approach uses deep reinforcement learning with only\nimage data. In this paper, we compare the results from several neural network\narchitectures and a preprocessing methodology which includes producing a\nsegmented image and downsampling.", "time": "2021-03-26T12:01:42Z", "link": "http://arxiv.org/abs/2103.14422v1", "id": "2103.14422v1", "title": "SegVisRL: Visuomotor Development for a Lunar Rover for Hazard Avoidance\n  using Camera Images"}
{"author": "Arash Mehrjou, Ashkan Soleymani, Amin Abyaneh, Samir Bhatt, Bernhard SchÃ¶lkopf, Stefan Bauer", "abstract": "Simulating the spread of infectious diseases in human communities is critical\nfor predicting the trajectory of an epidemic and verifying various policies to\ncontrol the devastating impacts of the outbreak. Many existing simulators are\nbased on compartment models that divide people into a few subsets and simulate\nthe dynamics among those subsets using hypothesized differential equations.\nHowever, these models lack the requisite granularity to study the effect of\nintelligent policies that influence every individual in a particular way. In\nthis work, we introduce a simulator software capable of modeling a population\nstructure and controlling the disease's propagation at an individualistic\nlevel. In order to estimate the confidence of the conclusions drawn from the\nsimulator, we employ a comprehensive probabilistic approach where the entire\npopulation is constructed as a hierarchical random variable. This approach\nmakes the inferred conclusions more robust against sampling artifacts and gives\nconfidence bounds for decisions based on the simulation results. To showcase\npotential applications, the simulator parameters are set based on the formal\nstatistics of the COVID-19 pandemic, and the outcome of a wide range of control\nmeasures is investigated. Furthermore, the simulator is used as the environment\nof a reinforcement learning problem to find the optimal policies to control the\npandemic. The obtained experimental results indicate the simulator's\nadaptability and capacity in making sound predictions and a successful policy\nderivation example based on real-world data. As an exemplary application, our\nresults show that the proposed policy discovery method can lead to control\nmeasures that produce significantly fewer infected individuals in the\npopulation and protect the health system against saturation.", "time": "2021-04-21T00:13:57Z", "link": "http://arxiv.org/abs/2103.15561v2", "id": "2103.15561v2", "title": "Pyfectious: An individual-level simulator to discover optimal\n  containment polices for epidemic diseases"}
{"author": "Alireza Khadem, Haojie Ye, Trevor Mudge", "abstract": "Computation and Data Reuse is critical for the resource-limited Convolutional\nNeural Network (CNN) accelerators. This paper presents Universal Computation\nReuse to exploit weight sparsity, repetition, and similarity simultaneously in\na convolutional layer. Moreover, CoDR decreases the cost of weight memory\naccess by proposing a customized Run-Length Encoding scheme and the number of\nmemory accesses to the intermediate results by introducing an input and output\nstationary dataflow. Compared to two recent compressed CNN accelerators with\nthe same area of 2.85 mm^2, CoDR decreases SRAM access by 5.08x and 7.99x, and\nconsumes 3.76x and 6.84x less energy.", "time": "2021-04-20T07:20:17Z", "link": "http://arxiv.org/abs/2104.09798v1", "id": "2104.09798v1", "title": "CoDR: Computation and Data Reuse Aware CNN Accelerator"}
{"author": "Ali Tavasoli, Teague Henry, Heman Shakeri", "abstract": "Networks are landmarks of many complex phenomena where interweaving\ninteractions between different agents transform simple local rule-sets into\nnonlinear emergent behaviors. While some recent studies unveil associations\nbetween the network structure and the underlying dynamical process, identifying\nstochastic nonlinear dynamical processes continues to be an outstanding\nproblem. Here we develop a simple data-driven framework based on\noperator-theoretic techniques to identify and control stochastic nonlinear\ndynamics taking place over large-scale networks. The proposed approach requires\nno prior knowledge of the network structure and identifies the underlying\ndynamics solely using a collection of two-step snapshots of the states. This\ndata-driven system identification is achieved by using the Koopman operator to\nfind a low dimensional representation of the dynamical patterns that evolve\nlinearly. Further, we use the global linear Koopman model to solve critical\ncontrol problems by applying to model predictive control (MPC)--typically, a\nchallenging proposition when applied to large networks. We show that our\nproposed approach tackles this by converting the original nonlinear programming\ninto a more tractable optimization problem that is both convex and with far\nfewer variables.", "time": "2021-08-01T03:57:10Z", "link": "http://arxiv.org/abs/2108.02005v1", "id": "2108.02005v1", "title": "A purely data-driven framework for prediction, optimization, and control\n  of networked processes: application to networked SIS epidemic model"}
{"author": "Mohammed Abouheaf, Shuzheng Qu, Wail Gueaieb, Rami Abielmona, Moufid Harb", "abstract": "This article elaborates on how machine learning (ML) can leverage the\nsolution of a contemporary problem related to the security of maritime domains.\nThe worldwide ``Illegal, Unreported, and Unregulated'' (IUU) fishing incidents\nhave led to serious environmental and economic consequences which involve\ndrastic changes in our ecosystems in addition to financial losses caused by the\ndepletion of natural resources. The Fisheries and Aquatic Department (FAD) of\nthe United Nation's Food and Agriculture Organization (FAO) issued a report\nwhich indicated that the annual losses due to IUU fishing reached $25 Billion.\nThis imposes negative impacts on the future-biodiversity of the marine\necosystem and domestic Gross National Product (GNP). Hence, robust interception\nmechanisms are increasingly needed for detecting and pursuing the unrelenting\nillegal fishing incidents in maritime territories. This article addresses the\nproblem of coordinating the motion of a fleet of marine vessels (pursuers) to\ncatch an IUU vessel while still in local waters. The problem is formulated as a\npursuer-evader problem that is tackled within an ML framework. One or more\npursuers, such as law enforcement vessels, intercept an evader (i.e., the\nillegal fishing ship) using an online reinforcement learning mechanism that is\nbased on a value iteration process. It employs real-time navigation\nmeasurements of the evader ship as well as those of the pursuing vessels and\nreturns back model-free interception strategies.", "time": "2021-08-05T04:18:07Z", "link": "http://arxiv.org/abs/2108.03169v1", "id": "2108.03169v1", "title": "Responding to Illegal Activities Along the Canadian Coastlines Using\n  Reinforcement Learning"}
{"author": "Ajinkya Jain, Stephen Giguere, Rudolf Lioutikov, Scott Niekum", "abstract": "We propose a method that efficiently learns distributions over articulation\nmodel parameters directly from depth images without the need to know\narticulation model categories a priori. By contrast, existing methods that\nlearn articulation models from raw observations typically only predict point\nestimates of the model parameters, which are insufficient to guarantee the safe\nmanipulation of articulated objects. Our core contributions include a novel\nrepresentation for distributions over rigid body transformations and\narticulation model parameters based on screw theory, von Mises-Fisher\ndistributions, and Stiefel manifolds. Combining these concepts allows for an\nefficient, mathematically sound representation that implicitly satisfies the\nconstraints that rigid body transformations and articulations must adhere to.\nLeveraging this representation, we introduce a novel deep learning based\napproach, DUST-net, that performs category-independent articulation model\nestimation while also providing model uncertainties. We evaluate our approach\non several benchmarking datasets and real-world objects and compare its\nperformance with two current state-of-the-art methods. Our results demonstrate\nthat DUST-net can successfully learn distributions over articulation models for\nnovel objects across articulation model categories, which generate point\nestimates with better accuracy than state-of-the-art methods and effectively\ncapture the uncertainty over predicted model parameters due to noisy inputs.\nProject webpage: https://pearl-utexas.github.io/DUST-net/", "time": "2021-10-25T05:06:39Z", "link": "http://arxiv.org/abs/2108.05875v2", "id": "2108.05875v2", "title": "Distributional Depth-Based Estimation of Object Articulation Models"}
{"author": "Rick Salay, Krzysztof Czarnecki, Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae, Vahdat Abdelzad, Chengjie Huang, Maximilian Kahn, Van Duong Nguyen", "abstract": "Safety assurance is a central concern for the development and societal\nacceptance of automated driving (AD) systems. Perception is a key aspect of AD\nthat relies heavily on Machine Learning (ML). Despite the known challenges with\nthe safety assurance of ML-based components, proposals have recently emerged\nfor unit-level safety cases addressing these components. Unfortunately, AD\nsafety cases express safety requirements at the system level and these efforts\nare missing the critical linking argument needed to integrate safety\nrequirements at the system level with component performance requirements at the\nunit level. In this paper, we propose the Integration Safety Case for\nPerception (ISCaP), a generic template for such a linking safety argument\nspecifically tailored for perception components. The template takes a deductive\nand formal approach to define strong traceability between levels. We\ndemonstrate the applicability of ISCaP with a detailed case study and discuss\nits use as a tool to support incremental development of perception components.", "time": "2022-09-06T19:20:43Z", "link": "http://arxiv.org/abs/2108.13294v4", "id": "2108.13294v4", "title": "The missing link: Developing a safety case for perception components in\n  automated driving"}
{"author": "Daniel Hein, Daniel Labisch", "abstract": "In this paper, genetic programming reinforcement learning (GPRL) is utilized\nto generate human-interpretable control policies for a Chylla-Haase\npolymerization reactor. Such continuously stirred tank reactors (CSTRs) with\njacket cooling are widely used in the chemical industry, in the production of\nfine chemicals, pigments, polymers, and medical products. Despite appearing\nrather simple, controlling CSTRs in real-world applications is quite a\nchallenging problem to tackle. GPRL utilizes already existing data from the\nreactor and generates fully automatically a set of optimized simplistic control\nstrategies, so-called policies, the domain expert can choose from. Note that\nthese policies are white-box models of low complexity, which makes them easy to\nvalidate and implement in the target control system, e.g., SIMATIC PCS 7.\nHowever, despite its low complexity the automatically-generated policy yields a\nhigh performance in terms of reactor temperature control deviation, which we\nempirically evaluate on the original reactor template.", "time": "2021-08-30T17:04:04Z", "link": "http://arxiv.org/abs/2108.13381v1", "id": "2108.13381v1", "title": "Trustworthy AI for Process Automation on a Chylla-Haase Polymerization\n  Reactor"}
{"author": "Rajeeva L. Karandikar, M. Vidyasagar", "abstract": "We begin by briefly surveying some results on the convergence of the\nStochastic Gradient Descent (SGD) Method, proved in a companion paper by the\npresent authors. These results are based on viewing SGD as a version of\nStochastic Approximation (SA). Ever since its introduction in the classic paper\nof Robbins and Monro in 1951, SA has become a standard tool for finding a\nsolution of an equation of the form $f(\\theta) = 0$, when only noisy\nmeasurements of $f(\\cdot)$ are available. In most situations, \\textit{every\ncomponent} of the putative solution $\\theta_t$ is updated at each step $t$. In\nsome applications in Reinforcement Learning (RL), \\textit{only one component}\nof $\\theta_t$ is updated at each $t$. This is known as \\textbf{asynchronous}\nSA. In this paper, we study \\textbf{Block Asynchronous SA (BASA)}, in which, at\neach step $t$, \\textit{some but not necessarily all} components of $\\theta_t$\nare updated. The theory presented here embraces both conventional (synchronous)\nSA as well as asynchronous SA, and all in-between possibilities. We provide\nsufficient conditions for the convergence of BASA, and also prove bounds on the\n\\textit{rate} of convergence of $\\theta_t$ to the solution. For the case of\nconventional SGD, these results reduce to those proved in our companion paper.\nThen we apply these results to the problem of finding a fixed point of a map\nwith only noisy measurements. This problem arises frequently in RL. We prove\nsufficient conditions for convergence as well as estimates for the rate of\nconvergence.", "time": "2024-08-06T06:19:46Z", "link": "http://arxiv.org/abs/2109.03445v6", "id": "2109.03445v6", "title": "Convergence of Batch Asynchronous Stochastic Approximation With\n  Applications to Reinforcement Learning"}
{"author": "Roy Zohar, Shie Mannor, Guy Tennenholtz", "abstract": "Cooperative multi-agent reinforcement learning (MARL) faces significant\nscalability issues due to state and action spaces that are exponentially large\nin the number of agents. As environments grow in size, effective credit\nassignment becomes increasingly harder and often results in infeasible learning\ntimes. Still, in many real-world settings, there exist simplified underlying\ndynamics that can be leveraged for more scalable solutions. In this work, we\nexploit such locality structures effectively whilst maintaining global\ncooperation. We propose a novel, value-based multi-agent algorithm called\nLOMAQ, which incorporates local rewards in the Centralized Training\nDecentralized Execution paradigm. Additionally, we provide a direct reward\ndecomposition method for finding these local rewards when only a global signal\nis provided. We test our method empirically, showing it scales well compared to\nother methods, significantly improving performance and convergence speed.", "time": "2021-09-22T10:08:15Z", "link": "http://arxiv.org/abs/2109.10632v1", "id": "2109.10632v1", "title": "Locality Matters: A Scalable Value Decomposition Approach for\n  Cooperative Multi-Agent Reinforcement Learning"}
{"author": "Satpreet Harcharan Singh, Floris van Breugel, Rajesh P. N. Rao, Bingni Wen Brunton", "abstract": "Tracking a turbulent plume to locate its source is a complex control problem\nbecause it requires multi-sensory integration and must be robust to\nintermittent odors, changing wind direction, and variable plume statistics.\nThis task is routinely performed by flying insects, often over long distances,\nin pursuit of food or mates. Several aspects of this remarkable behavior have\nbeen studied in detail in many experimental studies. Here, we take a\ncomplementary in silico approach, using artificial agents trained with\nreinforcement learning to develop an integrated understanding of the behaviors\nand neural computations that support plume tracking. Specifically, we use deep\nreinforcement learning (DRL) to train recurrent neural network (RNN) agents to\nlocate the source of simulated turbulent plumes. Interestingly, the agents'\nemergent behaviors resemble those of flying insects, and the RNNs learn to\nrepresent task-relevant variables, such as head direction and time since last\nodor encounter. Our analyses suggest an intriguing experimentally testable\nhypothesis for tracking plumes in changing wind direction -- that agents follow\nlocal plume shape rather than the current wind direction. While reflexive\nshort-memory behaviors are sufficient for tracking plumes in constant wind,\nlonger timescales of memory are essential for tracking plumes that switch\ndirection. At the level of neural dynamics, the RNNs' population activity is\nlow-dimensional and organized into distinct dynamical structures, with some\ncorrespondence to behavioral modules. Our in silico approach provides key\nintuitions for turbulent plume tracking strategies and motivates future\ntargeted experimental and theoretical developments.", "time": "2021-12-18T00:58:21Z", "link": "http://arxiv.org/abs/2109.12434v2", "id": "2109.12434v2", "title": "Emergent behavior and neural dynamics in artificial agents tracking\n  turbulent plumes"}
{"author": "Anthony Courchesne, Andrea Censi, Liam Paull", "abstract": "In many situations it is either impossible or impractical to develop and\nevaluate agents entirely on the target domain on which they will be deployed.\nThis is particularly true in robotics, where doing experiments on hardware is\nmuch more arduous than in simulation. This has become arguably more so in the\ncase of learning-based agents. To this end, considerable recent effort has been\ndevoted to developing increasingly realistic and higher fidelity simulators.\nHowever, we lack any principled way to evaluate how good a \"proxy domain\" is,\nspecifically in terms of how useful it is in helping us achieve our end\nobjective of building an agent that performs well in the target domain. In this\nwork, we investigate methods to address this need. We begin by clearly\nseparating two uses of proxy domains that are often conflated: 1) their ability\nto be a faithful predictor of agent performance and 2) their ability to be a\nuseful tool for learning. In this paper, we attempt to clarify the role of\nproxy domains and establish new proxy usefulness (PU) metrics to compare the\nusefulness of different proxy domains. We propose the relative predictive PU to\nassess the predictive ability of a proxy domain and the learning PU to quantify\nthe usefulness of a proxy as a tool to generate learning data. Furthermore, we\nargue that the value of a proxy is conditioned on the task that it is being\nused to help solve. We demonstrate how these new metrics can be used to\noptimize parameters of the proxy domain for which obtaining ground truth via\nsystem identification is not trivial.", "time": "2021-10-07T14:32:44Z", "link": "http://arxiv.org/abs/2109.14516v2", "id": "2109.14516v2", "title": "On Assessing the Usefulness of Proxy Domains for Developing and\n  Evaluating Embodied Agents"}
{"author": "Vladislav Kurenkov, Bulat Maksudov", "abstract": "In recent years, Evolutionary Strategies were actively explored in robotic\ntasks for policy search as they provide a simpler alternative to reinforcement\nlearning algorithms. However, this class of algorithms is often claimed to be\nextremely sample-inefficient. On the other hand, there is a growing interest in\nDifferentiable Robot Simulators (DRS) as they potentially can find successful\npolicies with only a handful of trajectories. But the resulting gradient is not\nalways useful for the first-order optimization. In this work, we demonstrate\nhow DRS gradient can be used in conjunction with Evolutionary Strategies.\nPreliminary results suggest that this combination can reduce sample complexity\nof Evolutionary Strategies by 3x-5x times in both simulation and the real\nworld.", "time": "2021-11-09T15:41:04Z", "link": "http://arxiv.org/abs/2110.00438v3", "id": "2110.00438v3", "title": "Guiding Evolutionary Strategies by Differentiable Robot Simulators"}
{"author": "Jikun Kang, Miao Liu, Abhinav Gupta, Chris Pal, Xue Liu, Jie Fu", "abstract": "Various automatic curriculum learning (ACL) methods have been proposed to\nimprove the sample efficiency and final performance of deep reinforcement\nlearning (DRL). They are designed to control how a DRL agent collects data,\nwhich is inspired by how humans gradually adapt their learning processes to\ntheir capabilities. For example, ACL can be used for subgoal generation, reward\nshaping, environment generation, or initial state generation. However, prior\nwork only considers curriculum learning following one of the aforementioned\npredefined paradigms. It is unclear which of these paradigms are complementary,\nand how the combination of them can be learned from interactions with the\nenvironment. Therefore, in this paper, we propose a unified automatic\ncurriculum learning framework to create multi-objective but coherent curricula\nthat are generated by a set of parametric curriculum modules. Each curriculum\nmodule is instantiated as a neural network and is responsible for generating a\nparticular curriculum. In order to coordinate those potentially conflicting\nmodules in unified parameter space, we propose a multi-task hyper-net learning\nframework that uses a single hyper-net to parameterize all those curriculum\nmodules. In addition to existing hand-designed curricula paradigms, we further\ndesign a flexible memory mechanism to learn an abstract curriculum, which may\notherwise be difficult to design manually. We evaluate our method on a series\nof robotic manipulation tasks and demonstrate its superiority over other\nstate-of-the-art ACL methods in terms of sample efficiency and final\nperformance.", "time": "2022-10-20T01:38:32Z", "link": "http://arxiv.org/abs/2110.03032v3", "id": "2110.03032v3", "title": "Learning Multi-Objective Curricula for Robotic Policy Learning"}
{"author": "Wenlong Huang, Igor Mordatch, Pieter Abbeel, Deepak Pathak", "abstract": "Dexterous manipulation of arbitrary objects, a fundamental daily task for\nhumans, has been a grand challenge for autonomous robotic systems. Although\ndata-driven approaches using reinforcement learning can develop specialist\npolicies that discover behaviors to control a single object, they often exhibit\npoor generalization to unseen ones. In this work, we show that policies learned\nby existing reinforcement learning algorithms can in fact be generalist when\ncombined with multi-task learning and a well-chosen object representation. We\nshow that a single generalist policy can perform in-hand manipulation of over\n100 geometrically-diverse real-world objects and generalize to new objects with\nunseen shape or size. Interestingly, we find that multi-task learning with\nobject point cloud representations not only generalizes better but even\noutperforms the single-object specialist policies on both training as well as\nheld-out test objects. Video results at\nhttps://huangwl18.github.io/geometry-dex", "time": "2021-11-04T17:59:56Z", "link": "http://arxiv.org/abs/2111.03062v1", "id": "2111.03062v1", "title": "Generalization in Dexterous Manipulation via Geometry-Aware Multi-Task\n  Learning"}
{"author": "Aditya M. Deshpande, Ali A. Minai, Manish Kumar", "abstract": "Deep reinforcement learning (RL) has made it possible to solve complex\nrobotics problems using neural networks as function approximators. However, the\npolicies trained on stationary environments suffer in terms of generalization\nwhen transferred from one environment to another. In this work, we use Robust\nMarkov Decision Processes (RMDP) to train the drone control policy, which\ncombines ideas from Robust Control and RL. It opts for pessimistic optimization\nto handle potential gaps between policy transfer from one environment to\nanother. The trained control policy is tested on the task of quadcopter\npositional control. RL agents were trained in a MuJoCo simulator. During\ntesting, different environment parameters (unseen during the training) were\nused to validate the robustness of the trained policy for transfer from one\nenvironment to another. The robust policy outperformed the standard agents in\nthese environments, suggesting that the added robustness increases generality\nand can adapt to non-stationary environments.\n  Codes: https://github.com/adipandas/gym_multirotor", "time": "2021-11-06T16:35:13Z", "link": "http://arxiv.org/abs/2111.03915v1", "id": "2111.03915v1", "title": "Robust Deep Reinforcement Learning for Quadcopter Control"}
{"author": "Boyuan Chen, Robert Kwiatkowski, Carl Vondrick, Hod Lipson", "abstract": "Internal computational models of physical bodies are fundamental to the\nability of robots and animals alike to plan and control their actions. These\n\"self-models\" allow robots to consider outcomes of multiple possible future\nactions, without trying them out in physical reality. Recent progress in fully\ndata-driven self-modeling has enabled machines to learn their own forward\nkinematics directly from task-agnostic interaction data. However,\nforward-kinema\\-tics models can only predict limited aspects of the morphology,\nsuch as the position of end effectors or velocity of joints and masses. A key\nchallenge is to model the entire morphology and kinematics, without prior\nknowledge of what aspects of the morphology will be relevant to future tasks.\nHere, we propose that instead of directly modeling forward-kinematics, a more\nuseful form of self-modeling is one that could answer space occupancy queries,\nconditioned on the robot's state. Such query-driven self models are continuous\nin the spatial domain, memory efficient, fully differentiable and kinematic\naware. In physical experiments, we demonstrate how a visual self-model is\naccurate to about one percent of the workspace, enabling the robot to perform\nvarious motion planning and control tasks. Visual self-modeling can also allow\nthe robot to detect, localize and recover from real-world damage, leading to\nimproved machine resiliency. Our project website is at:\nhttps://robot-morphology.cs.columbia.edu/", "time": "2021-11-21T19:10:14Z", "link": "http://arxiv.org/abs/2111.06389v2", "id": "2111.06389v2", "title": "Full-Body Visual Self-Modeling of Robot Morphologies"}
{"author": "Yuzi Yan, Xiaoxiang Li, Xinyou Qiu, Jiantao Qiu, Jian Wang, Yu Wang, Yuan Shen", "abstract": "Multi-agent formation as well as obstacle avoidance is one of the most\nactively studied topics in the field of multi-agent systems. Although some\nclassic controllers like model predictive control (MPC) and fuzzy control\nachieve a certain measure of success, most of them require precise global\ninformation which is not accessible in harsh environments. On the other hand,\nsome reinforcement learning (RL) based approaches adopt the leader-follower\nstructure to organize different agents' behaviors, which sacrifices the\ncollaboration between agents thus suffering from bottlenecks in maneuverability\nand robustness. In this paper, we propose a distributed formation and obstacle\navoidance method based on multi-agent reinforcement learning (MARL). Agents in\nour system only utilize local and relative information to make decisions and\ncontrol themselves distributively. Agent in the multi-agent system will\nreorganize themselves into a new topology quickly in case that any of them is\ndisconnected. Our method achieves better performance regarding formation error,\nformation convergence rate and on-par success rate of obstacle avoidance\ncompared with baselines (both classic control methods and another RL-based\nmethod). The feasibility of our method is verified by both simulation and\nhardware implementation with Ackermann-steering vehicles.", "time": "2021-11-14T13:02:45Z", "link": "http://arxiv.org/abs/2111.07334v1", "id": "2111.07334v1", "title": "Relative Distributed Formation and Obstacle Avoidance with Multi-agent\n  Reinforcement Learning"}
{"author": "Muhammad Shakeel, Katsutoshi Itoyama, Kenji Nishida, Kazuhiro Nakadai", "abstract": "We describe a novel metric-based learning approach that introduces a\nmultimodal framework and uses deep audio and geophone encoders in siamese\nconfiguration to design an adaptable and lightweight supervised model. This\nframework eliminates the need for expensive data labeling procedures and learns\ngeneral-purpose representations from low multisensory data obtained from\nomnipresent sensing systems. These sensing systems provide numerous\napplications and various use cases in activity recognition tasks. Here, we\nintend to explore the human footstep movements from indoor environments and\nanalyze representations from a small self-collected dataset of acoustic and\nvibration-based sensors. The core idea is to learn plausible similarities\nbetween two sensory traits and combining representations from audio and\ngeophone signals. We present a generalized framework to learn embeddings from\ntemporal and spatial features extracted from audio and geophone signals. We\nthen extract the representations in a shared space to maximize the learning of\na compatibility function between acoustic and geophone features. This, in turn,\ncan be used effectively to carry out a classification task from the learned\nmodel, as demonstrated by assigning high similarity to the pairs with a human\nfootstep movement and lower similarity to pairs containing no footstep\nmovement. Performance analyses show that our proposed multimodal framework\nachieves a 19.99\\% accuracy increase (in absolute terms) and avoided\noverfitting on the evaluation set when the training samples were increased from\n200 pairs to just 500 pairs while satisfactorily learning the audio and\ngeophone representations. Our results employ a metric-based contrastive\nlearning approach for multi-sensor data to mitigate the impact of data scarcity\nand perform human movement identification with limited data size.", "time": "2021-11-15T18:46:14Z", "link": "http://arxiv.org/abs/2111.07979v1", "id": "2111.07979v1", "title": "Metric-based multimodal meta-learning for human movement identification\n  via footstep recognition"}
{"author": "Hangyu Mao, Chao Wang, Xiaotian Hao, Yihuan Mao, Yiming Lu, Chengjie Wu, Jianye Hao, Dong Li, Pingzhong Tang", "abstract": "The MineRL competition is designed for the development of reinforcement\nlearning and imitation learning algorithms that can efficiently leverage human\ndemonstrations to drastically reduce the number of environment interactions\nneeded to solve the complex \\emph{ObtainDiamond} task with sparse rewards. To\naddress the challenge, in this paper, we present \\textbf{SEIHAI}, a\n\\textbf{S}ample-\\textbf{e}ff\\textbf{i}cient \\textbf{H}ierarchical \\textbf{AI},\nthat fully takes advantage of the human demonstrations and the task structure.\nSpecifically, we split the task into several sequentially dependent subtasks,\nand train a suitable agent for each subtask using reinforcement learning and\nimitation learning. We further design a scheduler to select different agents\nfor different subtasks automatically. SEIHAI takes the first place in the\npreliminary and final of the NeurIPS-2020 MineRL competition.", "time": "2021-11-17T01:36:40Z", "link": "http://arxiv.org/abs/2111.08857v1", "id": "2111.08857v1", "title": "SEIHAI: A Sample-efficient Hierarchical AI for the MineRL Competition"}
{"author": "Chao-Han Huck Yang, Zhengling Qi, Yifan Cui, Pin-Yu Chen", "abstract": "Deep Reinforcement Learning (DRL) has demonstrated great potentials in\nsolving sequential decision making problems in many applications. Despite its\npromising performance, practical gaps exist when deploying DRL in real-world\nscenarios. One main barrier is the over-fitting issue that leads to poor\ngeneralizability of the policy learned by DRL. In particular, for offline DRL\nwith observational data, model selection is a challenging task as there is no\nground truth available for performance demonstration, in contrast with the\nonline setting with simulated environments. In this work, we propose a\npessimistic model selection (PMS) approach for offline DRL with a theoretical\nguarantee, which features a provably effective framework for finding the best\npolicy among a set of candidate models. Two refined approaches are also\nproposed to address the potential bias of DRL model in identifying the optimal\npolicy. Numerical studies demonstrated the superior performance of our approach\nover existing methods.", "time": "2021-11-29T06:29:49Z", "link": "http://arxiv.org/abs/2111.14346v1", "id": "2111.14346v1", "title": "Pessimistic Model Selection for Offline Deep Reinforcement Learning"}
{"author": "Param Budhraja, Mayank Baranwal, Kunal Garg, Ashish Hota", "abstract": "Accelerated gradient methods are the cornerstones of large-scale, data-driven\noptimization problems that arise naturally in machine learning and other fields\nconcerning data analysis. We introduce a gradient-based optimization framework\nfor achieving acceleration, based on the recently introduced notion of\nfixed-time stability of dynamical systems. The method presents itself as a\ngeneralization of simple gradient-based methods suitably scaled to achieve\nconvergence to the optimizer in a fixed-time, independent of the\ninitialization. We achieve this by first leveraging a continuous-time framework\nfor designing fixed-time stable dynamical systems, and later providing a\nconsistent discretization strategy, such that the equivalent discrete-time\nalgorithm tracks the optimizer in a practically fixed number of iterations. We\nalso provide a theoretical analysis of the convergence behavior of the proposed\ngradient flows, and their robustness to additive disturbances for a range of\nfunctions obeying strong convexity, strict convexity, and possibly nonconvexity\nbut satisfying the Polyak-{\\L}ojasiewicz inequality. We also show that the\nregret bound on the convergence rate is constant by virtue of the fixed-time\nconvergence. The hyperparameters have intuitive interpretations and can be\ntuned to fit the requirements on the desired convergence rates. We validate the\naccelerated convergence properties of the proposed schemes on a range of\nnumerical examples against the state-of-the-art optimization algorithms. Our\nwork provides insights on developing novel optimization algorithms via\ndiscretization of continuous-time flows.", "time": "2022-03-20T05:26:17Z", "link": "http://arxiv.org/abs/2112.01363v2", "id": "2112.01363v2", "title": "Breaking the Convergence Barrier: Optimization via Fixed-Time Convergent\n  Flows"}
{"author": "Christos Mavridis, John Baras", "abstract": "The existence of a universal learning architecture in human cognition is a\nwidely spread conjecture supported by experimental findings from neuroscience.\nWhile no low-level implementation can be specified yet, an abstract outline of\nhuman perception and learning is believed to entail three basic properties: (a)\nhierarchical attention and processing, (b) memory-based knowledge\nrepresentation, and (c) progressive learning and knowledge compaction. We\napproach the design of such a learning architecture from a system-theoretic\nviewpoint, developing a closed-loop system with three main components: (i) a\nmulti-resolution analysis pre-processor, (ii) a group-invariant feature\nextractor, and (iii) a progressive knowledge-based learning module.\nMulti-resolution feedback loops are used for learning, i.e., for adapting the\nsystem parameters to online observations. To design (i) and (ii), we build upon\nthe established theory of wavelet-based multi-resolution analysis and the\nproperties of group convolution operators. Regarding (iii), we introduce a\nnovel learning algorithm that constructs progressively growing knowledge\nrepresentations in multiple resolutions. The proposed algorithm is an extension\nof the Online Deterministic Annealing (ODA) algorithm based on annealing\noptimization, solved using gradient-free stochastic approximation. ODA has\ninherent robustness and regularization properties and provides a means to\nprogressively increase the complexity of the learning model i.e. the number of\nthe neurons, as needed, through an intuitive bifurcation phenomenon. The\nproposed multi-resolution approach is hierarchical, progressive,\nknowledge-based, and interpretable. We illustrate the properties of the\nproposed architecture in the context of the state-of-the-art learning\nalgorithms and deep learning methods.", "time": "2021-12-04T05:54:33Z", "link": "http://arxiv.org/abs/2112.02256v1", "id": "2112.02256v1", "title": "Towards the One Learning Algorithm Hypothesis: A System-theoretic\n  Approach"}
{"author": "Boyuan Chen, Kuang Huang, Sunand Raghupathi, Ishaan Chandratreya, Qiang Du, Hod Lipson", "abstract": "All physical laws are described as relationships between state variables that\ngive a complete and non-redundant description of the relevant system dynamics.\nHowever, despite the prevalence of computing power and AI, the process of\nidentifying the hidden state variables themselves has resisted automation. Most\ndata-driven methods for modeling physical phenomena still assume that observed\ndata streams already correspond to relevant state variables. A key challenge is\nto identify the possible sets of state variables from scratch, given only\nhigh-dimensional observational data. Here we propose a new principle for\ndetermining how many state variables an observed system is likely to have, and\nwhat these variables might be, directly from video streams. We demonstrate the\neffectiveness of this approach using video recordings of a variety of physical\ndynamical systems, ranging from elastic double pendulums to fire flames.\nWithout any prior knowledge of the underlying physics, our algorithm discovers\nthe intrinsic dimension of the observed dynamics and identifies candidate sets\nof state variables. We suggest that this approach could help catalyze the\nunderstanding, prediction and control of increasingly complex systems. Project\nwebsite is at: https://www.cs.columbia.edu/~bchen/neural-state-variables", "time": "2021-12-20T18:56:59Z", "link": "http://arxiv.org/abs/2112.10755v1", "id": "2112.10755v1", "title": "Discovering State Variables Hidden in Experimental Data"}
{"author": "Osman Boyaci, Mohammad Rasoul Narimani, Katherine Davis, Erchin Serpedin", "abstract": "As a highly complex and integrated cyber-physical system, modern power grids\nare exposed to cyberattacks. False data injection attacks (FDIAs),\nspecifically, represent a major class of cyber threats to smart grids by\ntargeting the measurement data's integrity. Although various solutions have\nbeen proposed to detect those cyberattacks, the vast majority of the works have\nignored the inherent graph structure of the power grid measurements and\nvalidated their detectors only for small test systems with less than a few\nhundred buses. To better exploit the spatial correlations of smart grid\nmeasurements, this paper proposes a deep learning model for cyberattack\ndetection in large-scale AC power grids using Chebyshev Graph Convolutional\nNetworks (CGCN). By reducing the complexity of spectral graph filters and\nmaking them localized, CGCN provides a fast and efficient convolution operation\nto model the graph structural smart grid data. We numerically verify that the\nproposed CGCN based detector surpasses the state-of-the-art model by 7.86 in\ndetection rate and 9.67 in false alarm rate for a large-scale power grid with\n2848 buses. It is notable that the proposed approach detects cyberattacks under\n4 milliseconds for a 2848-bus system, which makes it a good candidate for\nreal-time detection of cyberattacks in large systems.", "time": "2021-12-25T01:35:22Z", "link": "http://arxiv.org/abs/2112.13166v1", "id": "2112.13166v1", "title": "Cyberattack Detection in Large-Scale Smart Grids using Chebyshev Graph\n  Convolutional Networks"}
{"author": "Yichen Yang, Phitchaya Mangpo Phothilimtha, Yisu Remy Wang, Max Willsey, Sudip Roy, Jacques Pienaar", "abstract": "One of the major optimizations employed in deep learning frameworks is graph\nrewriting. Production frameworks rely on heuristics to decide if rewrite rules\nshould be applied and in which order. Prior research has shown that one can\ndiscover more optimal tensor computation graphs if we search for a better\nsequence of substitutions instead of relying on heuristics. However, we observe\nthat existing approaches for tensor graph superoptimization both in production\nand research frameworks apply substitutions in a sequential manner. Such\nsequential search methods are sensitive to the order in which the substitutions\nare applied and often only explore a small fragment of the exponential space of\nequivalent graphs. This paper presents a novel technique for tensor graph\nsuperoptimization that employs equality saturation to apply all possible\nsubstitutions at once. We show that our approach can find optimized graphs with\nup to 16% speedup over state-of-the-art, while spending on average 48x less\ntime optimizing.", "time": "2021-03-17T15:05:06Z", "link": "http://arxiv.org/abs/2101.01332v2", "id": "2101.01332v2", "title": "Equality Saturation for Tensor Graph Superoptimization"}
{"author": "Yiwen Han, Shihao Shen, Xiaofei Wang, Shiqiang Wang, Victor C. M. Leung", "abstract": "Kubernetes (k8s) has the potential to merge the distributed edge and the\ncloud but lacks a scheduling framework specifically for edge-cloud systems.\nBesides, the hierarchical distribution of heterogeneous resources and the\ncomplex dependencies among requests and resources make the modeling and\nscheduling of k8s-oriented edge-cloud systems particularly sophisticated. In\nthis paper, we introduce KaiS, a learning-based scheduling framework for such\nedge-cloud systems to improve the long-term throughput rate of request\nprocessing. First, we design a coordinated multi-agent actor-critic algorithm\nto cater to decentralized request dispatch and dynamic dispatch spaces within\nthe edge cluster. Second, for diverse system scales and structures, we use\ngraph neural networks to embed system state information, and combine the\nembedding results with multiple policy networks to reduce the orchestration\ndimensionality by stepwise scheduling. Finally, we adopt a two-time-scale\nscheduling mechanism to harmonize request dispatch and service orchestration,\nand present the implementation design of deploying the above algorithms\ncompatible with native k8s components. Experiments using real workload traces\nshow that KaiS can successfully learn appropriate scheduling policies,\nirrespective of request arrival patterns and system scales. Moreover, KaiS can\nenhance the average system throughput rate by 14.3% while reducing scheduling\ncost by 34.7% compared to baselines.", "time": "2021-01-17T03:45:25Z", "link": "http://arxiv.org/abs/2101.06582v1", "id": "2101.06582v1", "title": "Tailored Learning-Based Scheduling for Kubernetes-Oriented Edge-Cloud\n  System"}
{"author": "Philipp-Jan Honysz, Sebastian BuschjÃ¤ger, Katharina Morik", "abstract": "The optimization of submodular functions constitutes a viable way to perform\nclustering. Strong approximation guarantees and feasible optimization w.r.t.\nstreaming data make this clustering approach favorable. Technically, submodular\nfunctions map subsets of data to real values, which indicate how\n\"representative\" a specific subset is. Optimal sets might then be used to\npartition the data space and to infer clusters. Exemplar-based clustering is\none of the possible submodular functions, but suffers from high computational\ncomplexity. However, for practical applications, the particular real-time or\nwall-clock run-time is decisive. In this work, we present a novel way to\nevaluate this particular function on GPUs, which keeps the necessities of\noptimizers in mind and reduces wall-clock run-time. To discuss our GPU\nalgorithm, we investigated both the impact of different run-time critical\nproblem properties, like data dimensionality and the number of data points in a\nsubset, and the influence of required floating-point precision. In reproducible\nexperiments, our GPU algorithm was able to achieve competitive speedups of up\nto 72x depending on whether multi-threaded computation on CPUs was used for\ncomparison and the type of floating-point precision required. Half-precision\nGPU computation led to large speedups of up to 452x compared to\nsingle-precision, single-thread CPU computations.", "time": "2021-01-21T18:23:44Z", "link": "http://arxiv.org/abs/2101.08763v1", "id": "2101.08763v1", "title": "GPU-Accelerated Optimizer-Aware Evaluation of Submodular Exemplar\n  Clustering"}
{"author": "Anshul Jindal, Paul Staab, Jorge Cardoso, Michael Gerndt, Vladimir Podolskiy", "abstract": "A memory leak in an application deployed on the cloud can affect the\navailability and reliability of the application. Therefore, to identify and\nultimately resolve it quickly is highly important. However, in the production\nenvironment running on the cloud, memory leak detection is a challenge without\nthe knowledge of the application or its internal object allocation details.\n  This paper addresses this challenge of online detection of memory leaks in\ncloud-based infrastructure without having any internal application knowledge by\nintroducing a novel machine learning based algorithm Precog. This algorithm\nsolely uses one metric i.e the system's memory utilization on which the\napplication is deployed for the detection of a memory leak. The developed\nalgorithm's accuracy was tested on 60 virtual machines manually labeled memory\nutilization data provided by our industry partner Huawei Munich Research Center\nand it was found that the proposed algorithm achieves the accuracy score of\n85\\% with less than half a second prediction time per virtual machine.", "time": "2021-01-24T20:48:45Z", "link": "http://arxiv.org/abs/2101.09799v1", "id": "2101.09799v1", "title": "Online Memory Leak Detection in the Cloud-based Infrastructures"}
{"author": "Dainius Jenkus, Fei Xia, Rishad Shafik, Alex Yakovlev", "abstract": "Web servers scaled across distributed systems necessitate complex runtime\ncontrols for providing quality of service (QoS) guarantees as well as\nminimizing the energy costs under dynamic workloads. This paper presents a\nQoS-aware runtime controller using horizontal scaling (node allocation) and\nvertical scaling (resource allocation within nodes) methods synergistically to\nprovide adaptation to workloads while minimizing the power consumption under\nQoS constraint (i.e., response time). A horizontal scaling determines the\nnumber of active nodes based on workload demands and the required QoS according\nto a set of rules. Then, it is coupled with vertical scaling using transfer\nQ-learning, which further tunes power/performance based on workload profile\nusing dynamic voltage/frequency scaling (DVFS). It transfers Q-values within\nminimally explored states reducing exploration requirements. In addition, the\napproach exploits a scalable architecture of the many-core server allowing to\nreuse available knowledge from fully or partially explored nodes. When\ncombined, these methods allow to reduce the exploration time and QoS violations\nwhen compared to model-free Q-learning. The technique balances design-time and\nruntime costs to maximize the portability and operational optimality\ndemonstrated through persistent power reductions with minimal QoS violations\nunder different workload scenarios on heterogeneous multi-processing nodes of a\nserver cluster.", "time": "2021-02-02T06:47:58Z", "link": "http://arxiv.org/abs/2102.01348v1", "id": "2102.01348v1", "title": "QoS-Aware Power Minimization of Distributed Many-Core Servers using\n  Transfer Q-Learning"}
{"author": "Huaizheng Zhang, Meng Shen, Yizheng Huang, Yonggang Wen, Yong Luo, Guanyu Gao, Kyle Guan", "abstract": "DNN-based video analytics have empowered many new applications (e.g.,\nautomated retail). Meanwhile, the proliferation of fog devices provides\ndevelopers with more design options to improve performance and save cost. To\nthe best of our knowledge, this paper presents the first serverless system that\ntakes full advantage of the client-fog-cloud synergy to better serve the\nDNN-based video analytics. Specifically, the system aims to achieve two goals:\n1) Provide the optimal analytics results under the constraints of lower\nbandwidth usage and shorter round-trip time (RTT) by judiciously managing the\ncomputational and bandwidth resources deployed in the client, fog, and cloud\nenvironment. 2) Free developers from tedious administration and operation\ntasks, including DNN deployment, cloud and fog's resource management. To this\nend, we implement a holistic cloud-fog system referred to as VPaaS\n(Video-Platform-as-a-Service). VPaaS adopts serverless computing to enable\ndevelopers to build a video analytics pipeline by simply programming a set of\nfunctions (e.g., model inference), which are then orchestrated to process\nvideos through carefully designed modules. To save bandwidth and reduce RTT,\nVPaaS provides a new video streaming protocol that only sends low-quality video\nto the cloud. The state-of-the-art (SOTA) DNNs deployed at the cloud can\nidentify regions of video frames that need further processing at the fog ends.\nAt the fog ends, misidentified labels in these regions can be corrected using a\nlight-weight DNN model. To address the data drift issues, we incorporate\nlimited human feedback into the system to verify the results and adopt\nincremental learning to improve our system continuously. The evaluation\ndemonstrates that VPaaS is superior to several SOTA systems: it maintains high\naccuracy while reducing bandwidth usage by up to 21%, RTT by up to 62.5%, and\ncloud monetary cost by up to 50%.", "time": "2021-02-05T05:59:36Z", "link": "http://arxiv.org/abs/2102.03012v1", "id": "2102.03012v1", "title": "A Serverless Cloud-Fog Platform for DNN-Based Video Analytics with\n  Incremental Learning"}
{"author": "JiÅÃ­ FilipoviÄ, Jana HozzovÃ¡, Amin Nezarat, Jaroslav OÄ¾ha, Filip PetroviÄ", "abstract": "We have developed several autotuning benchmarks in CUDA that take into\naccount performance-relevant source-code parameters and reach near\npeak-performance on various GPU architectures. We have used them during the\ndevelopment and evaluation of a novel search method for tuning space proposed\nin [1]. With our framework Kernel Tuning Toolkit, freely available at Github,\nwe measured computation times and hardware performance counters on several GPUs\nfor the complete tuning spaces of five benchmarks. These data, which we provide\nhere, might benefit research of search algorithms for the tuning spaces of GPU\ncodes or research of relation between applied code optimization, hardware\nperformance counters, and GPU kernels' performance.\n  Moreover, we describe the scripts we used for robust evaluation of our\nsearcher and comparison to others in detail. In particular, the script that\nsimulates the tuning, i.e., replaces time-demanding compiling and executing the\ntuned kernels with a quick reading of the computation time from our measured\ndata, makes it possible to inspect the convergence of tuning search over a\nlarge number of experiments. These scripts, freely available with our other\ncodes, make it easier to experiment with search algorithms and compare them in\na robust way.\n  During our research, we generated models for predicting values of performance\ncounters from values of tuning parameters of our benchmarks. Here, we provide\nthe models themselves and describe the scripts we implemented for their\ntraining. These data might benefit researchers who want to reproduce or build\non our research.", "time": "2021-02-10T07:51:09Z", "link": "http://arxiv.org/abs/2102.05299v1", "id": "2102.05299v1", "title": "Searching CUDA code autotuning spaces with hardware performance\n  counters: data from benchmarks running on various GPU architectures"}
{"author": "Soeren Becker, Florian Schmidt, Anton Gulenko, Alexander Acker, Odej Kao", "abstract": "Edge computing was introduced as a technical enabler for the demanding\nrequirements of new network technologies like 5G. It aims to overcome\nchallenges related to centralized cloud computing environments by distributing\ncomputational resources to the edge of the network towards the customers. The\ncomplexity of the emerging infrastructures increases significantly, together\nwith the ramifications of outages on critical use cases such as self-driving\ncars or health care. Artificial Intelligence for IT Operations (AIOps) aims to\nsupport human operators in managing complex infrastructures by using machine\nlearning methods. This paper describes the system design of an AIOps platform\nwhich is applicable in heterogeneous, distributed environments. The overhead of\na high-frequency monitoring solution on edge devices is evaluated and\nperformance experiments regarding the applicability of three anomaly detection\nalgorithms on edge devices are conducted. The results show, that it is feasible\nto collect metrics with a high frequency and simultaneously run specific\nanomaly detection algorithms directly on edge devices with a reasonable\noverhead on the resource utilization.", "time": "2021-02-12T09:33:00Z", "link": "http://arxiv.org/abs/2102.09001v1", "id": "2102.09001v1", "title": "Towards AIOps in Edge Computing Environments"}
{"author": "Abdelrahman Hosny, Sherief Reda", "abstract": "Cloud computing accelerates design space exploration in logic synthesis, and\nparameter tuning in physical design. However, deploying EDA jobs on the cloud\nrequires EDA teams to deeply understand the characteristics of their jobs in\ncloud environments. Unfortunately, there has been little to no public\ninformation on these characteristics. Thus, in this paper, we formulate the\nproblem of migrating EDA jobs to the cloud. First, we characterize the\nperformance of four main EDA applications, namely: synthesis, placement,\nrouting and static timing analysis. We show that different EDA jobs require\ndifferent machine configurations. Second, using observations from our\ncharacterization, we propose a novel model based on Graph Convolutional\nNetworks to predict the total runtime of a given application on different\nmachine configurations. Our model achieves a prediction accuracy of 87%. Third,\nwe develop a new formulation for optimizing cloud deployments in order to\nreduce deployment costs while meeting deadline constraints. We present a\npseudo-polynomial optimal solution using a multi-choice knapsack mapping that\nreduces costs by 35.29%.", "time": "2021-02-22T06:51:09Z", "link": "http://arxiv.org/abs/2102.10800v1", "id": "2102.10800v1", "title": "Characterizing and Optimizing EDA Flows for the Cloud"}
{"author": "Sheik Mohammad Mostakim Fattah, Athman Bouguettaya, Sajib Mistry", "abstract": "We propose a novel CP-Net based composition approach to qualitatively select\nan optimal set of consumers for an IaaS provider. The IaaS provider's and\nconsumers' qualitative preferences are captured using CP-Nets. We propose a\nCP-Net composability model using the semantic congruence property of a\nqualitative composition. A greedy-based and a heuristic-based consumer\nselection approaches are proposed that effectively reduce the search space of\ncandidate consumers in the composition. Experimental results prove the\nfeasibility of the proposed composition approach.", "time": "2021-02-24T11:21:20Z", "link": "http://arxiv.org/abs/2102.12221v1", "id": "2102.12221v1", "title": "A CP-Net based Qualitative Composition Approach for an IaaS Provider"}
{"author": "Renato L. F. Cunha, Lucas V. Real, Renan Souza, Bruno Silva, Marco A. S. Netto", "abstract": "Interactive computing notebooks, such as Jupyter notebooks, have become a\npopular tool for developing and improving data-driven models. Such notebooks\ntend to be executed either in the user's own machine or in a cloud environment,\nhaving drawbacks and benefits in both approaches. This paper presents a\nsolution developed as a Jupyter extension that automatically selects which\ncells, as well as in which scenarios, such cells should be migrated to a more\nsuitable platform for execution. We describe how we reduce the execution state\nof the notebook to decrease migration time and we explore the knowledge of user\ninteractivity patterns with the notebook to determine which blocks of cells\nshould be migrated. Using notebooks from Earth science (remote sensing), image\nrecognition, and hand written digit identification (machine learning), our\nexperiments show notebook state reductions of up to 55x and migration decisions\nleading to performance gains of up to 3.25x when the user interactivity with\nthe notebook is taken into consideration.", "time": "2021-07-01T02:33:18Z", "link": "http://arxiv.org/abs/2107.00187v1", "id": "2107.00187v1", "title": "Context-aware Execution Migration Tool for Data Science Jupyter\n  Notebooks on Hybrid Clouds"}
{"author": "Zimu Zheng, Qiong Chen, Chuang Hu, Dan Wang, Fangming Liu", "abstract": "On edge devices, data scarcity occurs as a common problem where transfer\nlearning serves as a widely-suggested remedy. Nevertheless, transfer learning\nimposes a heavy computation burden to resource-constrained edge devices.\nExisting task allocation works usually assume all submitted tasks are equally\nimportant, leading to inefficient resource allocation at a task level when\ndirectly applied in Multi-task Transfer Learning (MTL). To address these\nissues, we first reveal that it is crucial to measure the impact of tasks on\noverall decision performance improvement and quantify \\emph{task importance}.\nWe then show that task allocation with task importance for MTL (TATIM) is a\nvariant of the NP-complete Knapsack problem, where the complicated computation\nto solve this problem needs to be conducted repeatedly under varying contexts.\nTo solve TATIM with high computational efficiency, we propose a Data-driven\nCooperative Task Allocation (DCTA) approach. Finally, we evaluate the\nperformance of DCTA by not only a trace-driven simulation, but also a new\ncomprehensive real-world AIOps case study that bridges model and practice via a\nnew architecture and main components design within the AIOps system. Extensive\nexperiments show that our DCTA reduces 3.24 times of processing time, and saves\n48.4\\% energy consumption compared with the state-of-the-art when solving\nTATIM.", "time": "2021-07-06T08:24:25Z", "link": "http://arxiv.org/abs/2107.02466v1", "id": "2107.02466v1", "title": "On-edge Multi-task Transfer Learning: Model and Practice with\n  Data-driven Task Allocation"}
{"author": "Shaohuai Shi, Lin Zhang, Bo Li", "abstract": "Distributed training with synchronous stochastic gradient descent (SGD) on\nGPU clusters has been widely used to accelerate the training process of deep\nmodels. However, SGD only utilizes the first-order gradient in model parameter\nupdates, which may take days or weeks. Recent studies have successfully\nexploited approximate second-order information to speed up the training\nprocess, in which the Kronecker-Factored Approximate Curvature (KFAC) emerges\nas one of the most efficient approximation algorithms for training deep models.\nYet, when leveraging GPU clusters to train models with distributed KFAC\n(D-KFAC), it incurs extensive computation as well as introduces extra\ncommunications during each iteration. In this work, we propose D-KFAC\n(SPD-KFAC) with smart parallelism of computing and communication tasks to\nreduce the iteration time. Specifically, 1) we first characterize the\nperformance bottlenecks of D-KFAC, 2) we design and implement a pipelining\nmechanism for Kronecker factors computation and communication with dynamic\ntensor fusion, and 3) we develop a load balancing placement for inverting\nmultiple matrices on GPU clusters. We conduct real-world experiments on a\n64-GPU cluster with 100Gb/s InfiniBand interconnect. Experimental results show\nthat our proposed SPD-KFAC training scheme can achieve 10%-35% improvement over\nstate-of-the-art algorithms.", "time": "2021-07-14T08:01:07Z", "link": "http://arxiv.org/abs/2107.06533v1", "id": "2107.06533v1", "title": "Accelerating Distributed K-FAC with Smart Parallelism of Computing and\n  Communication Tasks"}
{"author": "Supun Kamburugamuve, Chathura Widanage, Niranda Perera, Vibhatha Abeykoon, Ahmet Uyar, Thejaka Amila Kanewala, Gregor von Laszewski, Geoffrey Fox", "abstract": "Data-intensive applications impact many domains, and their steadily\nincreasing size and complexity demands high-performance, highly usable\nenvironments. We integrate a set of ideas developed in various data science and\ndata engineering frameworks. They employ a set of operators on specific data\nabstractions that include vectors, matrices, tensors, graphs, and tables. Our\nkey concepts are inspired from systems like MPI, HPF (High-Performance\nFortran), NumPy, Pandas, Spark, Modin, PyTorch, TensorFlow, RAPIDS(NVIDIA), and\nOneAPI (Intel). Further, it is crucial to support different languages in\neveryday use in the Big Data arena, including Python, R, C++, and Java. We note\nthe importance of Apache Arrow and Parquet for enabling language agnostic high\nperformance and interoperability. In this paper, we propose High-Performance\nTensors, Matrices and Tables (HPTMT), an operator-based architecture for\ndata-intensive applications, and identify the fundamental principles needed for\nperformance and usability success. We illustrate these principles by a\ndiscussion of examples using our software environments, Cylon and Twister2 that\nembody HPTMT.", "time": "2021-07-30T01:12:23Z", "link": "http://arxiv.org/abs/2107.12807v2", "id": "2107.12807v2", "title": "HPTMT: Operator-Based Architecture for Scalable High-Performance\n  Data-Intensive Frameworks"}
{"author": "Tao Chen, Miqing Li", "abstract": "Automatically tuning software configuration for optimizing a single\nperformance attribute (e.g., minimizing latency) is not trivial, due to the\nnature of the configuration systems (e.g., complex landscape and expensive\nmeasurement). To deal with the problem, existing work has been focusing on\ndeveloping various effective optimizers. However, a prominent issue that all\nthese optimizers need to take care of is how to avoid the search being trapped\nin local optima -- a hard nut to crack for software configuration tuning due to\nits rugged and sparse landscape, and neighboring configurations tending to\nbehave very differently. Overcoming such in an expensive measurement setting is\neven more challenging. In this paper, we take a different perspective to tackle\nthis issue. Instead of focusing on improving the optimizer, we work on the\nlevel of optimization model. We do this by proposing a meta\nmulti-objectivization model (MMO) that considers an auxiliary performance\nobjective (e.g., throughput in addition to latency). What makes this model\nunique is that we do not optimize the auxiliary performance objective, but\nrather use it to make similarly-performing while different configurations less\ncomparable (i.e. Pareto nondominated to each other), thus preventing the search\nfrom being trapped in local optima.\n  Experiments on eight real-world software systems/environments with diverse\nperformance attributes reveal that our MMO model is statistically more\neffective than state-of-the-art single-objective counterparts in overcoming\nlocal optima (up to 42% gain), while using as low as 24% of their measurements\nto achieve the same (or better) performance result.", "time": "2021-05-31T03:03:53Z", "link": "http://arxiv.org/abs/2106.01331v1", "id": "2106.01331v1", "title": "Multi-Objectivizing Software Configuration Tuning (for a single\n  performance concern)"}
{"author": "Cheng Luo, Lei Qu, Youshan Miao, Peng Cheng, Yongqiang Xiong", "abstract": "Distributed deep learning workloads include throughput-intensive training\ntasks on the GPU clusters, where the Distributed Stochastic Gradient Descent\n(SGD) incurs significant communication delays after backward propagation,\nforces workers to wait for the gradient synchronization via a centralized\nparameter server or directly in decentralized workers. We present\nCrossoverScheduler, an algorithm that enables communication cycles of a\ndistributed training application to be filled by other applications through\npipelining communication and computation. With CrossoverScheduler, the running\nperformance of distributed training can be significantly improved without\nsacrificing convergence rate and network accuracy. We achieve so by introducing\nCrossover Synchronization which allows multiple distributed deep learning\napplications to time-share the same GPU alternately. The prototype of\nCrossoverScheduler is built and integrated with Horovod. Experiments on a\nvariety of distributed tasks show that CrossoverScheduler achieves 20% \\times\nspeedup for image classification tasks on ImageNet dataset.", "time": "2021-03-14T17:01:15Z", "link": "http://arxiv.org/abs/2103.07974v1", "id": "2103.07974v1", "title": "CrossoverScheduler: Overlapping Multiple Distributed Training\n  Applications in a Crossover Manner"}
{"author": "Kauotar El Maghraoui, Lorraine M. Herger, Chekuri Choudary, Kim Tran, Todd Deshane, David Hanson", "abstract": "A composable infrastructure is defined as resources, such as compute,\nstorage, accelerators and networking, that are shared in a pool and that can be\ngrouped in various configurations to meet application requirements. This\nfreedom to 'mix and match' resources dynamically allows for experimentation\nearly in the design cycle, prior to the final architectural design or hardware\nimplementation of a system. This design provides flexibility to serve a variety\nof workloads and provides a dynamic co-design platform that allows experiments\nand measurements in a controlled manner. For instance, key performance\nbottlenecks can be revealed early on in the experimentation phase thus avoiding\ncostly and time consuming mistakes. Additionally, various system-level\ntopologies can be evaluated when experimenting with new System on Chip (SoCs)\nand new accelerator types. This paper details the design of an enterprise\ncomposable infrastructure that we have implemented and made available to our\npartners in the IBM Research AI Hardware Center (AIHC). Our experimental\nevaluations on the composable system give insights into how the system works\nand evaluates the impact of various resource aggregations and reconfigurations\non representative deep learning benchmarks.", "time": "2021-03-19T17:15:42Z", "link": "http://arxiv.org/abs/2103.10911v1", "id": "2103.10911v1", "title": "Performance Analysis of Deep Learning Workloads on a Composable System"}
{"author": "Lars Bjertnes, Jacob O. TÃ¸rring, Anne C. Elster", "abstract": "The effectiveness of Machine Learning (ML) methods depend on access to large\nsuitable datasets. In this article, we present how we build the LS-CAT\n(Large-Scale CUDA AutoTuning) dataset sourced from GitHub for the purpose of\ntraining NLP-based ML models. Our dataset includes 19 683 CUDA kernels focused\non linear algebra. In addition to the CUDA codes, our LS-CAT dataset contains 5\n028 536 associated runtimes, with different combinations of kernels, block\nsizes and matrix sizes. The runtime are GPU benchmarks on both Nvidia GTX 980\nand Nvidia T4 systems. This information creates a foundation upon which\nNLP-based models can find correlations between source-code features and optimal\nchoice of thread block sizes.\n  There are several results that can be drawn out of our LS-CAT database. E.g.,\nour experimental results show that an optimal choice in thread block size can\ngain an average of 6% for the average case. We thus also analyze how much\nperformance increase can be achieved in general, finding that in 10% of the\ncases more than 20% performance increase can be achieved by using the optimal\nblock. A description of current and future work is also included.", "time": "2021-03-26T11:33:48Z", "link": "http://arxiv.org/abs/2103.14409v1", "id": "2103.14409v1", "title": "LS-CAT: A Large-Scale CUDA AutoTuning Dataset"}
{"author": "Adeyinka Akanbi", "abstract": "Distributed networks and real-time systems are becoming the most important\ncomponents for the new computer age, the Internet of Things (IoT), with huge\ndata streams or data sets generated from sensors and data generated from\nexisting legacy systems. The data generated offers the ability to measure,\ninfer and understand environmental indicators, from delicate ecologies and\nnatural resources to urban environments. This can be achieved through the\nanalysis of the heterogeneous data sources (structured and unstructured). In\nthis paper, we propose a distributed framework Event STream Processing Engine\nfor Environmental Monitoring Domain (ESTemd) for the application of stream\nprocessing on heterogeneous environmental data. Our work in this area\ndemonstrates the useful role big data techniques can play in an environmental\ndecision support system, early warning and forecasting systems. The proposed\nframework addresses the challenges of data heterogeneity from heterogeneous\nsystems and real time processing of huge environmental datasets through a\npublish/subscribe method via a unified data pipeline with the application of\nApache Kafka for real time analytics.", "time": "2021-04-02T15:04:15Z", "link": "http://arxiv.org/abs/2104.01082v1", "id": "2104.01082v1", "title": "ESTemd: A Distributed Processing Framework for Environmental Monitoring\n  based on Apache Kafka Streaming Engine"}
{"author": "Dimitrios Amaxilatis, Georgios Mylonas, Evangelos Theodoridis, Luis Diez, Katerina Deligiannidou", "abstract": "Although we have reached new levels in smart city installations and systems,\nefforts so far have focused on providing diverse sources of data to smart city\nservices consumers while neglecting to provide ways to simplify making good use\nof them. In this context, one first step that will bring added value to smart\ncities is knowledge creation in smart cities through anomaly detection and data\nannotation, supported in both an automated and a crowdsourced manner. We\npresent here LearningCity, our solution that has been validated over an\nexisting smart city deployment in Santander, and the OrganiCity\nexperimentation-as-a-service ecosystem. We discuss key challenges along with\ncharacteristic use cases, and report on our design and implementation, together\nwith some preliminary results derived from combining large smart city datasets\nwith machine learning.", "time": "2021-04-12T08:31:10Z", "link": "http://arxiv.org/abs/2104.05286v1", "id": "2104.05286v1", "title": "LearningCity: Knowledge Generation for Smart Cities"}
{"author": "Pulasthi Wickramasinghe, Geoffrey Fox", "abstract": "Multidimensional scaling of gene sequence data has long played a vital role\nin analysing gene sequence data to identify clusters and patterns. However the\ncomputation complexities and memory requirements of state-of-the-art\ndimensional scaling algorithms make it infeasible to scale to large datasets.\nIn this paper we present an autoencoder-based dimensional reduction model which\ncan easily scale to datasets containing millions of gene sequences, while\nattaining results comparable to state-of-the-art MDS algorithms with minimal\nresource requirements. The model also supports out-of-sample data points with a\n99.5%+ accuracy based on our experiments. The proposed model is evaluated\nagainst DAMDS with a real world fungi gene sequence dataset. The presented\nresults showcase the effectiveness of the autoencoder-based dimension reduction\nmodel and its advantages.", "time": "2021-04-19T02:14:17Z", "link": "http://arxiv.org/abs/2104.09014v1", "id": "2104.09014v1", "title": "Multidimensional Scaling for Gene Sequence Data with Autoencoders"}
{"author": "Ilche Georgievski", "abstract": "Cloud providers are facing a complex problem in configuring software\napplications ready for deployment on their infrastructures. Hierarchical Task\nNetwork (HTN) planning can provide effective means to solve such deployment\nproblems. We present an HTN planning domain that models deployment problems as\nfound in realistic Cloud environments.", "time": "2021-05-28T09:16:17Z", "link": "http://arxiv.org/abs/2104.10027v2", "id": "2104.10027v2", "title": "HTN Planning Domain for Deployment of Cloud Applications"}
{"author": "Mingrui Cao, Long Zhang, Bin Cao", "abstract": "Due to the distributed characteristics of Federated Learning (FL), the\nvulnerability of global model and coordination of devices are the main\nobstacle. As a promising solution of decentralization, scalability and\nsecurity, leveraging blockchain in FL has attracted much attention in recent\nyears. However, the traditional consensus mechanisms designed for blockchain\nlike Proof of Work (PoW) would cause extreme resource consumption, which\nreduces the efficiency of FL greatly, especially when the participating devices\nare wireless and resource-limited. In order to address device asynchrony and\nanomaly detection in FL while avoiding the extra resource consumption caused by\nblockchain, this paper introduces a framework for empowering FL using Direct\nAcyclic Graph (DAG)-based blockchain systematically (DAG-FL). Accordingly,\nDAG-FL is first introduced from a three-layer architecture in details, and then\ntwo algorithms DAG-FL Controlling and DAG-FL Updating are designed running on\ndifferent nodes to elaborate the operation of DAG-FL consensus mechanism. After\nthat, a Poisson process model is formulated to discuss that how to set\ndeployment parameters to maintain DAG-FL stably in different federated learning\ntasks. The extensive simulations and experiments show that DAG-FL can achieve\nbetter performance in terms of training efficiency and model accuracy compared\nwith the typical existing on-device federated learning systems as the\nbenchmarks.", "time": "2021-04-27T10:29:38Z", "link": "http://arxiv.org/abs/2104.13092v1", "id": "2104.13092v1", "title": "Towards On-Device Federated Learning: A Direct Acyclic Graph-based\n  Blockchain Approach"}
{"author": "Emna Baccour, Naram Mhaisen, Alaa Awad Abdellatif, Aiman Erbad, Amr Mohamed, Mounir Hamdi, Mohsen Guizani", "abstract": "Artificial intelligence (AI) has witnessed a substantial breakthrough in a\nvariety of Internet of Things (IoT) applications and services, spanning from\nrecommendation systems to robotics control and military surveillance. This is\ndriven by the easier access to sensory data and the enormous scale of\npervasive/ubiquitous devices that generate zettabytes (ZB) of real-time data\nstreams. Designing accurate models using such data streams, to predict future\ninsights and revolutionize the decision-taking process, inaugurates pervasive\nsystems as a worthy paradigm for a better quality-of-life. The confluence of\npervasive computing and artificial intelligence, Pervasive AI, expanded the\nrole of ubiquitous IoT systems from mainly data collection to executing\ndistributed computations with a promising alternative to centralized learning,\npresenting various challenges. In this context, a wise cooperation and resource\nscheduling should be envisaged among IoT devices (e.g., smartphones, smart\nvehicles) and infrastructure (e.g. edge nodes, and base stations) to avoid\ncommunication and computation overheads and ensure maximum performance. In this\npaper, we conduct a comprehensive survey of the recent techniques developed to\novercome these resource challenges in pervasive AI systems. Specifically, we\nfirst present an overview of the pervasive computing, its architecture, and its\nintersection with artificial intelligence. We then review the background,\napplications and performance metrics of AI, particularly Deep Learning (DL) and\nonline learning, running in a ubiquitous system. Next, we provide a deep\nliterature review of communication-efficient techniques, from both algorithmic\nand system perspectives, of distributed inference, training and online learning\ntasks across the combination of IoT devices, edge devices and cloud servers.\nFinally, we discuss our future vision and research challenges.", "time": "2022-08-27T13:54:51Z", "link": "http://arxiv.org/abs/2105.01798v2", "id": "2105.01798v2", "title": "Pervasive AI for IoT applications: A Survey on Resource-efficient\n  Distributed Artificial Intelligence"}
{"author": "Hyunho Ahn, Munkyu Lee, Cheol-Ho Hong, Blesson Varghese", "abstract": "Industrial Internet of Things (IIoT) applications can benefit from leveraging\nedge computing. For example, applications underpinned by deep neural networks\n(DNN) models can be sliced and distributed across the IIoT device and the edge\nof the network for improving the overall performance of inference and for\nenhancing privacy of the input data, such as industrial product images.\nHowever, low network performance between IIoT devices and the edge is often a\nbottleneck. In this study, we develop ScissionLite, a holistic framework for\naccelerating distributed DNN inference using the Transfer Layer (TL). The TL is\na traffic-aware layer inserted between the optimal slicing point of a DNN model\nslice in order to decrease the outbound network traffic without a significant\naccuracy drop. For the TL, we implement a new lightweight down/upsampling\nnetwork for performance-limited IIoT devices. In ScissionLite, we develop\nScissionTL, the Preprocessor, and the Offloader for end-to-end activities for\ndeploying DNN slices with the TL. They decide the optimal slicing point of the\nDNN, prepare pre-trained DNN slices including the TL, and execute the DNN\nslices on an IIoT device and the edge. Employing the TL for the sliced DNN\nmodels has a negligible overhead. ScissionLite improves the inference latency\nby up to 16 and 2.8 times when compared to execution on the local device and an\nexisting state-of-the-art model slicing approach respectively.", "time": "2021-05-05T12:38:58Z", "link": "http://arxiv.org/abs/2105.02019v1", "id": "2105.02019v1", "title": "ScissionLite: Accelerating Distributed Deep Neural Networks Using\n  Transfer Layer"}
{"author": "Federica Filippini, Danilo Ardagna, Marco Lattuada, Edoardo Amaldi, Michele Ciavotta, Maciek Riedl, Katarzyna Materka, PaweÅ Skrzypek, Fabrizio Magugliani, Marco Cicala", "abstract": "Artificial Intelligence (AI) and Deep Learning (DL) algorithms are currently\napplied to a wide range of products and solutions. DL training jobs are highly\nresource demanding and they experience great benefits when exploiting AI\naccelerators (e.g., GPUs). However, the effective management of GPU-powered\nclusters comes with great challenges. Among these, efficient scheduling and\nresource allocation solutions are crucial to maximize performance and minimize\nData Centers operational costs. In this paper we propose ANDREAS, an advanced\nscheduling solution that tackles these problems jointly, aiming at optimizing\nDL training runtime workloads and their energy consumption in accelerated\nclusters. Experiments based on simulation demostrate that we can achieve a cost\nreduction between 30 and 62% on average with respect to first-principle methods\nwhile the validation on a real cluster shows a worst case deviation below 13%\nbetween actual and predicted costs, proving the effectiveness of ANDREAS\nsolution in practical scenarios.", "time": "2021-05-11T14:36:19Z", "link": "http://arxiv.org/abs/2105.05080v1", "id": "2105.05080v1", "title": "ANDREAS: Artificial intelligence traiNing scheDuler foR accElerAted\n  resource clusterS"}
{"author": "Yuping Fan, Zhiling Lan", "abstract": "For decades, system administrators have been striving to design and tune\ncluster scheduling policies to improve the performance of high performance\ncomputing (HPC) systems. However, the increasingly complex HPC systems combined\nwith highly diverse workloads make such manual process challenging,\ntime-consuming, and error-prone. We present a reinforcement learning based HPC\nscheduling framework named DRAS-CQSim to automatically learn optimal scheduling\npolicy. DRAS-CQSim encapsulates simulation environments, agents, hyperparameter\ntuning options, and different reinforcement learning algorithms, which allows\nthe system administrators to quickly obtain customized scheduling policies.", "time": "2021-05-16T21:56:31Z", "link": "http://arxiv.org/abs/2105.07526v1", "id": "2105.07526v1", "title": "DRAS-CQSim: A Reinforcement Learning based Framework for HPC Cluster\n  Scheduling"}
{"author": "Philipp-Jan Honysz, Alexander Schulze-Struchtrup, Sebastian BuschjÃ¤ger, Katharina Morik", "abstract": "Data summarizations are a valuable tool to derive knowledge from large data\nstreams and have proven their usefulness in a great number of applications.\nSummaries can be found by optimizing submodular functions. These functions map\nsubsets of data to real values, which indicate their \"representativeness\" and\nwhich should be maximized to find a diverse summary of the underlying data. In\nthis paper, we studied Exemplar-based clustering as a submodular function and\nprovide a GPU algorithm to cope with its high computational complexity. We\nshow, that our GPU implementation provides speedups of up to 72x using\nsingle-precision and up to 452x using half-precision computation compared to\nconventional CPU algorithms. We also show, that the GPU algorithm not only\nprovides remarkable runtime benefits with workstation-grade GPUs but also with\nlow-power embedded computation units for which speedups of up to 35x are\npossible. Furthermore, we apply our algorithm to real-world data from injection\nmolding manufacturing processes and discuss how found summaries help with\nsteering this specific process to cut costs and reduce the manufacturing of bad\nparts. Beyond pure speedup considerations, we show, that our approach can\nprovide summaries within reasonable time frames for this kind of industrial,\nreal-world data.", "time": "2021-06-18T21:46:04Z", "link": "http://arxiv.org/abs/2105.12026v2", "id": "2105.12026v2", "title": "Providing Meaningful Data Summarizations Using Exemplar-based Clustering\n  in Industry 4.0"}
{"author": "Mohammad Sina Kiarostami", "abstract": "Due to the significant importance of Big Data analysis, especially in\nbusiness-related topics such as improving services, finding potential\ncustomers, and selecting practical approaches to manage income and expenses,\nmany companies attempt to collaborate with scientists to find how, why, and\nwhat they should analysis. In this work, we would like to compare and discuss\ntwo different approaches that employed in business analysis topic in Big Data\nwith more consideration on how they utilized Spark. Both studies have\ninvestigated Churn Prediction as their case study for their proposed approaches\nsince it is an essential topic in business analysis for companies to recognize\na customer intends to leave or stop using their services. Here, we focus on\nApache Spark since it has provided several solutions to handle a massive amount\nof data in recent years efficiently. This feature in Spark makes it one of the\nmost robust candidate tools to upfront with a Big Data problem, particularly\ntime and resource are concerns.", "time": "2021-05-28T12:19:30Z", "link": "http://arxiv.org/abs/2105.15147v1", "id": "2105.15147v1", "title": "Comparing Two Different Approaches in Big Data and Business Analysis for\n  Churn Prediction with the Focus on How Apache Spark Employed"}
{"author": "Xavier Vasques", "abstract": "The data center of tomorrow is a data center made up of heterogeneous\nsystems, which will run heterogeneous workloads. The systems will be located as\nclose as possible to the data. Heterogeneous systems will be equipped with\nbinary, biological inspired and quantum accelerators. These architectures will\nbe the foundations to address challenges. Like an orchestra conductor, the\nhybrid cloud will make it possible to set these systems to music thanks to a\nlayer of security and intelligent automation.", "time": "2021-11-04T13:02:01Z", "link": "http://arxiv.org/abs/2108.03997v2", "id": "2108.03997v2", "title": "A new step for computing"}
{"author": "Kordian Gontarska, Morgan Geldenhuys, Dominik Scheinert, Philipp Wiesner, Andreas Polze, Lauritz Thamsen", "abstract": "Distributed Stream Processing (DSP) systems enable processing large streams\nof continuous data to produce results in near to real time. They are an\nessential part of many data-intensive applications and analytics platforms. The\nrate at which events arrive at DSP systems can vary considerably over time,\nwhich may be due to trends, cyclic, and seasonal patterns within the data\nstreams. A priori knowledge of incoming workloads enables proactive approaches\nto resource management and optimization tasks such as dynamic scaling, live\nmigration of resources, and the tuning of configuration parameters during\nrun-times, thus leading to a potentially better Quality of Service.\n  In this paper we conduct a comprehensive evaluation of different load\nprediction techniques for DSP jobs. We identify three use-cases and formulate\nrequirements for making load predictions specific to DSP jobs. Automatically\noptimized classical and Deep Learning methods are being evaluated on nine\ndifferent datasets from typical DSP domains, i.e. the IoT, Web 2.0, and cluster\nmonitoring. We compare model performance with respect to overall accuracy and\ntraining duration. Our results show that the Deep Learning methods provide the\nmost accurate load predictions for the majority of the evaluated datasets.", "time": "2021-08-10T15:25:32Z", "link": "http://arxiv.org/abs/2108.04749v1", "id": "2108.04749v1", "title": "Evaluation of Load Prediction Techniques for Distributed Stream\n  Processing"}
{"author": "Vibhatha Abeykoon, Supun Kamburugamuve, Chathura Widanage, Niranda Perera, Ahmet Uyar, Thejaka Amila Kanewala, Gregor von Laszewski, Geoffrey Fox", "abstract": "Data-intensive applications are becoming commonplace in all science\ndisciplines. They are comprised of a rich set of sub-domains such as data\nengineering, deep learning, and machine learning. These applications are built\naround efficient data abstractions and operators that suit the applications of\ndifferent domains. Often lack of a clear definition of data structures and\noperators in the field has led to other implementations that do not work well\ntogether. The HPTMT architecture that we proposed recently, identifies a set of\ndata structures, operators, and an execution model for creating rich data\napplications that links all aspects of data engineering and data science\ntogether efficiently. This paper elaborates and illustrates this architecture\nusing an end-to-end application with deep learning and data engineering parts\nworking together.", "time": "2021-08-13T00:05:43Z", "link": "http://arxiv.org/abs/2108.06001v1", "id": "2108.06001v1", "title": "HPTMT Parallel Operators for High Performance Data Science & Data\n  Engineering"}
{"author": "Stephan Patrick Baller, Anshul Jindal, Mohak Chadha, Michael Gerndt", "abstract": "EdgeAI (Edge computing based Artificial Intelligence) has been most actively\nresearched for the last few years to handle variety of massively distributed AI\napplications to meet up the strict latency requirements. Meanwhile, many\ncompanies have released edge devices with smaller form factors (low power\nconsumption and limited resources) like the popular Raspberry Pi and Nvidia's\nJetson Nano for acting as compute nodes at the edge computing environments.\nAlthough the edge devices are limited in terms of computing power and hardware\nresources, they are powered by accelerators to enhance their performance\nbehavior. Therefore, it is interesting to see how AI-based Deep Neural Networks\nperform on such devices with limited resources.\n  In this work, we present and compare the performance in terms of inference\ntime and power consumption of the four Systems on a Chip (SoCs): Asus Tinker\nEdge R, Raspberry Pi 4, Google Coral Dev Board, Nvidia Jetson Nano, and one\nmicrocontroller: Arduino Nano 33 BLE, on different deep learning models and\nframeworks. We also provide a method for measuring power consumption, inference\ntime and accuracy for the devices, which can be easily extended to other\ndevices. Our results showcase that, for Tensorflow based quantized model, the\nGoogle Coral Dev Board delivers the best performance, both for inference time\nand power consumption. For a low fraction of inference computation time, i.e.\nless than 29.3% of the time for MobileNetV2, the Jetson Nano performs faster\nthan the other devices.", "time": "2021-08-21T08:13:22Z", "link": "http://arxiv.org/abs/2108.09457v1", "id": "2108.09457v1", "title": "DeepEdgeBench: Benchmarking Deep Neural Networks on Edge Devices"}
{"author": "Kai-Hsun Chen, Huan-Ping Su, Wei-Chiu Chuang, Hung-Chang Hsiao, Wangda Tan, Zhankun Tang, Xun Liu, Yanbo Liang, Wen-Chih Lo, Wanqiang Ji, Byron Hsu, Keqiu Hu, HuiYang Jian, Quan Zhou, Chien-Min Wang", "abstract": "As machine learning is applied more widely, it is necessary to have a machine\nlearning platform for both infrastructure administrators and users including\nexpert data scientists and citizen data scientists to improve their\nproductivity. However, existing machine learning platforms are ill-equipped to\naddress the \"Machine Learning tech debts\" such as glue code, reproducibility,\nand portability. Furthermore, existing platforms only take expert data\nscientists into consideration, and thus they are inflexible for infrastructure\nadministrators and non-user-friendly for citizen data scientists. We propose\nSubmarine, a unified machine learning platform, to address the challenges.", "time": "2021-08-22T01:47:33Z", "link": "http://arxiv.org/abs/2108.09615v1", "id": "2108.09615v1", "title": "Apache Submarine: A Unified Machine Learning Platform Made Simple"}
{"author": "Tharindu Adikari", "abstract": "This article is in the context of gradient compression. Gradient compression\nis a popular technique for mitigating the communication bottleneck observed\nwhen training large machine learning models in a distributed manner using\ngradient-based methods such as stochastic gradient descent. In this article,\nassuming a Gaussian distribution for the components in gradient, we find the\nrate distortion trade-off of gradient quantization schemes such as Scaled-sign\nand Top-K, and compare with the Shannon rate distortion limit. A similar\ncomparison with vector quantizers also is presented.", "time": "2021-08-23T02:33:51Z", "link": "http://arxiv.org/abs/2108.09899v1", "id": "2108.09899v1", "title": "Rate distortion comparison of a few gradient quantizers"}
{"author": "Salman Haidri, Yaksh J. Haranwala, Vania Bogorny, Chiara Renso, Vinicius Prado da Fonseca, Amilcar Soares", "abstract": "Trajectory data represent a trace of an object that changes its position in\nspace over time. This kind of data is complex to handle and analyze, since it\nis generally produced in huge quantities, often prone to errors generated by\nthe geolocation device, human mishandling, or area coverage limitation.\nTherefore, there is a need for software specifically tailored to preprocess\ntrajectory data. In this work we propose PTRAIL, a python package offering\nseveral trajectory preprocessing steps, including filtering, feature\nextraction, and interpolation. PTRAIL uses parallel computation and\nvectorization, being suitable for large datasets and fast compared to other\npython libraries.", "time": "2021-08-26T20:14:07Z", "link": "http://arxiv.org/abs/2108.13202v1", "id": "2108.13202v1", "title": "PTRAIL -- A python package for parallel trajectory data preprocessing"}
{"author": "Seungbeom Choi, Sunho Lee, Yeonjae Kim, Jongse Park, Youngjin Kwon, Jaehyuk Huh", "abstract": "As machine learning techniques are applied to a widening range of\napplications, high throughput machine learning (ML) inference servers have\nbecome critical for online service applications. Such ML inference servers pose\ntwo challenges: first, they must provide a bounded latency for each request to\nsupport consistent service-level objective (SLO), and second, they can serve\nmultiple heterogeneous ML models in a system as certain tasks involve\ninvocation of multiple models and consolidating multiple models can improve\nsystem utilization. To address the two requirements of ML inference servers,\nthis paper proposes a new ML inference scheduling framework for multi-model ML\ninference servers. The paper first shows that with SLO constraints, current\nGPUs are not fully utilized for ML inference tasks. To maximize the resource\nefficiency of inference servers, a key mechanism proposed in this paper is to\nexploit hardware support for spatial partitioning of GPU resources. With the\npartitioning mechanism, a new abstraction layer of GPU resources is created\nwith configurable GPU resources. The scheduler assigns requests to virtual\nGPUs, called gpu-lets, with the most effective amount of resources. The paper\nalso investigates a remedy for potential interference effects when two ML tasks\nare running concurrently in a GPU. Our prototype implementation proves that\nspatial partitioning enhances throughput by 102.6% on average while satisfying\nSLOs.", "time": "2021-09-01T04:46:46Z", "link": "http://arxiv.org/abs/2109.01611v1", "id": "2109.01611v1", "title": "Multi-model Machine Learning Inference Serving with GPU Spatial\n  Partitioning"}
{"author": "Aryan Naim, Ryan Alimo, Jay Braun", "abstract": "Emergency personnel respond to various situations ranging from fire, medical,\nhazardous materials, industrial accidents, to natural disasters. Situations\nsuch as natural disasters or terrorist acts require a multifaceted response of\nfirefighters, paramedics, hazmat teams, and other agencies. Engineering AI\nsystems that aid emergency personnel proves to be a difficult system\nengineering problem. Mission-critical \"edge AI\" situations require low-latency,\nreliable analytics. To further add complexity, a high degree of model accuracy\nis required when lives are at stake, creating a need for the deployment of\nhighly accurate, however computationally intensive models to\nresource-constrained devices. To address all these issues, we propose an\nagent-based architecture for deployment of AI agents via 5G service-based\narchitecture.", "time": "2021-09-10T03:24:50Z", "link": "http://arxiv.org/abs/2109.04646v1", "id": "2109.04646v1", "title": "AI Agents in Emergency Response Applications"}
{"author": "Keshi Ge, Yongquan Fu, Zhiquan Lai, Xiaoge Deng, Dongsheng Li", "abstract": "Distributed stochastic gradient descent (SGD) approach has been widely used\nin large-scale deep learning, and the gradient collective method is vital to\nensure the training scalability of the distributed deep learning system.\nCollective communication such as AllReduce has been widely adopted for the\ndistributed SGD process to reduce the communication time. However, AllReduce\nincurs large bandwidth resources while most gradients are sparse in many cases\nsince many gradient values are zeros and should be efficiently compressed for\nbandwidth saving. To reduce the sparse gradient communication overhead, we\npropose Sparse-Sketch Reducer (S2 Reducer), a novel sketch-based sparse\ngradient aggregation method with convergence guarantees. S2 Reducer reduces the\ncommunication cost by only compressing the non-zero gradients with count-sketch\nand bitmap, and enables the efficient AllReduce operators for parallel SGD\ntraining. We perform extensive evaluation against four state-of-the-art methods\nover five training models. Our results show that S2 Reducer converges to the\nsame accuracy, reduces 81\\% sparse communication overhead, and achieves 1.8$\n\\times $ speedup compared to state-of-the-art approaches.", "time": "2021-11-26T05:44:29Z", "link": "http://arxiv.org/abs/2110.02140v2", "id": "2110.02140v2", "title": "S2 Reducer: High-Performance Sparse Communication to Accelerate\n  Distributed Deep Learning"}
{"author": "Martin Uray, Eduard Hirsch, Gerold Katzinger, Michael Gadermayr", "abstract": "Enterprises and labs performing computationally expensive data science\napplications sooner or later face the problem of scale but unconnected\ninfrastructure. For this up-scaling process, an IT service provider can be\nhired or in-house personnel can attempt to implement a software stack. The\nfirst option can be quite expensive if it is just about connecting several\nmachines. For the latter option often experience is missing with the data\nscience staff in order to navigate through the software jungle. In this\ntechnical report, we illustrate the decision process towards an on-premises\ninfrastructure, our implemented system architecture, and the transformation of\nthe software stack towards a scaleable GPU cluster system.", "time": "2021-10-11T11:02:50Z", "link": "http://arxiv.org/abs/2110.05156v1", "id": "2110.05156v1", "title": "Beyond Desktop Computation: Challenges in Scaling a GPU Infrastructure"}
{"author": "Jiarong Xing, Leyuan Wang, Shang Zhang, Jack Chen, Ang Chen, Yibo Zhu", "abstract": "Today's auto-tuners (e.g., AutoTVM, Ansor) generate efficient tensor programs\nby navigating a large search space to identify effective implementations, but\nthey do so with opaque hardware details. Thus, their performance could fall\nbehind that of hardware-native libraries (e.g., cuBLAS, cuDNN), which are\nhand-optimized by device vendors to extract high performance. On the other\nhand, these vendor libraries have a fixed set of supported functions and lack\nthe customization and automation support afforded by auto-tuners. Bolt is based\non the recent trend that vendor libraries are increasingly modularized and\nreconfigurable via declarative control (e.g., CUTLASS). It enables a novel\napproach that bridges this gap and achieves the best of both worlds, via\nhardware-native templated search. Bolt provides new opportunities to rethink\nend-to-end tensor optimizations at the graph, operator, and model levels. Bolt\ndemonstrates this concept by prototyping on a popular auto-tuner in TVM and a\nclass of widely-used platforms (i.e., NVIDIA GPUs) -- both in large deployment\nin our production environment. Bolt improves the inference speed of common\nconvolutional neural networks by 2.5x on average over the state of the art, and\nit auto-tunes these models within 20 minutes.", "time": "2021-10-25T19:47:15Z", "link": "http://arxiv.org/abs/2110.15238v1", "id": "2110.15238v1", "title": "Bolt: Bridging the Gap between Auto-tuners and Hardware-native\n  Performance"}
{"author": "Chinmaya Kumar Dehury, Shivananda Poojara, Satish Narayana Srirama", "abstract": "Fog computing is introduced by shifting cloud resources towards the users'\nproximity to mitigate the limitations possessed by cloud computing. Fog\nenvironment made its limited resource available to a large number of users to\ndeploy their serverless applications, composed of several serverless functions.\nOne of the primary intentions behind introducing the fog environment is to\nfulfil the demand of latency and location-sensitive serverless applications\nthrough its limited resources. The recent research mainly focuses on assigning\nmaximum resources to such applications from the fog node and not taking full\nadvantage of the cloud environment. This introduces a negative impact in\nproviding the resources to a maximum number of connected users. To address this\nissue, in this paper, we investigated the optimum percentage of a user's\nrequest that should be fulfilled by fog and cloud. As a result, we proposed\nDeF-DReL, a Systematic Deployment of Serverless Functions in Fog and Cloud\nenvironments using Deep Reinforcement Learning, using several real-life\nparameters, such as distance and latency of the users from nearby fog node,\nuser's priority, the priority of the serverless applications and their resource\ndemand, etc. The performance of the DeF-DReL algorithm is further compared with\nrecent related algorithms. From the simulation and comparison results, its\nsuperiority over other algorithms and its applicability to the real-life\nscenario can be clearly observed.", "time": "2022-04-18T13:33:48Z", "link": "http://arxiv.org/abs/2110.15702v3", "id": "2110.15702v3", "title": "DeF-DReL: Systematic Deployment of Serverless Functions in Fog and Cloud\n  environments using Deep Reinforcement Learning"}
{"author": "Yuqing Tian, Zhaoyang Zhang, Zhaohui Yang, Qianqian Yang", "abstract": "The main challenge to deploy deep neural network (DNN) over a mobile edge\nnetwork is how to split the DNN model so as to match the network architecture\nas well as all the nodes' computation and communication capacity. This\nessentially involves two highly coupled procedures: model generating and model\nsplitting. In this paper, a joint model split and neural architecture search\n(JMSNAS) framework is proposed to automatically generate and deploy a DNN model\nover a mobile edge network. Considering both the computing and communication\nresource constraints, a computational graph search problem is formulated to\nfind the multi-split points of the DNN model, and then the model is trained to\nmeet some accuracy requirements. Moreover, the trade-off between model accuracy\nand completion latency is achieved through the proper design of the objective\nfunction. The experiment results confirm the superiority of the proposed\nframework over the state-of-the-art split machine learning design methods.", "time": "2021-11-16T03:10:23Z", "link": "http://arxiv.org/abs/2111.08206v1", "id": "2111.08206v1", "title": "JMSNAS: Joint Model Split and Neural Architecture Search for Learning\n  over Mobile Edge Networks"}
{"author": "Haili Wang, Jingda Guo, Xu Ma, Song Fu, Qing Yang, Yunzhong Xu", "abstract": "Modern cloud computing systems contain hundreds to thousands of computing and\nstorage servers. Such a scale, combined with ever-growing system complexity, is\ncausing a key challenge to failure and resource management for dependable cloud\ncomputing. Autonomic failure detection is a crucial technique for understanding\nemergent, cloud-wide phenomena and self-managing cloud resources for\nsystem-level dependability assurance. To detect failures, we need to monitor\nthe cloud execution and collect runtime performance data. These data are\nusually unlabeled, and thus a prior failure history is not always available in\nproduction clouds. In this paper, we present a \\emph{self-evolving anomaly\ndetection} (SEAD) framework for cloud dependability assurance. Our framework\nself-evolves by recursively exploring newly verified anomaly records and\ncontinuously updating the anomaly detector online. As a distinct advantage of\nour framework, cloud system administrators only need to check a small number of\ndetected anomalies, and their decisions are leveraged to update the detector.\nThus, the detector evolves following the upgrade of system hardware, update of\nthe software stack, and change of user workloads. Moreover, we design two types\nof detectors, one for general anomaly detection and the other for type-specific\nanomaly detection. With the help of self-evolving techniques, our detectors can\nachieve 88.94\\% in sensitivity and 94.60\\% in specificity on average, which\nmakes them suitable for real-world deployment.", "time": "2021-11-16T05:13:38Z", "link": "http://arxiv.org/abs/2111.08232v1", "id": "2111.08232v1", "title": "Online Self-Evolving Anomaly Detection in Cloud Computing Environments"}
{"author": "Sian Jin, Chengming Zhang, Xintong Jiang, Yunhe Feng, Hui Guan, Guanpeng Li, Shuaiwen Leon Song, Dingwen Tao", "abstract": "Training wide and deep neural networks (DNNs) require large amounts of\nstorage resources such as memory because the intermediate activation data must\nbe saved in the memory during forward propagation and then restored for\nbackward propagation. However, state-of-the-art accelerators such as GPUs are\nonly equipped with very limited memory capacities due to hardware design\nconstraints, which significantly limits the maximum batch size and hence\nperformance speedup when training large-scale DNNs. Traditional memory saving\ntechniques either suffer from performance overhead or are constrained by\nlimited interconnect bandwidth or specific interconnect technology. In this\npaper, we propose a novel memory-efficient CNN training framework (called\nCOMET) that leverages error-bounded lossy compression to significantly reduce\nthe memory requirement for training, to allow training larger models or to\naccelerate training. Different from the state-of-the-art solutions that adopt\nimage-based lossy compressors (such as JPEG) to compress the activation data,\nour framework purposely adopts error-bounded lossy compression with a strict\nerror-controlling mechanism. Specifically, we perform a theoretical analysis on\nthe compression error propagation from the altered activation data to the\ngradients, and empirically investigate the impact of altered gradients over the\ntraining process. Based on these analyses, we optimize the error-bounded lossy\ncompression and propose an adaptive error-bound control scheme for activation\ndata compression. We evaluate our design against state-of-the-art solutions\nwith five widely-adopted CNNs and ImageNet dataset. Experiments demonstrate\nthat our proposed framework can significantly reduce the training memory\nconsumption by up to 13.5X over the baseline training and 1.8X over another\nstate-of-the-art compression-based framework, respectively, with little or no\naccuracy loss.", "time": "2021-11-18T07:43:45Z", "link": "http://arxiv.org/abs/2111.09562v1", "id": "2111.09562v1", "title": "COMET: A Novel Memory-Efficient Deep Learning Training Framework by\n  Using Error-Bounded Lossy Compression"}
{"author": "Aaron Yi Ding, Ella Peltonen, Tobias Meuser, Atakan Aral, Christian Becker, Schahram Dustdar, Thomas Hiessl, Dieter Kranzlmuller, Madhusanka Liyanage, Setareh Magshudi, Nitinder Mohan, Joerg Ott, Jan S. Rellermeyer, Stefan Schulte, Henning Schulzrinne, Gurkan Solmaz, Sasu Tarkoma, Blesson Varghese, Lars Wolf", "abstract": "Based on the collective input of Dagstuhl Seminar (21342), this paper\npresents a comprehensive discussion on AI methods and capabilities in the\ncontext of edge computing, referred as Edge AI. In a nutshell, we envision Edge\nAI to provide adaptation for data-driven applications, enhance network and\nradio access, and allow the creation, optimization, and deployment of\ndistributed AI/ML pipelines with given quality of experience, trust, security\nand privacy targets. The Edge AI community investigates novel ML methods for\nthe edge computing environment, spanning multiple sub-fields of computer\nscience, engineering and ICT. The goal is to share an envisioned roadmap that\ncan bring together key actors and enablers to further advance the domain of\nEdge AI.", "time": "2021-11-27T16:48:20Z", "link": "http://arxiv.org/abs/2112.00616v1", "id": "2112.00616v1", "title": "Roadmap for Edge AI: A Dagstuhl Perspective"}
{"author": "Yiqing Ma, Hao Wang, Yiming Zhang, Kai Chen", "abstract": "ByteScheduler partitions and rearranges tensor transmissions to improve the\ncommunication efficiency of distributed Deep Neural Network (DNN) training. The\nconfiguration of hyper-parameters (i.e., the partition size and the credit\nsize) is critical to the effectiveness of partitioning and rearrangement.\nCurrently, ByteScheduler adopts Bayesian Optimization (BO) to find the optimal\nconfiguration for the hyper-parameters beforehand. In practice, however,\nvarious runtime factors (e.g., worker node status and network conditions)\nchange over time, making the statically-determined one-shot configuration\nresult suboptimal for real-world DNN training. To address this problem, we\npresent a real-time configuration method (called AutoByte) that automatically\nand timely searches the optimal hyper-parameters as the training systems\ndynamically change. AutoByte extends the ByteScheduler framework with a\nmeta-network, which takes the system's runtime statistics as its input and\noutputs predictions for speedups under specific configurations. Evaluation\nresults on various DNN models show that AutoByte can dynamically tune the\nhyper-parameters with low resource usage, and deliver up to 33.2\\% higher\nperformance than the best static configuration in ByteScheduler.", "time": "2021-12-27T04:54:49Z", "link": "http://arxiv.org/abs/2112.13509v1", "id": "2112.13509v1", "title": "Automatic Configuration for Optimal Communication Scheduling in DNN\n  Training"}
{"author": "Xinheng Liu, Yao Chen, Prakhar Ganesh, Junhao Pan, Jinjun Xiong, Deming Chen", "abstract": "Quantization for Convolutional Neural Network (CNN) has shown significant\nprogress with the intention of reducing the cost of computation and storage\nwith low-bitwidth data inputs. There are, however, no systematic studies on how\nan existing full-bitwidth processing unit, such as CPUs and DSPs, can be better\nutilized to carry out significantly higher computation throughput for\nconvolution under various quantized bitwidths. In this study, we propose\nHiKonv, a unified solution that maximizes the compute throughput of a given\nunderlying processing unit to process low-bitwidth quantized data inputs\nthrough novel bit-wise parallel computation. We establish theoretical\nperformance bounds using a full-bitwidth multiplier for highly parallelized\nlow-bitwidth convolution, and demonstrate new breakthroughs for\nhigh-performance computing in this critical domain. For example, a single\n32-bit processing unit can deliver 128 binarized convolution operations\n(multiplications and additions) under one CPU instruction, and a single 27x18\nDSP core can deliver eight convolution operations with 4-bit inputs in one\ncycle. We demonstrate the effectiveness of HiKonv on CPU and FPGA for both\nconvolutional layers or a complete DNN model. For a convolutional layer\nquantized to 4-bit, HiKonv achieves a 3.17x latency improvement over the\nbaseline implementation using C++ on CPU. Compared to the DAC-SDC 2020 champion\nmodel for FPGA, HiKonv achieves a 2.37x throughput improvement and 2.61x DSP\nefficiency improvement, respectively.", "time": "2021-12-28T03:03:39Z", "link": "http://arxiv.org/abs/2112.13972v1", "id": "2112.13972v1", "title": "HiKonv: High Throughput Quantized Convolution With Novel Bit-wise\n  Management and Computation"}
{"author": "Naveen T. R. Babu, Christopher Stewart", "abstract": "Multiple applications running on Edge computers can be orchestrated to\nachieve the desired goal. Orchestration of applications is prominent when\nworking with Internet of Things based applications, Autonomous driving and\nAutonomous Aerial vehicles. As the applications receive modified\nclassifiers/code, there will be multiple applications that need to be updated.\nIf all the classifiers are synchronously updated there would be increased\nthroughput and bandwidth degradation. On the other hand, delaying updates of\napplications which need immediate update hinders performance and delays\nprogress towards end goal. The updates of applications should be prioritized\nand updates should happen according to this priority. This paper explores the\nsetup and benchmarks to understand the impact of updates when multiple\napplications working to achieve same objective are orchestrated with\nprioritized updates. We discuss methods to build a distributed, reliable and\nscalable system called \"DSOC\"(Docker Swarm Orchestration Component).", "time": "2021-01-02T07:47:34Z", "link": "http://arxiv.org/abs/2101.00397v1", "id": "2101.00397v1", "title": "Early Work on Efficient Patching for Coordinating Edge Applications"}
{"author": "Andreas Grammenos, Themistoklis Charalambous, Evangelia Kalyvianaki", "abstract": "We propose an asynchronous iterative scheme that allows a set of\ninterconnected nodes to distributively reach an agreement within a\npre-specified bound in a finite number of steps. While this scheme could be\nadopted in a wide variety of applications, we discuss it within the context of\ntask scheduling for data centers. In this context, the algorithm is guaranteed\nto approximately converge to the optimal scheduling plan, given the available\nresources, in a finite number of steps. Furthermore, by being asynchronous, the\nproposed scheme is able to take into account the uncertainty that can be\nintroduced from straggler nodes or communication issues in the form of latency\nvariability while still converging to the target objective. In addition, by\nusing extensive empirical evaluation through simulations we show that the\nproposed method exhibits state-of-the-art performance.", "time": "2023-01-12T00:54:14Z", "link": "http://arxiv.org/abs/2101.06139v3", "id": "2101.06139v3", "title": "CPU Scheduling in Data Centers Using Asynchronous Finite-Time\n  Distributed Coordination Mechanisms"}
{"author": "S. H. Alsamhi, B. Lee, M. Guizani, N. Kumar, Y. Qiao, Xuan Liu", "abstract": "Currently, drones represent a promising technology for combating Coronavirus\ndisease 2019 (COVID-19) due to the transport of goods, medical supplies to a\ngiven target location in the quarantine areas experiencing an epidemic\noutbreak. Drone missions will increasingly rely on drone collaboration, which\nrequires the drones to reduce communication complexity and be controlled in a\ndecentralized fashion. Blockchain technology becomes a must in industrial\napplications because it provides decentralized data, accessibility,\nimmutability, and irreversibility. Therefore, Blockchain makes data public for\nall drones and enables drones to log information concerning world states, time,\nlocation, resources, delivery data, and drone relation to all neighbors drones.\nThis paper introduces decentralized independent multi-drones to accomplish the\ntask collaboratively. Improving blockchain with a consensus algorithm can\nimprove network partitioning and scalability in order to combat COVID-19. The\nmulti-drones task is to combat COVID-19 via monitoring and detecting, social\ndistancing, sanitization, data analysis, delivering goods and medical supplies,\nand announcement while avoiding collisions with one another. We discuss End to\nEnd (E2E) delivery application of combination blockchain and multi-drone in\ncombating COVID-19 and beyond future pandemics. Furthermore, the challenges and\nopportunities of our proposed framework are highlighted.", "time": "2021-02-01T16:54:30Z", "link": "http://arxiv.org/abs/2102.00969v1", "id": "2102.00969v1", "title": "Blockchain for Decentralized Multi-Drone to Combat COVID-19"}
{"author": "Nima Nikmehr, Mikhail A. Bragin, Peter B. Luh, Peng Zhang", "abstract": "Smart programmable microgrids (SPM) is an emerging technology for making\nmicrogrids more software-defined and less hardware-independent such that\nconverting distributed energy resources (DERs) to networked community\nmicrogrids becomes affordable, autonomic, and secure. As one of the\ncornerstones of SPM, this paper pioneers a concept of software-defined\noperation optimization for networked microgrids, where operation objectives,\ngrid connection, and DER participation will be defined by software and\nplug-and-play, and can be quickly reconfigured, based on the development of\nmodularized and tightened models and a novel asynchronous price-based\ndecomposition-and-coordination method. Key contributions include: (1) design\nthe architecture of the operational optimization of networked microgrids which\ncan be readily implemented to ensure the programmability of islanded microgrids\nin solving the distributed optimization models, (2) realize a novel discrete\nmodel of droop controller, and (3) introduce a powerful distributed and\nasynchronous method Distributed and Asynchronous Surrogate Lagrangian\nRelaxation (DA-SLR) to efficiently coordinate microgrids asynchronously. Two\ncase studies are tested to demonstrate the efficiency of developed DA-SLR, and\nspecifically, the testing results show the superiority of DA-SLR as compared to\nprevious methods such as ADMM.", "time": "2021-02-06T03:20:42Z", "link": "http://arxiv.org/abs/2102.03496v1", "id": "2102.03496v1", "title": "Distributed and Asynchronous Operational Optimization of Networked\n  Microgrids"}
{"author": "Saeid Barati, Lee Ehudin, Hank Hoffmann", "abstract": "Much recent research is devoted to exploring tradeoffs between computational\naccuracy and energy efficiency at different levels of the system stack.\nApproximation at the floating point unit (FPU) allows saving energy by simply\nreducing the number of computed floating point bits in return for accuracy\nloss. Although, finding the most energy efficient approximation for various\napplications with minimal effort is the main challenge. To address this issue,\nwe propose NEAT: a pin tool that helps users automatically explore the\naccuracy-energy tradeoff space induced by various floating point\nimplementations. NEAT helps programmers explore the effects of simultaneously\nusing multiple floating point implementations to achieve the lowest energy\nconsumption for an accuracy constraint or vice versa. NEAT accepts one or more\nuser-defined floating point implementations and programmable placement rules\nfor where/when to apply them. NEAT then automatically replaces floating point\noperations with different implementations based on the user-specified rules\nduring the runtime and explores the resulting tradeoff space to find the best\nuse of approximate floating point implementations for the precision tuning\nthroughout the program. We evaluate NEAT by enforcing combinations of 24/53\ndifferent floating point implementations with three sets of placement rules on\na wide range of benchmarks. We find that heuristic precision tuning at the\nfunction level provides up to 22% and 48% energy savings at 1% and 10% accuracy\nloss comparing to applying a single implementation for the whole application.\nAlso, NEAT is applicable to neural networks where it finds the optimal\nprecision level for each layer considering an accuracy target for the model.", "time": "2021-02-17T03:23:38Z", "link": "http://arxiv.org/abs/2102.08547v1", "id": "2102.08547v1", "title": "NEAT: A Framework for Automated Exploration of Floating Point\n  Approximations"}
{"author": "Yuanqiu Mo, Soura Dasgupta, Jacob Beal", "abstract": "Spreading information through a network of devices is a core activity for\nmost distributed systems. As such, self-stabilizing algorithms implementing\ninformation spreading are one of the key building blocks enabling aggregate\ncomputing to provide resilient coordination in open complex distributed\nsystems. This paper improves a general spreading block in the aggregate\ncomputing literature by making it resilient to network perturbations,\nestablishes its global uniform asymptotic stability and proves that it is\nultimately bounded under persistent disturbances. The ultimate bounds depend\nonly on the magnitude of the largest perturbation and the network diameter, and\nthree design parameters trade off competing aspects of performance. For\nexample, as in many dynamical systems, values leading to greater resilience to\nnetwork perturbations slow convergence and vice versa.", "time": "2021-02-20T11:59:20Z", "link": "http://arxiv.org/abs/2102.10319v1", "id": "2102.10319v1", "title": "Stability and Resilience of Distributed Information Spreading in\n  Aggregate Computing"}
{"author": "Guilherme Ramos, Daniel Silvestre, Carlos Silvestre", "abstract": "We tackle the problem of a set of agents achieving resilient consensus in the\npresence of attacked agents. We present a discrete-time reputation-based\nconsensus algorithm for synchronous and asynchronous networks by developing a\nlocal strategy where, at each time, each agent assigns a reputation (between\nzero and one) to each neighbor. The reputation is then used to weigh the\nneighbors' values in the update of its state. Under mild assumptions, we show\nthat: (i) the proposed method converges exponentially to the consensus of the\nregular agents; (ii) if a regular agent identifies a neighbor as an attacked\nnode, then it is indeed an attacked node; (iii) if the consensus value of the\nnormal nodes differs from that of any of the attacked nodes' values, then the\nreputation that a regular agent assigns to the attacked neighbors goes to zero.\nFurther, we extend our method to achieve resilience in the scenarios where\nthere are noisy nodes, dynamic networks and stochastic node selection. Finally,\nwe illustrate our algorithm with several examples, and we delineate some\nattacking scenarios that can be dealt by the current proposal but not by the\nstate-of-the-art approaches.", "time": "2021-07-01T13:26:57Z", "link": "http://arxiv.org/abs/2107.00431v1", "id": "2107.00431v1", "title": "A Discrete-time Reputation-based Resilient Consensus Algorithm for\n  Synchronous or Asynchronous Communications"}
{"author": "Mohit Garg, Cian Johnston, MÃ©lanie Bouroche", "abstract": "Connected autonomous vehicles (CAVs) can supplement the information from\ntheir own sensors with information from surrounding CAVs for decision making\nand control. This has the potential to improve traffic efficiency. CAVs face\nadditional challenges in their driving, however, when they interact with\nhuman-driven vehicles (HDVs) in mixed-traffic environments due to the\nuncertainty in human's driving behavior e.g. larger reaction times, perception\nerrors, etc. While a lot of research has investigated the impact of CAVs on\ntraffic safety and efficiency at different penetration rates, all have assumed\neither perfect communication or very simple scenarios with imperfect\ncommunication. In practice, the presence of communication delays and packet\nlosses means that CAVs might receive only partial information from surrounding\nvehicles, and this can have detrimental effects on their performance. This\npaper investigates the impact of CAVs on traffic efficiency in realistic\ncommunication and road network scenarios (i.e. imperfect communication and\nlarge-scale road network). We analyze the effect of unreliable communication\nlinks on CAVs operation in mixed traffic with various penetration rates and\nevaluate traffic performance in congested traffic scenarios on a large-scale\nroad network (the M50 motorway, in Ireland). Results show that CAVs can\nsignificantly improve traffic efficiency in congested traffic scenarios at high\npenetration rates. The scale of the improvement depends on communication\nreliability, with a packet drop rate of 70% leading to an increase in traffic\ncongestion by 28.7% and 11.88% at 40% and 70% penetration rates respectively\ncompared to perfect communication.", "time": "2021-07-11T18:46:23Z", "link": "http://arxiv.org/abs/2107.03078v2", "id": "2107.03078v2", "title": "Can Connected Autonomous Vehicles really improve mixed traffic\n  efficiency in realistic scenarios?"}
{"author": "Hunza Zainab, Giorgio Audrito, Soura Dasgupta, Jacob Beal", "abstract": "Distributed data collection is a fundamental task in open systems. In such\nnetworks, data is aggregated across a network to produce a single aggregated\nresult at a source device. Though self-stabilizing, algorithms performing data\ncollection can produce large overestimates in the transient phase. For example,\nin [1] we demonstrated that in a line graph, a switch of sources after initial\nstabilization may produce overestimates that are quadratic in the network\ndiameter. We also proposed monotonic filtering as a strategy for removing such\nlarge overestimates. Monotonic filtering prevents the transfer of data from\ndevice A to device B unless the distance estimate at A is more than that at B\nat the previous iteration. For a line graph, [1] shows that monotonic filtering\nprevents quadratic overestimates. This paper analyzes monotonic filtering for\nan arbitrary graph topology, showing that for an N device network, the largest\noverestimate after switching sources is at most 2N.", "time": "2021-07-13T00:32:36Z", "link": "http://arxiv.org/abs/2107.05791v1", "id": "2107.05791v1", "title": "Monotonic Filtering for Distributed Collection"}
{"author": "Tiago MÃ¼ck, Bryan Donyanavard, Biswadip Maity, Kasra Moazzemi, Nikil Dutt", "abstract": "Self-adaptive approaches for runtime resource management of manycore\ncomputing platforms often require a runtime model of the system that represents\nthe software organization or the architecture of the target platform. The\nincreasing heterogeneity in a platform's resource types and the interactions\nbetween resources pose challenges for coordinated model-based decision making\nin the face of dynamic workloads. Self-awareness properties address these\nchallenges for emerging heterogeneous manycore processing (HMP) platforms\nthrough reflective resource managers. However, with HMP computing platform\narchitectures evolving rapidly, porting the self-aware decision logic across\ndifferent hardware platforms is challenging, requiring resource managers to\nupdate their models and platform-specific interfaces. We propose MARS\n(Middleware for Adaptive and Reflective Systems), a cross-layer and\nmulti-platform framework that allows users to easily create resource managers\nby composing system models and resource management policies in a flexible and\ncoordinated manner. MARS consists of a generic user-level sensing/actuation\ninterface that allows for portable policy design, and a reflective system model\nused to coordinate multiple policies. We demonstrate MARS' interaction across\nmultiple layers of the system stack through a dynamic voltage and frequency\nscaling (DVFS) policy example which can run on any Linux-based HMP computing\nplatform.", "time": "2021-07-23T18:58:52Z", "link": "http://arxiv.org/abs/2107.11417v1", "id": "2107.11417v1", "title": "MARS: Middleware for Adaptive Reflective Computer Systems"}
{"author": "Qing Yang, Hao Wang, Xiaoxiao Wu, Taotao Wang, Shengli Zhang", "abstract": "This work presents the design and implementation of a blockchain system that\nenables the trustable transactive energy management for distributed energy\nresources (DERs). We model the interactions among DERs, including energy\ntrading and flexible appliance scheduling, as a cost minimization problem.\nConsidering the dispersed nature and diverse ownership of DERs, we develop a\ndistributed algorithm to solve the optimization problem using the alternating\ndirection method of multipliers (ADMM) method. Furthermore, we develop a\nblockchain system, on which we implement the proposed algorithm with the smart\ncontract, to guarantee the transparency and correctness of the energy\nmanagement. We prototype the blockchain in a small-scale test network and\nevaluate it through experiments using real-world data. The experimental results\nvalidate the feasibility and effectiveness of our design.", "time": "2021-06-04T14:55:20Z", "link": "http://arxiv.org/abs/2106.02529v1", "id": "2106.02529v1", "title": "Blockchain for Transactive Energy Management of Distributed Energy\n  Resources in Smart Grid"}
{"author": "Sarper AydÄ±n, Ceyhun Eksin", "abstract": "Multiple autonomous agents interact over a random communication network to\nmaximize their individual utility functions which depend on the actions of\nother agents. We consider decentralized best-response with inertia type\nalgorithms in which agents form beliefs about the future actions of other\nplayers based on local information, and take an action that maximizes their\nexpected utility computed with respect to these beliefs or continue to take\ntheir previous action. We show convergence of these types of algorithms to a\nNash equilibrium in weakly acyclic games under the condition that the belief\nupdate and information exchange protocols successfully learn the actions of\nother players with positive probability in finite time given a static\nenvironment, i.e., when other agents' actions do not change. We design a\ndecentralized fictitious play algorithm with voluntary and limited\ncommunication (DFP-VL) protocols that satisfy this condition. In the voluntary\ncommunication protocol, each agent decides whom to exchange information with by\nassessing the novelty of its information and the potential effect of its\ninformation on others' assessments of their utility functions. The limited\ncommunication protocol entails agents sending only their most frequent action\nto agents that they decide to communicate with. Numerical experiments on a\ntarget assignment game demonstrate that the voluntary and limited communication\nprotocol can more than halve the number of communication attempts while\nretaining the same convergence rate as DFP in which agents constantly attempt\nto communicate.", "time": "2021-06-13T20:04:57Z", "link": "http://arxiv.org/abs/2106.07079v1", "id": "2106.07079v1", "title": "Decentralized Inertial Best-Response with Voluntary and Limited\n  Communication in Random Communication Networks"}
{"author": "T. Mancini, F. Mari, I. Melatti, I. Salvo, E. Tronci, J. K. Gruber, B. Hayes, M. Prodanovic, L. Elmegaard", "abstract": "By using small computing devices deployed at user premises, Autonomous Demand\nResponse (ADR) adapts users electricity consumption to given time-dependent\nelectricity tariffs. This allows end-users to save on their electricity bill\nand Distribution System Operators to optimise (through suitable time-dependent\ntariffs) management of the electric grid by avoiding demand peaks.\nUnfortunately, even with ADR, users power consumption may deviate from the\nexpected (minimum cost) one, e.g., because ADR devices fail to correctly\nforecast energy needs at user premises. As a result, the aggregated power\ndemand may present undesirable peaks. In this paper we address such a problem\nby presenting methods and a software tool (APD-Analyser) implementing them,\nenabling Distribution System Operators to effectively verify that a given\ntime-dependent electricity tariff achieves the desired goals even when\nend-users deviate from their expected behaviour. We show feasibility of the\nproposed approach through a realistic scenario from a medium voltage Danish\ndistribution network.", "time": "2021-06-20T13:11:10Z", "link": "http://arxiv.org/abs/2106.10692v1", "id": "2106.10692v1", "title": "Parallel Statistical Model Checking for Safety Verification in Smart\n  Grids"}
{"author": "Wenlu Xuan, Zhongqi Zhao, Lei Fan, Zhu Han", "abstract": "Network function virtualization (NFV) is a crucial technology for the 5G\nnetwork development because it can improve the flexibility of employing\nhardware and reduce the construction of base stations. There are vast service\nchains in NFV to meet users' requests, which are composed of a sequence of\nnetwork functions. These virtual network functions (VNFs) are implemented in\nvirtual machines by software and virtual environment. How to deploy VMs to\nprocess VNFs of the service chains as soon as possible when users' requests are\nreceived is very challenging to solve by traditional algorithms on a large\nscale. Compared with traditional algorithms, quantum computing has better\ncomputational performance because of quantum parallelism. We build an integer\nlinear programming model of the VNF scheduling problem with the objective of\nminimizing delays and transfer it into the quadratic unconstrained binary\noptimization (QUBO) model. Our proposed heuristic algorithm employs a quantum\nannealer to solve the model. Finally, we evaluate the computational results and\nexplore the feasibility of leveraging quantum computing to solve the VNFs\nscheduling problem.", "time": "2021-06-20T14:52:28Z", "link": "http://arxiv.org/abs/2106.10707v1", "id": "2106.10707v1", "title": "Minimizing Delay in Network Function Visualization with Quantum\n  Computing"}
{"author": "Ana Radovanovic, Ross Koningstein, Ian Schneider, Bokan Chen, Alexandre Duarte, Binz Roy, Diyue Xiao, Maya Haridasan, Patrick Hung, Nick Care, Saurav Talukdar, Eric Mullen, Kendal Smith, MariEllen Cottman, Walfredo Cirne", "abstract": "The amount of CO$_2$ emitted per kilowatt-hour on an electricity grid varies\nby time of day and substantially varies by location due to the types of\ngeneration. Networked collections of warehouse scale computers, sometimes\ncalled Hyperscale Computing, emit more carbon than needed if operated without\nregard to these variations in carbon intensity. This paper introduces Google's\nsystem for Carbon-Intelligent Compute Management, which actively minimizes\nelectricity-based carbon footprint and power infrastructure costs by delaying\ntemporally flexible workloads. The core component of the system is a suite of\nanalytical pipelines used to gather the next day's carbon intensity forecasts,\ntrain day-ahead demand prediction models, and use risk-aware optimization to\ngenerate the next day's carbon-aware Virtual Capacity Curves (VCCs) for all\ndatacenter clusters across Google's fleet. VCCs impose hourly limits on\nresources available to temporally flexible workloads while preserving overall\ndaily capacity, enabling all such workloads to complete within a day. Data from\noperation shows that VCCs effectively limit hourly capacity when the grid's\nenergy supply mix is carbon intensive and delay the execution of temporally\nflexible workloads to \"greener\" times.", "time": "2021-06-11T04:17:06Z", "link": "http://arxiv.org/abs/2106.11750v1", "id": "2106.11750v1", "title": "Carbon-Aware Computing for Datacenters"}
{"author": "Meisam Ansari, Mostafa Ansari", "abstract": "The use of distributed generation resources, in addition to considerable\nbenefits, causes some problems in the power system. One of the most critical\nproblems in the case of disruption is increasing short-circuit current level in\ngrids, which leads to change the protection devices settings in the downstream\nand upstream grid. By using fault current limiters (FCL), short-circuit\ncurrents in grids with distributed generation can be reduced to acceptable\nlevels, so there is no needed to change the protection relays settings of the\ndownstream grid (including distributed generations). However, by locating the\nFCL in the tie-feeder, the downstream grid is not more effective than the\nupstream grid and thus its reliability indices also will be changed. Therefore,\nthis paper shows that by locating the unidirectional fault current limiter\n(UFCL) in the tie-feeder, the necessity of changing in the relay protection\nsettings of upstream grids is prevented. In this paper, the proposed method is\nimplemented, and its efficiency is reported in six scenarios.", "time": "2021-06-23T13:50:05Z", "link": "http://arxiv.org/abs/2106.12406v1", "id": "2106.12406v1", "title": "Mitigating the Impact of Distributed Generations on Relay Coordination\n  Using Fault Current Limiters"}
{"author": "Francisco Barreras, Mikhail Hayhoe, Hamed Hassani, Victor M. Preciado", "abstract": "We present an Extended Kalman Filter framework for system identification and\ncontrol of a stochastic high-dimensional epidemic model. The scale and severity\nof the COVID-19 emergency have highlighted the need for accurate forecasts of\nthe state of the pandemic at a high resolution. Mechanistic compartmental\nmodels are widely used to produce such forecasts and assist in the design of\ncontrol and relief policies. Unfortunately, the scale and stochastic nature of\nmany of these models often makes the estimation of their parameters difficult.\nWith the goal of calibrating a high dimensional COVID-19 model using low-level\nmobility data, we introduce a method for tractable maximum likelihood\nestimation that combines tools from Bayesian inference with scalable\noptimization techniques from machine learning. The proposed approach uses\nautomatic backward-differentiation to directly compute the gradient of the\nlikelihood of COVID-19 incidence and death data. The likelihood of the\nobservations is estimated recursively using an Extended Kalman Filter and can\nbe easily optimized using gradient-based methods to compute maximum likelihood\nestimators. Our compartmental model is trained using GPS mobility data that\nmeasures the mobility patterns of millions of mobile phones across the United\nStates. We show that, after calibrating against incidence and deaths data from\nthe city of Philadelphia, our model is able to produce an accurate 30-day\nforecast of the evolution of the pandemic.", "time": "2021-06-28T00:49:07Z", "link": "http://arxiv.org/abs/2106.14357v1", "id": "2106.14357v1", "title": "AutoEKF: Scalable System Identification for COVID-19 Forecasting from\n  Large-Scale GPS Data"}
{"author": "Alessio Netti, Michael Ott, Carla Guillen, Daniele Tafani, Martin Schulz", "abstract": "As HPC systems grow in complexity, efficient and manageable operation is\nincreasingly critical. Many centers are thus starting to explore the use of\nOperational Data Analytics (ODA) techniques, which extract knowledge from\nmassive amounts of monitoring data and use it for control and visualization\npurposes. As ODA is a multi-faceted problem, much effort has gone into\nresearching its separate aspects: however, accounts of production ODA\nexperiences are still hard to come across.\n  In this work we aim to bridge the gap between ODA research and production use\nby presenting our experiences with ODA in production, involving in particular\nthe control of cooling infrastructures and visualization of job data on two HPC\nsystems. We cover the entire development process, from design to deployment,\nhighlighting our insights in an effort to drive the community forward. We rely\non open-source tools, which make for a generic ODA framework suitable for most\nscenarios.", "time": "2021-06-28T07:03:08Z", "link": "http://arxiv.org/abs/2106.14423v1", "id": "2106.14423v1", "title": "Operational Data Analytics in Practice: Experiences from Design to\n  Deployment in Production HPC Environments"}
{"author": "Sebastian Burckhardt, Chris Gillum, David Justo, Konstantinos Kallas, Connor McMahon, Christopher S. Meiklejohn", "abstract": "Serverless is an increasingly popular choice for service architects because\nit can provide elasticity and load-based billing with minimal developer effort.\nA common and important use case is to compose serverless functions and cloud\nstorage into reliable workflows. However, existing solutions for authoring\nworkflows provide a rudimentary experience compared to writing standard code in\na modern programming language. Furthermore, executing workflows reliably in an\nelastic serverless environment poses significant performance challenges.\n  To address these, we propose Durable Functions, a programming model for\nserverless workflows, and Netherite, a distributed execution engine to execute\nthem efficiently. Workflows in Durable Functions are expressed as task-parallel\ncode in a host language of choice. Internally, the workflows are translated to\nfine-grained stateful communicating processes, which are load-balanced over an\nelastic cluster. The main challenge is to minimize the cost of reliably\npersisting progress to storage while supporting elastic scale. Netherite solves\nthis by introducing partitioning, recovery logs, asynchronous snapshots, and\nspeculative communication.\n  Our results show that Durable Functions simplifies the expression of complex\nworkflows, and that Netherite achieves lower latency and higher throughput than\nthe prevailing approaches for serverless workflows in Azure and AWS, by orders\nof magnitude in some cases.", "time": "2021-02-26T19:51:58Z", "link": "http://arxiv.org/abs/2103.00033v1", "id": "2103.00033v1", "title": "Serverless Workflows with Durable Functions and Netherite"}
{"author": "Chao Chen, Greg Eisenhauer, Santosh Pande", "abstract": "Due to the system scaling, transient errors caused by external noises, e.g.,\nheat fluxes and particle strikes, have become a growing concern for the current\nand upcoming extreme-scale high-performance-computing (HPC) systems. However,\nsince such errors are still quite rare as compared to no-fault cases, desirable\nsolutions call for low/no-overhead systems that do not compromise the\nperformance under no-fault conditions and also allow very fast fault recovery\nto minimize downtime. In this paper, we present IterPro, a light-weight\ncompiler-assisted resilience technique to quickly and accurately recover\nprocesses from transient-error-induced crashes. IterPro repairs the corrupted\nprocess states on-the-fly upon occurrences of errors, enabling applications to\ncontinue their executions instead of being terminated. IterPro also exploits\nside effects introduced by induction variable based code optimization\ntechniques to improve its recovery capability. To this end, two new code\ntransformation passes are introduced to expose the side effects for resilience\npurposes. We evaluated IterPro with 4 scientific workloads as well as the NPB\nbenchmarks suite. During their normal execution, IterPro incurs almost zero\nruntime overhead and a small, fixed 27MB memory overhead. Meanwhile, IterPro\ncan recover on an average 83.55% of crash-causing errors within dozens of\nmilliseconds with negligible downtime. With such an effective recovery\nmechanism, IterPro could tremendously mitigate the overheads and resource\nrequirements of the resilience subsystem in future extreme-scale systems.", "time": "2021-03-09T02:32:14Z", "link": "http://arxiv.org/abs/2103.05185v1", "id": "2103.05185v1", "title": "Near-zero Downtime Recovery from Transient-error-induced Crashes"}
{"author": "Qing Yang, Hao Wang", "abstract": "The fast growth of distributed energy resources (DERs), such as distributed\nrenewables (e.g., rooftop PV panels), energy storage systems, electric\nvehicles, and controllable appliances, drives the power system toward a\ndecentralized system with bidirectional power flow. The coordination of DERs\nthrough an aggregator, such as a utility, system operator, or a third-party\ncoordinator, emerges as a promising paradigm. However, it is not well\nunderstood how to enable trust between the aggregator and DERs to integrate\nDERs efficiently. In this paper, we develop a trustable and distributed\ncoordination system for DERs using blockchain technology. We model various DERs\nand formulate a cost minimization problem for DERs to optimize their energy\ntrading, scheduling, and demand response. We use the alternating direction\nmethod of multipliers (ADMM) to solve the problem in a distributed fashion. To\nimplement the distributed algorithm in a trustable way, we design a smart\ncontract to update multipliers and communicate with DERs in a blockchain\nnetwork. We validate our design by experiments using real-world data, and the\nsimulation results demonstrate the effectiveness of our algorithm.", "time": "2021-03-10T13:32:33Z", "link": "http://arxiv.org/abs/2103.06046v1", "id": "2103.06046v1", "title": "Exploring Blockchain for The Coordination of Distributed Energy\n  Resources"}
{"author": "Chalee Boonprasop, Yuhui Lin, Adam Barker", "abstract": "Cloud providers offer end-users various pricing schemes to allow them to\ntailor VMs to their needs, e.g., a pay-as-you-go billing scheme, called\n\\textit{on-demand}, and a discounted contract scheme, called \\textit{reserved\ninstances}. This paper presents a cloud broker which offers users both the\nflexibility of on-demand instances and some level of discounts found in\nreserved instances. The broker employs a buy-low-and-sell-high strategy that\nplaces user requests into a resource pool of pre-purchased discounted cloud\nresources. By analysing user request time-series data, the broker takes a\nrisk-oriented approach to dynamically adjust the resource pool.\n  This approach does not require a training process which is useful at\nprocessing the large data stream. The broker is evaluated with high-frequency\nreal cloud datasets from Alibaba. The results show that the overall profit of\nthe broker is close to the theoretical optimal scenario where user requests can\nbe perfectly predicted.", "time": "2021-03-12T08:11:19Z", "link": "http://arxiv.org/abs/2103.07133v1", "id": "2103.07133v1", "title": "A Risk-taking Broker Model to Optimise User Requests placement on\n  On-demand and Contract VMs"}
{"author": "Michael Kuperberg", "abstract": "Railway operations require control systems to ensure safety and efficiency,\nand to coordinate infrastructure elements such as switches, signals and train\nprotection. To compete with the traditional approaches to these systems, a\nblockchain-based approach has been proposed, with the intent to build a more\nresilient, integrated and cost-efficient system. Additionally, the developed\nblockchain-based architecture enables to run safety-relevant and\nsecurity-focused business logic on off-the-shelf platforms such as cloud,\nrather than on specialized (and expensive) secure hardware. After implementing\na prototype of the blockchain-based railway control system, scaling the\napproach to real-world mainline and branch operations required a thorough\nvalidation of the design choices. In this technical report, we show how\nperformance calculations, long-term technology perspectives and law-mandated\nnorms have impacted the architecture, the technology choices, and the\nmake-buy-reuse decisions.", "time": "2021-03-11T12:33:35Z", "link": "http://arxiv.org/abs/2103.08304v1", "id": "2103.08304v1", "title": "Scaling a Blockchain-based Railway Control System Prototype for Mainline\n  Railways: a Progress Report"}
{"author": "Shih-Chun Lin, Kwang-Cheng Chen, Ali Karimoddini", "abstract": "New paradigm shifts and 6G technological revolution in vehicular services\nhave emerged toward unmanned driving, automated transportation, and\nself-driving vehicles. As the technology for autonomous vehicles becomes\nmature, real challenges come from reliable, safe, real-time connected\ntransportation operations to achieve ubiquitous and prompt information\nexchanges with massive connected and autonomous vehicles. This article aims at\nintroducing novel wireless distributed architectures that embed the edge\ncomputing capability inside software-defined vehicular networking\ninfrastructure. Such edge networks consist of open-loop grant-free\ncommunications and computing-based control frameworks, which enable dynamic\neco-routing with ultra-low latency and mobile data-driven orchestration. Thus,\nthis work advances the frontiers of machine learning potentials and\nnext-generation mobile system realization in vehicular networking applications.", "time": "2021-03-26T02:25:42Z", "link": "http://arxiv.org/abs/2103.14225v1", "id": "2103.14225v1", "title": "SD-VEC: Software-Defined Vehicular Edge Computing with Ultra-Low Latency"}
{"author": "Carmen Amo Alonso, Shih-Hao Tseng", "abstract": "To effectively control large-scale distributed systems online, model\npredictive control (MPC) has to swiftly solve the underlying high-dimensional\noptimization. There are multiple techniques applied to accelerate the solving\nprocess in the literature, mainly attributed to software-based algorithmic\nadvancements and hardware-assisted computation enhancements. However, those\nmethods focus on arithmetic accelerations and overlook the benefits of the\nunderlying system's structure. In particular, the existing decoupled\nsoftware-hardware algorithm design that naively parallelizes the arithmetic\noperations by the hardware does not tackle the hardware overheads such as\nCPU-GPU and thread-to-thread communications in a principled manner. Also, the\nadvantages of parallelizable subproblem decomposition in distributed MPC are\nnot well recognized and exploited. As a result, we have not reached the full\npotential of hardware acceleration for MPC. In this paper, we explore those\nopportunities by leveraging GPU to parallelize the distributed and localized\nMPC (DLMPC) algorithm. We exploit the locality constraints embedded in the\nDLMPC formulation to reduce the hardware-intrinsic communication overheads. Our\nparallel implementation achieves up to 50x faster runtime than its CPU\ncounterparts under various parameters. Furthermore, we find that the\nlocality-aware GPU parallelization could halve the optimization runtime\ncomparing to the naive acceleration. Overall, our results demonstrate the\nperformance gains brought by software-hardware co-design with the information\nexchange structure in mind.", "time": "2021-03-27T20:34:55Z", "link": "http://arxiv.org/abs/2103.14990v1", "id": "2103.14990v1", "title": "Effective GPU Parallelization of Distributed and Localized Model\n  Predictive Control"}
{"author": "Simo SÃ¤rkkÃ¤, Ãngel F. GarcÃ­a-FernÃ¡ndez", "abstract": "This paper proposes a general formulation for temporal parallelisation of\ndynamic programming for optimal control problems. We derive the elements and\nassociative operators to be able to use parallel scans to solve these problems\nwith logarithmic time complexity rather than linear time complexity. We apply\nthis methodology to problems with finite state and control spaces, linear\nquadratic tracking control problems, and to a class of nonlinear control\nproblems. The computational benefits of the parallel methods are demonstrated\nvia numerical simulations run on a graphics processing unit.", "time": "2022-01-24T10:13:24Z", "link": "http://arxiv.org/abs/2104.03186v2", "id": "2104.03186v2", "title": "Temporal Parallelisation of Dynamic Programming and Linear Quadratic\n  Control"}
{"author": "Tudor Cioara, Ionut Anghel, Marcel Antal, Ioan Salomie, Claudia Antal, Arcas Gabriel Ioan", "abstract": "The Digital Twins offer promising solutions for smart grid challenges related\nto the optimal operation, management, and control of energy assets, for safe\nand reliable distribution of energy. These challenges are more pressing\nnowadays than ever due to the large-scale adoption of distributed renewable\nresources at the edge of the grid. The digital twins are leveraging\ntechnologies such as the Internet of Things, big data analytics, machine\nlearning, and cloud computing, to analyze data from different energy sensors,\nview and verify the status of physical energy assets and extract useful\ninformation to predict and optimize the assets performance. In this paper, we\nwill provide an overview of the Digital Twins application domains in the smart\ngrid while analyzing existing the state of the art literature. We have focused\non the following application domains: energy asset modeling, fault and security\ndiagnosis, operational optimization, and business models. Most of the relevant\nliterature approaches found are published in the last three years showing that\nthe domain of Digital Twins application in smart grid is hot and gradually\ndeveloping. Anyway, there is no unified view on the Digital Twins\nimplementation and integration with energy management processes, thus, much\nwork still needs to be done to understand and automatize the smart grid\nmanagement.", "time": "2021-04-16T06:03:05Z", "link": "http://arxiv.org/abs/2104.07904v1", "id": "2104.07904v1", "title": "An Overview of Digital Twins Application Domains in Smart Energy Grid"}
{"author": "Sayed Mehdi Meshkani, Bilal Farooq", "abstract": "In this study, we propose a novel heuristic two-step algorithm for shared\nridehailing in which users can share their rides with only one more user. The\nalgorithm, which is centrally formulated, starts with matching users and\ncreating a set of passenger pairs in step 1 and is followed by solving an\nassignment problem to assign passenger pairs to the vehicles. To solve the\nproblem of high computational time in dynamic ride-matching problems, we\npropose a distributed system that is based on vehicle to infrastructure (V2I)\nand infrastructure to infrastructure (I2I) communication. To evaluate the\ndistributed system's performance, we compare it with the proposed centralized\nridehailing algorithm. Both centralized and distributed systems are implemented\nin a micro-traffic simulator to assess their performance and their impact on\ntraffic congestion. Downtown Toronto road network was chosen as the study area.\nBased on our obtained results, the service rate of the distributed system was\n91.59% which is close to 95.80% in the centralized system. However, the\ndistributed system yielded much lower computational time compared to\ncentralized. Furthermore, the scalability of the distributed system was shown\nby testing it on a small network and comparing with the entire network.", "time": "2021-04-17T18:40:45Z", "link": "http://arxiv.org/abs/2104.10022v1", "id": "2104.10022v1", "title": "A Decentralized Shared CAV System Design and Application"}
{"author": "Qing Yang, Hao Wang, Taotao Wang, Shengli Zhang, Xiaoxiao Wu, Hui Wang", "abstract": "The advent of distributed energy resources (DERs), such as distributed\nrenewables, energy storage, electric vehicles, and controllable loads,\n\\rv{brings} a significantly disruptive and transformational impact on the\ncentralized power system. It is widely accepted that a paradigm shift to a\ndecentralized power system with bidirectional power flow is necessary to the\nintegration of DERs. The virtual power plant (VPP) emerges as a promising\nparadigm for managing DERs to participate in the power system. In this paper,\nwe develop a blockchain-based VPP energy management platform to facilitate a\nrich set of transactive energy activities among residential users with\nrenewables, energy storage, and flexible loads in a VPP. Specifically, users\ncan interact with each other to trade energy for mutual benefits and provide\nnetwork services, such as feed-in energy, reserve, and demand response, through\nthe VPP. To respect the users' independence and preserve their privacy, we\ndesign a decentralized optimization algorithm to optimize the users' energy\nscheduling, energy trading, and network services. Then we develop a prototype\nblockchain network for VPP energy management and implement the proposed\nalgorithm on the blockchain network. By experiments using real-world\ndata-trace, we validated the feasibility and effectiveness of our algorithm and\nthe blockchain system. The simulation results demonstrate that our\nblockchain-based VPP energy management platform reduces the users' cost by up\nto 38.6% and reduces the overall system cost by 11.2%.", "time": "2021-05-31T06:52:05Z", "link": "http://arxiv.org/abs/2105.00174v2", "id": "2105.00174v2", "title": "Blockchain-Based Decentralized Energy Management Platform for\n  Residential Distributed Energy Resources in A Virtual Power Plant"}
{"author": "Aryan Deshwal, Syrine Belakaria, Ganapati Bhat, Janardhan Rao Doppa, Partha Pratim Pande", "abstract": "Mobile system-on-chips (SoCs) are growing in their complexity and\nheterogeneity (e.g., Arm's Big-Little architecture) to meet the needs of\nemerging applications, including games and artificial intelligence. This makes\nit very challenging to optimally manage the resources (e.g., controlling the\nnumber and frequency of different types of cores) at runtime to meet the\ndesired trade-offs among multiple objectives such as performance and energy.\nThis paper proposes a novel information-theoretic framework referred to as\nPaRMIS to create Pareto-optimal resource management policies for given target\napplications and design objectives. PaRMIS specifies parametric policies to\nmanage resources and learns statistical models from candidate policy evaluation\ndata in the form of target design objective values. The key idea is to select a\ncandidate policy for evaluation in each iteration guided by statistical models\nthat maximize the information gain about the true Pareto front. Experiments on\na commercial heterogeneous SoC show that PaRMIS achieves better Pareto fronts\nand is easily usable to optimize complex objectives (e.g., performance per\nWatt) when compared to prior methods.", "time": "2021-04-14T05:14:52Z", "link": "http://arxiv.org/abs/2105.09282v1", "id": "2105.09282v1", "title": "Learning Pareto-Frontier Resource Management Policies for Heterogeneous\n  SoCs: An Information-Theoretic Approach"}
{"author": "Houssam-Eddine Zahaf, Ignacio Sanudo Olmedo, Jayati Singh, Nicola Capodieci, Sebastien Faucou", "abstract": "In order to satisfy timing constraints, modern real-time applications require\nmassively parallel accelerators such as General Purpose Graphic Processing\nUnits (GPGPUs). Generation after generation, the number of computing clusters\nmade available in novel GPU architectures is steadily increasing, hence,\ninvestigating suitable scheduling approaches is now mandatory. Such scheduling\napproaches are related to mapping different and concurrent compute kernels\nwithin the GPU computing clusters, hence grouping GPU computing clusters into\nschedulable partitions. In this paper we propose novel techniques to define GPU\npartitions; this allows us to define suitable task-to-partition allocation\nmechanisms in which tasks are GPU compute kernels featuring different timing\nrequirements. Such mechanisms will take into account the interference that GPU\nkernels experience when running in overlapping time windows. Hence, an\neffective and simple way to quantify the magnitude of such interference is also\npresented. We demonstrate the efficiency of the proposed approaches against the\nclassical techniques that considered the GPU as a single, non-partitionable\nresource.", "time": "2021-05-21T12:28:55Z", "link": "http://arxiv.org/abs/2105.10312v1", "id": "2105.10312v1", "title": "Contention-Aware GPU Partitioning and Task-to-Partition Allocation for\n  Real-Time Workloads"}
{"author": "Davide Callegaro, Marco Levorato, Francesco Restuccia", "abstract": "Edge computing enables Mobile Autonomous Systems (MASs) to execute continuous\nstreams of heavy-duty mission-critical processing tasks, such as real-time\nobstacle detection and navigation. However, in practical applications, erratic\npatterns in channel quality, network load, and edge server load can interrupt\nthe task flow execution, which necessarily leads to severe disruption of the\nsystem's key operations. Existing work has mostly tackled the problem with\nreactive approaches, which cannot guarantee task-level reliability. Conversely,\nin this paper we focus on learning-based predictive edge computing to achieve\nself-resilient task offloading. By conducting a preliminary experimental\nevaluation, we show that there is no dominant feature that can predict the\nedge-MAS system reliability, which calls for an ensemble and selection of\nweaker features. To tackle the complexity of the problem, we propose SeReMAS, a\ndata-driven optimization framework. We first mathematically formulate a\nRedundant Task Offloading Problem (RTOP), where a MAS may connect to multiple\nedge servers for redundancy, and needs to select which server(s) to transmit\nits computing tasks in order to maximize the probability of task execution\nwhile minimizing channel and edge resource utilization. We then create a\npredictor based on Deep Reinforcement Learning (DRL), which produces the\noptimum task assignment based on application-, network- and telemetry-based\nfeatures. We prototype SeReMAS on a testbed composed by a drone, mounting a\nPixHawk flight controller, a Jetson Nano board, and three 802.11n WiFi\ninterfaces. We extensively evaluate SeReMAS by considering an application where\none drone offloads high-resolution images for real-time analysis to three edge\nservers on the ground. Experimental results show that SeReMAS improves task\nexecution probability by $17\\%$ with respect to existing reactive-based\napproaches.", "time": "2021-07-11T16:59:05Z", "link": "http://arxiv.org/abs/2105.15105v2", "id": "2105.15105v2", "title": "SeReMAS: Self-Resilient Mobile Autonomous Systems Through Predictive\n  Edge Computing"}
{"author": "Houssam-Eddine Zahaf, Nicola Capodieci", "abstract": "Hard real-time systems like image processing, autonomous driving, etc.\nrequire an increasing need of computational power that classical multi-core\nplatforms can not provide, to fulfill with their timing constraints.\nHeterogeneous Instruction Set Architecture (ISA) platforms allow accelerating\nreal-time workloads on application-specific cores (e.g. GPU, DSP, ASICs) etc.\nand are suitable for these applications. In addition, these platforms provide\nlarger design choices as a given functionnality can be implemented onto several\ntypes of compute elements. HPC-DAG (Heterogeneous Parallel Directed Acyclic\nGraph) task model has been recently proposed to capture real-time workload\nexecution on heterogeneous platforms. It expresses the ISA heterogeneity, and\nsome specific characteristics of hardware accelerators, as the absence of\npreemption or costly preemption, alternative implementations and on-line\nconditional execution. In this paper, we propose a time-table scheduling\napproach to allocate and schedule a set of HPC-DAG tasks onto a set of\nheterogeneous cores, by the mean Integer Linear Programming (ILP). Our design\nallows to handle heterogeniety of resources, on-line execution costs, and a\nfaster solving time, by exploring gradually the design space", "time": "2021-08-31T14:28:34Z", "link": "http://arxiv.org/abs/2108.13871v1", "id": "2108.13871v1", "title": "Building Time-Triggered Schedules for typed-DAG Tasks with alternative\n  implementations"}
{"author": "Liang Hu, Jiangcheng Zhu, Zirui Zhou, Ruiqing Cheng, Xiaolong Bai, Yong Zhang", "abstract": "Cloud training platforms, such as Amazon Web Services and Huawei Cloud\nprovide users with computational resources to train their deep learning jobs.\nElastic training is a service embedded in cloud training platforms that\ndynamically scales up or down the resources allocated to a job. The core\ntechnique of an elastic training system is to best allocate limited resources\namong heterogeneous jobs in terms of shorter queueing delay and higher training\nefficiency. This paper presents an optimal resource allocator for elastic\ntraining system that leverages a mixed-integer programming (MIP) model to\nmaximize the training progress of deep learning jobs. We take advantage of the\nreal-world job data obtained from ModelArts, the deep learning training\nplatform of Huawei Cloud and conduct simulation experiments to compare the\noptimal resource allocator with a greedy one as benchmark. Numerical results\nshow that the proposed allocator can reduce queuing time by up to 32% and\naccelerate training efficiency by up to 24% relative to the greedy resource\nallocator, thereby greatly improving user experience with Huawei ModelArts and\npotentially enabling the realization of higher profits for the product. Also,\nthe optimal resource allocator is fast in decision-making, taking merely 0.4\nseconds on average.", "time": "2021-09-08T01:38:09Z", "link": "http://arxiv.org/abs/2109.03389v1", "id": "2109.03389v1", "title": "An Optimal Resource Allocator of Elastic Training for Deep Learning Jobs\n  on Cloud"}
{"author": "Sanjay Lall, Calin Cascaval, Martin Izzard, Tammo Spalink", "abstract": "Distributed system applications rely on a fine-grain common sense of time.\nExisting systems maintain the common sense of time by keeping each independent\nmachine as close as possible to wall-clock time through a combination of\nsoftware protocols like NTP and GPS signals and/or precision references like\natomic clocks. This approach is expensive and has tolerance limitations that\nrequire protocols to deal with asynchrony and its performance consequences.\nMoreover, at data-center scale it is impractical to distribute a physical clock\nas is done on a chip or printed circuit board. In this paper we introduce a\ndistributed system design that removes the need for physical clock distribution\nor mechanisms for maintaining close alignment to wall-clock time, and instead\nprovides applications with a perfectly synchronized logical clock. We discuss\nthe abstract frame model (AFM), a mathematical model that underpins the system\nsynchronization. The model is based on the rate of communication between nodes\nin a topology without requiring a global clock. We show that there are families\nof controllers that satisfy the properties required for existence and\nuniqueness of solutions to the AFM, and give examples.", "time": "2022-03-31T20:06:10Z", "link": "http://arxiv.org/abs/2109.14111v2", "id": "2109.14111v2", "title": "Modeling and Control of bittide Synchronization"}
{"author": "Premathas Somasekaram, Radu Calinescu, Rajkumar Buyya", "abstract": "The delivery of key services in domains ranging from finance and\nmanufacturing to healthcare and transportation is underpinned by a rapidly\ngrowing number of mission-critical enterprise applications. Ensuring the\ncontinuity of these complex applications requires the use of software-managed\ninfrastructures called high-availability clusters (HACs). HACs employ\nsophisticated techniques to monitor the health of key enterprise application\nlayers and of the resources they use, and to seamlessly restart or relocate\napplication components after failures. In this paper, we first describe the\nmanifold uses of HACs to protect essential layers of a critical application and\npresent the architecture of high availability clusters. We then propose a\ntaxonomy that covers all key aspects of HACs -- deployment patterns,\napplication areas, types of cluster, topology, cluster management, failure\ndetection and recovery, consistency and integrity, and data synchronisation;\nand we use this taxonomy to provide a comprehensive survey of the end-to-end\nsoftware solutions available for the HAC deployment of enterprise applications.\nFinally, we discuss the limitations and challenges of existing HAC solutions,\nand we identify opportunities for future research in the area.", "time": "2022-09-22T00:28:39Z", "link": "http://arxiv.org/abs/2109.15139v3", "id": "2109.15139v3", "title": "High-Availability Clusters: A Taxonomy, Survey, and Future Directions"}
{"author": "Sanjay Lall, Calin Cascaval, Martin Izzard, Tammo Spalink", "abstract": "We discuss control of bittide distributed systems, which are designed to\nprovide logical synchronization between networked machines by observing data\nflow rates between adjacent systems at the physical network layer and\ncontrolling local reference clock frequencies. We analyze the performance of\napproximate proportional-integral control of the synchronization mechanism and\ndevelop a simple continuous-time model to show the resulting dynamics are\nstable for any positive choice of gains. We then construct explicit formulae to\nshow that closed-loop performance measured using the L2 norm is a product of\ntwo terms, one depending only on resistance distances in the graph, and the\nother depending only on controller gains.", "time": "2022-03-31T19:49:15Z", "link": "http://arxiv.org/abs/2111.05296v2", "id": "2111.05296v2", "title": "Resistance Distance and Control Performance for bittide Synchronization"}
{"author": "Yang Xiao, Shanghao Shi, Wenjing Lou, Chonggang Wang, Xu Li, Ning Zhang, Y. Thomas Hou, Jeffrey H. Reed", "abstract": "Spectrum access system (SAS) is widely considered the de facto solution to\ncoordinating dynamic spectrum sharing (DSS) and protecting incumbent users. The\ncurrent SAS paradigm prescribed by the FCC for the CBRS band and standardized\nby the WInnForum follows a centralized service model in that a spectrum user\nsubscribes to a SAS server for spectrum allocation service. This model,\nhowever, neither tolerates SAS server failures (crash or Byzantine) nor resists\ndishonest SAS administrators, leading to serious concerns on SAS system\nreliability and trustworthiness. This is especially concerning for the evolving\nDSS landscape where an increasing number of SAS service providers and\nheterogeneous user requirements are coming up. To address these challenges, we\npropose a novel blockchain-based decentralized SAS architecture called BD-SAS\nthat provides SAS services securely and efficiently, without relying on the\ntrust of each individual SAS server for the overall system trustworthiness. In\nBD-SAS, a global blockchain (G-Chain) is used for spectrum regulatory\ncompliance while smart contract-enabled local blockchains (L-Chains) are\ninstantiated in individual spectrum zones for automating spectrum access\nassignment per user request. We hope our vision of a decentralized SAS, the\nBD-SAS architecture, and discussion on future challenges can open up a new\ndirection towards reliable spectrum management in a decentralized manner.", "time": "2021-12-10T15:47:54Z", "link": "http://arxiv.org/abs/2112.05612v1", "id": "2112.05612v1", "title": "Decentralized Spectrum Access System: Vision, Challenges, and a\n  Blockchain Solution"}
{"author": "Nimish Shah, Laura Isabel Galindez Olascoaga, Shirui Zhao, Wannes Meert, Marian Verhelst", "abstract": "Computation in several real-world applications like probabilistic machine\nlearning, sparse linear algebra, and robotic navigation, can be modeled as\nirregular directed acyclic graphs (DAGs). The irregular data dependencies in\nDAGs pose challenges to parallel execution on general-purpose CPUs and GPUs,\nresulting in severe under-utilization of the hardware. This paper proposes DPU,\na specialized processor designed for the efficient execution of irregular DAGs.\nThe DPU is equipped with parallel compute units that execute different\nsubgraphs of a DAG independently. The compute units can synchronize within a\ncycle using a hardware-supported synchronization primitive, and communicate via\nan efficient interconnect to a global banked scratchpad. Furthermore, a\nprecision-scalable posit arithmetic unit is developed to enable\napplication-dependent precision. The DPU is taped-out in 28nm CMOS, achieving a\nspeedup of 5.1$\\times$ and 20.6$\\times$ over state-of-the-art CPU and GPU\nimplementations on DAGs of sparse linear algebra and probabilistic machine\nlearning workloads. This performance is achieved while operating at a power\nbudget of 0.23W, as opposed to 55W and 98W of the CPU and GPU, resulting in a\npeak efficiency of 538 GOPS/W with DPU, which is 1350$\\times$ and 9000$\\times$\nhigher than the CPU and GPU, respectively. Thus, with specialized architecture,\nDPU enables low-power execution of irregular DAG workloads.", "time": "2021-12-10T16:44:47Z", "link": "http://arxiv.org/abs/2112.05660v1", "id": "2112.05660v1", "title": "DPU: DAG Processing Unit for Irregular Graphs with Precision-Scalable\n  Posit Arithmetic in 28nm"}
{"author": "Sakshi Mishra, Roohallah Khatami, Yu Christine Chen", "abstract": "We present a hierarchical framework aimed at decentralizing the distribution\nsystems market operations using localized peer-to-peer energy markets.\nHierarchically designed decision-making algorithm approaches the power systems\nmarket operations from a bottom-up perspective. The three layers of the\nhierarchical framework operate in orchestration to enable prosumers (the\ngrass-root actors) to maximize their revenues - hence, a prosumer-centric\nframework. The design of the framework incorporates existing smart grid\ntechnologies (Virtual Power Plants, Microgrids, Distributed Energy Resources)\nand redefine their functional objectives to align them with the\ndecentralization paradigm focused on empowering bottom-up grid operations\napproach. On one hand, the framework is enabling prosumers with simultaneous\naccess to the buy-sell choices that help them maximize their cost savings while\nensuring their consumption patterns and preferences are not being tradeoff as a\nresult of top-down operational decisions. On the other hand, it is designed to\noperate in harmony with the existing top-down grid operations mechanisms -\nthereby reducing the potential friction in its adaptation. This marriage of the\ntop-down and bottom-up operational approaches is facilitated through meticulous\norchestration of operational timescales. Framework's novel design also\nincorporates scalability and interoperability considerations, thereby tackling\nthe challenge of decentralization holistically.", "time": "2023-11-17T20:12:20Z", "link": "http://arxiv.org/abs/2112.09756v2", "id": "2112.09756v2", "title": "Towards Harmonious Decentralization of Energy Systems: A Vision of\n  Interoperable Peer-to-Peer Energy Markets"}
{"author": "Runze Gao, Yuanqing Xia, Guan Wang, Liwen Yang, Yufeng Zhan", "abstract": "Subspace identification (SID) has been widely used in system identification\nand control fields since it can estimate system models only relying on the\ninput and output data by reliable numerical operations such as singular value\ndecomposition (SVD). However, high-dimension Hankel matrices are involved to\nstore these data and used to obtain the system models, which increases the\ncomputation amount of SID and leads SID not suitable for the large-scale or\nreal-time identification tasks. In this paper, a novel fast SID method based on\ncloud workflow processing and container technology is proposed to accelerate\nthe traditional algorithm. First, a workflow-based structure of SID is designed\nto match the distributed cloud environment, based on the computational feature\nof each calculation stage. Second, a containerised cloud workflow processing\nsystem is established to execute the logic- and data- dependent SID workflow\nmission based on Kubernetes system. Finally, the experiments show that the\ncomputation time is reduced by at most $91.6\\%$ for large-scale SID mission and\ndecreased to within 20 ms for the real-time mission parameter.", "time": "2021-12-29T01:06:13Z", "link": "http://arxiv.org/abs/2112.14349v1", "id": "2112.14349v1", "title": "Fast Subspace Identification Method Based on Containerised Cloud\n  Workflow Processing System"}
{"author": "Vighnesh Sachidananda, Anirudh Sivaraman", "abstract": "As cloud applications shift from monoliths to loosely coupled microservices,\napplication developers must decide how many compute resources (e.g., number of\nreplicated containers) to assign to each microservice within an application.\nThis decision affects both (1) the dollar cost to the application developer and\n(2) the end-to-end latency perceived by the application user. Today, individual\nmicroservices are autoscaled independently by adding VMs whenever\nper-microservice CPU or memory utilization crosses a configurable threshold.\nHowever, an application user's end-to-end latency consists of time spent on\nmultiple microservices and each microservice might need a different number of\nVMs to achieve an overall end-to-end latency.\n  We present COLA, an autoscaler for microservice-based applications, which\ncollectively allocates VMs to microservices with a global goal of minimizing\ndollar cost while keeping end-to-end application latency under a given target.\nUsing 5 open-source applications, we compared COLA to several utilization and\nmachine learning based autoscalers. We evaluate COLA across different compute\nsettings on Google Kubernetes Engine (GKE) in which users manage compute\nresources, GKE standard, and a new mode of operation in which the cloud\nprovider manages compute infrastructure, GKE Autopilot. COLA meets a desired\nmedian or tail latency target on 53 of 63 workloads where it provides a cost\nreduction of 19.3%, on average, over the next cheapest autoscaler. COLA is the\nmost cost effective autoscaling policy for 48 of these 53 workloads. The cost\nsavings from managing a cluster with COLA result in COLA paying for its\ntraining cost in a few days. On smaller applications, for which we can\nexhaustively search microservice configurations, we find that COLA is optimal\nfor 90% of cases and near optimal otherwise.", "time": "2022-08-07T23:20:06Z", "link": "http://arxiv.org/abs/2112.14845v3", "id": "2112.14845v3", "title": "Collective Autoscaling for Cloud Microservices"}
{"author": "Stefano Savazzi, Monica Nicoli, Mehdi Bennis, Sanaz Kianoush, Luca Barbieri", "abstract": "Next-generation autonomous and networked industrial systems (i.e., robots,\nvehicles, drones) have driven advances in ultra-reliable, low latency\ncommunications (URLLC) and computing. These networked multi-agent systems\nrequire fast, communication-efficient and distributed machine learning (ML) to\nprovide mission critical control functionalities. Distributed ML techniques,\nincluding federated learning (FL), represent a mushrooming multidisciplinary\nresearch area weaving in sensing, communication and learning. FL enables\ncontinual model training in distributed wireless systems: rather than fusing\nraw data samples at a centralized server, FL leverages a cooperative fusion\napproach where networked agents, connected via URLLC, act as distributed\nlearners that periodically exchange their locally trained model parameters.\nThis article explores emerging opportunities of FL for the next-generation\nnetworked industrial systems. Open problems are discussed, focusing on\ncooperative driving in connected automated vehicles and collaborative robotics\nin smart manufacturing.", "time": "2021-01-12T22:42:24Z", "link": "http://arxiv.org/abs/2101.03367v2", "id": "2101.03367v2", "title": "Opportunities of Federated Learning in Connected, Cooperative and\n  Automated Industrial Systems"}
{"author": "Priyanka Mary Mammen", "abstract": "Federated Learning (FL) is a concept first introduced by Google in 2016, in\nwhich multiple devices collaboratively learn a machine learning model without\nsharing their private data under the supervision of a central server. This\noffers ample opportunities in critical domains such as healthcare, finance etc,\nwhere it is risky to share private user information to other organisations or\ndevices. While FL appears to be a promising Machine Learning (ML) technique to\nkeep the local data private, it is also vulnerable to attacks like other ML\nmodels. Given the growing interest in the FL domain, this report discusses the\nopportunities and challenges in federated learning.", "time": "2021-01-14T02:44:28Z", "link": "http://arxiv.org/abs/2101.05428v1", "id": "2101.05428v1", "title": "Federated Learning: Opportunities and Challenges"}
{"author": "Akanksha Atrey, Prashant Shenoy, David Jensen", "abstract": "The ubiquity of mobile devices has led to the proliferation of mobile\nservices that provide personalized and context-aware content to their users.\nModern mobile services are distributed between end-devices, such as\nsmartphones, and remote servers that reside in the cloud. Such services thrive\non their ability to predict future contexts to pre-fetch content or make\ncontext-specific recommendations. An increasingly common method to predict\nfuture contexts, such as location, is via machine learning (ML) models. Recent\nwork in context prediction has focused on ML model personalization where a\npersonalized model is learned for each individual user in order to tailor\npredictions or recommendations to a user's mobile behavior. While the use of\npersonalized models increases efficacy of the mobile service, we argue that it\nincreases privacy risk since a personalized model encodes contextual behavior\nunique to each user. To demonstrate these privacy risks, we present several\nattribute inference-based privacy attacks and show that such attacks can leak\nprivacy with up to 78% efficacy for top-3 predictions. We present Pelican, a\nprivacy-preserving personalization system for context-aware mobile services\nthat leverages both device and cloud resources to personalize ML models while\nminimizing the risk of privacy leakage for users. We evaluate Pelican using\nreal world traces for location-aware mobile services and show that Pelican can\nsubstantially reduce privacy leakage by up to 75%.", "time": "2021-04-21T23:18:05Z", "link": "http://arxiv.org/abs/2101.05855v2", "id": "2101.05855v2", "title": "Preserving Privacy in Personalized Models for Distributed Mobile\n  Services"}
{"author": "Atousa Zarindast, Anuj Sharma", "abstract": "With the era of big data, an explosive amount of information is now\navailable. This enormous increase of Big Data in both academia and industry\nrequires large-scale data processing systems. A large body of research is\nbehind optimizing Spark's performance to make it state of the art, a fast and\ngeneral data processing system. Many science and engineering fields have\nadvanced with Big Data analytics, such as Biology, finance, and transportation.\nIntelligent transportation systems (ITS) gain popularity and direct benefit\nfrom the richness of information. The objective is to improve the safety and\nmanagement of transportation networks by reducing congestion and incidents. The\nfirst step toward the goal is better understanding, modeling, and detecting\ncongestion across a network efficiently and effectively. In this study, we\nintroduce an efficient congestion detection model. The underlying network\nconsists of 3017 segments in I-35, I-80, I-29, and I-380 freeways with an\noverall length of 1570 miles and averaged (0.4-0.6) miles per segment. The\nresult of congestion detection shows the proposed method is 90% accurate while\nhas reduced computation time by 99.88%.", "time": "2021-01-16T21:26:11Z", "link": "http://arxiv.org/abs/2101.06524v1", "id": "2101.06524v1", "title": "Big Data application in congestion detection and classification using\n  Apache spark"}
{"author": "Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He", "abstract": "Large-scale model training has been a playing ground for a limited few\nrequiring complex model refactoring and access to prohibitively expensive GPU\nclusters. ZeRO-Offload changes the large model training landscape by making\nlarge model training accessible to nearly everyone. It can train models with\nover 13 billion parameters on a single GPU, a 10x increase in size compared to\npopular framework such as PyTorch, and it does so without requiring any model\nchange from the data scientists or sacrificing computational efficiency.\nZeRO-Offload enables large model training by offloading data and compute to\nCPU. To preserve compute efficiency, it is designed to minimize the data\nmovement to/from GPU, and reduce CPU compute time while maximizing memory\nsavings on GPU. As a result, ZeRO-Offload can achieve 40 TFlops/GPU on a single\nNVIDIA V100 GPU for 10B parameter model compared to 30TF using PyTorch alone\nfor a 1.4B parameter model, the largest that can be trained without running out\nof memory. ZeRO-Offload is also designed to scale on multiple-GPUs when\navailable, offering near linear speedup on up to 128 GPUs. Additionally, it can\nwork together with model parallelism to train models with over 70 billion\nparameters on a single DGX-2 box, a 4.5x increase in model size compared to\nusing model parallelism alone. By combining compute and memory efficiency with\nease-of-use, ZeRO-Offload democratizes large-scale model training making it\naccessible to even data scientists with access to just a single GPU.", "time": "2021-01-18T02:11:25Z", "link": "http://arxiv.org/abs/2101.06840v1", "id": "2101.06840v1", "title": "ZeRO-Offload: Democratizing Billion-Scale Model Training"}
{"author": "Jun Li, Yumeng Shao, Kang Wei, Ming Ding, Chuan Ma, Long Shi, Zhu Han, H. Vincent Poor", "abstract": "Federated learning (FL), as a distributed machine learning paradigm, promotes\npersonal privacy by local data processing at each client. However, relying on a\ncentralized server for model aggregation, standard FL is vulnerable to server\nmalfunctions, untrustworthy server, and external attacks. To address this\nissue, we propose a decentralized FL framework by integrating blockchain into\nFL, namely, blockchain assisted decentralized federated learning (BLADE-FL). In\na round of the proposed BLADE-FL, each client broadcasts the trained model to\nother clients, aggregates its own model with received ones, and then competes\nto generate a block before its local training of the next round. We evaluate\nthe learning performance of BLADE-FL, and develop an upper bound on the global\nloss function. Then we verify that this bound is convex with respect to the\nnumber of overall aggregation rounds K, and optimize the computing resource\nallocation for minimizing the upper bound. We also note that there is a\ncritical problem of training deficiency, caused by lazy clients who plagiarize\nothers' trained models and add artificial noises to disguise their cheating\nbehaviors. Focusing on this problem, we explore the impact of lazy clients on\nthe learning performance of BLADE-FL, and characterize the relationship among\nthe optimal K, the learning parameters, and the proportion of lazy clients.\nBased on MNIST and Fashion-MNIST datasets, we show that the experimental\nresults are consistent with the analytical ones. To be specific, the gap\nbetween the developed upper bound and experimental results is lower than 5%,\nand the optimized K based on the upper bound can effectively minimize the loss\nfunction.", "time": "2021-05-25T13:41:18Z", "link": "http://arxiv.org/abs/2101.06905v2", "id": "2101.06905v2", "title": "Blockchain Assisted Decentralized Federated Learning (BLADE-FL):\n  Performance Analysis and Resource Allocation"}
{"author": "Adnan Qayyum, Kashif Ahmad, Muhammad Ahtazaz Ahsan, Ala Al-Fuqaha, Junaid Qadir", "abstract": "Despite significant improvements over the last few years, cloud-based\nhealthcare applications continue to suffer from poor adoption due to their\nlimitations in meeting stringent security, privacy, and quality of service\nrequirements (such as low latency). The edge computing trend, along with\ntechniques for distributed machine learning such as federated learning, have\ngained popularity as a viable solution in such settings. In this paper, we\nleverage the capabilities of edge computing in medicine by analyzing and\nevaluating the potential of intelligent processing of clinical visual data at\nthe edge allowing the remote healthcare centers, lacking advanced diagnostic\nfacilities, to benefit from the multi-modal data securely. To this aim, we\nutilize the emerging concept of clustered federated learning (CFL) for an\nautomatic diagnosis of COVID-19. Such an automated system can help reduce the\nburden on healthcare systems across the world that has been under a lot of\nstress since the COVID-19 pandemic emerged in late 2019. We evaluate the\nperformance of the proposed framework under different experimental setups on\ntwo benchmark datasets. Promising results are obtained on both datasets\nresulting in comparable results against the central baseline where the\nspecialized models (i.e., each on a specific type of COVID-19 imagery) are\ntrained with central data, and improvements of 16\\% and 11\\% in overall\nF1-Scores have been achieved over the multi-modal model trained in the\nconventional Federated Learning setup on X-ray and Ultrasound datasets,\nrespectively. We also discuss in detail the associated challenges,\ntechnologies, tools, and techniques available for deploying ML at the edge in\nsuch privacy and delay-sensitive applications.", "time": "2021-01-19T08:40:59Z", "link": "http://arxiv.org/abs/2101.07511v1", "id": "2101.07511v1", "title": "Collaborative Federated Learning For Healthcare: Multi-Modal COVID-19\n  Diagnosis at the Edge"}
{"author": "Nikoli Dryden, Roman BÃ¶hringer, Tal Ben-Nun, Torsten Hoefler", "abstract": "I/O is emerging as a major bottleneck for machine learning training,\nespecially in distributed environments. Indeed, at large scale, I/O takes as\nmuch as 85% of training time. Addressing this I/O bottleneck necessitates\ncareful optimization, as optimal data ingestion pipelines differ between\nsystems, and require a delicate balance between access to local storage,\nexternal filesystems, and remote nodes. We introduce NoPFS, a machine learning\nI/O middleware, which provides a scalable, flexible, and easy-to-use solution\nto the I/O bottleneck. NoPFS uses clairvoyance: Given the seed generating the\nrandom access pattern for training with SGD, it can exactly predict when and\nwhere a sample will be accessed. We combine this with an analysis of access\npatterns and a performance model to provide distributed caching policies that\nadapt to different datasets and storage hierarchies. NoPFS reduces I/O times\nand improves end-to-end training by up to 5.4x on the ImageNet-1k,\nImageNet-22k, and CosmoFlow datasets.", "time": "2021-06-10T12:28:20Z", "link": "http://arxiv.org/abs/2101.08734v2", "id": "2101.08734v2", "title": "Clairvoyant Prefetching for Distributed Machine Learning I/O"}
{"author": "Song WenJie, Shen Xuan", "abstract": "As data privacy is gradually valued by people, federated learning(FL) has\nemerged because of its potential to protect data. FL uses homomorphic\nencryption and differential privacy encryption on the promise of ensuring data\nsecurity to realize distributed machine learning by exchanging encrypted\ninformation between different data providers. However, there are still many\nproblems in FL, such as the communication efficiency between the client and the\nserver and the data is non-iid. In order to solve the two problems mentioned\nabove, we propose a novel vertical federated learning framework based on the\nDFP and the BFGS(denoted as BDFL), then apply it to logistic regression.\nFinally, we perform experiments using real datasets to test efficiency of BDFL\nframework.", "time": "2021-01-23T06:15:04Z", "link": "http://arxiv.org/abs/2101.09428v1", "id": "2101.09428v1", "title": "Vertical federated learning based on DFP and BFGS"}
{"author": "Ahmed M. Abdelmoniem, Ahmed Elzanaty, Mohamed-Slim Alouini, Marco Canini", "abstract": "The recent many-fold increase in the size of deep neural networks makes\nefficient distributed training challenging. Many proposals exploit the\ncompressibility of the gradients and propose lossy compression techniques to\nspeed up the communication stage of distributed training. Nevertheless,\ncompression comes at the cost of reduced model quality and extra computation\noverhead. In this work, we design an efficient compressor with minimal\noverhead. Noting the sparsity of the gradients, we propose to model the\ngradients as random variables distributed according to some sparsity-inducing\ndistributions (SIDs). We empirically validate our assumption by studying the\nstatistical characteristics of the evolution of gradient vectors over the\ntraining process. We then propose Sparsity-Inducing Distribution-based\nCompression (SIDCo), a threshold-based sparsification scheme that enjoys\nsimilar threshold estimation quality to deep gradient compression (DGC) while\nbeing faster by imposing lower compression overhead. Our extensive evaluation\nof popular machine learning benchmarks involving both recurrent neural network\n(RNN) and convolution neural network (CNN) models shows that SIDCo speeds up\ntraining by up to 41:7%, 7:6%, and 1:9% compared to the no-compression\nbaseline, Topk, and DGC compressors, respectively.", "time": "2021-03-17T18:18:47Z", "link": "http://arxiv.org/abs/2101.10761v2", "id": "2101.10761v2", "title": "An Efficient Statistical-based Gradient Compression Technique for\n  Distributed Training Systems"}
{"author": "Haibo Yang, Minghong Fang, Jia Liu", "abstract": "Federated learning (FL) is a distributed machine learning architecture that\nleverages a large number of workers to jointly learn a model with decentralized\ndata. FL has received increasing attention in recent years thanks to its data\nprivacy protection, communication efficiency and a linear speedup for\nconvergence in training (i.e., convergence performance increases linearly with\nrespect to the number of workers). However, existing studies on linear speedup\nfor convergence are only limited to the assumptions of i.i.d. datasets across\nworkers and/or full worker participation, both of which rarely hold in\npractice. So far, it remains an open question whether or not the linear speedup\nfor convergence is achievable under non-i.i.d. datasets with partial worker\nparticipation in FL. In this paper, we show that the answer is affirmative.\nSpecifically, we show that the federated averaging (FedAvg) algorithm (with\ntwo-sided learning rates) on non-i.i.d. datasets in non-convex settings\nachieves a convergence rate $\\mathcal{O}(\\frac{1}{\\sqrt{mKT}} + \\frac{1}{T})$\nfor full worker participation and a convergence rate\n$\\mathcal{O}(\\frac{\\sqrt{K}}{\\sqrt{nT}} + \\frac{1}{T})$ for partial worker\nparticipation, where $K$ is the number of local steps, $T$ is the number of\ntotal communication rounds, $m$ is the total worker number and $n$ is the\nworker number in one communication round if for partial worker participation.\nOur results also reveal that the local steps in FL could help the convergence\nand show that the maximum number of local steps can be improved to $T/m$ in\nfull worker participation. We conduct extensive experiments on MNIST and\nCIFAR-10 to verify our theoretical results.", "time": "2021-05-04T03:30:18Z", "link": "http://arxiv.org/abs/2101.11203v3", "id": "2101.11203v3", "title": "Achieving Linear Speedup with Partial Worker Participation in Non-IID\n  Federated Learning"}
{"author": "Kang Wei, Jun Li, Ming Ding, Chuan Ma, Yo-Seb Jeon, H. Vincent Poor", "abstract": "Federated learning (FL), as a type of distributed machine learning\nframeworks, is vulnerable to external attacks on FL models during parameters\ntransmissions. An attacker in FL may control a number of participant clients,\nand purposely craft the uploaded model parameters to manipulate system outputs,\nnamely, model poisoning (MP). In this paper, we aim to propose effective MP\nalgorithms to combat state-of-the-art defensive aggregation mechanisms (e.g.,\nKrum and Trimmed mean) implemented at the server without being noticed, i.e.,\ncovert MP (CMP). Specifically, we first formulate the MP as an optimization\nproblem by minimizing the Euclidean distance between the manipulated model and\ndesignated one, constrained by a defensive aggregation rule. Then, we develop\nCMP algorithms against different defensive mechanisms based on the solutions of\ntheir corresponding optimization problems. Furthermore, to reduce the\noptimization complexity, we propose low complexity CMP algorithms with a slight\nperformance degradation. In the case that the attacker does not know the\ndefensive aggregation mechanism, we design a blind CMP algorithm, in which the\nmanipulated model will be adjusted properly according to the aggregated model\ngenerated by the unknown defensive aggregation. Our experimental results\ndemonstrate that the proposed CMP algorithms are effective and substantially\noutperform existing attack mechanisms.", "time": "2021-01-28T03:28:18Z", "link": "http://arxiv.org/abs/2101.11799v1", "id": "2101.11799v1", "title": "Covert Model Poisoning Against Federated Learning: Algorithm Design and\n  Optimization"}
{"author": "Nima Mohammadi, Jianan Bai, Qiang Fan, Yifei Song, Yang Yi, Lingjia Liu", "abstract": "The performance of federated learning systems is bottlenecked by\ncommunication costs and training variance. The communication overhead problem\nis usually addressed by three communication-reduction techniques, namely, model\ncompression, partial device participation, and periodic aggregation, at the\ncost of increased training variance. Different from traditional distributed\nlearning systems, federated learning suffers from data heterogeneity (since the\ndevices sample their data from possibly different distributions), which induces\nadditional variance among devices during training. Various variance-reduced\ntraining algorithms have been introduced to combat the effects of data\nheterogeneity, while they usually cost additional communication resources to\ndeliver necessary control information. Additionally, data privacy remains a\ncritical issue in FL, and thus there have been attempts at bringing\nDifferential Privacy to this framework as a mediator between utility and\nprivacy requirements. This paper investigates the trade-offs between\ncommunication costs and training variance under a resource-constrained\nfederated system theoretically and experimentally, and how communication\nreduction techniques interplay in a differentially private setting. The results\nprovide important insights into designing practical privacy-aware federated\nlearning systems.", "time": "2021-01-28T19:20:56Z", "link": "http://arxiv.org/abs/2101.12240v1", "id": "2101.12240v1", "title": "Differential Privacy Meets Federated Learning under Communication\n  Constraints"}
{"author": "Syed Zawad, Ahsan Ali, Pin-Yu Chen, Ali Anwar, Yi Zhou, Nathalie Baracaldo, Yuan Tian, Feng Yan", "abstract": "Data heterogeneity has been identified as one of the key features in\nfederated learning but often overlooked in the lens of robustness to\nadversarial attacks. This paper focuses on characterizing and understanding its\nimpact on backdooring attacks in federated learning through comprehensive\nexperiments using synthetic and the LEAF benchmarks. The initial impression\ndriven by our experimental results suggests that data heterogeneity is the\ndominant factor in the effectiveness of attacks and it may be a redemption for\ndefending against backdooring as it makes the attack less efficient, more\nchallenging to design effective attack strategies, and the attack result also\nbecomes less predictable. However, with further investigations, we found data\nheterogeneity is more of a curse than a redemption as the attack effectiveness\ncan be significantly boosted by simply adjusting the client-side backdooring\ntiming. More importantly,data heterogeneity may result in overfitting at the\nlocal training of benign clients, which can be utilized by attackers to\ndisguise themselves and fool skewed-feature based defenses. In addition,\neffective attack strategies can be made by adjusting attack data distribution.\nFinally, we discuss the potential directions of defending the curses brought by\ndata heterogeneity. The results and lessons learned from our extensive\nexperiments and analysis offer new insights for designing robust federated\nlearning methods and systems", "time": "2021-02-01T06:06:21Z", "link": "http://arxiv.org/abs/2102.00655v1", "id": "2102.00655v1", "title": "Curse or Redemption? How Data Heterogeneity Affects the Robustness of\n  Federated Learning"}
{"author": "Wentai Wu, Ligang He, Weiwei Lin, Carsten Maple", "abstract": "Federated Learning (FL) has shown great potential as a privacy-preserving\nsolution to learning from decentralized data that are only accessible to end\ndevices (i.e., clients). In many scenarios, however, a large proportion of the\nclients are probably in possession of low-quality data that are biased, noisy\nor even irrelevant. As a result, they could significantly slow down the\nconvergence of the global model we aim to build and also compromise its\nquality. In light of this, we propose FedProf, a novel algorithm for optimizing\nFL under such circumstances without breaching data privacy. The key of our\napproach is a distributional representation profiling and matching scheme that\nuses the global model to dynamically profile data representations and allows\nfor low-cost, lightweight representation matching. Based on the scheme we\nadaptively score each client and adjust its participation probability so as to\nmitigate the impact of low-value clients on the training process. We have\nconducted extensive experiments on public datasets using various FL settings.\nThe results show that the selective behaviour of our algorithm leads to a\nsignificant reduction in the number of communication rounds and the amount of\ntime (up to 2.4x speedup) for the global model to converge and also provides\naccuracy gain.", "time": "2022-01-28T12:30:13Z", "link": "http://arxiv.org/abs/2102.01733v9", "id": "2102.01733v9", "title": "FedProf: Selective Federated Learning with Representation Profiling"}
{"author": "Qinbin Li, Yiqun Diao, Quan Chen, Bingsheng He", "abstract": "Due to the increasing privacy concerns and data regulations, training data\nhave been increasingly fragmented, forming distributed databases of multiple\n\"data silos\" (e.g., within different organizations and countries). To develop\neffective machine learning services, there is a must to exploit data from such\ndistributed databases without exchanging the raw data. Recently, federated\nlearning (FL) has been a solution with growing interests, which enables\nmultiple parties to collaboratively train a machine learning model without\nexchanging their local data. A key and common challenge on distributed\ndatabases is the heterogeneity of the data distribution among the parties. The\ndata of different parties are usually non-independently and identically\ndistributed (i.e., non-IID). There have been many FL algorithms to address the\nlearning effectiveness under non-IID data settings. However, there lacks an\nexperimental study on systematically understanding their advantages and\ndisadvantages, as previous studies have very rigid data partitioning strategies\namong parties, which are hardly representative and thorough. In this paper, to\nhelp researchers better understand and study the non-IID data setting in\nfederated learning, we propose comprehensive data partitioning strategies to\ncover the typical non-IID data cases. Moreover, we conduct extensive\nexperiments to evaluate state-of-the-art FL algorithms. We find that non-IID\ndoes bring significant challenges in learning accuracy of FL algorithms, and\nnone of the existing state-of-the-art FL algorithms outperforms others in all\ncases. Our experiments provide insights for future studies of addressing the\nchallenges in \"data silos\".", "time": "2021-10-28T15:22:21Z", "link": "http://arxiv.org/abs/2102.02079v4", "id": "2102.02079v4", "title": "Federated Learning on Non-IID Data Silos: An Experimental Study"}
{"author": "Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He", "abstract": "Scalable training of large models (like BERT and GPT-3) requires careful\noptimization rooted in model design, architecture, and system capabilities.\nFrom a system standpoint, communication has become a major bottleneck,\nespecially on commodity systems with standard TCP interconnects that offer\nlimited network bandwidth. Communication compression is an important technique\nto reduce training time on such systems. One of the most effective methods is\nerror-compensated compression, which offers robust convergence speed even under\n1-bit compression. However, state-of-the-art error compensation techniques only\nwork with basic optimizers like SGD and momentum SGD, which are linearly\ndependent on the gradients. They do not work with non-linear gradient-based\noptimizers like Adam, which offer state-of-the-art convergence efficiency and\naccuracy for models like BERT. In this paper, we propose 1-bit Adam that\nreduces the communication volume by up to $5\\times$, offers much better\nscalability, and provides the same convergence speed as uncompressed Adam. Our\nkey finding is that Adam's variance (non-linear term) becomes stable (after a\nwarmup phase) and can be used as a fixed precondition for the rest of the\ntraining (compression phase). Experiments on up to 256 GPUs show that 1-bit\nAdam enables up to $3.3\\times$ higher throughput for BERT-Large pre-training\nand up to $2.9\\times$ higher throughput for SQuAD fine-tuning. In addition, we\nprovide theoretical analysis for our proposed work.", "time": "2021-06-29T18:25:26Z", "link": "http://arxiv.org/abs/2102.02888v2", "id": "2102.02888v2", "title": "1-bit Adam: Communication Efficient Large-Scale Training with Adam's\n  Convergence Speed"}
{"author": "Wenting Zou, Li Li, Zichen Xu, Chengzhong Xu", "abstract": "Federated learning struggles with their heavy energy footprint on\nbattery-powered devices. The learning process keeps all devices awake while\ndraining expensive battery power to train a shared model collaboratively, yet\nit may still leak sensitive personal information. Traditional energy management\ntechniques in system kernel mode can force the training device entering low\npower states, but it may violate the SLO of the collaborative learning. To\naddress the conflict between learning SLO and energy efficiency, we propose\nDEAL, an energy efficient learning system that saves energy and preserves\nprivacy with a decremental learning design. DEAL reduces the energy footprint\nfrom two layers: 1) an optimization layer that selects a subset of workers with\nsufficient capacity and maximum rewards. 2) a specified decremental learning\nalgorithm that actively provides a decremental and incremental update\nfunctions, which allows kernel to correctly tune the local DVFS. We prototyped\nDEAL in containerized services with modern smartphone profiles and evaluated it\nwith several learning benchmarks with realistic traces. We observed that DEAL\nachieves 75.6%-82.4% less energy footprint in different datasets, compared to\nthe traditional methods. All learning processes are faster than\nstate-of-the-practice FL frameworks up to 2-4X in model convergence.", "time": "2021-02-05T08:31:42Z", "link": "http://arxiv.org/abs/2102.03051v1", "id": "2102.03051v1", "title": "DEAL: Decremental Energy-Aware Learning in a Federated System"}
{"author": "Karan Singhal, Hakim Sidahmed, Zachary Garrett, Shanshan Wu, Keith Rush, Sushant Prakash", "abstract": "Personalization methods in federated learning aim to balance the benefits of\nfederated and local training for data availability, communication cost, and\nrobustness to client heterogeneity. Approaches that require clients to\ncommunicate all model parameters can be undesirable due to privacy and\ncommunication constraints. Other approaches require always-available or\nstateful clients, impractical in large-scale cross-device settings. We\nintroduce Federated Reconstruction, the first model-agnostic framework for\npartially local federated learning suitable for training and inference at\nscale. We motivate the framework via a connection to model-agnostic meta\nlearning, empirically demonstrate its performance over existing approaches for\ncollaborative filtering and next word prediction, and release an open-source\nlibrary for evaluating approaches in this setting. We also describe the\nsuccessful deployment of this approach at scale for federated collaborative\nfiltering in a mobile keyboard application.", "time": "2022-04-27T04:29:57Z", "link": "http://arxiv.org/abs/2102.03448v6", "id": "2102.03448v6", "title": "Federated Reconstruction: Partially Local Federated Learning"}
{"author": "Anirban Das, Stacy Patterson", "abstract": "We consider decentralized model training in tiered communication networks.\nOur network model consists of a set of silos, each holding a vertical partition\nof the data. Each silo contains a hub and a set of clients, with the silo's\nvertical data shard partitioned horizontally across its clients. We propose\nTiered Decentralized Coordinate Descent (TDCD), a communication-efficient\ndecentralized training algorithm for such two-tiered networks. To reduce\ncommunication overhead, the clients in each silo perform multiple local\ngradient steps before sharing updates with their hub. Each hub adjusts its\ncoordinates by averaging its workers' updates, and then hubs exchange\nintermediate updates with one another. We present a theoretical analysis of our\nalgorithm and show the dependence of the convergence rate on the number of\nvertical partitions, the number of local updates, and the number of clients in\neach hub. We further validate our approach empirically via simulation-based\nexperiments using a variety of datasets and both convex and non-convex\nobjectives.", "time": "2021-02-06T17:34:14Z", "link": "http://arxiv.org/abs/2102.03620v1", "id": "2102.03620v1", "title": "Multi-Tier Federated Learning for Vertically Partitioned Data"}
{"author": "An Xu, Heng Huang", "abstract": "Communication efficiency is crucial for federated learning (FL). Conducting\nlocal training steps in clients to reduce the communication frequency between\nclients and the server is a common method to address this issue. However, this\nstrategy leads to the client drift problem due to \\textit{non-i.i.d.} data\ndistributions in different clients which severely deteriorates the performance.\nIn this work, we propose a new method to improve the training performance in\ncross-silo FL via maintaining double momentum buffers. In our algorithm, one\nmomentum buffer is used to track the server model updating direction, and the\nother one is adopted to track the local model updating direction. More\nimportant, we introduce a novel momentum fusion technique to coordinate the\nserver and local momentum buffers. We also derive the first theoretical\nconvergence analysis involving both the server and local standard momentum SGD.\nExtensive deep FL experimental results verify that our new approach has a\nbetter training performance than the FedAvg and existing standard momentum SGD\nvariants.", "time": "2022-01-19T05:51:50Z", "link": "http://arxiv.org/abs/2102.03970v2", "id": "2102.03970v2", "title": "Coordinating Momenta for Cross-silo Federated Learning"}
{"author": "Xingyu Li, Zhe Qu, Bo Tang, Zhuo Lu", "abstract": "Federated learning (FL) is a new machine learning framework which trains a\njoint model across a large amount of decentralized computing devices. Existing\nmethods, e.g., Federated Averaging (FedAvg), are able to provide an\noptimization guarantee by synchronously training the joint model, but usually\nsuffer from stragglers, i.e., IoT devices with low computing power or\ncommunication bandwidth, especially on heterogeneous optimization problems. To\nmitigate the influence of stragglers, this paper presents a novel FL algorithm,\nnamely Hybrid Federated Learning (HFL), to achieve a learning balance in\nefficiency and effectiveness. It consists of two major components: synchronous\nkernel and asynchronous updater. Unlike traditional synchronous FL methods, our\nHFL introduces the asynchronous updater which actively pulls unsynchronized and\ndelayed local weights from stragglers. An adaptive approximation method,\nAdaptive Delayed-SGD (AD-SGD), is proposed to merge the delayed local updates\ninto the joint model. The theoretical analysis of HFL shows that the\nconvergence rate of the proposed algorithm is $\\mathcal{O}(\\frac{1}{t+\\tau})$\nfor both convex and non-convex optimization problems.", "time": "2021-02-12T02:27:44Z", "link": "http://arxiv.org/abs/2102.06329v1", "id": "2102.06329v1", "title": "Stragglers Are Not Disaster: A Hybrid Federated Learning Algorithm with\n  Delayed Gradients"}
{"author": "Canh T. Dinh, Tung T. Vu, Nguyen H. Tran, Minh N. Dao, Hongyu Zhang", "abstract": "Non-Independent and Identically Distributed (non- IID) data distribution\namong clients is considered as the key factor that degrades the performance of\nfederated learning (FL). Several approaches to handle non-IID data such as\npersonalized FL and federated multi-task learning (FMTL) are of great interest\nto research communities. In this work, first, we formulate the FMTL problem\nusing Laplacian regularization to explicitly leverage the relationships among\nthe models of clients for multi-task learning. Then, we introduce a new view of\nthe FMTL problem, which in the first time shows that the formulated FMTL\nproblem can be used for conventional FL and personalized FL. We also propose\ntwo algorithms FedU and dFedU to solve the formulated FMTL problem in\ncommunication-centralized and decentralized schemes, respectively.\nTheoretically, we prove that the convergence rates of both algorithms achieve\nlinear speedup for strongly convex and sublinear speedup of order 1/2 for\nnonconvex objectives. Experimentally, we show that our algorithms outperform\nthe algorithm FedAvg, FedProx, SCAFFOLD, and AFL in FL settings, MOCHA in FMTL\nsettings, as well as pFedMe and Per-FedAvg in personalized FL settings.", "time": "2022-10-11T11:54:45Z", "link": "http://arxiv.org/abs/2102.07148v5", "id": "2102.07148v5", "title": "A New Look and Convergence Rate of Federated Multi-Task Learning with\n  Laplacian Regularization"}
{"author": "Xinchi Qiu, Titouan Parcollet, Javier Fernandez-Marques, Pedro Porto Buarque de Gusmao, Yan Gao, Daniel J. Beutel, Taner Topal, Akhil Mathur, Nicholas D. Lane", "abstract": "Despite impressive results, deep learning-based technologies also raise\nsevere privacy and environmental concerns induced by the training procedure\noften conducted in data centers. In response, alternatives to centralized\ntraining such as Federated Learning (FL) have emerged. Perhaps unexpectedly, FL\nis starting to be deployed at a global scale by companies that must adhere to\nnew legal demands and policies originating from governments and social groups\nadvocating for privacy protection. \\textit{However, the potential environmental\nimpact related to FL remains unclear and unexplored. This paper offers the\nfirst-ever systematic study of the carbon footprint of FL.} First, we propose a\nrigorous model to quantify the carbon footprint, hence facilitating the\ninvestigation of the relationship between FL design and carbon emissions. Then,\nwe compare the carbon footprint of FL to traditional centralized learning. Our\nfindings show that, depending on the configuration, FL can emit up to two order\nof magnitude more carbon than centralized machine learning. However, in certain\nsettings, it can be comparable to centralized learning due to the reduced\nenergy consumption of embedded devices. We performed extensive experiments\nacross different types of datasets, settings and various deep learning models\nwith FL. Finally, we highlight and connect the reported results to the future\nchallenges and trends in FL to reduce its environmental impact, including\nalgorithms efficiency, hardware capabilities, and stronger industry\ntransparency.", "time": "2023-05-22T15:07:08Z", "link": "http://arxiv.org/abs/2102.07627v6", "id": "2102.07627v6", "title": "A first look into the carbon footprint of federated learning"}
{"author": "Dimitris Stripelis, Jose Luis Ambite, Pradeep Lam, Paul Thompson", "abstract": "The amount of biomedical data continues to grow rapidly. However, the ability\nto analyze these data is limited due to privacy and regulatory concerns.\nMachine learning approaches that require data to be copied to a single location\nare hampered by the challenges of data sharing. Federated Learning is a\npromising approach to learn a joint model over data silos. This architecture\ndoes not share any subject data across sites, only aggregated parameters, often\nin encrypted environments, thus satisfying privacy and regulatory requirements.\nHere, we describe our Federated Learning architecture and training policies. We\ndemonstrate our approach on a brain age prediction model on structural MRI\nscans distributed across multiple sites with diverse amounts of data and\nsubject (age) distributions. In these heterogeneous environments, our\nSemi-Synchronous protocol provides faster convergence.", "time": "2021-02-16T20:30:04Z", "link": "http://arxiv.org/abs/2102.08440v1", "id": "2102.08440v1", "title": "Scaling Neuroscience Research using Federated Learning"}
{"author": "Afaf Taik, Zoubeir Mlika, Soumaya Cherkaoui", "abstract": "Federated Edge Learning (FEEL) involves the collaborative training of machine\nlearning models among edge devices, with the orchestration of a server in a\nwireless edge network. Due to frequent model updates, FEEL needs to be adapted\nto the limited communication bandwidth, scarce energy of edge devices, and the\nstatistical heterogeneity of edge devices' data distributions. Therefore, a\ncareful scheduling of a subset of devices for training and uploading models is\nnecessary. In contrast to previous work in FEEL where the data aspects are\nunder-explored, we consider data properties at the heart of the proposed\nscheduling algorithm. To this end, we propose a new scheduling scheme for\nnon-independent and-identically-distributed (non-IID) and unbalanced datasets\nin FEEL. As the data is the key component of the learning, we propose a new set\nof considerations for data characteristics in wireless scheduling algorithms in\nFEEL. In fact, the data collected by the devices depends on the local\nenvironment and usage pattern. Thus, the datasets vary in size and\ndistributions among the devices. In the proposed algorithm, we consider both\ndata and resource perspectives. In addition to minimizing the completion time\nof FEEL as well as the transmission energy of the participating devices, the\nalgorithm prioritizes devices with rich and diverse datasets. We first define a\ngeneral framework for the data-aware scheduling and the main axes and\nrequirements for diversity evaluation. Then, we discuss diversity aspects and\nsome exploitable techniques and metrics. Next, we formulate the problem and\npresent our FEEL scheduling algorithm. Evaluations in different scenarios show\nthat our proposed FEEL scheduling algorithm can help achieve high accuracy in\nfew rounds with a reduced cost.", "time": "2022-01-27T02:18:30Z", "link": "http://arxiv.org/abs/2102.09491v2", "id": "2102.09491v2", "title": "Data-Aware Device Scheduling for Federated Edge Learning"}
{"author": "Bradley T. Baker, Aashis Khanal, Vince D. Calhoun, Barak Pearlmutter, Sergey M. Plis", "abstract": "Although distributed machine learning has opened up many new and exciting\nresearch frontiers, fragmentation of models and data across different machines,\nnodes, and sites still results in considerable communication overhead, impeding\nreliable training in real-world contexts.\n  The focus on gradients as the primary shared statistic during training has\nspawned a number of intuitive algorithms for distributed deep learning;\nhowever, gradient-centric training of large deep neural networks (DNNs) tends\nto be communication-heavy, often requiring additional adaptations such as\nsparsity constraints, compression, quantization, and more, to curtail\nbandwidth.\n  We introduce an innovative, communication-friendly approach for training\ndistributed DNNs, which capitalizes on the outer-product structure of the\ngradient as revealed by the mechanics of auto-differentiation. The exposed\nstructure of the gradient evokes a new class of distributed learning algorithm,\nwhich is naturally more communication-efficient than full gradient sharing. Our\napproach, called distributed auto-differentiation (dAD), builds off a marriage\nof rank-based compression and the innate structure of the gradient as an\nouter-product. We demonstrate that dAD trains more efficiently than other state\nof the art distributed methods on modern architectures, such as transformers,\nwhen applied to large-scale text and imaging datasets. The future of\ndistributed learning, we determine, need not be dominated by gradient-centric\nalgorithms.", "time": "2022-02-03T18:06:39Z", "link": "http://arxiv.org/abs/2102.09631v3", "id": "2102.09631v3", "title": "Peering Beyond the Gradient Veil with Distributed Auto Differentiation"}
{"author": "Sajib Mistry, Athman Bouguettaya, Lie Qu", "abstract": "We propose a novel generic reputation bootstrapping framework for composite\nservices. Multiple reputation-related indicators are considered in a\nlayer-based framework to implicitly reflect the reputation of the component\nservices. The importance of an indicator on the future performance of a\ncomponent service is learned using a modified Random Forest algorithm. We\npropose a topology-aware Forest Deep Neural Network (fDNN) to find the\ncorrelations between the reputation of a composite service and reputation\nindicators of component services. The trained fDNN model predicts the\nreputation of a new composite service with the confidence value. Experimental\nresults with real-world dataset prove the efficiency of the proposed approach.", "time": "2021-02-01T15:00:30Z", "link": "http://arxiv.org/abs/2102.09951v1", "id": "2102.09951v1", "title": "Layer-based Composite Reputation Bootstrapping"}
{"author": "Kaan Ozkara, Navjot Singh, Deepesh Data, Suhas Diggavi", "abstract": "Traditionally, federated learning (FL) aims to train a single global model\nwhile collaboratively using multiple clients and a server. Two natural\nchallenges that FL algorithms face are heterogeneity in data across clients and\ncollaboration of clients with {\\em diverse resources}. In this work, we\nintroduce a \\textit{quantized} and \\textit{personalized} FL algorithm QuPeL\nthat facilitates collective training with heterogeneous clients while\nrespecting resource diversity. For personalization, we allow clients to learn\n\\textit{compressed personalized models} with different quantization parameters\ndepending on their resources. Towards this, first we propose an algorithm for\nlearning quantized models through a relaxed optimization problem, where\nquantization values are also optimized over. When each client participating in\nthe (federated) learning process has different requirements of the quantized\nmodel (both in value and precision), we formulate a quantized personalization\nframework by introducing a penalty term for local client objectives against a\nglobally trained model to encourage collaboration. We develop an alternating\nproximal gradient update for solving this quantized personalization problem,\nand we analyze its convergence properties. Numerically, we show that optimizing\nover the quantization levels increases the performance and we validate that\nQuPeL outperforms both FedAvg and local training of clients in a heterogeneous\nsetting.", "time": "2021-02-23T16:43:51Z", "link": "http://arxiv.org/abs/2102.11786v1", "id": "2102.11786v1", "title": "QuPeL: Quantized Personalization with Applications to Federated Learning"}
{"author": "Sajib Mistry, Sheik Mohammad Mostakim Fattah, Athman Bouguettaya", "abstract": "We propose a novel IaaS composition framework that selects an optimal set of\nconsumer requests according to the provider's qualitative preferences on\nlong-term service provisions. Decision variables are included in the temporal\nconditional preference networks (TempCP-net) to represent qualitative\npreferences for both short-term and long-term consumers. The global preference\nranking of a set of requests is computed using a \\textit{k}-d tree indexing\nbased temporal similarity measure approach. We propose an extended\nthree-dimensional Q-learning approach to maximize the global preference\nranking. We design the on-policy based sequential selection learning approach\nthat applies the length of request to accept or reject requests in a\ncomposition. The proposed on-policy based learning method reuses historical\nexperiences or policies of sequential optimization using an agglomerative\nclustering approach. Experimental results prove the feasibility of the proposed\nframework.", "time": "2021-02-24T23:16:01Z", "link": "http://arxiv.org/abs/2102.12598v1", "id": "2102.12598v1", "title": "Sequential Learning-based IaaS Composition"}
{"author": "Shaoxiong Ji, Yue Tan, Teemu Saravirta, Zhiqin Yang, Yixin Liu, Lauri Vasankari, Shirui Pan, Guodong Long, Anwar Walid", "abstract": "Federated learning is a new learning paradigm that decouples data collection\nand model training via multi-party computation and model aggregation. As a\nflexible learning setting, federated learning has the potential to integrate\nwith other learning frameworks. We conduct a focused survey of federated\nlearning in conjunction with other learning algorithms. Specifically, we\nexplore various learning algorithms to improve the vanilla federated averaging\nalgorithm and review model fusion methods such as adaptive aggregation,\nregularization, clustered methods, and Bayesian methods. Following the emerging\ntrends, we also discuss federated learning in the intersection with other\nlearning paradigms, termed federated X learning, where X includes multitask\nlearning, meta-learning, transfer learning, unsupervised learning, and\nreinforcement learning. In addition to reviewing state-of-the-art studies, this\npaper also identifies key challenges and applications in this field, while also\nhighlighting promising future directions.", "time": "2024-03-27T09:07:29Z", "link": "http://arxiv.org/abs/2102.12920v5", "id": "2102.12920v5", "title": "Emerging Trends in Federated Learning: From Model Fusion to Federated X\n  Learning"}
{"author": "Brennan Saeta, Denys Shabalin, Marc Rasi, Brad Larson, Xihui Wu, Parker Schuh, Michelle Casbon, Daniel Zheng, Saleem Abdulrasool, Aleksandr Efremov, Dave Abrahams, Chris Lattner, Richard Wei", "abstract": "Swift for TensorFlow is a deep learning platform that scales from mobile\ndevices to clusters of hardware accelerators in data centers. It combines a\nlanguage-integrated automatic differentiation system and multiple Tensor\nimplementations within a modern ahead-of-time compiled language oriented around\nmutable value semantics. The resulting platform has been validated through use\nin over 30 deep learning models and has been employed across data center and\nmobile applications.", "time": "2021-02-26T00:21:15Z", "link": "http://arxiv.org/abs/2102.13243v1", "id": "2102.13243v1", "title": "Swift for TensorFlow: A portable, flexible platform for deep learning"}
{"author": "Samuel Horvath, Stefanos Laskaridis, Mario Almeida, Ilias Leontiadis, Stylianos I. Venieris, Nicholas D. Lane", "abstract": "Federated Learning (FL) has been gaining significant traction across\ndifferent ML tasks, ranging from vision to keyboard predictions. In large-scale\ndeployments, client heterogeneity is a fact and constitutes a primary problem\nfor fairness, training performance and accuracy. Although significant efforts\nhave been made into tackling statistical data heterogeneity, the diversity in\nthe processing capabilities and network bandwidth of clients, termed as system\nheterogeneity, has remained largely unexplored. Current solutions either\ndisregard a large portion of available devices or set a uniform limit on the\nmodel's capacity, restricted by the least capable participants. In this work,\nwe introduce Ordered Dropout, a mechanism that achieves an ordered, nested\nrepresentation of knowledge in deep neural networks (DNNs) and enables the\nextraction of lower footprint submodels without the need of retraining. We\nfurther show that for linear maps our Ordered Dropout is equivalent to SVD. We\nemploy this technique, along with a self-distillation methodology, in the realm\nof FL in a framework called FjORD. FjORD alleviates the problem of client\nsystem heterogeneity by tailoring the model width to the client's capabilities.\nExtensive evaluation on both CNNs and RNNs across diverse modalities shows that\nFjORD consistently leads to significant performance gains over state-of-the-art\nbaselines, while maintaining its nested structure.", "time": "2022-01-11T09:31:23Z", "link": "http://arxiv.org/abs/2102.13451v5", "id": "2102.13451v5", "title": "FjORD: Fair and Accurate Federated Learning under heterogeneous targets\n  with Ordered Dropout"}
{"author": "Shaoduo Gan, Xiangru Lian, Rui Wang, Jianbin Chang, Chengjun Liu, Hongmei Shi, Shengzhuo Zhang, Xianghong Li, Tengxu Sun, Jiawei Jiang, Binhang Yuan, Sen Yang, Ji Liu, Ce Zhang", "abstract": "Recent years have witnessed a growing list of systems for distributed\ndata-parallel training. Existing systems largely fit into two paradigms, i.e.,\nparameter server and MPI-style collective operations. On the algorithmic side,\nresearchers have proposed a wide range of techniques to lower the communication\nvia system relaxations: quantization, decentralization, and communication\ndelay. However, most, if not all, existing systems only rely on standard\nsynchronous and asynchronous stochastic gradient (SG) based optimization,\ntherefore, cannot take advantage of all possible optimizations that the machine\nlearning community has been developing recently. Given this emerging gap\nbetween the current landscapes of systems and theory, we build BAGUA, a\nMPI-style communication library, providing a collection of primitives, that is\nboth flexible and modular to support state-of-the-art system relaxation\ntechniques of distributed training. Powered by this design, BAGUA has a great\nability to implement and extend various state-of-the-art distributed learning\nalgorithms. In a production cluster with up to 16 machines (128 GPUs), BAGUA\ncan outperform PyTorch-DDP, Horovod and BytePS in the end-to-end training time\nby a significant margin (up to 2 times) across a diverse range of tasks.\nMoreover, we conduct a rigorous tradeoff exploration showing that different\nalgorithms and system relaxations achieve the best performance over different\nnetwork conditions.", "time": "2021-11-25T06:39:43Z", "link": "http://arxiv.org/abs/2107.01499v4", "id": "2107.01499v4", "title": "BAGUA: Scaling up Distributed Learning with System Relaxations"}
{"author": "J. Gregory Pauloski, Qi Huang, Lei Huang, Shivaram Venkataraman, Kyle Chard, Ian Foster, Zhao Zhang", "abstract": "Kronecker-factored Approximate Curvature (K-FAC) has recently been shown to\nconverge faster in deep neural network (DNN) training than stochastic gradient\ndescent (SGD); however, K-FAC's larger memory footprint hinders its\napplicability to large models. We present KAISA, a K-FAC-enabled, Adaptable,\nImproved, and ScAlable second-order optimizer framework that adapts the memory\nfootprint, communication, and computation given specific models and hardware to\nimprove performance and increase scalability. We quantify the tradeoffs between\nmemory and communication cost and evaluate KAISA on large models, including\nResNet-50, Mask R-CNN, U-Net, and BERT, on up to 128 NVIDIA A100 GPUs. Compared\nto the original optimizers, KAISA converges 18.1-36.3% faster across\napplications with the same global batch size. Under a fixed memory budget,\nKAISA converges 32.5% and 41.6% faster in ResNet-50 and BERT-Large,\nrespectively. KAISA can balance memory and communication to achieve scaling\nefficiency equal to or better than the baseline optimizers. KAISA is open\nsource and available at https://github.com/gpauloski/kfac_pytorch.", "time": "2021-09-20T14:47:11Z", "link": "http://arxiv.org/abs/2107.01739v2", "id": "2107.01739v2", "title": "KAISA: An Adaptive Second-Order Optimizer Framework for Deep Neural\n  Networks"}
{"author": "Van-Dinh Nguyen, Symeon Chatzinotas, Bjorn Ottersten, Trung Q. Duong", "abstract": "Federated learning (FL) is capable of performing large distributed machine\nlearning tasks across multiple edge users by periodically aggregating trained\nlocal parameters. To address key challenges of enabling FL over a wireless\nfog-cloud system (e.g., non-i.i.d. data, users' heterogeneity), we first\npropose an efficient FL algorithm based on Federated Averaging (called FedFog)\nto perform the local aggregation of gradient parameters at fog servers and\nglobal training update at the cloud. Next, we employ FedFog in wireless\nfog-cloud systems by investigating a novel network-aware FL optimization\nproblem that strikes the balance between the global loss and completion time.\nAn iterative algorithm is then developed to obtain a precise measurement of the\nsystem performance, which helps design an efficient stopping criteria to output\nan appropriate number of global rounds. To mitigate the straggler effect, we\npropose a flexible user aggregation strategy that trains fast users first to\nobtain a certain level of accuracy before allowing slow users to join the\nglobal training updates. Extensive numerical results using several real-world\nFL tasks are provided to verify the theoretical convergence of FedFog. We also\nshow that the proposed co-design of FL and communication is essential to\nsubstantially improve resource utilization while achieving comparable accuracy\nof the learning model.", "time": "2022-02-10T11:20:21Z", "link": "http://arxiv.org/abs/2107.02755v3", "id": "2107.02755v3", "title": "FedFog: Network-Aware Optimization of Federated Learning over Wireless\n  Fog-Cloud Systems"}
{"author": "Di Wu, Rehmat Ullah, Paul Harvey, Peter Kilpatrick, Ivor Spence, Blesson Varghese", "abstract": "Applying Federated Learning (FL) on Internet-of-Things devices is\nnecessitated by the large volumes of data they produce and growing concerns of\ndata privacy. However, there are three challenges that need to be addressed to\nmake FL efficient: (i) execution on devices with limited computational\ncapabilities, (ii) accounting for stragglers due to computational heterogeneity\nof devices, and (iii) adaptation to the changing network bandwidths. This paper\npresents FedAdapt, an adaptive offloading FL framework to mitigate the\naforementioned challenges. FedAdapt accelerates local training in\ncomputationally constrained devices by leveraging layer offloading of deep\nneural networks (DNNs) to servers. Further, FedAdapt adopts reinforcement\nlearning based optimization and clustering to adaptively identify which layers\nof the DNN should be offloaded for each individual device on to a server to\ntackle the challenges of computational heterogeneity and changing network\nbandwidth. Experimental studies are carried out on a lab-based testbed and it\nis demonstrated that by offloading a DNN from the device to the server FedAdapt\nreduces the training time of a typical IoT device by over half compared to\nclassic FL. The training time of extreme stragglers and the overall training\ntime can be reduced by up to 57%. Furthermore, with changing network bandwidth,\nFedAdapt is demonstrated to reduce the training time by up to 40% when compared\nto classic FL, without sacrificing accuracy.", "time": "2022-05-18T12:59:31Z", "link": "http://arxiv.org/abs/2107.04271v5", "id": "2107.04271v5", "title": "FedAdapt: Adaptive Offloading for IoT Devices in Federated Learning"}
{"author": "Kabir Nagrecha", "abstract": "As deep learning becomes more expensive, both in terms of time and compute,\ninefficiencies in machine learning (ML) training prevent practical usage of\nstate-of-the-art models for most users. The newest model architectures are\nsimply too large to be fit onto a single processor. To address the issue, many\nML practitioners have turned to model parallelism as a method of distributing\nthe computational requirements across several devices. Unfortunately, the\nsequential nature of neural networks causes very low efficiency and device\nutilization in model parallel training jobs. We propose a new form of \"shard\nparallelism\" combining task and model parallelism, then package it into a\nframework we name Hydra. Hydra recasts the problem of model parallelism in the\nmulti-model context to produce a fine-grained parallel workload of independent\nmodel shards, rather than independent models. This new parallel design promises\ndramatic speedups relative to the traditional model parallelism paradigm.", "time": "2021-07-14T03:20:37Z", "link": "http://arxiv.org/abs/2107.06469v1", "id": "2107.06469v1", "title": "Model-Parallel Model Selection for Deep Learning Systems"}
{"author": "David Roschewitz, Mary-Anne Hartley, Luca Corinzia, Martin Jaggi", "abstract": "Recently, the ever-growing demand for privacy-oriented machine learning has\nmotivated researchers to develop federated and decentralized learning\ntechniques, allowing individual clients to train models collaboratively without\ndisclosing their private datasets. However, widespread adoption has been\nlimited in domains relying on high levels of user trust, where assessment of\ndata compatibility is essential. In this work, we define and address low\ninteroperability induced by underlying client data inconsistencies in federated\nlearning for tabular data. The proposed method, iFedAvg, builds on federated\naveraging adding local element-wise affine layers to allow for a personalized\nand granular understanding of the collaborative learning process. Thus,\nenabling the detection of outlier datasets in the federation and also learning\nthe compensation for local data distribution shifts without sharing any\noriginal data. We evaluate iFedAvg using several public benchmarks and a\npreviously unstudied collection of real-world datasets from the 2014 - 2016\nWest African Ebola epidemic, jointly forming the largest such dataset in the\nworld. In all evaluations, iFedAvg achieves competitive average performance\nwith negligible overhead. It additionally shows substantial improvement on\noutlier clients, highlighting increased robustness to individual dataset\nshifts. Most importantly, our method provides valuable client-specific insights\nat a fine-grained level to guide interoperable federated learning.", "time": "2021-07-14T09:54:00Z", "link": "http://arxiv.org/abs/2107.06580v1", "id": "2107.06580v1", "title": "IFedAvg: Interpretable Data-Interoperability for Federated Learning"}
{"author": "Matthias Reisser, Christos Louizos, Efstratios Gavves, Max Welling", "abstract": "Federated learning (FL) has emerged as the predominant approach for\ncollaborative training of neural network models across multiple users, without\nthe need to gather the data at a central location. One of the important\nchallenges in this setting is data heterogeneity, i.e. different users have\ndifferent data characteristics. For this reason, training and using a single\nglobal model might be suboptimal when considering the performance of each of\nthe individual user's data. In this work, we tackle this problem via Federated\nMixture of Experts, FedMix, a framework that allows us to train an ensemble of\nspecialized models. FedMix adaptively selects and trains a user-specific\nselection of the ensemble members. We show that users with similar data\ncharacteristics select the same members and therefore share statistical\nstrength while mitigating the effect of non-i.i.d data. Empirically, we show\nthrough an extensive experimental evaluation that FedMix improves performance\ncompared to using a single global model across a variety of different sources\nof non-i.i.d.-ness.", "time": "2021-07-14T14:15:24Z", "link": "http://arxiv.org/abs/2107.06724v1", "id": "2107.06724v1", "title": "Federated Mixture of Experts"}
{"author": "Shigang Li, Torsten Hoefler", "abstract": "Training large deep learning models at scale is very challenging. This paper\nproposes Chimera, a novel pipeline parallelism scheme which combines\nbidirectional pipelines for efficiently training large-scale models. Chimera is\na synchronous approach and therefore no loss of accuracy, which is more\nconvergence-friendly than asynchronous approaches. Compared with the latest\nsynchronous pipeline approach, Chimera reduces the number of bubbles by up to\n50%; benefiting from the sophisticated scheduling of bidirectional pipelines,\nChimera has a more balanced activation memory consumption. Evaluations are\nconducted on Transformer based language models. For a GPT-2 model with 1.3\nbillion parameters running on 2,048 GPU nodes of the Piz Daint supercomputer,\nChimera improves the training throughput by 1.16x-2.34x over the\nstate-of-the-art synchronous and asynchronous pipeline approaches.", "time": "2022-02-25T10:49:12Z", "link": "http://arxiv.org/abs/2107.06925v3", "id": "2107.06925v3", "title": "Chimera: Efficiently Training Large-Scale Neural Networks with\n  Bidirectional Pipelines"}
{"author": "Young Geun Kim, Carole-Jean Wu", "abstract": "Federated learning enables a cluster of decentralized mobile devices at the\nedge to collaboratively train a shared machine learning model, while keeping\nall the raw training samples on device. This decentralized training approach is\ndemonstrated as a practical solution to mitigate the risk of privacy leakage.\nHowever, enabling efficient FL deployment at the edge is challenging because of\nnon-IID training data distribution, wide system heterogeneity and\nstochastic-varying runtime effects in the field. This paper jointly optimizes\ntime-to-convergence and energy efficiency of state-of-the-art FL use cases by\ntaking into account the stochastic nature of edge execution. We propose AutoFL\nby tailor-designing a reinforcement learning algorithm that learns and\ndetermines which K participant devices and per-device execution targets for\neach FL model aggregation round in the presence of stochastic runtime variance,\nsystem and data heterogeneity. By considering the unique characteristics of FL\nedge deployment judiciously, AutoFL achieves 3.6 times faster model convergence\ntime and 4.7 and 5.2 times higher energy efficiency for local clients and\nglobally over the cluster of K participants, respectively.", "time": "2021-07-16T23:41:26Z", "link": "http://arxiv.org/abs/2107.08147v1", "id": "2107.08147v1", "title": "AutoFL: Enabling Heterogeneity-Aware Energy Efficient Federated Learning"}
{"author": "Guang Yang, Ke Mu, Chunhe Song, Zhijia Yang, Tierui Gong", "abstract": "Federated learning is a widely used distributed deep learning framework that\nprotects the privacy of each client by exchanging model parameters rather than\nraw data. However, federated learning suffers from high communication costs, as\na considerable number of model parameters need to be transmitted many times\nduring the training process, making the approach inefficient, especially when\nthe communication network bandwidth is limited. This article proposes RingFed,\na novel framework to reduce communication overhead during the training process\nof federated learning. Rather than transmitting parameters between the center\nserver and each client, as in original federated learning, in the proposed\nRingFed, the updated parameters are transmitted between each client in turn,\nand only the final result is transmitted to the central server, thereby\nreducing the communication overhead substantially. After several local updates,\nclients first send their parameters to another proximal client, not to the\ncenter server directly, to preaggregate. Experiments on two different public\ndatasets show that RingFed has fast convergence, high model accuracy, and low\ncommunication cost.", "time": "2021-07-19T13:43:10Z", "link": "http://arxiv.org/abs/2107.08873v1", "id": "2107.08873v1", "title": "RingFed: Reducing Communication Costs in Federated Learning on Non-IID\n  Data"}
{"author": "Kostas Kolomvatsos, Christos Anagnostopoulos", "abstract": "The combination of the infrastructure provided by the Internet of Things\n(IoT) with numerous processing nodes present at the Edge Computing (EC)\necosystem opens up new pathways to support intelligent applications. Such\napplications can be provided upon humongous volumes of data collected by IoT\ndevices being transferred to the edge nodes through the network. Various\nprocessing activities can be performed on the discussed data and multiple\ncollaborative opportunities between EC nodes can facilitate the execution of\nthe desired tasks. In order to support an effective interaction between edge\nnodes, the knowledge about the geographically distributed data should be\nshared. Obviously, the migration of large amounts of data will harm the\nstability of the network stability and its performance. In this paper, we\nrecommend the exchange of data synopses than real data between EC nodes to\nprovide them with the necessary knowledge about peer nodes owning similar data.\nThis knowledge can be valuable when considering decisions such as data/service\nmigration and tasks offloading. We describe an continuous reasoning model that\nbuilds a temporal similarity map of the available datasets to get nodes\nunderstanding the evolution of data in their peers. We support the proposed\ndecision making mechanism through an intelligent similarity extraction scheme\nbased on an unsupervised machine learning model, and, at the same time, combine\nit with a statistical measure that represents the trend of the so-called\ndiscrepancy quantum. Our model can reveal the differences in the exchanged\nsynopses and provide a datasets similarity map which becomes the appropriate\nknowledge base to support the desired processing activities. We present the\nproblem under consideration and suggest a solution for that, while, at the same\ntime, we reveal its advantages and disadvantages through a large number of\nexperiments.", "time": "2021-07-22T10:22:37Z", "link": "http://arxiv.org/abs/2107.10558v1", "id": "2107.10558v1", "title": "A Proactive Management Scheme for Data Synopses at the Edge"}
{"author": "Muhammad Asad, Ahmed Moustafa, Takayuki Ito", "abstract": "In the past few decades, machine learning has revolutionized data processing\nfor large scale applications. Simultaneously, increasing privacy threats in\ntrending applications led to the redesign of classical data training models. In\nparticular, classical machine learning involves centralized data training,\nwhere the data is gathered, and the entire training process executes at the\ncentral server. Despite significant convergence, this training involves several\nprivacy threats on participants' data when shared with the central cloud\nserver. To this end, federated learning has achieved significant importance\nover distributed data training. In particular, the federated learning allows\nparticipants to collaboratively train the local models on local data without\nrevealing their sensitive information to the central cloud server. In this\npaper, we perform a convergence comparison between classical machine learning\nand federated learning on two publicly available datasets, namely,\nlogistic-regression-MNIST dataset and image-classification-CIFAR-10 dataset.\nThe simulation results demonstrate that federated learning achieves higher\nconvergence within limited communication rounds while maintaining participants'\nanonymity. We hope that this research will show the benefits and help federated\nlearning to be implemented widely.", "time": "2021-07-22T17:14:35Z", "link": "http://arxiv.org/abs/2107.10976v1", "id": "2107.10976v1", "title": "Federated Learning Versus Classical Machine Learning: A Convergence\n  Comparison"}
{"author": "Wei Liu, Li Chen, Wenyi Zhang", "abstract": "Decentralized stochastic gradient descent (SGD) is a driving engine for\ndecentralized federated learning (DFL). The performance of decentralized SGD is\njointly influenced by inter-node communications and local updates. In this\npaper, we propose a general DFL framework, which implements both multiple local\nupdates and multiple inter-node communications periodically, to strike a\nbalance between communication efficiency and model consensus. It can provide a\ngeneral decentralized SGD analytical framework. We establish strong convergence\nguarantees for the proposed DFL algorithm without the assumption of convex\nobjectives. The convergence rate of DFL can be optimized to achieve the balance\nof communication and computing costs under constrained resources. For improving\ncommunication efficiency of DFL, compressed communication is further introduced\nto the proposed DFL as a new scheme, named DFL with compressed communication\n(C-DFL). The proposed C-DFL exhibits linear convergence for strongly convex\nobjectives. Experiment results based on MNIST and CIFAR-10 datasets illustrate\nthe superiority of DFL over traditional decentralized SGD methods and show that\nC-DFL further enhances communication efficiency.", "time": "2022-02-11T04:15:35Z", "link": "http://arxiv.org/abs/2107.12048v4", "id": "2107.12048v4", "title": "Decentralized Federated Learning: Balancing Communication and Computing\n  Costs"}
{"author": "Dominik Scheinert, Lauritz Thamsen, Houkun Zhu, Jonathan Will, Alexander Acker, Thorsten Wittkopp, Odej Kao", "abstract": "Distributed dataflow systems enable the use of clusters for scalable data\nanalytics. However, selecting appropriate cluster resources for a processing\njob is often not straightforward. Performance models trained on historical\nexecutions of a concrete job are helpful in such situations, yet they are\nusually bound to a specific job execution context (e.g. node type, software\nversions, job parameters) due to the few considered input parameters. Even in\ncase of slight context changes, such supportive models need to be retrained and\ncannot benefit from historical execution data from related contexts.\n  This paper presents Bellamy, a novel modeling approach that combines\nscale-outs, dataset sizes, and runtimes with additional descriptive properties\nof a dataflow job. It is thereby able to capture the context of a job\nexecution. Moreover, Bellamy is realizing a two-step modeling approach. First,\na general model is trained on all the available data for a specific scalable\nanalytics algorithm, hereby incorporating data from different contexts.\nSubsequently, the general model is optimized for the specific situation at\nhand, based on the available data for the concrete context. We evaluate our\napproach on two publicly available datasets consisting of execution data from\nvarious dataflow jobs carried out in different environments, showing that\nBellamy outperforms state-of-the-art methods.", "time": "2021-10-17T18:32:09Z", "link": "http://arxiv.org/abs/2107.13921v2", "id": "2107.13921v2", "title": "Bellamy: Reusing Performance Models for Distributed Dataflow Jobs Across\n  Contexts"}
{"author": "Firas Hamze", "abstract": "We present a methodology for parallel acceleration of learning in the\npresence of matrix orthogonality and unitarity constraints of interest in\nseveral branches of machine learning. We show how an apparently sequential\nelementary rotation parametrization can be restructured into blocks of\ncommutative operations using a well-known tool for coloring the edges of\ncomplete graphs, in turn widely applied to schedule round-robin\n(all-against-all) sports tournaments. The resulting decomposition admits an\nalgorithm to compute a fully-parametrized orthogonal matrix from its rotation\nparameters in $O(n)$ sequential steps and one to compute the gradient of a\ntraining loss with respect to its parameters in $O(n\\log n)$ steps. We discuss\nparametric restrictions of interest to generative modeling and present\npromising performance results with a prototype GPU implementation.", "time": "2021-05-30T00:47:03Z", "link": "http://arxiv.org/abs/2106.00003v1", "id": "2106.00003v1", "title": "Parallelized Computation and Backpropagation Under Angle-Parametrized\n  Orthogonal Matrices"}
{"author": "Antonello Rosato, Massimo Panella, Denis Kleyko", "abstract": "In the supervised learning domain, considering the recent prevalence of\nalgorithms with high computational cost, the attention is steering towards\nsimpler, lighter, and less computationally extensive training and inference\napproaches. In particular, randomized algorithms are currently having a\nresurgence, given their generalized elementary approach. By using randomized\nneural networks, we study distributed classification, which can be employed in\nsituations were data cannot be stored at a central location nor shared. We\npropose a more efficient solution for distributed classification by making use\nof a lossy compression approach applied when sharing the local classifiers with\nother agents. This approach originates from the framework of hyperdimensional\ncomputing, and is adapted herein. The results of experiments on a collection of\ndatasets demonstrate that the proposed approach has usually higher accuracy\nthan local classifiers and getting close to the benchmark - the centralized\nclassifier. This work can be considered as the first step towards analyzing the\nvariegated horizon of distributed randomized neural networks.", "time": "2021-06-02T01:33:56Z", "link": "http://arxiv.org/abs/2106.00881v1", "id": "2106.00881v1", "title": "Hyperdimensional Computing for Efficient Distributed Classification with\n  Randomized Neural Networks"}
{"author": "Yongchao Liu, Houyi Li, Guowei Zhang, Xintan Zeng, Yongyong Li, Bin Huang, Peng Zhang, Zhao Li, Xiaowei Zhu, Changhua He, Wenguang Chen", "abstract": "Graph neural networks (GNNs) have been demonstrated as a powerful tool for\nanalyzing non-Euclidean graph data. However, the lack of efficient distributed\ngraph learning systems severely hinders applications of GNNs, especially when\ngraphs are big and GNNs are relatively deep. Herein, we present GraphTheta, the\nfirst distributed and scalable graph learning system built upon vertex-centric\ndistributed graph processing with neural network operators implemented as\nuser-defined functions. This system supports multiple training strategies and\nenables efficient and scalable big-graph learning on distributed (virtual)\nmachines with low memory. To facilitate graph convolutions, GraphTheta puts\nforward a new graph learning abstraction named NN-TGAR to bridge the gap\nbetween graph processing and graph deep learning. A distributed graph engine is\nproposed to conduct the stochastic gradient descent optimization with a\nhybrid-parallel execution, and a new cluster-batched training strategy is\nsupported. We evaluate GraphTheta using several datasets with network sizes\nranging from small-, modest- to large-scale. Experimental results show that\nGraphTheta can scale well to 1,024 workers for training an in-house developed\nGNN on an industry-scale Alipay dataset of 1.4 billion nodes and 4.1 billion\nattributed edges, with a cluster of CPU virtual machines (dockers) of small\nmemory each (5$\\sim$12GB). Moreover, GraphTheta can outperform DistDGL by up to\n$2.02\\times$, with better scalability, and GraphLearn by up to $30.56\\times$.\nAs for model accuracy, GraphTheta is capable of learning as good GNNs as\nexisting frameworks. To the best of our knowledge, this work presents the\nlargest edge-attributed GNN learning task in the literature.", "time": "2023-01-17T01:25:47Z", "link": "http://arxiv.org/abs/2104.10569v3", "id": "2104.10569v3", "title": "GraphTheta: A Distributed Graph Neural Network Learning System With\n  Flexible Training Strategy"}
{"author": "Chien-Yu Lin, Liang Luo, Luis Ceze", "abstract": "Graph neural networks (GNNs), an emerging deep learning model class, can\nextract meaningful representations from highly expressive graph-structured data\nand are therefore gaining popularity for wider ranges of applications. However,\ncurrent GNNs suffer from the poor performance of their sparse-dense matrix\nmultiplication (SpMM) operator, even when using powerful GPUs. Our analysis\nshows that 95% of the inference time could be spent on SpMM when running\npopular GNN models on NVIDIA's advanced V100 GPU. Such SpMM performance\nbottleneck hinders GNNs' applicability to large-scale problems or the\ndevelopment of more sophisticated GNN models. To address this inference time\nbottleneck, we introduce ES-SpMM, a cache-first edge sampling mechanism and\ncodesigned SpMM kernel. ES-SpMM uses edge sampling to downsize the graph to fit\ninto GPU's shared memory. It thus reduces the computation cost and improves\nSpMM's cache locality. To evaluate ES-SpMM's performance, we integrated it with\na popular GNN framework, DGL, and tested it using representative GNN models and\ndatasets. Our results show that ES-SpMM outperforms the highly optimized\ncuSPARSE SpMM kernel by up to 4.35x with no accuracy loss and by 45.3x with\nless than a 1% accuracy loss.", "time": "2021-04-23T20:50:10Z", "link": "http://arxiv.org/abs/2104.10716v2", "id": "2104.10716v2", "title": "Accelerating SpMM Kernel with Cache-First Edge Sampling for Graph Neural\n  Networks"}
{"author": "Kun Li, Liang Yuan, Yunquan Zhang, Gongwei Chen", "abstract": "As the data size in Machine Learning fields grows exponentially, it is\ninevitable to accelerate the computation by utilizing the ever-growing large\nnumber of available cores provided by high-performance computing hardware.\nHowever, existing parallel methods for clustering or regression often suffer\nfrom problems of low accuracy, slow convergence, and complex\nhyperparameter-tuning. Furthermore, the parallel efficiency is usually\ndifficult to improve while striking a balance between preserving model\nproperties and partitioning computing workloads on distributed systems. In this\npaper, we propose a novel and simple data structure capturing the most\nimportant information among data samples. It has several advantageous\nproperties supporting a hierarchical clustering strategy that is irrelevant to\nthe hardware parallelism, well-defined metrics for determining optimal\nclustering, balanced partition for maintaining the compactness property, and\nefficient parallelization for accelerating computation phases. Then we combine\nthe clustering with regression techniques as a parallel library and utilize a\nhybrid structure of data and model parallelism to make predictions. Experiments\nillustrate that our library obtains remarkable performance on convergence,\naccuracy, and scalability.", "time": "2021-04-22T01:34:29Z", "link": "http://arxiv.org/abs/2104.10819v1", "id": "2104.10819v1", "title": "An Accurate and Efficient Large-scale Regression Method through Best\n  Friend Clustering"}
{"author": "Shradha Shinde, Jay Joshi, Sowmya Mareedu, Yeon Pyo Kim, Jongwook Woo", "abstract": "COVID 19 is an acute disease that started spreading throughout the world,\nbeginning in December 2019. It has spread worldwide and has affected more than\n7 million people, and 200 thousand people have died due to this infection as of\nOct 2020. In this paper, we have forecasted the number of deaths and the\nconfirmed cases in Los Angeles and New York of the United States using the\ntraditional and Big Data platforms based on the Times Series: ARIMA and ETS. We\nalso implemented a more sophisticated time-series forecast model using Facebook\nProphet API. Furthermore, we developed the classification models: Logistic\nRegression and Random Forest regression to show that the Weather does not\naffect the number of the confirmed cases. The models are built and run in\nlegacy systems (Azure ML Studio) and Big Data systems (Oracle Cloud and\nDatabricks). Besides, we present the accuracy of the models.", "time": "2021-04-22T23:08:13Z", "link": "http://arxiv.org/abs/2104.11349v1", "id": "2104.11349v1", "title": "Scalable Predictive Time-Series Analysis of COVID-19: Cases and\n  Fatalities"}
{"author": "Tai Le Quy, Arjun Roy, Gunnar Friege, Eirini Ntoutsi", "abstract": "Traditionally, clustering algorithms focus on partitioning the data into\ngroups of similar instances. The similarity objective, however, is not\nsufficient in applications where a fair-representation of the groups in terms\nof protected attributes like gender or race, is required for each cluster.\nMoreover, in many applications, to make the clusters useful for the end-user, a\nbalanced cardinality among the clusters is required. Our motivation comes from\nthe education domain where studies indicate that students might learn better in\ndiverse student groups and of course groups of similar cardinality are more\npractical e.g., for group assignments. To this end, we introduce the\nfair-capacitated clustering problem that partitions the data into clusters of\nsimilar instances while ensuring cluster fairness and balancing cluster\ncardinalities. We propose a two-step solution to the problem: i) we rely on\nfairlets to generate minimal sets that satisfy the fair constraint and ii) we\npropose two approaches, namely hierarchical clustering and partitioning-based\nclustering, to obtain the fair-capacitated clustering. The hierarchical\napproach embeds the additional cardinality requirements during the merging step\nwhile the partitioning-based one alters the assignment step using a knapsack\nproblem formulation to satisfy the additional requirements. Our experiments on\nfour educational datasets show that our approaches deliver well-balanced\nclusters in terms of both fairness and cardinality while maintaining a good\nclustering quality.", "time": "2021-04-28T07:24:28Z", "link": "http://arxiv.org/abs/2104.12116v2", "id": "2104.12116v2", "title": "Fair-Capacitated Clustering"}
{"author": "Zhefeng Qiao, Xianghao Yu, Jun Zhang, Khaled B. Letaief", "abstract": "Federated learning (FL) is a promising and powerful approach for training\ndeep learning models without sharing the raw data of clients. During the\ntraining process of FL, the central server and distributed clients need to\nexchange a vast amount of model information periodically. To address the\nchallenge of communication-intensive training, we propose a new training\nmethod, referred to as federated learning with dual-side low-rank compression\n(FedDLR), where the deep learning model is compressed via low-rank\napproximations at both the server and client sides. The proposed FedDLR not\nonly reduces the communication overhead during the training stage but also\ndirectly generates a compact model to speed up the inference process. We shall\nprovide convergence analysis, investigate the influence of the key parameters,\nand empirically show that FedDLR outperforms the state-of-the-art solutions in\nterms of both the communication and computation efficiency.", "time": "2021-04-26T09:13:31Z", "link": "http://arxiv.org/abs/2104.12416v1", "id": "2104.12416v1", "title": "Communication-Efficient Federated Learning with Dual-Side Low-Rank\n  Compression"}
{"author": "Aman Priyanshu, Mudit Sinha, Shreyans Mehta", "abstract": "Social media platforms such as Twitter, Facebook etc can be utilised as an\nimportant source of information during disaster events. This information can be\nused for disaster response and crisis management if processed accurately and\nquickly. However, the data present in such situations is ever-changing, and\nusing considerable resources during such a crisis is not feasible. Therefore,\nwe have to develop a low resource and continually learning system that\nincorporates text classification models which are robust against noisy and\nunordered data. We utilised Distributed learning which enabled us to learn on\nresource-constrained devices, then to alleviate catastrophic forgetting in our\ntarget neural networks we utilized regularization. We then applied federated\naveraging for distributed learning and to aggregate the central model for\ncontinual learning.", "time": "2021-07-01T05:19:29Z", "link": "http://arxiv.org/abs/2104.12876v2", "id": "2104.12876v2", "title": "Continual Distributed Learning for Crisis Management"}
{"author": "Yue Tan, Guodong Long, Lu Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, Chengqi Zhang", "abstract": "Heterogeneity across clients in federated learning (FL) usually hinders the\noptimization convergence and generalization performance when the aggregation of\nclients' knowledge occurs in the gradient space. For example, clients may\ndiffer in terms of data distribution, network latency, input/output space,\nand/or model architecture, which can easily lead to the misalignment of their\nlocal gradients. To improve the tolerance to heterogeneity, we propose a novel\nfederated prototype learning (FedProto) framework in which the clients and\nserver communicate the abstract class prototypes instead of the gradients.\nFedProto aggregates the local prototypes collected from different clients, and\nthen sends the global prototypes back to all clients to regularize the training\nof local models. The training on each client aims to minimize the\nclassification error on the local data while keeping the resulting local\nprototypes sufficiently close to the corresponding global ones. Moreover, we\nprovide a theoretical analysis to the convergence rate of FedProto under\nnon-convex objectives. In experiments, we propose a benchmark setting tailored\nfor heterogeneous FL, with FedProto outperforming several recent FL approaches\non multiple datasets.", "time": "2022-03-05T01:09:14Z", "link": "http://arxiv.org/abs/2105.00243v4", "id": "2105.00243v4", "title": "FedProto: Federated Prototype Learning across Heterogeneous Clients"}
{"author": "Saeed Vahidian, Mahdi Morafah, Bill Lin", "abstract": "The traditional approach in FL tries to learn a single global model\ncollaboratively with the help of many clients under the orchestration of a\ncentral server. However, learning a single global model might not work well for\nall clients participating in the FL under data heterogeneity. Therefore, the\npersonalization of the global model becomes crucial in handling the challenges\nthat arise with statistical heterogeneity and the non-IID distribution of data.\nUnlike prior works, in this work we propose a new approach for obtaining a\npersonalized model from a client-level objective. This further motivates all\nclients to participate in federation even under statistical heterogeneity in\norder to improve their performance, instead of merely being a source of data\nand model training for the central server. To realize this personalization, we\nleverage finding a small subnetwork for each client by applying hybrid pruning\n(combination of structured and unstructured pruning), and unstructured pruning.\nThrough a range of experiments on different benchmarks, we observed that the\nclients with similar data (labels) share similar personal parameters. By\nfinding a subnetwork for each client ...", "time": "2021-05-10T00:43:37Z", "link": "http://arxiv.org/abs/2105.00562v2", "id": "2105.00562v2", "title": "Personalized Federated Learning by Structured and Unstructured Pruning\n  under Data Heterogeneity"}
{"author": "Pavlos S. Bouzinis, Panagiotis D. Diamantoulakis, George K. Karagiannidis", "abstract": "Conventional machine learning techniques are conducted in a centralized\nmanner. Recently, the massive volume of generated wireless data, the privacy\nconcerns and the increasing computing capabilities of wireless end-devices have\nled to the emergence of a promising decentralized solution, termed as Wireless\nFederated Learning (WFL). In this first of the two parts paper, we present the\napplication of WFL in the sixth generation of wireless networks (6G), which is\nenvisioned to be an integrated communication and computing platform. After\nanalyzing the key concepts of WFL, we discuss the core challenges of WFL\nimposed by the wireless (or mobile communication) environment. Finally, we shed\nlight to the future directions of WFL, aiming to compose a constructive\nintegration of FL into the future wireless networks.", "time": "2021-04-24T18:10:05Z", "link": "http://arxiv.org/abs/2105.00842v1", "id": "2105.00842v1", "title": "Wireless Federated Learning (WFL) for 6G Networks -- Part I: Research\n  Challenges and Future Trends"}
{"author": "Xin Zhang, Jia Liu, Zhengyuan Zhu, Elizabeth S. Bentley", "abstract": "Decentralized nonconvex optimization has received increasing attention in\nrecent years in machine learning due to its advantages in system robustness,\ndata privacy, and implementation simplicity. However, three fundamental\nchallenges in designing decentralized optimization algorithms are how to reduce\ntheir sample, communication, and memory complexities. In this paper, we propose\na \\underline{g}radient-\\underline{t}racking-based \\underline{sto}chastic\n\\underline{r}ecursive \\underline{m}omentum (GT-STORM) algorithm for efficiently\nsolving nonconvex optimization problems. We show that to reach an\n$\\epsilon^2$-stationary solution, the total number of sample evaluations of our\nalgorithm is $\\tilde{O}(m^{1/2}\\epsilon^{-3})$ and the number of communication\nrounds is $\\tilde{O}(m^{-1/2}\\epsilon^{-3})$, which improve the\n$O(\\epsilon^{-4})$ costs of sample evaluations and communications for the\nexisting decentralized stochastic gradient algorithms. We conduct extensive\nexperiments with a variety of learning models, including non-convex logistical\nregression and convolutional neural networks, to verify our theoretical\nfindings. Collectively, our results contribute to the state of the art of\ntheories and algorithms for decentralized network optimization.", "time": "2021-05-19T04:28:42Z", "link": "http://arxiv.org/abs/2105.01231v2", "id": "2105.01231v2", "title": "GT-STORM: Taming Sample, Communication, and Memory Complexities in\n  Decentralized Non-Convex Learning"}
{"author": "Marco Serafini, Hui Guan", "abstract": "Graph Neural Networks (GNNs) are a new and increasingly popular family of\ndeep neural network architectures to perform learning on graphs. Training them\nefficiently is challenging due to the irregular nature of graph data. The\nproblem becomes even more challenging when scaling to large graphs that exceed\nthe capacity of single devices. Standard approaches to distributed DNN\ntraining, such as data and model parallelism, do not directly apply to GNNs.\nInstead, two different approaches have emerged in the literature: whole-graph\nand sample-based training.\n  In this paper, we review and compare the two approaches. Scalability is\nchallenging with both approaches, but we make a case that research should focus\non sample-based training since it is a more promising approach. Finally, we\nreview recent systems supporting sample-based training.", "time": "2021-05-05T20:44:10Z", "link": "http://arxiv.org/abs/2105.02315v1", "id": "2105.02315v1", "title": "Scalable Graph Neural Network Training: The Case for Sampling"}
{"author": "Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman, Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang, Noam Shazeer, Shibo Wang, Tao Wang, Yonghui Wu, Zhifeng Chen", "abstract": "We present GSPMD, an automatic, compiler-based parallelization system for\ncommon machine learning computations. It allows users to write programs in the\nsame way as for a single device, then give hints through a few annotations on\nhow to distribute tensors, based on which GSPMD will parallelize the\ncomputation. Its representation of partitioning is simple yet general, allowing\nit to express different or mixed paradigms of parallelism on a wide variety of\nmodels.\n  GSPMD infers the partitioning for every operator based on limited user\nannotations, making it convenient to scale existing single-device programs. It\nsolves several technical challenges for production usage, allowing GSPMD to\nachieve 50% to 62% compute utilization on up to 2048 Cloud TPUv3 cores for\nmodels with up to one trillion parameters.", "time": "2021-12-23T21:29:57Z", "link": "http://arxiv.org/abs/2105.04663v2", "id": "2105.04663v2", "title": "GSPMD: General and Scalable Parallelization for ML Computation Graphs"}
{"author": "Tahseen Khan, Wenhong Tian, Rajkumar Buyya", "abstract": "Cloud computing has rapidly emerged as model for delivering Internet-based\nutility computing services. In cloud computing, Infrastructure as a Service\n(IaaS) is one of the most important and rapidly growing fields. Cloud providers\nprovide users/machines resources such as virtual machines, raw (block) storage,\nfirewalls, load balancers, and network devices in this service model. One of\nthe most important aspects of cloud computing for IaaS is resource management.\nScalability, quality of service, optimum utility, reduced overheads, increased\nthroughput, reduced latency, specialised environment, cost effectiveness, and a\nstreamlined interface are some of the advantages of resource management for\nIaaS in cloud computing. Traditionally, resource management has been done\nthrough static policies, which impose certain limitations in various dynamic\nscenarios, prompting cloud service providers to adopt data-driven,\nmachine-learning-based approaches. Machine learning is being used to handle a\nvariety of resource management tasks, including workload estimation, task\nscheduling, VM consolidation, resource optimization, and energy optimization,\namong others. This paper provides a detailed review of challenges in ML-based\nresource management in current research, as well as current approaches to\nresolve these challenges, as well as their advantages and limitations. Finally,\nwe propose potential future research directions based on identified challenges\nand limitations in current research.", "time": "2021-05-09T08:03:58Z", "link": "http://arxiv.org/abs/2105.05079v1", "id": "2105.05079v1", "title": "Machine Learning (ML)-Centric Resource Management in Cloud Computing: A\n  Review and Future Directions"}
{"author": "Chuan Ma, Jun Li, Ming Ding, Kang Wei, Wen Chen, H. Vincent Poor", "abstract": "Owing to the low communication costs and privacy-promoting capabilities,\nFederated Learning (FL) has become a promising tool for training effective\nmachine learning models among distributed clients. However, with the\ndistributed architecture, low quality models could be uploaded to the\naggregator server by unreliable clients, leading to a degradation or even a\ncollapse of training. In this paper, we model these unreliable behaviors of\nclients and propose a defensive mechanism to mitigate such a security risk.\nSpecifically, we first investigate the impact on the models caused by\nunreliable clients by deriving a convergence upper bound on the loss function\nbased on the gradient descent updates. Our theoretical bounds reveal that with\na fixed amount of total computational resources, there exists an optimal number\nof local training iterations in terms of convergence performance. We further\ndesign a novel defensive mechanism, named deep neural network based secure\naggregation (DeepSA). Our experimental results validate our theoretical\nanalysis. In addition, the effectiveness of DeepSA is verified by comparing\nwith other state-of-the-art defensive mechanisms.", "time": "2021-07-31T12:36:55Z", "link": "http://arxiv.org/abs/2105.06256v2", "id": "2105.06256v2", "title": "Federated Learning with Unreliable Clients: Performance Analysis and\n  Mechanism Design"}
{"author": "G Anthony Reina, Alexey Gruzdev, Patrick Foley, Olga Perepelkina, Mansi Sharma, Igor Davidyuk, Ilya Trushkin, Maksim Radionov, Aleksandr Mokrov, Dmitry Agapov, Jason Martin, Brandon Edwards, Micah J. Sheller, Sarthak Pati, Prakash Narayana Moorthy, Shih-han Wang, Prashant Shah, Spyridon Bakas", "abstract": "Federated learning (FL) is a computational paradigm that enables\norganizations to collaborate on machine learning (ML) projects without sharing\nsensitive data, such as, patient records, financial data, or classified\nsecrets. Open Federated Learning (OpenFL https://github.com/intel/openfl) is an\nopen-source framework for training ML algorithms using the data-private\ncollaborative learning paradigm of FL. OpenFL works with training pipelines\nbuilt with both TensorFlow and PyTorch, and can be easily extended to other ML\nand deep learning frameworks. Here, we summarize the motivation and development\ncharacteristics of OpenFL, with the intention of facilitating its application\nto existing ML model training in a production environment. Finally, we describe\nthe first use of the OpenFL framework to train consensus ML models in a\nconsortium of international healthcare organizations, as well as how it\nfacilitates the first computational competition on FL.", "time": "2021-05-13T16:40:19Z", "link": "http://arxiv.org/abs/2105.06413v1", "id": "2105.06413v1", "title": "OpenFL: An open-source framework for Federated Learning"}
{"author": "Jiawei Jiang, Shaoduo Gan, Yue Liu, Fanlin Wang, Gustavo Alonso, Ana Klimovic, Ankit Singla, Wentao Wu, Ce Zhang", "abstract": "The appeal of serverless (FaaS) has triggered a growing interest on how to\nuse it in data-intensive applications such as ETL, query processing, or machine\nlearning (ML). Several systems exist for training large-scale ML models on top\nof serverless infrastructures (e.g., AWS Lambda) but with inconclusive results\nin terms of their performance and relative advantage over \"serverful\"\ninfrastructures (IaaS). In this paper we present a systematic, comparative\nstudy of distributed ML training over FaaS and IaaS. We present a design space\ncovering design choices such as optimization algorithms and synchronization\nprotocols, and implement a platform, LambdaML, that enables a fair comparison\nbetween FaaS and IaaS. We present experimental results using LambdaML, and\nfurther develop an analytic model to capture cost/performance tradeoffs that\nmust be considered when opting for a serverless infrastructure. Our results\nindicate that ML training pays off in serverless only for models with efficient\n(i.e., reduced) communication and that quickly converge. In general, FaaS can\nbe much faster but it is never significantly cheaper than IaaS.", "time": "2021-05-17T13:19:23Z", "link": "http://arxiv.org/abs/2105.07806v1", "id": "2105.07806v1", "title": "Towards Demystifying Serverless Machine Learning Training"}
{"author": "Yuanming Li, Huaizheng Zhang, Shanshan Jiang, Fan Yang, Yonggang Wen, Yong Luo", "abstract": "AI engineering has emerged as a crucial discipline to democratize deep neural\nnetwork (DNN) models among software developers with a diverse background. In\nparticular, altering these DNN models in the deployment stage posits a\ntremendous challenge. In this research, we propose and develop a low-code\nsolution, ModelPS (an acronym for \"Model Photoshop\"), to enable and empower\ncollaborative DNN model editing and intelligent model serving. The ModelPS\nsolution embodies two transformative features: 1) a user-friendly web interface\nfor a developer team to share and edit DNN models pictorially, in a low-code\nfashion, and 2) a model genie engine in the backend to aid developers in\ncustomizing model editing configurations for given deployment requirements or\nconstraints. Our case studies with a wide range of deep learning (DL) models\nshow that the system can tremendously reduce both development and communication\noverheads with improved productivity.", "time": "2021-08-16T08:44:31Z", "link": "http://arxiv.org/abs/2105.08275v3", "id": "2105.08275v3", "title": "ModelPS: An Interactive and Collaborative Platform for Editing\n  Pre-trained Models at Scale"}
{"author": "Yiming Chen, Kun Yuan, Yingya Zhang, Pan Pan, Yinghui Xu, Wotao Yin", "abstract": "Communication overhead hinders the scalability of large-scale distributed\ntraining. Gossip SGD, where each node averages only with its neighbors, is more\ncommunication-efficient than the prevalent parallel SGD. However, its\nconvergence rate is reversely proportional to quantity $1-\\beta$ which measures\nthe network connectivity. On large and sparse networks where $1-\\beta \\to 0$,\nGossip SGD requires more iterations to converge, which offsets against its\ncommunication benefit. This paper introduces Gossip-PGA, which adds Periodic\nGlobal Averaging into Gossip SGD. Its transient stage, i.e., the iterations\nrequired to reach asymptotic linear speedup stage, improves from\n$\\Omega(\\beta^4 n^3/(1-\\beta)^4)$ to $\\Omega(\\beta^4 n^3 H^4)$ for non-convex\nproblems. The influence of network topology in Gossip-PGA can be controlled by\nthe averaging period $H$. Its transient-stage complexity is also superior to\nLocal SGD which has order $\\Omega(n^3 H^4)$. Empirical results of large-scale\ntraining on image classification (ResNet50) and language modeling (BERT)\nvalidate our theoretical findings.", "time": "2021-05-19T11:59:25Z", "link": "http://arxiv.org/abs/2105.09080v1", "id": "2105.09080v1", "title": "Accelerating Gossip SGD with Periodic Global Averaging"}
{"author": "Zhuangdi Zhu, Junyuan Hong, Jiayu Zhou", "abstract": "Federated Learning (FL) is a decentralized machine-learning paradigm, in\nwhich a global server iteratively averages the model parameters of local users\nwithout accessing their data. User heterogeneity has imposed significant\nchallenges to FL, which can incur drifted global models that are slow to\nconverge. Knowledge Distillation has recently emerged to tackle this issue, by\nrefining the server model using aggregated knowledge from heterogeneous users,\nother than directly averaging their model parameters. This approach, however,\ndepends on a proxy dataset, making it impractical unless such a prerequisite is\nsatisfied. Moreover, the ensemble knowledge is not fully utilized to guide\nlocal model learning, which may in turn affect the quality of the aggregated\nmodel. Inspired by the prior art, we propose a data-free knowledge\ndistillation} approach to address heterogeneous FL, where the server learns a\nlightweight generator to ensemble user information in a data-free manner, which\nis then broadcasted to users, regulating local training using the learned\nknowledge as an inductive bias. Empirical studies powered by theoretical\nimplications show that, our approach facilitates FL with better generalization\nperformance using fewer communication rounds, compared with the\nstate-of-the-art.", "time": "2021-06-09T19:31:35Z", "link": "http://arxiv.org/abs/2105.10056v2", "id": "2105.10056v2", "title": "Data-Free Knowledge Distillation for Heterogeneous Federated Learning"}
{"author": "Haijin Wang, Caomingzhe Si, Junhua Zhao, Guolong Liu, Fushuan Wen", "abstract": "Non-intrusive load monitoring (NILM) is essential for understanding\ncustomer's power consumption patterns and may find wide applications like\ncarbon emission reduction and energy conservation. The training of NILM models\nrequires massive load data containing different types of appliances. However,\ninadequate load data and the risk of power consumer privacy breaches may be\nencountered by local data owners during the NILM model training. To prevent\nsuch potential risks, a novel NILM method named Fed-NILM which is based on\nFederated Learning (FL) is proposed in this paper. In Fed-NILM, local model\nparameters instead of local load data are shared among multiple data owners.\nThe global model is obtained by weighted averaging the parameters. Experiments\nbased on two measured load datasets are conducted to explore the generalization\nability of Fed-NILM. Besides, a comparison of Fed-NILM with locally-trained\nNILMs and the centrally-trained NILM is conducted. The experimental results\nshow that Fed-NILM has superior performance in scalability and convergence.\nFed-NILM outperforms locally-trained NILMs operated by local data owners and\napproximates the centrally-trained NILM which is trained on the entire load\ndataset without privacy protection. The proposed Fed-NILM significantly\nimproves the co-modeling capabilities of local data owners while protecting\npower consumers' privacy.", "time": "2021-06-25T13:04:03Z", "link": "http://arxiv.org/abs/2105.11085v2", "id": "2105.11085v2", "title": "Fed-NILM: A Federated Learning-based Non-Intrusive Load Monitoring\n  Method for Privacy-Protection"}
{"author": "John Thorpe, Yifan Qiao, Jonathan Eyolfson, Shen Teng, Guanzhou Hu, Zhihao Jia, Jinliang Wei, Keval Vora, Ravi Netravali, Miryung Kim, Guoqing Harry Xu", "abstract": "A graph neural network (GNN) enables deep learning on structured graph data.\nThere are two major GNN training obstacles: 1) it relies on high-end servers\nwith many GPUs which are expensive to purchase and maintain, and 2) limited\nmemory on GPUs cannot scale to today's billion-edge graphs. This paper presents\nDorylus: a distributed system for training GNNs. Uniquely, Dorylus can take\nadvantage of serverless computing to increase scalability at a low cost.\n  The key insight guiding our design is computation separation. Computation\nseparation makes it possible to construct a deep, bounded-asynchronous pipeline\nwhere graph and tensor parallel tasks can fully overlap, effectively hiding the\nnetwork latency incurred by Lambdas. With the help of thousands of Lambda\nthreads, Dorylus scales GNN training to billion-edge graphs. Currently, for\nlarge graphs, CPU servers offer the best performance-per-dollar over GPU\nservers. Just using Lambdas on top of CPU servers offers up to 2.75x more\nperformance-per-dollar than training only with CPU servers. Concretely, Dorylus\nis 1.22x faster and 4.83x cheaper than GPU servers for massive sparse graphs.\nDorylus is up to 3.8x faster and 10.7x cheaper compared to existing\nsampling-based systems.", "time": "2021-05-25T01:14:05Z", "link": "http://arxiv.org/abs/2105.11118v2", "id": "2105.11118v2", "title": "Dorylus: Affordable, Scalable, and Accurate GNN Training with\n  Distributed CPU Servers and Serverless Threads"}
{"author": "Shenggui Li, Fuzhao Xue, Chaitanya Baranwal, Yongbin Li, Yang You", "abstract": "Transformer achieves promising results on various tasks. However,\nself-attention suffers from quadratic memory requirements with respect to the\nsequence length. Existing work focuses on reducing time and space complexity\nfrom an algorithm perspective. In this work, we propose sequence parallelism, a\nmemory-efficient parallelism method to help us break input sequence length\nlimitation and train with longer sequences on GPUs efficiently. Our approach is\ncompatible with most existing parallelisms (e.g. data parallelism, pipeline\nparallelism and tensor parallelism), which means our sequence parallelism makes\n4D parallelism possible. More importantly, we no longer require a single device\nto hold the whole sequence. That is, with sparse attention, our sequence\nparallelism enables us to train transformer with infinite long sequence.\nSpecifically, we split the input sequence into multiple chunks and feed each\nchunk into its corresponding device (i.e. GPU). To compute the attention\noutput, we integrated ring-style communication with self-attention calculation\nand proposed Ring Self-Attention (RSA). Experiments show that sequence\nparallelism performs well when scaling with batch size and sequence length.\nCompared with tensor parallelism, our approach achieved $13.7\\times$ and\n$3.0\\times$ maximum batch size and sequence length respectively when scaling up\nto 64 NVIDIA P100 GPUs. With sparse attention, sequence can handle sequence\nwith over 114K tokens, which is over $27\\times$ longer than existing sparse\nattention works holding the whole sequence on a single device.", "time": "2022-05-21T06:03:54Z", "link": "http://arxiv.org/abs/2105.13120v3", "id": "2105.13120v3", "title": "Sequence Parallelism: Long Sequence Training from System Perspective"}
{"author": "Jia Yan, Suzhi Bi, Ying-Jun Angela Zhang", "abstract": "Device-edge co-inference opens up new possibilities for resource-constrained\nwireless devices (WDs) to execute deep neural network (DNN)-based applications\nwith heavy computation workloads. In particular, the WD executes the first few\nlayers of the DNN and sends the intermediate features to the edge server that\nprocesses the remaining layers of the DNN. By adapting the model splitting\ndecision, there exists a tradeoff between local computation cost and\ncommunication overhead. In practice, the DNN model is re-trained and updated\nperiodically at the edge server. Once the DNN parameters are regenerated, part\nof the updated model must be placed at the WD to facilitate on-device\ninference. In this paper, we study the joint optimization of the model\nplacement and online model splitting decisions to minimize the energy-and-time\ncost of device-edge co-inference in presence of wireless channel fading. The\nproblem is challenging because the model placement and model splitting\ndecisions are strongly coupled, while involving two different time scales. We\nfirst tackle online model splitting by formulating an optimal stopping problem,\nwhere the finite horizon of the problem is determined by the model placement\ndecision. In addition to deriving the optimal model splitting rule based on\nbackward induction, we further investigate a simple one-stage look-ahead rule,\nfor which we are able to obtain analytical expressions of the model splitting\ndecision. The analysis is useful for us to efficiently optimize the model\nplacement decision in a larger time scale. In particular, we obtain a\nclosed-form model placement solution for the fully-connected multilayer\nperceptron with equal neurons. Simulation results validate the superior\nperformance of the joint optimal model placement and splitting with various DNN\nstructures.", "time": "2021-05-28T06:55:04Z", "link": "http://arxiv.org/abs/2105.13618v1", "id": "2105.13618v1", "title": "Optimal Model Placement and Online Model Splitting for Device-Edge\n  Co-Inference"}
{"author": "Guangye Chen, Luis ChacÃ³n, Truong B. Nguyen", "abstract": "We propose an unsupervised machine-learning checkpoint-restart (CR) lossy\nalgorithm for particle-in-cell (PIC) algorithms using Gaussian mixtures (GM).\nThe algorithm features a particle compression stage and a particle\nreconstruction stage, where a continuum particle distribution function is\nconstructed and resampled, respectively. To guarantee fidelity of the CR\nprocess, we ensure the exact preservation of charge, momentum, and energy for\nboth compression and reconstruction stages, everywhere on the mesh. We also\nensure the preservation of Gauss' law after particle reconstruction. As a\nresult, the GM CR algorithm is shown to provide a clean, conservative restart\ncapability while potentially affording orders of magnitude savings in\ninput/output requirements. We demonstrate the algorithm using a recently\ndeveloped exactly energy- and charge-conserving PIC algorithm on physical\nproblems of interest, with compression factors $\\gtrsim75$ with no appreciable\nimpact on the quality of the restarted dynamics.", "time": "2021-03-17T01:38:02Z", "link": "http://arxiv.org/abs/2105.13797v1", "id": "2105.13797v1", "title": "An unsupervised machine-learning checkpoint-restart algorithm using\n  Gaussian mixtures for particle-in-cell simulations"}
{"author": "Jiahao Xie, Chao Zhang, Zebang Shen, Weijie Liu, Hui Qian", "abstract": "Minimax problems arise in a wide range of important applications including\nrobust adversarial learning and Generative Adversarial Network (GAN) training.\nRecently, algorithms for minimax problems in the Federated Learning (FL)\nparadigm have received considerable interest. Existing federated algorithms for\ngeneral minimax problems require the full aggregation (i.e., aggregation of\nlocal model information from all clients) in each training round. Thus, they\nare inapplicable to an important setting of FL known as the cross-device\nsetting, which involves numerous unreliable mobile/IoT devices. In this paper,\nwe develop the first practical algorithm named CDMA for general minimax\nproblems in the cross-device FL setting. CDMA is based on a\nStart-Immediately-With-Enough-Responses mechanism, in which the server first\nsignals a subset of clients to perform local computation and then starts to\naggregate the local results reported by clients once it receives responses from\nenough clients in each round. With this mechanism, CDMA is resilient to the low\nclient availability. In addition, CDMA is incorporated with a lightweight\nglobal correction in the local update steps of clients, which mitigates the\nimpact of slow network connections. We establish theoretical guarantees of CDMA\nunder different choices of hyperparameters and conduct experiments on AUC\nmaximization, robust adversarial network training, and GAN training tasks.\nTheoretical and experimental results demonstrate the efficiency of CDMA.", "time": "2023-06-29T02:41:05Z", "link": "http://arxiv.org/abs/2105.14216v4", "id": "2105.14216v4", "title": "CDMA: A Practical Cross-Device Federated Learning Algorithm for General\n  Minimax Problems"}
{"author": "Xubo Yue, Maher Nouiehed, Raed Al Kontar", "abstract": "In this paper we propose \\texttt{GIFAIR-FL}: a framework that imposes\n\\textbf{G}roup and \\textbf{I}ndividual \\textbf{FAIR}ness to \\textbf{F}ederated\n\\textbf{L}earning settings. By adding a regularization term, our algorithm\npenalizes the spread in the loss of client groups to drive the optimizer to\nfair solutions. Our framework \\texttt{GIFAIR-FL} can accommodate both global\nand personalized settings. Theoretically, we show convergence in non-convex and\nstrongly convex settings. Our convergence guarantees hold for both $i.i.d.$ and\nnon-$i.i.d.$ data. To demonstrate the empirical performance of our algorithm,\nwe apply our method to image classification and text prediction tasks. Compared\nto existing algorithms, our method shows improved fairness results while\nretaining superior or similar prediction accuracy.", "time": "2022-03-08T00:30:52Z", "link": "http://arxiv.org/abs/2108.02741v2", "id": "2108.02741v2", "title": "GIFAIR-FL: A Framework for Group and Individual Fairness in Federated\n  Learning"}
{"author": "Yuwei Sun, Hideya Ochiai, Hiroshi Esaki", "abstract": "Wider coverage and a better solution to a latency reduction in 5G necessitate\nits combination with multi-access edge computing (MEC) technology.\nDecentralized deep learning (DDL) such as federated learning and swarm learning\nas a promising solution to privacy-preserving data processing for millions of\nsmart edge devices, leverages distributed computing of multi-layer neural\nnetworks within the networking of local clients, whereas, without disclosing\nthe original local training data. Notably, in industries such as finance and\nhealthcare where sensitive data of transactions and personal medical records is\ncautiously maintained, DDL can facilitate the collaboration among these\ninstitutes to improve the performance of trained models while protecting the\ndata privacy of participating clients. In this survey paper, we demonstrate the\ntechnical fundamentals of DDL that benefit many walks of society through\ndecentralized learning. Furthermore, we offer a comprehensive overview of the\ncurrent state-of-the-art in the field by outlining the challenges of DDL and\nthe most relevant solutions from novel perspectives of communication efficiency\nand trustworthiness.", "time": "2021-12-06T11:54:57Z", "link": "http://arxiv.org/abs/2108.03980v4", "id": "2108.03980v4", "title": "Decentralized Deep Learning for Multi-Access Edge Computing: A Survey on\n  Communication Efficiency and Trustworthiness"}
{"author": "Mengmeng Tian, Yuxin Chen, Yuan Liu, Zehui Xiong, Cyril Leung, Chunyan Miao", "abstract": "Federated learning (FL) serves as a data privacy-preserved machine learning\nparadigm, and realizes the collaborative model trained by distributed clients.\nTo accomplish an FL task, the task publisher needs to pay financial incentives\nto the FL server and FL server offloads the task to the contributing FL\nclients. It is challenging to design proper incentives for the FL clients due\nto the fact that the task is privately trained by the clients. This paper aims\nto propose a contract theory based FL task training model towards minimizing\nincentive budget subject to clients being individually rational (IR) and\nincentive compatible (IC) in each FL training round. We design a\ntwo-dimensional contract model by formally defining two private types of\nclients, namely data quality and computation effort. To effectively aggregate\nthe trained models, a contract-based aggregator is proposed. We analyze the\nfeasible and optimal contract solutions to the proposed contract model.\n%Experimental results demonstrate that the proposed framework and contract\nmodel can effective improve the generation accuracy of FL tasks. Experimental\nresults show that the generalization accuracy of the FL tasks can be improved\nby the proposed incentive mechanism where contract-based aggregation is\napplied.", "time": "2021-08-12T07:30:42Z", "link": "http://arxiv.org/abs/2108.05568v1", "id": "2108.05568v1", "title": "A Contract Theory based Incentive Mechanism for Federated Learning"}
{"author": "Zihan Chen, Kai Fong Ernest Chong, Tony Q. S. Quek", "abstract": "Federated learning (FL) offers a solution to train a global machine learning\nmodel while still maintaining data privacy, without needing access to data\nstored locally at the clients. However, FL suffers performance degradation when\nclient data distribution is non-IID, and a longer training duration to combat\nthis degradation may not necessarily be feasible due to communication\nlimitations. To address this challenge, we propose a new adaptive training\nalgorithm $\\texttt{AdaFL}$, which comprises two components: (i) an\nattention-based client selection mechanism for a fairer training scheme among\nthe clients; and (ii) a dynamic fraction method to balance the trade-off\nbetween performance stability and communication efficiency. Experimental\nresults show that our $\\texttt{AdaFL}$ algorithm outperforms the usual\n$\\texttt{FedAvg}$ algorithm, and can be incorporated to further improve various\nstate-of-the-art FL algorithms, with respect to three aspects: model accuracy,\nperformance stability, and communication efficiency.", "time": "2021-08-12T14:18:05Z", "link": "http://arxiv.org/abs/2108.05765v1", "id": "2108.05765v1", "title": "Dynamic Attention-based Communication-Efficient Federated Learning"}
{"author": "Jiarui Fang, Zilin Zhu, Shenggui Li, Hui Su, Yang Yu, Jie Zhou, Yang You", "abstract": "The pre-trained model (PTM) is revolutionizing Artificial Intelligence (AI)\ntechnology. However, the hardware requirement of PTM training is prohibitively\nhigh, making it a game for a small proportion of people. Therefore, we proposed\nPatrickStar system to lower the hardware requirements of PTMs and make them\naccessible to everyone. PatrickStar uses the CPU-GPU heterogeneous memory space\nto store the model data. Different from existing works, we organize the model\ndata in memory chunks and dynamically distribute them in the heterogeneous\nmemory. Guided by the runtime memory statistics collected in a warm-up\niteration, chunks are orchestrated efficiently in heterogeneous memory and\ngenerate lower CPU-GPU data transmission volume and higher bandwidth\nutilization. Symbiosis with the Zero Redundancy Optimizer, PatrickStar scales\nto multiple GPUs on multiple nodes. % using data parallelism. The system can\ntrain tasks on bigger models and larger batch sizes, which cannot be\naccomplished by existing works. Experimental results show that PatrickStar\nextends model scales 2.27 and 2.5 times of DeepSpeed, and consistently exhibits\nsignificantly higher execution speed. PatricStar also successfully runs the\n175B GPT3 training task on a 32 GPU cluster. Our code is publicly available at\nhttps://github.com/Tencent/PatrickStar.", "time": "2022-08-01T07:17:56Z", "link": "http://arxiv.org/abs/2108.05818v4", "id": "2108.05818v4", "title": "PatrickStar: Parallel Training of Pre-trained Models via Chunk-based\n  Memory Management"}
{"author": "Conglong Li, Minjia Zhang, Yuxiong He", "abstract": "Recent works have demonstrated great success in pre-training large-scale\nautoregressive language models on massive GPUs. To reduce the wall-clock\ntraining time, a common practice is to increase the batch size and learning\nrate. However, such practice is often brittle and leads to a so-called\nstability-efficiency dilemma: increasing the batch sizes and learning rates\nleads to better training efficiency but can also result in training\ninstability, leading to poor generalization accuracy or failed runs. To better\nunderstand this phenomenon, we conduct an in-depth analysis on large-scale\npre-training experiments replicating the GPT-2 model. We find that there is a\nstrong correlation between training instability and extreme values of gradient\nvariance, and that samples with long sequence lengths contribute to these\nextreme gradient variance values, especially at the beginning of the training,\nindicating that long sequence length can be a main source of training\ninstability. Based on the analysis, we present a Sequence Length Warmup method\nthat aims to solve the training stability-efficiency dilemma. Experiments\nreplicating GPT-2 models show that our approach enables stable training with 8x\nlarger batch size and 4x larger learning rate, whereas the baseline approach\nstruggles with training instability. To achieve the same or better zero-shot\nevaluation results, our method reduces the required number of training tokens\nand wall clock time by up to 2.2x and 3.7x, respectively. Experiments\nreplicating GPT-3 model (125M) show that our approach enables stable training\nwith 8x larger batch size and 40x larger learning rate, and retains 99% of the\nzero-shot accuracy on 11 tasks using 10x less data and 17x less time compared\nto the original GPT-3 training recipe, while the baseline diverges under the\nsame settings and only retain 95% of accuracy under lower learning rate.", "time": "2022-10-16T08:42:29Z", "link": "http://arxiv.org/abs/2108.06084v4", "id": "2108.06084v4", "title": "The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup\n  for Training GPT Models"}
{"author": "Ye Xue, Diego Klabjan, Yuan Luo", "abstract": "Federated learning is a distributed machine learning paradigm where multiple\ndata owners (clients) collaboratively train one machine learning model while\nkeeping data on their own devices. The heterogeneity of client datasets is one\nof the most important challenges of federated learning algorithms. Studies have\nfound performance reduction with standard federated algorithms, such as FedAvg,\non non-IID data. Many existing works on handling non-IID data adopt the same\naggregation framework as FedAvg and focus on improving model updates either on\nthe server side or on clients. In this work, we tackle this challenge in a\ndifferent view by introducing redistribution rounds that delay the aggregation.\nWe perform experiments on multiple tasks and show that the proposed framework\nsignificantly improves the performance on non-IID data.", "time": "2021-08-17T04:06:10Z", "link": "http://arxiv.org/abs/2108.07433v1", "id": "2108.07433v1", "title": "Aggregation Delayed Federated Learning"}
{"author": "Lin Ning, Karan Singhal, Ellie X. Zhou, Sushant Prakash", "abstract": "Deep retrieval models are widely used for learning entity representations and\nrecommendations. Federated learning provides a privacy-preserving way to train\nthese models without requiring centralization of user data. However, federated\ndeep retrieval models usually perform much worse than their centralized\ncounterparts due to non-IID (independent and identically distributed) training\ndata on clients, an intrinsic property of federated learning that limits\nnegatives available for training. We demonstrate that this issue is distinct\nfrom the commonly studied client drift problem. This work proposes\nbatch-insensitive losses as a way to alleviate the non-IID negatives issue for\nfederated movie recommendations. We explore a variety of techniques and\nidentify that batch-insensitive losses can effectively improve the performance\nof federated deep retrieval models, increasing the relative recall of the\nfederated model by up to 93.15% and reducing the relative gap in recall between\nit and a centralized model from 27.22% - 43.14% to 0.53% - 2.42%. We also\nopen-source our code framework to accelerate further research and applications\nof federated deep retrieval models.", "time": "2021-11-02T04:50:31Z", "link": "http://arxiv.org/abs/2108.07931v2", "id": "2108.07931v2", "title": "Learning Federated Representations and Recommendations with Limited\n  Negatives"}
{"author": "Abdullatif Albaseer, Mohamed Abdallah, Ala Al-Fuqaha, Aiman Erbad", "abstract": "Clustered Federated Multitask Learning (CFL) was introduced as an efficient\nscheme to obtain reliable specialized models when data is imbalanced and\ndistributed in a non-i.i.d. (non-independent and identically distributed)\nfashion amongst clients. While a similarity measure metric, like the cosine\nsimilarity, can be used to endow groups of the client with a specialized model,\nthis process can be arduous as the server should involve all clients in each of\nthe federated learning rounds. Therefore, it is imperative that a subset of\nclients is selected periodically due to the limited bandwidth and latency\nconstraints at the network edge. To this end, this paper proposes a new client\nselection algorithm that aims to accelerate the convergence rate for obtaining\nspecialized machine learning models that achieve high test accuracies for all\nclient groups. Specifically, we introduce a client selection approach that\nleverages the devices' heterogeneity to schedule the clients based on their\nround latency and exploits the bandwidth reuse for clients that consume more\ntime to update the model. Then, the server performs model averaging and\nclusters the clients based on predefined thresholds. When a specific cluster\nreaches a stationary point, the proposed algorithm uses a greedy scheduling\nalgorithm for that group by selecting the clients with less latency to update\nthe model. Extensive experiments show that the proposed approach lowers the\ntraining time and accelerates the convergence rate by up to 50% while imbuing\neach client with a specialized model that is fit for its local data\ndistribution.", "time": "2021-08-16T21:38:22Z", "link": "http://arxiv.org/abs/2108.08768v1", "id": "2108.08768v1", "title": "Client Selection Approach in Support of Clustered Federated Learning\n  over Wireless Edge Networks"}
{"author": "Anirban Das, Timothy Castiglia, Shiqiang Wang, Stacy Patterson", "abstract": "We consider federated learning in tiered communication networks. Our network\nmodel consists of a set of silos, each holding a vertical partition of the\ndata. Each silo contains a hub and a set of clients, with the silo's vertical\ndata shard partitioned horizontally across its clients. We propose Tiered\nDecentralized Coordinate Descent (TDCD), a communication-efficient\ndecentralized training algorithm for such two-tiered networks. The clients in\neach silo perform multiple local gradient steps before sharing updates with\ntheir hub to reduce communication overhead. Each hub adjusts its coordinates by\naveraging its workers' updates, and then hubs exchange intermediate updates\nwith one another. We present a theoretical analysis of our algorithm and show\nthe dependence of the convergence rate on the number of vertical partitions and\nthe number of local updates. We further validate our approach empirically via\nsimulation-based experiments using a variety of datasets and objectives.", "time": "2024-04-25T05:01:05Z", "link": "http://arxiv.org/abs/2108.08930v4", "id": "2108.08930v4", "title": "Cross-Silo Federated Learning for Multi-Tier Networks with Vertical and\n  Horizontal Data Partitioning"}
{"author": "Jed Mills, Jia Hu, Geyong Min, Rui Jin, Siwei Zheng, Jin Wang", "abstract": "Federated Learning (FL) is a recent development in distributed machine\nlearning that collaboratively trains models without training data leaving\nclient devices, preserving data privacy. In real-world FL, the training set is\ndistributed over clients in a highly non-Independent and Identically\nDistributed (non-IID) fashion, harming model convergence speed and final\nperformance. To address this challenge, we propose a novel, generalised\napproach for incorporating adaptive optimisation into FL with the Federated\nGlobal Biased Optimiser (FedGBO) algorithm. FedGBO accelerates FL by employing\na set of global biased optimiser values during training, reducing\n'client-drift' from non-IID data whilst benefiting from adaptive optimisation.\nWe show that in FedGBO, updates to the global model can be reformulated as\ncentralised training using biased gradients and optimiser updates, and apply\nthis framework to prove FedGBO's convergence on nonconvex objectives when using\nthe momentum-SGD (SGDm) optimiser. We also conduct extensive experiments using\n4 FL benchmark datasets (CIFAR100, Sent140, FEMNIST, Shakespeare) and 3 popular\noptimisers (SGDm, RMSProp, Adam) to compare FedGBO against six state-of-the-art\nFL algorithms. The results demonstrate that FedGBO displays superior or\ncompetitive performance across the datasets whilst having low data-upload and\ncomputational costs, and provide practical insights into the trade-offs\nassociated with different adaptive-FL algorithms and optimisers.", "time": "2022-10-05T21:27:40Z", "link": "http://arxiv.org/abs/2108.09134v3", "id": "2108.09134v3", "title": "Accelerating Federated Learning with a Global Biased Optimiser"}
{"author": "Mark Zhao, Niket Agarwal, Aarti Basant, Bugra Gedik, Satadru Pan, Mustafa Ozdal, Rakesh Komuravelli, Jerry Pan, Tianshu Bao, Haowei Lu, Sundaram Narayanan, Jack Langman, Kevin Wilfong, Harsha Rastogi, Carole-Jean Wu, Christos Kozyrakis, Parik Pol", "abstract": "Datacenter-scale AI training clusters consisting of thousands of\ndomain-specific accelerators (DSA) are used to train increasingly-complex deep\nlearning models. These clusters rely on a data storage and ingestion (DSI)\npipeline, responsible for storing exabytes of training data and serving it at\ntens of terabytes per second. As DSAs continue to push training efficiency and\nthroughput, the DSI pipeline is becoming the dominating factor that constrains\nthe overall training performance and capacity. Innovations that improve the\nefficiency and performance of DSI systems and hardware are urgent, demanding a\ndeep understanding of DSI characteristics and infrastructure at scale.\n  This paper presents Meta's end-to-end DSI pipeline, composed of a central\ndata warehouse built on distributed storage and a Data PreProcessing Service\nthat scales to eliminate data stalls. We characterize how hundreds of models\nare collaboratively trained across geo-distributed datacenters via diverse and\ncontinuous training jobs. These training jobs read and heavily filter massive\nand evolving datasets, resulting in popular features and samples used across\ntraining jobs. We measure the intense network, memory, and compute resources\nrequired by each training job to preprocess samples during training. Finally,\nwe synthesize key takeaways based on our production infrastructure\ncharacterization. These include identifying hardware bottlenecks, discussing\nopportunities for heterogeneous DSI hardware, motivating research in datacenter\nscheduling and benchmark datasets, and assimilating lessons learned in\noptimizing DSI infrastructure.", "time": "2022-04-22T23:51:04Z", "link": "http://arxiv.org/abs/2108.09373v4", "id": "2108.09373v4", "title": "Understanding Data Storage and Ingestion for Large-Scale Deep\n  Recommendation Model Training"}
{"author": "Moming Duan, Duo Liu, Xinyuan Ji, Yu Wu, Liang Liang, Xianzhang Chen, Yujuan Tan", "abstract": "Federated Learning (FL) enables the multiple participating devices to\ncollaboratively contribute to a global neural network model while keeping the\ntraining data locally. Unlike the centralized training setting, the non-IID,\nimbalanced (statistical heterogeneity) and distribution shifted training data\nof FL is distributed in the federated network, which will increase the\ndivergences between the local models and the global model, further degrading\nperformance. In this paper, we propose a flexible clustered federated learning\n(CFL) framework named FlexCFL, in which we 1) group the training of clients\nbased on the similarities between the clients' optimization directions for\nlower training divergence; 2) implement an efficient newcomer device cold start\nmechanism for framework scalability and practicality; 3) flexibly migrate\nclients to meet the challenge of client-level data distribution shift. FlexCFL\ncan achieve improvements by dividing joint optimization into groups of\nsub-optimization and can strike a balance between accuracy and communication\nefficiency in the distribution shift environment. The convergence and\ncomplexity are analyzed to demonstrate the efficiency of FlexCFL. We also\nevaluate FlexCFL on several open datasets and made comparisons with related CFL\nframeworks. The results show that FlexCFL can significantly improve absolute\ntest accuracy by +10.6% on FEMNIST compared to FedAvg, +3.5% on FashionMNIST\ncompared to FedProx, +8.4% on MNIST compared to FeSEM. The experiment results\nshow that FlexCFL is also communication efficient in the distribution shift\nenvironment.", "time": "2021-08-22T15:11:39Z", "link": "http://arxiv.org/abs/2108.09749v1", "id": "2108.09749v1", "title": "Flexible Clustered Federated Learning for Client-Level Data Distribution\n  Shift"}
{"author": "Haibo Yang, Xin Zhang, Prashant Khanduri, Jia Liu", "abstract": "Present-day federated learning (FL) systems deployed over edge networks\nconsists of a large number of workers with high degrees of heterogeneity in\ndata and/or computing capabilities, which call for flexible worker\nparticipation in terms of timing, effort, data heterogeneity, etc. To satisfy\nthe need for flexible worker participation, we consider a new FL paradigm\ncalled \"Anarchic Federated Learning\" (AFL) in this paper. In stark contrast to\nconventional FL models, each worker in AFL has the freedom to choose i) when to\nparticipate in FL, and ii) the number of local steps to perform in each round\nbased on its current situation (e.g., battery level, communication channels,\nprivacy concerns). However, such chaotic worker behaviors in AFL impose many\nnew open questions in algorithm design. In particular, it remains unclear\nwhether one could develop convergent AFL training algorithms, and if yes, under\nwhat conditions and how fast the achievable convergence speed is. Toward this\nend, we propose two Anarchic Federated Averaging (AFA) algorithms with\ntwo-sided learning rates for both cross-device and cross-silo settings, which\nare named AFA-CD and AFA-CS, respectively. Somewhat surprisingly, we show that,\nunder mild anarchic assumptions, both AFL algorithms achieve the best known\nconvergence rate as the state-of-the-art algorithms for conventional FL.\nMoreover, they retain the highly desirable {\\em linear speedup effect} with\nrespect of both the number of workers and local steps in the new AFL paradigm.\nWe validate the proposed algorithms with extensive experiments on real-world\ndatasets.", "time": "2022-06-10T13:34:28Z", "link": "http://arxiv.org/abs/2108.09875v4", "id": "2108.09875v4", "title": "Anarchic Federated Learning"}
{"author": "Guodong Long, Yue Tan, Jing Jiang, Chengqi Zhang", "abstract": "Open banking enables individual customers to own their banking data, which\nprovides fundamental support for the boosting of a new ecosystem of data\nmarketplaces and financial services. In the near future, it is foreseeable to\nhave decentralized data ownership in the finance sector using federated\nlearning. This is a just-in-time technology that can learn intelligent models\nin a decentralized training manner. The most attractive aspect of federated\nlearning is its ability to decompose model training into a centralized server\nand distributed nodes without collecting private data. This kind of decomposed\nlearning framework has great potential to protect users' privacy and sensitive\ndata. Therefore, federated learning combines naturally with an open banking\ndata marketplaces. This chapter will discuss the possible challenges for\napplying federated learning in the context of open banking, and the\ncorresponding solutions have been explored as well.", "time": "2021-08-24T14:06:16Z", "link": "http://arxiv.org/abs/2108.10749v1", "id": "2108.10749v1", "title": "Federated Learning for Open Banking"}
{"author": "Nirupam Gupta, Thinh T. Doan, Nitin Vaidya", "abstract": "We consider the problem of Byzantine fault-tolerance in federated machine\nlearning. In this problem, the system comprises multiple agents each with local\ndata, and a trusted centralized coordinator. In fault-free setting, the agents\ncollaborate with the coordinator to find a minimizer of the aggregate of their\nlocal cost functions defined over their local data. We consider a scenario\nwhere some agents ($f$ out of $N$) are Byzantine faulty. Such agents need not\nfollow a prescribed algorithm correctly, and may communicate arbitrary\nincorrect information to the coordinator. In the presence of Byzantine agents,\na more reasonable goal for the non-faulty agents is to find a minimizer of the\naggregate cost function of only the non-faulty agents. This particular goal is\ncommonly referred as exact fault-tolerance. Recent work has shown that exact\nfault-tolerance is achievable if only if the non-faulty agents satisfy the\nproperty of $2f$-redundancy. Now, under this property, techniques are known to\nimpart exact fault-tolerance to the distributed implementation of the classical\nstochastic gradient-descent (SGD) algorithm. However, we do not know of any\nsuch techniques for the federated local SGD algorithm - a more commonly used\nmethod for federated machine learning. To address this issue, we propose a\nnovel technique named comparative elimination (CE). We show that, under\n$2f$-redundancy, the federated local SGD algorithm with CE can indeed obtain\nexact fault-tolerance in the deterministic setting when the non-faulty agents\ncan accurately compute gradients of their local cost functions. In the general\nstochastic case, when agents can only compute unbiased noisy estimates of their\nlocal gradients, our algorithm achieves approximate fault-tolerance with\napproximation error proportional to the variance of stochastic gradients and\nthe fraction of Byzantine agents.", "time": "2021-08-26T13:04:28Z", "link": "http://arxiv.org/abs/2108.11769v1", "id": "2108.11769v1", "title": "Byzantine Fault-Tolerance in Federated Local SGD under 2f-Redundancy"}
{"author": "Stefan Kesselheim, Andreas Herten, Kai Krajsek, Jan Ebert, Jenia Jitsev, Mehdi Cherti, Michael Langguth, Bing Gong, Scarlet Stadtler, Amirpasha Mozaffari, Gabriele Cavallaro, Rocco Sedona, Alexander Schug, Alexandre Strube, Roshni Kamath, Martin G. Schultz, Morris Riedel, Thomas Lippert", "abstract": "In this article, we present JUWELS Booster, a recently commissioned\nhigh-performance computing system at the J\\\"ulich Supercomputing Center. With\nits system architecture, most importantly its large number of powerful Graphics\nProcessing Units (GPUs) and its fast interconnect via InfiniBand, it is an\nideal machine for large-scale Artificial Intelligence (AI) research and\napplications. We detail its system architecture, parallel, distributed model\ntraining, and benchmarks indicating its outstanding performance. We exemplify\nits potential for research application by presenting large-scale AI research\nhighlights from various scientific fields that require such a facility.", "time": "2021-06-30T21:37:02Z", "link": "http://arxiv.org/abs/2108.11976v1", "id": "2108.11976v1", "title": "JUWELS Booster -- A Supercomputer for Large-Scale AI Research"}
{"author": "Harshit Daga, Yiwen Chen, Aastha Agrawal, Ada Gavrilovska", "abstract": "For highly distributed environments such as edge computing, collaborative\nlearning approaches eschew the dependence on a global, shared model, in favor\nof models tailored for each location. Creating tailored models for individual\nlearning contexts reduces the amount of data transfer, while collaboration\namong peers provides acceptable model performance. Collaboration assumes,\nhowever, the availability of knowledge transfer mechanisms, which are not\ntrivial for deep learning models where knowledge isn't easily attributed to\nprecise model slices. We present Canoe - a framework that facilitates knowledge\ntransfer for neural networks. Canoe provides new system support for dynamically\nextracting significant parameters from a helper node's neural network and uses\nthis with a multi-model boosting-based approach to improve the predictive\nperformance of the target node. The evaluation of Canoe with different PyTorch\nand TensorFlow neural network models demonstrates that the knowledge transfer\nmechanism improves the model's adaptiveness to changes up to 3.5X compared to\nlearning in isolation, while affording several magnitudes reduction in data\nmovement costs compared to federated learning.", "time": "2021-08-30T01:01:58Z", "link": "http://arxiv.org/abs/2108.12124v2", "id": "2108.12124v2", "title": "Canoe : A System for Collaborative Learning for Neural Nets"}
{"author": "Dominik Scheinert, Houkun Zhu, Lauritz Thamsen, Morgan K. Geldenhuys, Jonathan Will, Alexander Acker, Odej Kao", "abstract": "Distributed dataflow systems like Spark and Flink enable the use of clusters\nfor scalable data analytics. While runtime prediction models can be used to\ninitially select appropriate cluster resources given target runtimes, the\nactual runtime performance of dataflow jobs depends on several factors and\nvaries over time. Yet, in many situations, dynamic scaling can be used to meet\nformulated runtime targets despite significant performance variance.\n  This paper presents Enel, a novel dynamic scaling approach that uses message\npropagation on an attributed graph to model dataflow jobs and, thus, allows for\nderiving effective rescaling decisions. For this, Enel incorporates descriptive\nproperties that capture the respective execution context, considers statistics\nfrom individual dataflow tasks, and propagates predictions through the job\ngraph to eventually find an optimized new scale-out. Our evaluation of Enel\nwith four iterative Spark jobs shows that our approach is able to identify\neffective rescaling actions, reacting for instance to node failures, and can be\nreused across different execution contexts.", "time": "2022-01-26T12:34:53Z", "link": "http://arxiv.org/abs/2108.12211v3", "id": "2108.12211v3", "title": "Enel: Context-Aware Dynamic Scaling of Distributed Dataflow Jobs using\n  Graph Propagation"}
{"author": "Hanfei Yu, Hao Wang, Jian Li, Xu Yuan, Seung-Jong Park", "abstract": "Serverless computing automates fine-grained resource scaling and simplifies\nthe development and deployment of online services with stateless functions.\nHowever, it is still non-trivial for users to allocate appropriate resources\ndue to various function types, dependencies, and input sizes. Misconfiguration\nof resource allocations leaves functions either under-provisioned or\nover-provisioned and leads to continuous low resource utilization. This paper\npresents Freyr, a new resource manager (RM) for serverless platforms that\nmaximizes resource efficiency by dynamically harvesting idle resources from\nover-provisioned functions to under-provisioned functions. Freyr monitors each\nfunction's resource utilization in real-time, detects over-provisioning and\nunder-provisioning, and learns to harvest idle resources safely and accelerates\nfunctions efficiently by applying deep reinforcement learning algorithms along\nwith a safeguard mechanism. We have implemented and deployed a Freyr prototype\nin a 13-node Apache OpenWhisk cluster. Experimental results show that 38.8% of\nfunction invocations have idle resources harvested by Freyr, and 39.2% of\ninvocations are accelerated by the harvested resources. Freyr reduces the\n99th-percentile function response latency by 32.1% compared to the baseline\nRMs.", "time": "2022-02-17T00:01:38Z", "link": "http://arxiv.org/abs/2108.12717v2", "id": "2108.12717v2", "title": "Accelerating Serverless Computing by Harvesting Idle Resources"}
{"author": "Koichi Bando, Kenji Tanaka", "abstract": "With the recent progress of information technology, the use of networked\ninformation systems has rapidly expanded. Electronic commerce and electronic\npayments between banks and companies, and online shopping and social networking\nservices used by the general public are examples of such systems. Therefore, in\norder to maintain and improve the dependability of these systems, we are\nconstructing a failure database from past failure cases. When importing new\nfailure cases to the database, it is necessary to classify these cases\naccording to failure type. The problems are the accuracy and efficiency of the\nclassification. Especially when working with multiple individuals, unification\nof classification is required. Therefore, we are attempting to automate\nclassification using machine learning. As evaluation models, we selected the\nmultilayer perceptron (MLP), the convolutional neural network (CNN), and the\nrecurrent neural network (RNN), which are models that use neural networks. As a\nresult, the optimal model in terms of accuracy is first the MLP followed by the\nCNN, and the processing time of the classification is practical.", "time": "2021-09-01T09:53:52Z", "link": "http://arxiv.org/abs/2108.12788v2", "id": "2108.12788v2", "title": "Attempt to Predict Failure Case Classification in a Failure Database by\n  using Neural Network Models"}
{"author": "Yujing Chen, Zheng Chai, Yue Cheng, Huzefa Rangwala", "abstract": "Federated learning (FL) involves multiple distributed devices jointly\ntraining a shared model without any of the participants having to reveal their\nlocal data to a centralized server. Most of previous FL approaches assume that\ndata on devices are fixed and stationary during the training process. However,\nthis assumption is unrealistic because these devices usually have varying\nsampling rates and different system configurations. In addition, the underlying\ndistribution of the device data can change dynamically over time, which is\nknown as concept drift. Concept drift makes the learning process complicated\nbecause of the inconsistency between existing and upcoming data. Traditional\nconcept drift handling techniques such as chunk based and ensemble\nlearning-based methods are not suitable in the federated learning frameworks\ndue to the heterogeneity of local devices. We propose a novel approach,\nFedConD, to detect and deal with the concept drift on local devices and\nminimize the effect on the performance of models in asynchronous FL. The drift\ndetection strategy is based on an adaptive mechanism which uses the historical\nperformance of the local models. The drift adaptation is realized by adjusting\nthe regularization parameter of objective function on each local device.\nAdditionally, we design a communication strategy on the server side to select\nlocal updates in a prudent fashion and speed up model convergence. Experimental\nevaluations on three evolving data streams and two image datasets show that\n\\model~detects and handles concept drift, and also reduces the overall\ncommunication cost compared to other baseline methods.", "time": "2021-09-01T02:06:42Z", "link": "http://arxiv.org/abs/2109.00151v1", "id": "2109.00151v1", "title": "Asynchronous Federated Learning for Sensor Data with Concept Drift"}
{"author": "Qinghao Hu, Peng Sun, Shengen Yan, Yonggang Wen, Tianwei Zhang", "abstract": "Modern GPU datacenters are critical for delivering Deep Learning (DL) models\nand services in both the research community and industry. When operating a\ndatacenter, optimization of resource scheduling and management can bring\nsignificant financial benefits. Achieving this goal requires a deep\nunderstanding of the job features and user behaviors. We present a\ncomprehensive study about the characteristics of DL jobs and resource\nmanagement. First, we perform a large-scale analysis of real-world job traces\nfrom SenseTime. We uncover some interesting conclusions from the perspectives\nof clusters, jobs and users, which can facilitate the cluster system designs.\nSecond, we introduce a general-purpose framework, which manages resources based\non historical data. As case studies, we design: a Quasi-Shortest-Service-First\nscheduling service, which can minimize the cluster-wide average job completion\ntime by up to 6.5x; and a Cluster Energy Saving service, which improves overall\ncluster utilization by up to 13%.", "time": "2021-09-06T01:26:38Z", "link": "http://arxiv.org/abs/2109.01313v2", "id": "2109.01313v2", "title": "Characterization and Prediction of Deep Learning Workloads in\n  Large-Scale GPU Datacenters"}
{"author": "Jing Ma, Qiuchen Zhang, Jian Lou, Li Xiong, Sivasubramanium Bhavani, Joyce C. Ho", "abstract": "Tensor factorization has been proved as an efficient unsupervised learning\napproach for health data analysis, especially for computational phenotyping,\nwhere the high-dimensional Electronic Health Records (EHRs) with patients'\nhistory of medical procedures, medications, diagnosis, lab tests, etc., are\nconverted to meaningful and interpretable medical concepts. Federated tensor\nfactorization distributes the tensor computation to multiple workers under the\ncoordination of a central server, which enables jointly learning the phenotypes\nacross multiple hospitals while preserving the privacy of the patient\ninformation. However, existing federated tensor factorization algorithms\nencounter the single-point-failure issue with the involvement of the central\nserver, which is not only easily exposed to external attacks but also limits\nthe number of clients sharing information with the server under restricted\nuplink bandwidth. In this paper, we propose CiderTF, a communication-efficient\ndecentralized generalized tensor factorization, which reduces the uplink\ncommunication cost by leveraging a four-level communication reduction strategy\ndesigned for a generalized tensor factorization, which has the flexibility of\nmodeling different tensor distribution with multiple kinds of loss functions.\nExperiments on two real-world EHR datasets demonstrate that CiderTF achieves\ncomparable convergence with a communication reduction up to 99.99%.", "time": "2022-11-03T06:15:31Z", "link": "http://arxiv.org/abs/2109.01718v2", "id": "2109.01718v2", "title": "Communication Efficient Generalized Tensor Factorization for\n  Decentralized Healthcare Networks"}
{"author": "Kun Zhai, Qiang Ren, Junli Wang, Chungang Yan", "abstract": "Federated learning is a novel framework that enables resource-constrained\nedge devices to jointly learn a model, which solves the problem of data\nprotection and data islands. However, standard federated learning is vulnerable\nto Byzantine attacks, which will cause the global model to be manipulated by\nthe attacker or fail to converge. On non-iid data, the current methods are not\neffective in defensing against Byzantine attacks. In this paper, we propose a\nByzantine-robust framework for federated learning via credibility assessment on\nnon-iid data (BRCA). Credibility assessment is designed to detect Byzantine\nattacks by combing adaptive anomaly detection model and data verification.\nSpecially, an adaptive mechanism is incorporated into the anomaly detection\nmodel for the training and prediction of the model. Simultaneously, a unified\nupdate algorithm is given to guarantee that the global model has a consistent\ndirection. On non-iid data, our experiments demonstrate that the BRCA is more\nrobust to Byzantine attacks compared with conventional methods", "time": "2021-09-06T12:18:02Z", "link": "http://arxiv.org/abs/2109.02396v1", "id": "2109.02396v1", "title": "Byzantine-Robust Federated Learning via Credibility Assessment on\n  Non-IID Data"}
{"author": "Frank Po-Chen Lin, Seyyedali Hosseinalipour, Sheikh Shams Azam, Christopher G. Brinton, NicolÃ² Michelusi", "abstract": "Federated learning has emerged as a popular technique for distributing model\ntraining across the network edge. Its learning architecture is conventionally a\nstar topology between the devices and a central server. In this paper, we\npropose two timescale hybrid federated learning (TT-HF), which migrates to a\nmore distributed topology via device-to-device (D2D) communications. In TT-HF,\nlocal model training occurs at devices via successive gradient iterations, and\nthe synchronization process occurs at two timescales: (i) macro-scale, where\nglobal aggregations are carried out via device-server interactions, and (ii)\nmicro-scale, where local aggregations are carried out via D2D cooperative\nconsensus formation in different device clusters. Our theoretical analysis\nreveals how device, cluster, and network-level parameters affect the\nconvergence of TT-HF, and leads to a set of conditions under which a\nconvergence rate of O(1/t) is guaranteed. Experimental results demonstrate the\nimprovements in convergence and utilization that can be obtained by TT-HF over\nstate-of-the-art federated learning baselines.", "time": "2021-09-12T04:29:09Z", "link": "http://arxiv.org/abs/2109.03350v2", "id": "2109.03350v2", "title": "Federated Learning Beyond the Star: Local D2D Model Consensus with\n  Global Cluster Sampling"}
{"author": "Jin Wang, Jia Hu, Jed Mills, Geyong Min, Ming Xia", "abstract": "Federated learning (FL) is a privacy-preserving distributed machine learning\nparadigm that enables collaborative training among geographically distributed\nand heterogeneous devices without gathering their data. Extending FL beyond the\nsupervised learning models, federated reinforcement learning (FRL) was proposed\nto handle sequential decision-making problems in edge computing systems.\nHowever, the existing FRL algorithms directly combine model-free RL with FL,\nthus often leading to high sample complexity and lacking theoretical\nguarantees. To address the challenges, we propose a novel FRL algorithm that\neffectively incorporates model-based RL and ensemble knowledge distillation\ninto FL for the first time. Specifically, we utilise FL and knowledge\ndistillation to create an ensemble of dynamics models for clients, and then\ntrain the policy by solely using the ensemble model without interacting with\nthe environment. Furthermore, we theoretically prove that the monotonic\nimprovement of the proposed algorithm is guaranteed. The extensive experimental\nresults demonstrate that our algorithm obtains much higher sample efficiency\ncompared to classic model-free FRL algorithms in the challenging continuous\ncontrol benchmark environments under edge computing settings. The results also\nhighlight the significant impact of heterogeneous client data and local model\nupdate steps on the performance of FRL, validating the insights obtained from\nour theoretical analysis.", "time": "2023-04-01T14:47:33Z", "link": "http://arxiv.org/abs/2109.05549v3", "id": "2109.05549v3", "title": "Federated Ensemble Model-based Reinforcement Learning in Edge Computing"}
{"author": "Venkatesan T. Chakaravarthy, Shivmaran S. Pandian, Saurabh Raje, Yogish Sabharwal, Toyotaro Suzumura, Shashanka Ubaru", "abstract": "We present distributed algorithms for training dynamic Graph Neural Networks\n(GNN) on large scale graphs spanning multi-node, multi-GPU systems. To the best\nof our knowledge, this is the first scaling study on dynamic GNN. We devise\nmechanisms for reducing the GPU memory usage and identify two execution time\nbottlenecks: CPU-GPU data transfer; and communication volume. Exploiting\nproperties of dynamic graphs, we design a graph difference-based strategy to\nsignificantly reduce the transfer time. We develop a simple, but effective data\ndistribution technique under which the communication volume remains fixed and\nlinear in the input size, for any number of GPUs. Our experiments using\nbillion-size graphs on a system of 128 GPUs shows that: (i) the distribution\nscheme achieves up to 30x speedup on 128 GPUs; (ii) the graph-difference\ntechnique reduces the transfer time by a factor of up to 4.1x and the overall\nexecution time by up to 40%", "time": "2021-09-16T11:51:20Z", "link": "http://arxiv.org/abs/2109.07893v1", "id": "2109.07893v1", "title": "Efficient Scaling of Dynamic Graph Neural Networks"}
{"author": "Cheng Tan, Zhichao Li, Jian Zhang, Yu Cao, Sikai Qi, Zherui Liu, Yibo Zhu, Chuanxiong Guo", "abstract": "Multi-Instance GPU (MIG) is a new feature introduced by NVIDIA A100 GPUs that\npartitions one physical GPU into multiple GPU instances. With MIG, A100 can be\nthe most cost-efficient GPU ever for serving Deep Neural Networks (DNNs).\nHowever, discovering the most efficient GPU partitions is challenging. The\nunderlying problem is NP-hard; moreover, it is a new abstract problem, which we\ndefine as the Reconfigurable Machine Scheduling Problem (RMS). This paper\nstudies serving DNNs with MIG, a new case of RMS. We further propose a\nsolution, MIG-serving. MIG- serving is an algorithm pipeline that blends a\nvariety of newly designed algorithms and customized classic algorithms,\nincluding a heuristic greedy algorithm, Genetic Algorithm (GA), and Monte Carlo\nTree Search algorithm (MCTS). We implement MIG-serving on Kubernetes. Our\nexperiments show that compared to using A100 as-is, MIG-serving can save up to\n40% of GPUs while providing the same throughput.", "time": "2021-09-18T19:57:13Z", "link": "http://arxiv.org/abs/2109.11067v1", "id": "2109.11067v1", "title": "Serving DNN Models with Multi-Instance GPUs: A Case of the\n  Reconfigurable Machine Scheduling Problem"}
{"author": "William Won, Saeed Rashidi, Sudarshan Srinivasan, Tushar Krishna", "abstract": "As model sizes in machine learning continue to scale, distributed training is\nnecessary to accommodate model weights within each device and to reduce\ntraining time. However, this comes with the expense of increased communication\noverhead due to the exchange of gradients and activations, which become the\ncritical bottleneck of the end-to-end training process. In this work, we\nmotivate the design of multi-dimensional networks within machine learning\nsystems as a cost-efficient mechanism to enhance overall network bandwidth. We\nalso identify that optimal bandwidth allocation is pivotal for\nmulti-dimensional networks to ensure efficient resource utilization. We\nintroduce LIBRA, a framework specifically focused on optimizing\nmulti-dimensional fabric architectures. Through case studies, we demonstrate\nthe value of LIBRA, both in architecting optimized fabrics under diverse\nconstraints and in enabling co-optimization opportunities.", "time": "2024-05-05T05:53:40Z", "link": "http://arxiv.org/abs/2109.11762v2", "id": "2109.11762v2", "title": "LIBRA: Enabling Workload-aware Multi-dimensional Network Topology\n  Optimization for Distributed Training of Large AI Models"}
{"author": "Xutong Mu, Yulong Shen, Ke Cheng, Xueli Geng, Jiaxuan Fu, Tao Zhang, Zhiwei Zhang", "abstract": "Federated learning allows multiple clients to collaborate to train\nhigh-performance deep learning models while keeping the training data locally.\nHowever, when the local data of all clients are not independent and identically\ndistributed (i.e., non-IID), it is challenging to implement this form of\nefficient collaborative learning. Although significant efforts have been\ndedicated to addressing this challenge, the effect on the image classification\ntask is still not satisfactory. In this paper, we propose FedProc: prototypical\ncontrastive federated learning, which is a simple and effective federated\nlearning framework. The key idea is to utilize the prototypes as global\nknowledge to correct the local training of each client. We design a local\nnetwork architecture and a global prototypical contrastive loss to regulate the\ntraining of local models, which makes local objectives consistent with the\nglobal optima. Eventually, the converged global model obtains a good\nperformance on non-IID data. Experimental results show that, compared to\nstate-of-the-art federated learning methods, FedProc improves the accuracy by\n$1.6\\%\\sim7.9\\%$ with acceptable computation cost.", "time": "2021-09-25T04:32:23Z", "link": "http://arxiv.org/abs/2109.12273v1", "id": "2109.12273v1", "title": "FedProc: Prototypical Contrastive Federated Learning on Non-IID data"}
{"author": "Qingsong Zhang, Bin Gu, Cheng Deng, Songxiang Gu, Liefeng Bo, Jian Pei, Heng Huang", "abstract": "Vertical federated learning (VFL) is an effective paradigm of training the\nemerging cross-organizational (e.g., different corporations, companies and\norganizations) collaborative learning with privacy preserving. Stochastic\ngradient descent (SGD) methods are the popular choices for training VFL models\nbecause of the low per-iteration computation. However, existing SGD-based VFL\nalgorithms are communication-expensive due to a large number of communication\nrounds. Meanwhile, most existing VFL algorithms use synchronous computation\nwhich seriously hamper the computation resource utilization in real-world\napplications. To address the challenges of communication and computation\nresource utilization, we propose an asynchronous stochastic quasi-Newton\n(AsySQN) framework for VFL, under which three algorithms, i.e. AsySQN-SGD,\n-SVRG and -SAGA, are proposed. The proposed AsySQN-type algorithms making\ndescent steps scaled by approximate (without calculating the inverse Hessian\nmatrix explicitly) Hessian information convergence much faster than SGD-based\nmethods in practice and thus can dramatically reduce the number of\ncommunication rounds. Moreover, the adopted asynchronous computation can make\nbetter use of the computation resource. We theoretically prove the convergence\nrates of our proposed algorithms for strongly convex problems. Extensive\nnumerical experiments on real-word datasets demonstrate the lower communication\ncosts and better computation resource utilization of our algorithms compared\nwith state-of-the-art VFL algorithms.", "time": "2021-09-26T07:56:10Z", "link": "http://arxiv.org/abs/2109.12519v1", "id": "2109.12519v1", "title": "AsySQN: Faster Vertical Federated Learning Algorithms with Better\n  Computation Resource Utilization"}
{"author": "Lorenzo Valerio, Andrea Passarella, Marco Conti", "abstract": "Due to the pervasive diffusion of personal mobile and IoT devices, many\n``smart environments'' (e.g., smart cities and smart factories) will be, among\nothers, generators of huge amounts of data. Currently, this is typically\nachieved through centralised cloud-based data analytics services. However,\naccording to many studies, this approach may present significant issues from\nthe standpoint of data ownership, and even wireless network capacity. One\npossibility to cope with these shortcomings is to move data analytics closer to\nwhere data is generated. In this paper, we tackle this issue by proposing and\nanalyzing a distributed learning framework, whereby data analytics are\nperformed at the edge of the network, i.e., on locations very close to where\ndata is generated. Specifically, in our framework, partial data analytics are\nperformed directly on the nodes that generate the data, or on nodes close by\n(e.g., some of the data generators can take this role on behalf of subsets of\nother nodes nearby). Then, nodes exchange partial models and refine them\naccordingly. Our framework is general enough to host different analytics\nservices. In the specific case analysed in the paper, we focus on a learning\ntask, considering two distributed learning algorithms. Using an activity\nrecognition and a pattern recognition task, both on reference datasets, we\ncompare the two learning algorithms between each other and with a central cloud\nsolution (i.e., one that has access to the complete datasets). Our results show\nthat using distributed machine learning techniques, it is possible to\ndrastically reduce the network overhead, while obtaining performance comparable\nto the cloud solution in terms of learning accuracy. The analysis also shows\nwhen each distributed learning approach is preferable, based on the specific\ndistribution of the data on the nodes.", "time": "2021-09-27T13:44:34Z", "link": "http://arxiv.org/abs/2109.13049v1", "id": "2109.13049v1", "title": "A communication efficient distributed learning framework for smart\n  environments"}
{"author": "Hyungjun Oh, Hyungjun Oh, HyeongJu Kim, Jiwon Seo", "abstract": "Neural network training requires a large amount of computation and thus GPUs\nare often used for the acceleration. While they improve the performance, GPUs\nare underutilized during the training.This paper proposes out-of-order (ooo)\nbackprop, an effective scheduling technique for neural network training. By\nexploiting the dependencies of gradient computations, ooo backprop enables to\nreorder their executions to make the most of the GPU resources. We show that\nthe GPU utilization in single-GPU, data-parallel, and pipeline-parallel\ntraining can be commonly improve by applying ooo back-prop and prioritizing\ncritical operations. We propose three scheduling algorithms based on ooo\nbackprop. For single-GPU training, we schedule with multi-stream out-of-order\ncomputation to mask the kernel launch overhead. In data-parallel training, we\nreorder the gradient computations to maximize the overlapping of computation\nand parameter communication; in pipeline-parallel training, we prioritize\ncritical gradient computations to reduce the pipeline stalls.We evaluate our\noptimizations with twelve neural networks including a light-weight computer\nvision model (MobileNet) and largeNLP models (BERT and GPT-3) with up to forty\neight V100 GPUs.Our scheduling algorithms effectively improve the performance\nof single-GPU training as well as data- and pipeline-parallel training.Compared\nto the respective state of the art training systems, the throughput is\nsubstantially improved for single-GPU, data-parallel, and pipeline-parallel\ntraining.", "time": "2021-10-03T05:45:06Z", "link": "http://arxiv.org/abs/2110.00929v1", "id": "2110.00929v1", "title": "Scheduling Optimization Techniques for Neural Network Training"}
{"author": "Kavya Kopparapu, Eric Lin", "abstract": "TinyML has rose to popularity in an era where data is everywhere. However,\nthe data that is in most demand is subject to strict privacy and security\nguarantees. In addition, the deployment of TinyML hardware in the real world\nhas significant memory and communication constraints that traditional ML fails\nto address. In light of these challenges, we present TinyFedTL, the first\nimplementation of federated transfer learning on a resource-constrained\nmicrocontroller.", "time": "2021-10-03T21:43:02Z", "link": "http://arxiv.org/abs/2110.01107v1", "id": "2110.01107v1", "title": "TinyFedTL: Federated Transfer Learning on Tiny Devices"}
{"author": "Lingjiao Chen, Leshang Chen, Hongyi Wang, Susan Davidson, Edgar Dobriban", "abstract": "There has been a growing need to provide Byzantine-resilience in distributed\nmodel training. Existing robust distributed learning algorithms focus on\ndeveloping sophisticated robust aggregators at the parameter servers, but pay\nless attention to balancing the communication cost and robustness. In this\npaper, we propose Solon, an algorithmic framework that exploits gradient\nredundancy to provide communication efficiency and Byzantine robustness\nsimultaneously. Our theoretical analysis shows a fundamental trade-off among\ncomputational load, communication cost, and Byzantine robustness. We also\ndevelop a concrete algorithm to achieve the optimal trade-off, borrowing ideas\nfrom coding theory and sparse recovery. Empirical experiments on various\ndatasets demonstrate that Solon provides significant speedups over existing\nmethods to achieve the same accuracy, over 10 times faster than Bulyan and 80%\nfaster than Draco. We also show that carefully designed Byzantine attacks break\nSignum and Bulyan, but do not affect the successful convergence of Solon.", "time": "2021-10-09T06:34:14Z", "link": "http://arxiv.org/abs/2110.01595v2", "id": "2110.01595v2", "title": "Solon: Communication-efficient Byzantine-resilient Distributed Training\n  via Redundant Gradients"}
{"author": "Vincent Dumont, Casey Garner, Anuradha Trivedi, Chelsea Jones, Vidya Ganapati, Juliane Mueller, Talita Perciano, Mariam Kiran, Marc Day", "abstract": "We present a new software, HYPPO, that enables the automatic tuning of\nhyperparameters of various deep learning (DL) models. Unlike other\nhyperparameter optimization (HPO) methods, HYPPO uses adaptive surrogate models\nand directly accounts for uncertainty in model predictions to find accurate and\nreliable models that make robust predictions. Using asynchronous nested\nparallelism, we are able to significantly alleviate the computational burden of\ntraining complex architectures and quantifying the uncertainty. HYPPO is\nimplemented in Python and can be used with both TensorFlow and PyTorch\nlibraries. We demonstrate various software features on time-series prediction\nand image classification problems as well as a scientific application in\ncomputed tomography image reconstruction. Finally, we show that (1) we can\nreduce by an order of magnitude the number of evaluations necessary to find the\nmost optimal region in the hyperparameter space and (2) we can reduce by two\norders of magnitude the throughput for such HPO process to complete.", "time": "2021-10-04T20:14:22Z", "link": "http://arxiv.org/abs/2110.01698v1", "id": "2110.01698v1", "title": "HYPPO: A Surrogate-Based Multi-Level Parallelism Tool for Hyperparameter\n  Optimization"}
{"author": "Yuhao Chen, Qianqian Yang, Shibo He, Zhiguo Shi, Jiming Chen", "abstract": "With the increased penetration and proliferation of Internet of Things (IoT)\ndevices, there is a growing trend towards distributing the power of deep\nlearning (DL) across edge devices rather than centralizing it in the cloud.\nThis development enables better privacy preservation, real-time responses, and\nuser-specific models. To deploy deep and complex models to edge devices with\nlimited resources, model partitioning of deep neural networks (DNN) model is\nnecessary, and has been widely studied. However, most of the existing\nliterature only considers distributing the inference model while still relying\ncentralized cloud infrastructure to generate this model through training. In\nthis paper, we propose FTPipeHD, a novel DNN training framework that trains DNN\nmodels across distributed heterogeneous devices with fault tolerance mechanism.\nTo accelerate the training with time-varying computing power of each device, we\noptimize the partition points dynamically according to real-time computing\ncapacities. We also propose a novel weight redistribution approach that\nreplicates the weights to both the neighboring nodes and the central node\nperiodically, which combats the failure of multiple devices during training\nwhile incurring limited communication cost. Our numerical results demonstrate\nthat FTPipeHD is 6.8x faster in training than the state of the art method when\nthe computing capacity of the best device is 10x greater than the worst one. It\nis also shown that the proposed method is able to accelerate the training even\nwith the existence of device failures.", "time": "2021-10-06T14:00:22Z", "link": "http://arxiv.org/abs/2110.02781v1", "id": "2110.02781v1", "title": "FTPipeHD: A Fault-Tolerant Pipeline-Parallel Distributed Training\n  Framework for Heterogeneous Edge Devices"}
{"author": "Dhruv Guliani, Lillian Zhou, Changwan Ryu, Tien-Ju Yang, Harry Zhang, Yonghui Xiao, Francoise Beaufays, Giovanni Motta", "abstract": "Federated learning can be used to train machine learning models on the edge\non local data that never leave devices, providing privacy by default. This\npresents a challenge pertaining to the communication and computation costs\nassociated with clients' devices. These costs are strongly correlated with the\nsize of the model being trained, and are significant for state-of-the-art\nautomatic speech recognition models.\n  We propose using federated dropout to reduce the size of client models while\ntraining a full-size model server-side. We provide empirical evidence of the\neffectiveness of federated dropout, and propose a novel approach to vary the\ndropout rate applied at each layer. Furthermore, we find that federated dropout\nenables a set of smaller sub-models within the larger model to independently\nhave low word error rates, making it easier to dynamically adjust the size of\nthe model deployed for inference.", "time": "2021-10-07T17:22:40Z", "link": "http://arxiv.org/abs/2110.03634v1", "id": "2110.03634v1", "title": "Enabling On-Device Training of Speech Recognition Models with Federated\n  Dropout"}
{"author": "Zhen Xu, Sergio Escalera, Isabelle Guyon, Adrien PavÃ£o, Magali Richard, Wei-Wei Tu, Quanming Yao, Huan Zhao", "abstract": "Obtaining standardized crowdsourced benchmark of computational methods is a\nmajor issue in data science communities. Dedicated frameworks enabling fair\nbenchmarking in a unified environment are yet to be developed. Here we\nintroduce Codabench, an open-source, community-driven platform for benchmarking\nalgorithms or software agents versus datasets or tasks. A public instance of\nCodabench (https://www.codabench.org/) is open to everyone, free of charge, and\nallows benchmark organizers to compare fairly submissions, under the same\nsetting (software, hardware, data, algorithms), with custom protocols and data\nformats. Codabench has unique features facilitating the organization of\nbenchmarks flexibly, easily and reproducibly, such as the possibility of\nre-using templates of benchmarks, and supplying compute resources on-demand.\nCodabench has been used internally and externally on various applications,\nreceiving more than 130 users and 2500 submissions. As illustrative use cases,\nwe introduce 4 diverse benchmarks covering Graph Machine Learning, Cancer\nHeterogeneity, Clinical Diagnosis and Reinforcement Learning.", "time": "2022-02-25T08:20:35Z", "link": "http://arxiv.org/abs/2110.05802v2", "id": "2110.05802v2", "title": "Codabench: Flexible, Easy-to-Use and Reproducible Benchmarking Platform"}
{"author": "Jayashree Mohan, Amar Phanishayee, Janardhan Kulkarni, Vijay Chidambaram", "abstract": "Training Deep Neural Networks (DNNs) is a widely popular workload in both\nenterprises and cloud data centers. Existing schedulers for DNN training\nconsider GPU as the dominant resource, and allocate other resources such as CPU\nand memory proportional to the number of GPUs requested by the job.\nUnfortunately, these schedulers do not consider the impact of a job's\nsensitivity to allocation of CPU, memory, and storage resources. In this work,\nwe propose Synergy, a resource-sensitive scheduler for shared GPU clusters.\nSynergy infers the sensitivity of DNNs to different resources using optimistic\nprofiling; some jobs might benefit from more than the GPU-proportional\nallocation and some jobs might not be affected by less than GPU-proportional\nallocation. Synergy performs such multi-resource workload-aware assignments\nacross a set of jobs scheduled on shared multi-tenant clusters using a new\nnear-optimal online algorithm. Our experiments show that workload-aware CPU and\nmemory allocations can improve average JCT up to 3.4x when compared to\ntraditional GPU-proportional scheduling.", "time": "2022-08-24T07:52:38Z", "link": "http://arxiv.org/abs/2110.06073v2", "id": "2110.06073v2", "title": "Synergy: Resource Sensitive DNN Scheduling in Multi-Tenant Clusters"}
{"author": "Luca Cappelletti, Tommaso Fontana, Elena Casiraghi, Vida Ravanmehr, Tiffany J. Callahan, Carlos Cano, Marcin P. Joachimiak, Christopher J. Mungall, Peter N. Robinson, Justin Reese, Giorgio Valentini", "abstract": "Graph Representation Learning (GRL) methods opened new avenues for addressing\ncomplex, real-world problems represented by graphs. However, many graphs used\nin these applications comprise millions of nodes and billions of edges and are\nbeyond the capabilities of current methods and software implementations. We\npresent GRAPE, a software resource for graph processing and embedding that can\nscale with big graphs by using specialized and smart data structures,\nalgorithms, and a fast parallel implementation of random walk-based methods.\nCompared with state-of-the-art software resources, GRAPE shows an improvement\nof orders of magnitude in empirical space and time complexity, as well as a\ncompetitive edge and node label prediction performance. GRAPE comprises about\n1.7 million well-documented lines of Python and Rust code and provides 69 node\nembedding methods, 25 inference models, a collection of efficient graph\nprocessing utilities and over 80,000 graphs from the literature and other\nsources. Standardized interfaces allow seamless integration of third-party\nlibraries, while ready-to-use and modular pipelines permit an easy-to-use\nevaluation of GRL methods, therefore also positioning GRAPE as a software\nresource to perform a fair comparison between methods and libraries for graph\nprocessing and embedding.", "time": "2023-05-07T17:43:19Z", "link": "http://arxiv.org/abs/2110.06196v3", "id": "2110.06196v3", "title": "GRAPE for Fast and Scalable Graph Processing and random walk-based\n  Embedding"}
{"author": "Azita Nouri, Philip E. Davis, Pradeep Subedi, Manish Parashar", "abstract": "Graph embedding techniques have attracted growing interest since they convert\nthe graph data into continuous and low-dimensional space. Effective graph\nanalytic provides users a deeper understanding of what is behind the data and\nthus can benefit a variety of machine learning tasks. With the current scale of\nreal-world applications, most graph analytic methods suffer high computation\nand space costs. These methods and systems can process a network with thousands\nto a few million nodes. However, scaling to large-scale networks remains a\nchallenge. The complexity of training graph embedding system requires the use\nof existing accelerators such as GPU. In this paper, we introduce a hybrid\nCPU-GPU framework that addresses the challenges of learning embedding of\nlarge-scale graphs. The performance of our method is compared qualitatively and\nquantitatively with the existing embedding systems on common benchmarks. We\nalso show that our system can scale training to datasets with an order of\nmagnitude greater than a single machine's total memory capacity. The\neffectiveness of the learned embedding is evaluated within multiple downstream\napplications. The experimental results indicate the effectiveness of the\nlearned embedding in terms of performance and accuracy.", "time": "2022-01-19T22:50:16Z", "link": "http://arxiv.org/abs/2110.06991v2", "id": "2110.06991v2", "title": "Scalable Graph Embedding LearningOn A Single GPU"}
{"author": "Yujing Ma, Florin Rusu, Kesheng Wu, Alexander Sim", "abstract": "Motivated by extreme multi-label classification applications, we consider\ntraining deep learning models over sparse data in multi-GPU servers. The\nvariance in the number of non-zero features across training batches and the\nintrinsic GPU heterogeneity combine to limit accuracy and increase the time to\nconvergence. We address these challenges with Adaptive SGD, an adaptive elastic\nmodel averaging stochastic gradient descent algorithm for heterogeneous\nmulti-GPUs that is characterized by dynamic scheduling, adaptive batch size\nscaling, and normalized model merging. Instead of statically partitioning\nbatches to GPUs, batches are routed based on the relative processing speed.\nBatch size scaling assigns larger batches to the faster GPUs and smaller\nbatches to the slower ones, with the goal to arrive at a steady state in which\nall the GPUs perform the same number of model updates. Normalized model merging\ncomputes optimal weights for every GPU based on the assigned batches such that\nthe combined model achieves better accuracy. We show experimentally that\nAdaptive SGD outperforms four state-of-the-art solutions in time-to-accuracy\nand is scalable with the number of GPUs.", "time": "2021-10-13T20:58:15Z", "link": "http://arxiv.org/abs/2110.07029v1", "id": "2110.07029v1", "title": "Adaptive Elastic Training for Sparse Deep Learning on Heterogeneous\n  Multi-GPU Servers"}
{"author": "Chaitanya Poolla, Rahul Saxena", "abstract": "The problem of learning parallel computer performance is investigated in the\ncontext of multicore processors. Given a fixed workload, the effect of varying\nsystem configuration on performance is sought. Conventionally, the performance\nspeedup due to a single resource enhancement is formulated using Amdahl's law.\nHowever, in case of multiple configurable resources the conventional\nformulation results in several disconnected speedup equations that cannot be\ncombined together to determine the overall speedup. To solve this problem, we\npropose to (1) extend Amdahl's law to accommodate multiple configurable\nresources into the overall speedup equation, and (2) transform the speedup\nequation into a multivariable regression problem suitable for machine learning.\nUsing experimental data from fifty-eight tests spanning two benchmarks (SPECCPU\n2017 and PCMark 10) and four hardware platforms (Intel Xeon 8180M, AMD EPYC\n7702P, Intel CoffeeLake 8700K, and AMD Ryzen 3900X), analytical models are\ndeveloped and cross-validated. Findings indicate that in most cases, the models\nresult in an average cross-validated accuracy higher than 95%, thereby\nvalidating the proposed extension of Amdahl's law. The proposed methodology\nenables rapid generation of multivariable analytical models to support future\nindustrial development, optimization, and simulation needs.", "time": "2022-09-26T20:17:23Z", "link": "http://arxiv.org/abs/2110.07822v2", "id": "2110.07822v2", "title": "On Extending Amdahl's law to Learn Computer Performance"}
{"author": "Vineeth S", "abstract": "Hardware compute power has been growing at an unprecedented rate in recent\nyears. The utilization of such advancements plays a key role in producing\nbetter results in less time -- both in academia and industry. However, merging\nthe existing hardware with the latest hardware within the same ecosystem poses\na challenging task. One of the key challenges, in this case, is varying compute\npower. In this paper, we consider the training of deep neural networks on a\ndistributed system of workers with varying compute power. A naive\nimplementation of synchronous distributed training will result in the faster\nworkers waiting for the slowest worker to complete processing. To mitigate this\nissue, we propose to dynamically adjust the data assigned for each worker\nduring the training. We assign each worker a partition of total data\nproportional to its computing power. Our experiments show that dynamically\nadjusting the data partition helps to improve the utilization of the system and\nsignificantly reduces the time taken for training. Code is available at the\nrepository: \\url{https://github.com/vineeths96/Heterogeneous-Systems}.", "time": "2021-10-03T11:21:49Z", "link": "http://arxiv.org/abs/2110.08941v1", "id": "2110.08941v1", "title": "Distributed Optimization using Heterogeneous Compute Systems"}
{"author": "Chan Yun Hin, Ngai Edith", "abstract": "Federated learning (FL) is able to manage edge devices to cooperatively train\na model while maintaining the training data local and private. One common\nassumption in FL is that all edge devices share the same machine learning model\nin training, for example, identical neural network architecture. However, the\ncomputation and store capability of different devices may not be the same.\nMoreover, reducing communication overheads can improve the training efficiency\nthough it is still a challenging problem in FL. In this paper, we propose a\nnovel FL method, called FedHe, inspired by knowledge distillation, which can\ntrain heterogeneous models and support asynchronous training processes with\nsignificantly reduced communication overheads. Our analysis and experimental\nresults demonstrate that the performance of our proposed method is better than\nthe state-of-the-art algorithms in terms of communication overheads and model\naccuracy.", "time": "2021-10-19T12:18:37Z", "link": "http://arxiv.org/abs/2110.09910v1", "id": "2110.09910v1", "title": "FedHe: Heterogeneous Models and Communication-Efficient Federated\n  Learning"}
{"author": "Amro Alabsi Aljundi, Taha Atahan AkyÄ±ldÄ±z, Kamer Kaya", "abstract": "Graphs are ubiquitous, and they can model unique characteristics and complex\nrelations of real-life systems. Although using machine learning (ML) on graphs\nis promising, their raw representation is not suitable for ML algorithms. Graph\nembedding represents each node of a graph as a d-dimensional vector which is\nmore suitable for ML tasks. However, the embedding process is expensive, and\nCPU-based tools do not scale to real-world graphs. In this work, we present\nGOSH, a GPU-based tool for embedding large-scale graphs with minimum hardware\nconstraints. GOSH employs a novel graph coarsening algorithm to enhance the\nimpact of updates and minimize the work for embedding. It also incorporates a\ndecomposition schema that enables any arbitrarily large graph to be embedded\nwith a single GPU. As a result, GOSH sets a new state-of-the-art in link\nprediction both in accuracy and speed, and delivers high-quality embeddings for\nnode classification at a fraction of the time compared to the state-of-the-art.\nFor instance, it can embed a graph with over 65 million vertices and 1.8\nbillion edges in less than 30 minutes on a single GPU.", "time": "2021-10-19T15:25:04Z", "link": "http://arxiv.org/abs/2110.10049v1", "id": "2110.10049v1", "title": "Boosting Graph Embedding on a Single GPU"}
{"author": "Shuo Liu, Nirupam Gupta, Nitin Vaidya", "abstract": "This paper considers the problem of resilient distributed optimization and\nstochastic machine learning in a server-based architecture. The system\ncomprises a server and multiple agents, where each agent has a local cost\nfunction. The agents collaborate with the server to find a minimum of their\naggregate cost functions. We consider the case when some of the agents may be\nasynchronous and/or Byzantine faulty. In this case, the classical algorithm of\ndistributed gradient descent (DGD) is rendered ineffective. Our goal is to\ndesign techniques improving the efficacy of DGD with asynchrony and Byzantine\nfailures. To do so, we start by proposing a way to model the agents' cost\nfunctions by the generic notion of $(f, \\,r; \\epsilon)$-redundancy where $f$\nand $r$ are the parameters of Byzantine failures and asynchrony, respectively,\nand $\\epsilon$ characterizes the closeness between agents' cost functions. This\nallows us to quantify the level of redundancy present amongst the agents' cost\nfunctions, for any given distributed optimization problem. We demonstrate, both\ntheoretically and empirically, the merits of our proposed redundancy model in\nimproving the robustness of DGD against asynchronous and Byzantine agents, and\ntheir extensions to distributed stochastic gradient descent (D-SGD) for robust\ndistributed machine learning with asynchronous and Byzantine agents.", "time": "2021-10-21T02:41:19Z", "link": "http://arxiv.org/abs/2110.10858v1", "id": "2110.10858v1", "title": "Utilizing Redundancy in Cost Functions for Resilience in Distributed\n  Optimization and Learning"}
{"author": "Joost Verbraeken, Martijn de Vos, Johan Pouwelse", "abstract": "Federated learning (FL) is a privacy-friendly type of machine learning where\ndevices locally train a model on their private data and typically communicate\nmodel updates with a server. In decentralized FL (DFL), peers communicate model\nupdates with each other instead. However, DFL is challenging since (1) the\ntraining data possessed by different peers is often non-i.i.d. (i.e.,\ndistributed differently between the peers) and (2) malicious, or Byzantine,\nattackers can share arbitrary model updates with other peers to subvert the\ntraining process.\n  We address these two challenges and present Bristle, middleware between the\nlearning application and the decentralized network layer. Bristle leverages\ntransfer learning to predetermine and freeze the non-output layers of a neural\nnetwork, significantly speeding up model training and lowering communication\ncosts. To securely update the output layer with model updates from other peers,\nwe design a fast distance-based prioritizer and a novel performance-based\nintegrator. Their combined effect results in high resilience to Byzantine\nattackers and the ability to handle non-i.i.d. classes.\n  We empirically show that Bristle converges to a consistent 95% accuracy in\nByzantine environments, outperforming all evaluated baselines. In non-Byzantine\nenvironments, Bristle requires 83% fewer iterations to achieve 90% accuracy\ncompared to state-of-the-art methods. We show that when the training classes\nare non-i.i.d., Bristle significantly outperforms the accuracy of the most\nByzantine-resilient baselines by 2.3x while reducing communication costs by\n90%.", "time": "2021-10-21T09:20:48Z", "link": "http://arxiv.org/abs/2110.11006v1", "id": "2110.11006v1", "title": "Bristle: Decentralized Federated Learning in Byzantine, Non-i.i.d.\n  Environments"}
{"author": "Steven Farrell, Murali Emani, Jacob Balma, Lukas Drescher, Aleksandr Drozd, Andreas Fink, Geoffrey Fox, David Kanter, Thorsten Kurth, Peter Mattson, Dawei Mu, Amit Ruhela, Kento Sato, Koichi Shirahata, Tsuguchika Tabaru, Aristeidis Tsaris, Jan Balewski, Ben Cumming, Takumi Danjo, Jens Domke, Takaaki Fukai, Naoto Fukumoto, Tatsuya Fukushi, Balazs Gerofi, Takumi Honda, Toshiyuki Imamura, Akihiko Kasagi, Kentaro Kawakami, Shuhei Kudo, Akiyoshi Kuroda, Maxime Martinasso, Satoshi Matsuoka, Henrique MendonÃ§a, Kazuki Minami, Prabhat Ram, Takashi Sawada, Mallikarjun Shankar, Tom St. John, Akihiro Tabuchi, Venkatram Vishwanath, Mohamed Wahib, Masafumi Yamazaki, Junqi Yin", "abstract": "Scientific communities are increasingly adopting machine learning and deep\nlearning models in their applications to accelerate scientific insights. High\nperformance computing systems are pushing the frontiers of performance with a\nrich diversity of hardware resources and massive scale-out capabilities. There\nis a critical need to understand fair and effective benchmarking of machine\nlearning applications that are representative of real-world scientific use\ncases. MLPerf is a community-driven standard to benchmark machine learning\nworkloads, focusing on end-to-end performance metrics. In this paper, we\nintroduce MLPerf HPC, a benchmark suite of large-scale scientific machine\nlearning training applications driven by the MLCommons Association. We present\nthe results from the first submission round, including a diverse set of some of\nthe world's largest HPC systems. We develop a systematic framework for their\njoint analysis and compare them in terms of data staging, algorithmic\nconvergence, and compute performance. As a result, we gain a quantitative\nunderstanding of optimizations on different subsystems such as staging and\non-node loading of data, compute-unit utilization, and communication\nscheduling, enabling overall $>10 \\times$ (end-to-end) performance improvements\nthrough system scaling. Notably, our analysis shows a scale-dependent interplay\nbetween the dataset size, a system's memory hierarchy, and training convergence\nthat underlines the importance of near-compute storage. To overcome the\ndata-parallel scalability challenge at large batch sizes, we discuss specific\nlearning techniques and hybrid data-and-model parallelism that are effective on\nlarge systems. We conclude by characterizing each benchmark with respect to\nlow-level memory, I/O, and network behavior to parameterize extended roofline\nperformance models in future rounds.", "time": "2021-10-26T20:56:01Z", "link": "http://arxiv.org/abs/2110.11466v2", "id": "2110.11466v2", "title": "MLPerf HPC: A Holistic Benchmark Suite for Scientific Machine Learning\n  on HPC Systems"}
{"author": "Mohamed Yassine Boukhari, Akash Dhasade, Anne-Marie Kermarrec, Rafael Pires, Othmane Safsafi, Rishi Sharma", "abstract": "Federated learning (FL) enables a set of client devices to collaboratively\ntrain a model without sharing raw data. This process, though, operates under\nthe constrained computation and communication resources of edge devices. These\nconstraints combined with systems heterogeneity force some participating\nclients to perform fewer local updates than expected by the server, thus\nslowing down convergence. Exhaustive tuning of hyperparameters in FL,\nfurthermore, can be resource-intensive, without which the convergence is\nadversely affected. In this work, we propose GeL, the guess and learn\nalgorithm. GeL enables constrained edge devices to perform additional learning\nthrough guessed updates on top of gradient-based steps. These guesses are\ngradientless, i.e., participating clients leverage them for free. Our generic\nguessing algorithm (i) can be flexibly combined with several state-of-the-art\nalgorithms including FedProx, FedNova or FedYogi; and (ii) achieves\nsignificantly improved performance when the learning rates are not best tuned.\nWe conduct extensive experiments and show that GeL can boost empirical\nconvergence by up to 40% in resource-constrained networks while relieving the\nneed for exhaustive learning rate tuning.", "time": "2023-12-11T13:54:43Z", "link": "http://arxiv.org/abs/2110.11486v2", "id": "2110.11486v2", "title": "Boosting Federated Learning in Resource-Constrained Networks"}
{"author": "Bingyan Liu, Yifeng Cai, Ziqi Zhang, Yuanchun Li, Leye Wang, Ding Li, Yao Guo, Xiangqun Chen", "abstract": "Federated learning (FL) has emerged as an effective solution to decentralized\nand privacy-preserving machine learning for mobile clients. While traditional\nFL has demonstrated its superiority, it ignores the non-iid (independently\nidentically distributed) situation, which widely exists in mobile scenarios.\nFailing to handle non-iid situations could cause problems such as performance\ndecreasing and possible attacks. Previous studies focus on the \"symptoms\"\ndirectly, as they try to improve the accuracy or detect possible attacks by\nadding extra steps to conventional FL models. However, previous techniques\noverlook the root causes for the \"symptoms\": blindly aggregating models with\nthe non-iid distributions. In this paper, we try to fundamentally address the\nissue by decomposing the overall non-iid situation into several iid clusters\nand conducting aggregation in each cluster. Specifically, we propose\n\\textbf{DistFL}, a novel framework to achieve automated and accurate\n\\textbf{Dist}ribution-aware \\textbf{F}ederated \\textbf{L}earning in a\ncost-efficient way. DistFL achieves clustering via extracting and comparing the\n\\textit{distribution knowledge} from the uploaded models. With this framework,\nwe are able to generate multiple personalized models with distinctive\ndistributions and assign them to the corresponding clients. Extensive\nexperiments on mobile scenarios with popular model architectures have\ndemonstrated the effectiveness of DistFL.", "time": "2021-10-22T06:58:48Z", "link": "http://arxiv.org/abs/2110.11619v1", "id": "2110.11619v1", "title": "DistFL: Distribution-aware Federated Learning for Mobile Scenarios"}
{"author": "Byunggook Na, Jaehee Jang, Seongsik Park, Seijoon Kim, Joonoo Kim, Moon Sik Jeong, Kwang Choon Kim, Seon Heo, Yoonsang Kim, Sungroh Yoon", "abstract": "Various deep learning applications on smartphones have been rapidly rising,\nbut training deep neural networks (DNNs) has too large computational burden to\nbe executed on a single smartphone. A portable cluster, which connects\nsmartphones with a wireless network and supports parallel computation using\nthem, can be a potential approach to resolve the issue. However, by our\nfindings, the limitations of wireless communication restrict the cluster size\nto up to 30 smartphones. Such small-scale clusters have insufficient\ncomputational power to train DNNs from scratch. In this paper, we propose a\nscalable smartphone cluster enabling deep learning training by removing the\nportability to increase its computational efficiency. The cluster connects 138\nGalaxy S10+ devices with a wired network using Ethernet. We implemented\nlarge-batch synchronous training of DNNs based on Caffe, a deep learning\nlibrary. The smartphone cluster yielded 90% of the speed of a P100 when\ntraining ResNet-50, and approximately 43x speed-up of a V100 when training\nMobileNet-v1.", "time": "2021-10-23T08:48:24Z", "link": "http://arxiv.org/abs/2110.12172v1", "id": "2110.12172v1", "title": "Scalable Smartphone Cluster for Deep Learning"}
{"author": "Mohammad Goudarzi, Marimuthu Palaniswami, Rajkumar Buyya", "abstract": "Fog/Edge computing is a novel computing paradigm supporting\nresource-constrained Internet of Things (IoT) devices by the placement of their\ntasks on the edge and/or cloud servers. Recently, several Deep Reinforcement\nLearning (DRL)-based placement techniques have been proposed in fog/edge\ncomputing environments, which are only suitable for centralized setups. The\ntraining of well-performed DRL agents requires manifold training data while\nobtaining training data is costly. Hence, these centralized DRL-based\ntechniques lack generalizability and quick adaptability, thus failing to\nefficiently tackle application placement problems. Moreover, many IoT\napplications are modeled as Directed Acyclic Graphs (DAGs) with diverse\ntopologies. Satisfying dependencies of DAG-based IoT applications incur\nadditional constraints and increase the complexity of placement problems. To\novercome these challenges, we propose an actor-critic-based distributed\napplication placement technique, working based on the IMPortance weighted\nActor-Learner Architectures (IMPALA). IMPALA is known for efficient distributed\nexperience trajectory generation that significantly reduces the exploration\ncosts of agents. Besides, it uses an adaptive off-policy correction method for\nfaster convergence to optimal solutions. Our technique uses recurrent layers to\ncapture temporal behaviors of input data and a replay buffer to improve the\nsample efficiency. The performance results, obtained from simulation and\ntestbed experiments, demonstrate that our technique significantly improves the\nexecution cost of IoT applications up to 30\\% compared to its counterparts.", "time": "2021-10-24T11:25:03Z", "link": "http://arxiv.org/abs/2110.12415v1", "id": "2110.12415v1", "title": "A Distributed Deep Reinforcement Learning Technique for Application\n  Placement in Edge and Fog Computing Environments"}
{"author": "XinYu Piao, DoangJoo Synn, JooYoung Park, Jong-Kook Kim", "abstract": "Recent deep learning models are difficult to train using a large batch size,\nbecause commodity machines may not have enough memory to accommodate both the\nmodel and a large data batch size. The batch size is one of the\nhyper-parameters used in the training model, and it is dependent on and is\nlimited by the target machine memory capacity because the batch size can only\nfit into the remaining memory after the model is uploaded. Moreover, the data\nitem size is also an important factor because if each data item size is larger\nthen the batch size that can fit into the remaining memory becomes smaller.\nThis paper proposes a method called Micro-Batch Processing (MBP) to address\nthis problem. This method helps deep learning models to train by providing a\nbatch processing method that splits a batch into a size that can fit in the\nremaining memory and processes them sequentially. After processing the small\nbatches individually, a loss normalization algorithm based on the gradient\naccumulation is used to maintain the performance. The purpose of our method is\nto allow deep learning models to train using larger batch sizes that exceed the\nmemory capacity of a system without increasing the memory size or using\nmultiple devices (GPUs).", "time": "2024-07-02T13:33:39Z", "link": "http://arxiv.org/abs/2110.12484v3", "id": "2110.12484v3", "title": "Enabling Large Batch Size Training for DNN Models Beyond the Memory\n  Limit While Maintaining Performance"}
{"author": "Masaki Furukawa, Hiroki Matsutani", "abstract": "A computing cluster that interconnects multiple compute nodes is used to\naccelerate distributed reinforcement learning based on DQN (Deep Q-Network). In\ndistributed reinforcement learning, Actor nodes acquire experiences by\ninteracting with a given environment and a Learner node optimizes their DQN\nmodel. Since data transfer between Actor and Learner nodes increases depending\non the number of Actor nodes and their experience size, communication overhead\nbetween them is one of major performance bottlenecks. In this paper, their\ncommunication is accelerated by DPDK-based network optimizations, and\nDPDK-based low-latency experience replay memory server is deployed between\nActor and Learner nodes interconnected with a 40GbE (40Gbit Ethernet) network.\nEvaluation results show that, as a network optimization technique, kernel\nbypassing by DPDK reduces network access latencies to a shared memory server by\n32.7% to 58.9%. As another network optimization technique, an in-network\nexperience replay memory server between Actor and Learner nodes reduces access\nlatencies to the experience replay memory by 11.7% to 28.1% and communication\nlatencies for prioritized experience sampling by 21.9% to 29.1%.", "time": "2023-03-10T17:44:00Z", "link": "http://arxiv.org/abs/2110.13506v3", "id": "2110.13506v3", "title": "Accelerating Distributed Deep Reinforcement Learning by In-Network\n  Experience Sampling"}
{"author": "Yue Zhao, George H. Chen, Zhihao Jia", "abstract": "Outlier detection (OD) is a key learning task for finding rare and deviant\ndata samples, with many time-critical applications such as fraud detection and\nintrusion detection. In this work, we propose TOD, the first tensor-based\nsystem for efficient and scalable outlier detection on distributed multi-GPU\nmachines. A key idea behind TOD is decomposing complex OD applications into a\nsmall collection of basic tensor algebra operators. This decomposition enables\nTOD to accelerate OD computations by leveraging recent advances in deep\nlearning infrastructure in both hardware and software. Moreover, to deploy\nmemory-intensive OD applications on modern GPUs with limited on-device memory,\nwe introduce two key techniques. First, provable quantization speeds up OD\ncomputations and reduces its memory footprint by automatically performing\nspecific floating-point operations in lower precision while provably\nguaranteeing no accuracy loss. Second, to exploit the aggregated compute\nresources and memory capacity of multiple GPUs, we introduce automatic\nbatching, which decomposes OD computations into small batches for both\nsequential execution on a single GPU and parallel execution on multiple GPUs.\n  TOD supports a diverse set of OD algorithms. Extensive evaluation on 11 real\nand 3 synthetic OD datasets shows that TOD is on average 10.9x faster than the\nleading CPU-based OD system PyOD (with a maximum speedup of 38.9x), and can\nhandle much larger datasets than existing GPU-based OD systems. In addition,\nTOD allows easy integration of new OD operators, enabling fast prototyping of\nemerging and yet-to-discovered OD algorithms.", "time": "2022-09-17T02:50:12Z", "link": "http://arxiv.org/abs/2110.14007v3", "id": "2110.14007v3", "title": "TOD: GPU-accelerated Outlier Detection via Tensor Operations"}
{"author": "Yang Hu, Connor Imes, Xuanang Zhao, Souvik Kundu, Peter A. Beerel, Stephen P. Crago, John Paul N. Walters", "abstract": "Deep neural networks with large model sizes achieve state-of-the-art results\nfor tasks in computer vision (CV) and natural language processing (NLP).\nHowever, these large-scale models are too compute- or memory-intensive for\nresource-constrained edge devices. Prior works on parallel and distributed\nexecution primarily focus on training -- rather than inference -- using\nhomogeneous accelerators in data centers. We propose EdgePipe, a distributed\nframework for edge systems that uses pipeline parallelism to both speed up\ninference and enable running larger (and more accurate) models that otherwise\ncannot fit on single edge devices. EdgePipe achieves these results by using an\noptimal partition strategy that considers heterogeneity in compute, memory, and\nnetwork bandwidth. Our empirical evaluation demonstrates that EdgePipe achieves\n$10.59\\times$ and $11.88\\times$ speedup using 16 edge devices for the ViT-Large\nand ViT-Huge models, respectively, with no accuracy loss. Similarly, EdgePipe\nimproves ViT-Huge throughput by $3.93\\times$ over a 4-node baseline using 16\nedge devices, which independently cannot fit the model in memory. Finally, we\nshow up to $4.16\\times$ throughput improvement over the state-of-the-art\nPipeDream when using a heterogeneous set of devices.", "time": "2021-10-28T05:20:51Z", "link": "http://arxiv.org/abs/2110.14895v1", "id": "2110.14895v1", "title": "Pipeline Parallelism for Inference on Heterogeneous Edge Computing"}
{"author": "Abdelrahman Hosny, Marina Neseem, Sherief Reda", "abstract": "Training on the Edge enables neural networks to learn continuously from new\ndata after deployment on memory-constrained edge devices. Previous work is\nmostly concerned with reducing the number of model parameters which is only\nbeneficial for inference. However, memory footprint from activations is the\nmain bottleneck for training on the edge. Existing incremental training methods\nfine-tune the last few layers sacrificing accuracy gains from re-training the\nwhole model. In this work, we investigate the memory footprint of training deep\nlearning models, and use our observations to propose BitTrain. In BitTrain, we\nexploit activation sparsity and propose a novel bitmap compression technique\nthat reduces the memory footprint during training. We save the activations in\nour proposed bitmap compression format during the forward pass of the training,\nand restore them during the backward pass for the optimizer computations. The\nproposed method can be integrated seamlessly in the computation graph of modern\ndeep learning frameworks. Our implementation is safe by construction, and has\nno negative impact on the accuracy of model training. Experimental results show\nup to 34% reduction in the memory footprint at a sparsity level of 50%. Further\npruning during training results in more than 70% sparsity, which can lead to up\nto 56% reduction in memory footprint. BitTrain advances the efforts towards\nbringing more machine learning capabilities to edge devices. Our source code is\navailable at https://github.com/scale-lab/BitTrain.", "time": "2021-10-29T16:30:57Z", "link": "http://arxiv.org/abs/2110.15362v1", "id": "2110.15362v1", "title": "BitTrain: Sparse Bitmap Compression for Memory-Efficient Training on the\n  Edge"}
{"author": "Pavana Prakash, Jiahao Ding, Maoqiang Wu, Minglei Shu, Rong Yu, Miao Pan", "abstract": "Federated learning (FL), an emerging distributed machine learning paradigm,\nin conflux with edge computing is a promising area with novel applications over\nmobile edge devices. In FL, since mobile devices collaborate to train a model\nbased on their own data under the coordination of a central server by sharing\njust the model updates, training data is maintained private. However, without\nthe central availability of data, computing nodes need to communicate the model\nupdates often to attain convergence. Hence, the local computation time to\ncreate local model updates along with the time taken for transmitting them to\nand from the server result in a delay in the overall time. Furthermore,\nunreliable network connections may obstruct an efficient communication of these\nupdates. To address these, in this paper, we propose a delay-efficient FL\nmechanism that reduces the overall time (consisting of both the computation and\ncommunication latencies) and communication rounds required for the model to\nconverge. Exploring the impact of various parameters contributing to delay, we\nseek to balance the trade-off between wireless communication (to talk) and\nlocal computation (to work). We formulate a relation with overall time as an\noptimization problem and demonstrate the efficacy of our approach through\nextensive simulations.", "time": "2021-11-01T00:35:32Z", "link": "http://arxiv.org/abs/2111.00637v1", "id": "2111.00637v1", "title": "To Talk or to Work: Delay Efficient Federated Learning over Mobile Edge\n  Devices"}
{"author": "Ahmed M. Abdelmoniem, Atal Narayan Sahu, Marco Canini, Suhaib A. Fahmy", "abstract": "Federated Learning (FL) enables distributed training by learners using local\ndata, thereby enhancing privacy and reducing communication. However, it\npresents numerous challenges relating to the heterogeneity of the data\ndistribution, device capabilities, and participant availability as deployments\nscale, which can impact both model convergence and bias. Existing FL schemes\nuse random participant selection to improve fairness; however, this can result\nin inefficient use of resources and lower quality training. In this work, we\nsystematically address the question of resource efficiency in FL, showing the\nbenefits of intelligent participant selection, and incorporation of updates\nfrom straggling participants. We demonstrate how these factors enable resource\nefficiency while also improving trained model quality.", "time": "2022-11-04T13:00:09Z", "link": "http://arxiv.org/abs/2111.01108v2", "id": "2111.01108v2", "title": "Resource-Efficient Federated Learning"}
{"author": "Jossekin Beilharz, Bjarne Pfitzner, Robert Schmid, Paul Geppert, Bert Arnrich, Andreas Polze", "abstract": "Federated learning allows a group of distributed clients to train a common\nmachine learning model on private data. The exchange of model updates is\nmanaged either by a central entity or in a decentralized way, e.g. by a\nblockchain. However, the strong generalization across all clients makes these\napproaches unsuited for non-independent and identically distributed (non-IID)\ndata.\n  We propose a unified approach to decentralization and personalization in\nfederated learning that is based on a directed acyclic graph (DAG) of model\nupdates. Instead of training a single global model, clients specialize on their\nlocal data while using the model updates from other clients dependent on the\nsimilarity of their respective data. This specialization implicitly emerges\nfrom the DAG-based communication and selection of model updates. Thus, we\nenable the evolution of specialized models, which focus on a subset of the data\nand therefore cover non-IID data better than federated learning in a\ncentralized or blockchain-based setup.\n  To the best of our knowledge, the proposed solution is the first to unite\npersonalization and poisoning robustness in fully decentralized federated\nlearning. Our evaluation shows that the specialization of models emerges\ndirectly from the DAG-based communication of model updates on three different\ndatasets. Furthermore, we show stable model accuracy and less variance across\nclients when compared to federated averaging.", "time": "2021-11-03T08:09:29Z", "link": "http://arxiv.org/abs/2111.01257v2", "id": "2111.01257v2", "title": "Implicit Model Specialization through DAG-based Decentralized Federated\n  Learning"}
{"author": "Fahao Chen, Peng Li, Toshiaki Miyazaki, Celimuge Wu", "abstract": "Federated learning has attracted much research attention due to its privacy\nprotection in distributed machine learning. However, existing work of federated\nlearning mainly focuses on Convolutional Neural Network (CNN), which cannot\nefficiently handle graph data that are popular in many applications. Graph\nConvolutional Network (GCN) has been proposed as one of the most promising\ntechniques for graph learning, but its federated setting has been seldom\nexplored. In this paper, we propose FedGraph for federated graph learning among\nmultiple computing clients, each of which holds a subgraph. FedGraph provides\nstrong graph learning capability across clients by addressing two unique\nchallenges. First, traditional GCN training needs feature data sharing among\nclients, leading to risk of privacy leakage. FedGraph solves this issue using a\nnovel cross-client convolution operation. The second challenge is high GCN\ntraining overhead incurred by large graph size. We propose an intelligent graph\nsampling algorithm based on deep reinforcement learning, which can\nautomatically converge to the optimal sampling policies that balance training\nspeed and accuracy. We implement FedGraph based on PyTorch and deploy it on a\ntestbed for performance evaluation. The experimental results of four popular\ndatasets demonstrate that FedGraph significantly outperforms existing work by\nenabling faster convergence to higher accuracy.", "time": "2021-11-02T04:58:03Z", "link": "http://arxiv.org/abs/2111.01370v1", "id": "2111.01370v1", "title": "FedGraph: Federated Graph Learning with Intelligent Sampling"}
{"author": "Jamie Cui, Cen Chen, Tiandi Ye, Li Wang", "abstract": "Recently, Niu, et. al. introduced a new variant of Federated Learning (FL),\ncalled Federated Submodel Learning (FSL). Different from traditional FL, each\nclient locally trains the submodel (e.g., retrieved from the servers) based on\nits private data and uploads a submodel at its choice to the servers. Then all\nclients aggregate all their submodels and finish the iteration. Inevitably, FSL\nintroduces two privacy-preserving computation tasks, i.e., Private Submodel\nRetrieval (PSR) and Secure Submodel Aggregation (SSA). Existing work fails to\nprovide a loss-less scheme, or has impractical efficiency. In this work, we\nleverage Distributed Point Function (DPF) and cuckoo hashing to construct a\npractical and light-weight secure FSL scheme in the two-server setting. More\nspecifically, we propose two basic protocols with few optimisation techniques,\nwhich ensures our protocol practicality on specific real-world FSL tasks. Our\nexperiments show that our proposed protocols can finish in less than 1 minute\nwhen weight sizes $\\leq 2^{15}$, we also demonstrate protocol efficiency by\ncomparing with existing work and by handling a real-world FSL task.", "time": "2021-11-02T08:47:27Z", "link": "http://arxiv.org/abs/2111.01432v1", "id": "2111.01432v1", "title": "Practical and Light-weight Secure Aggregation for Federated Submodel\n  Learning"}
{"author": "Rehmat Ullah, Di Wu, Paul Harvey, Peter Kilpatrick, Ivor Spence, Blesson Varghese", "abstract": "Federated learning (FL) is a privacy-preserving distributed machine learning\ntechnique that trains models while keeping all the original data generated on\ndevices locally. Since devices may be resource constrained, offloading can be\nused to improve FL performance by transferring computational workload from\ndevices to edge servers. However, due to mobility, devices participating in FL\nmay leave the network during training and need to connect to a different edge\nserver. This is challenging because the offloaded computations from edge server\nneed to be migrated. In line with this assertion, we present FedFly, which is,\nto the best of our knowledge, the first work to migrate a deep neural network\n(DNN) when devices move between edge servers during FL training. Our empirical\nresults on the CIFAR10 dataset, with both balanced and imbalanced data\ndistribution, support our claims that FedFly can reduce training time by up to\n33% when a device moves after 50% of the training is completed, and by up to\n45% when 90% of the training is completed when compared to state-of-the-art\noffloading approach in FL. FedFly has negligible overhead of up to two seconds\nand does not compromise accuracy. Finally, we highlight a number of open\nresearch issues for further investigation. FedFly can be downloaded from\nhttps://github.com/qub-blesson/FedFly.", "time": "2022-07-14T22:46:58Z", "link": "http://arxiv.org/abs/2111.01516v2", "id": "2111.01516v2", "title": "FedFly: Towards Migration in Edge-based Distributed Federated Learning"}
{"author": "Jun-Liang Lin, Sheng-De Wang", "abstract": "The inference of Neural Networks is usually restricted by the resources\n(e.g., computing power, memory, bandwidth) on edge devices. In addition to\nimproving the hardware design and deploying efficient models, it is possible to\naggregate the computing power of many devices to enable the machine learning\nmodels. In this paper, we proposed a novel method of exploiting model\nparallelism to separate a neural network for distributed inferences. To achieve\na better balance between communication latency, computation latency, and\nperformance, we adopt neural architecture search (NAS) to search for the best\ntransmission policy and reduce the amount of communication. The best model we\nfound decreases by 86.6% of the amount of data transmission compared to the\nbaseline and does not impact performance much. Under proper specifications of\ndevices and configurations of models, our experiments show that the inference\nof large neural networks on edge clusters can be distributed and accelerated,\nwhich provides a new solution for the deployment of intelligent applications in\nthe internet of things (IoT).", "time": "2021-11-03T19:30:28Z", "link": "http://arxiv.org/abs/2111.02489v1", "id": "2111.02489v1", "title": "Communication-Efficient Separable Neural Network for Distributed\n  Inference on Edge Devices"}
{"author": "Jie Zhang, Song Guo, Xiaosong Ma, Haozhao Wang, Wencao Xu, Feijie Wu", "abstract": "In recent years, personalized federated learning (pFL) has attracted\nincreasing attention for its potential in dealing with statistical\nheterogeneity among clients. However, the state-of-the-art pFL methods rely on\nmodel parameters aggregation at the server side, which require all models to\nhave the same structure and size, and thus limits the application for more\nheterogeneous scenarios. To deal with such model constraints, we exploit the\npotentials of heterogeneous model settings and propose a novel training\nframework to employ personalized models for different clients. Specifically, we\nformulate the aggregation procedure in original pFL into a personalized group\nknowledge transfer training algorithm, namely, KT-pFL, which enables each\nclient to maintain a personalized soft prediction at the server side to guide\nthe others' local training. KT-pFL updates the personalized soft prediction of\neach client by a linear combination of all local soft predictions using a\nknowledge coefficient matrix, which can adaptively reinforce the collaboration\namong clients who own similar data distribution. Furthermore, to quantify the\ncontributions of each client to others' personalized training, the knowledge\ncoefficient matrix is parameterized so that it can be trained simultaneously\nwith the models. The knowledge coefficient matrix and the model parameters are\nalternatively updated in each round following the gradient descent way.\nExtensive experiments on various datasets (EMNIST, Fashion\\_MNIST, CIFAR-10)\nare conducted under different settings (heterogeneous models and data\ndistributions). It is demonstrated that the proposed framework is the first\nfederated learning paradigm that realizes personalized model training via\nparameterized group knowledge transfer while achieving significant performance\ngain comparing with state-of-the-art algorithms.", "time": "2021-11-04T13:41:45Z", "link": "http://arxiv.org/abs/2111.02862v1", "id": "2111.02862v1", "title": "Parameterized Knowledge Transfer for Personalized Federated Learning"}
{"author": "Junya Chen, Sijia Wang, Lawrence Carin, Chenyang Tao", "abstract": "Distributed learning has become an integral tool for scaling up machine\nlearning and addressing the growing need for data privacy. Although more robust\nto the network topology, decentralized learning schemes have not gained the\nsame level of popularity as their centralized counterparts for being less\ncompetitive performance-wise. In this work, we attribute this issue to the lack\nof synchronization among decentralized learning workers, showing both\nempirically and theoretically that the convergence rate is tied to the\nsynchronization level among the workers. Such motivated, we present a novel\ndecentralized learning framework based on nonlinear gossiping (NGO), that\nenjoys an appealing finite-time consensus property to achieve better\nsynchronization. We provide a careful analysis of its convergence and discuss\nits merits for modern distributed optimization applications, such as deep\nneural networks. Our analysis on how communication delay and randomized chats\naffect learning further enables the derivation of practical variants that\naccommodate asynchronous and randomized communications. To validate the\neffectiveness of our proposal, we benchmark NGO against competing solutions\nthrough an extensive set of tests, with encouraging results reported.", "time": "2021-11-13T20:38:48Z", "link": "http://arxiv.org/abs/2111.02949v2", "id": "2111.02949v2", "title": "Finite-Time Consensus Learning for Decentralized Optimization with\n  Nonlinear Gossiping"}
{"author": "Durmus Alp Emre Acar, Yue Zhao, Ramon Matas Navarro, Matthew Mattina, Paul N. Whatmough, Venkatesh Saligrama", "abstract": "We propose a novel federated learning method for distributively training\nneural network models, where the server orchestrates cooperation between a\nsubset of randomly chosen devices in each round. We view Federated Learning\nproblem primarily from a communication perspective and allow more device level\ncomputations to save transmission costs. We point out a fundamental dilemma, in\nthat the minima of the local-device level empirical loss are inconsistent with\nthose of the global empirical loss. Different from recent prior works, that\neither attempt inexact minimization or utilize devices for parallelizing\ngradient computation, we propose a dynamic regularizer for each device at each\nround, so that in the limit the global and device solutions are aligned. We\ndemonstrate both through empirical results on real and synthetic data as well\nas analytical results that our scheme leads to efficient training, in both\nconvex and non-convex settings, while being fully agnostic to device\nheterogeneity and robust to large number of devices, partial participation and\nunbalanced data.", "time": "2021-11-09T16:37:10Z", "link": "http://arxiv.org/abs/2111.04263v2", "id": "2111.04263v2", "title": "Federated Learning Based on Dynamic Regularization"}
{"author": "Bicheng Ying, Kun Yuan, Hanbin Hu, Yiming Chen, Wotao Yin", "abstract": "Decentralized algorithm is a form of computation that achieves a global goal\nthrough local dynamics that relies on low-cost communication between\ndirectly-connected agents. On large-scale optimization tasks involving\ndistributed datasets, decentralized algorithms have shown strong, sometimes\nsuperior, performance over distributed algorithms with a central node.\nRecently, developing decentralized algorithms for deep learning has attracted\ngreat attention. They are considered as low-communication-overhead alternatives\nto those using a parameter server or the Ring-Allreduce protocol. However, the\nlack of an easy-to-use and efficient software package has kept most\ndecentralized algorithms merely on paper. To fill the gap, we introduce\nBlueFog, a python library for straightforward, high-performance implementations\nof diverse decentralized algorithms. Based on a unified abstraction of various\ncommunication operations, BlueFog offers intuitive interfaces to implement a\nspectrum of decentralized algorithms, from those using a static, undirected\ngraph for synchronous operations to those using dynamic and directed graphs for\nasynchronous operations. BlueFog also adopts several system-level acceleration\ntechniques to further optimize the performance on the deep learning tasks. On\nmainstream DNN training tasks, BlueFog reaches a much higher throughput and\nachieves an overall $1.2\\times \\sim 1.8\\times$ speedup over Horovod, a\nstate-of-the-art distributed deep learning package based on Ring-Allreduce.\nBlueFog is open source at https://github.com/Bluefog-Lib/bluefog.", "time": "2021-11-08T06:06:39Z", "link": "http://arxiv.org/abs/2111.04287v1", "id": "2111.04287v1", "title": "BlueFog: Make Decentralized Algorithms Practical for Optimization and\n  Deep Learning"}
{"author": "Renato Cardoso, Dejan Golubovic, Ignacio Peluaga Lozada, Ricardo Rocha, JoÃ£o Fernandes, Sofia Vallecorsa", "abstract": "With the increasing number of Machine and Deep Learning applications in High\nEnergy Physics, easy access to dedicated infrastructure represents a\nrequirement for fast and efficient R&D. This work explores different types of\ncloud services to train a Generative Adversarial Network (GAN) in a parallel\nenvironment, using Tensorflow data parallel strategy. More specifically, we\nparallelize the training process on multiple GPUs and Google Tensor Processing\nUnits (TPU) and we compare two algorithms: the TensorFlow built-in logic and a\ncustom loop, optimised to have higher control of the elements assigned to each\nGPU worker or TPU core. The quality of the generated data is compared to Monte\nCarlo simulation. Linear speed-up of the training process is obtained, while\nretaining most of the performance in terms of physics results. Additionally, we\nbenchmark the aforementioned approaches, at scale, over multiple GPU nodes,\ndeploying the training process on different public cloud providers, seeking for\noverall efficiency and cost-effectiveness. The combination of data science,\ncloud deployment options and associated economics allows to burst out\nheterogeneously, exploring the full potential of cloud-based services.", "time": "2021-11-08T16:59:15Z", "link": "http://arxiv.org/abs/2111.04628v1", "id": "2111.04628v1", "title": "Accelerating GAN training using highly parallel hardware on public cloud"}
{"author": "Aashaka Shah, Vijay Chidambaram, Meghan Cowan, Saeed Maleki, Madan Musuvathi, Todd Mytkowicz, Jacob Nelson, Olli Saarikivi, Rachee Singh", "abstract": "Machine learning models are increasingly being trained across multiple GPUs\nand servers. In this setting, data is transferred between GPUs using\ncommunication collectives such as AlltoAll and AllReduce, which can become a\nsignificant bottleneck in training large models. Thus, it is important to use\nefficient algorithms for collective communication. We develop TACCL, a tool\nthat enables algorithm designers to guide a synthesizer into automatically\ngenerating algorithms for a given hardware configuration and communication\ncollective. TACCL uses a novel communication sketch abstraction to get crucial\ninformation from the designer to significantly reduce the search space and\nguide the synthesizer towards better algorithms. TACCL also uses a novel\nencoding of the problem that allows it to scale beyond single-node topologies.\nWe use TACCL to synthesize algorithms for three collectives and two hardware\ntopologies: DGX-2 and NDv2. We demonstrate that the algorithms synthesized by\nTACCL outperform the Nvidia Collective Communication Library (NCCL) by up to\n6.7x. We also show that TACCL can speed up end-to-end training of\nTransformer-XL and BERT models by 11%--2.3x for different batch sizes.", "time": "2022-10-05T05:01:59Z", "link": "http://arxiv.org/abs/2111.04867v4", "id": "2111.04867v4", "title": "TACCL: Guiding Collective Algorithm Synthesis using Communication\n  Sketches"}
{"author": "Dzmitry Huba, John Nguyen, Kshitiz Malik, Ruiyu Zhu, Mike Rabbat, Ashkan Yousefpour, Carole-Jean Wu, Hongyuan Zhan, Pavel Ustinov, Harish Srinivas, Kaikai Wang, Anthony Shoumikhin, Jesik Min, Mani Malek", "abstract": "Cross-device Federated Learning (FL) is a distributed learning paradigm with\nseveral challenges that differentiate it from traditional distributed learning,\nvariability in the system characteristics on each device, and millions of\nclients coordinating with a central server being primary ones. Most FL systems\ndescribed in the literature are synchronous - they perform a synchronized\naggregation of model updates from individual clients. Scaling synchronous FL is\nchallenging since increasing the number of clients training in parallel leads\nto diminishing returns in training speed, analogous to large-batch training.\nMoreover, stragglers hinder synchronous FL training. In this work, we outline a\nproduction asynchronous FL system design. Our work tackles the aforementioned\nissues, sketches of some of the system design challenges and their solutions,\nand touches upon principles that emerged from building a production FL system\nfor millions of clients. Empirically, we demonstrate that asynchronous FL\nconverges faster than synchronous FL when training across nearly one hundred\nmillion devices. In particular, in high concurrency settings, asynchronous FL\nis 5x faster and has nearly 8x less communication overhead than synchronous FL.", "time": "2022-04-25T19:42:58Z", "link": "http://arxiv.org/abs/2111.04877v2", "id": "2111.04877v2", "title": "Papaya: Practical, Private, and Scalable Federated Learning"}
{"author": "Keshav Santhanam, Siddharth Krishna, Ryota Tomioka, Tim Harris, Matei Zaharia", "abstract": "The rapidly growing size of deep neural network (DNN) models and datasets has\ngiven rise to a variety of distribution strategies such as data, tensor-model,\npipeline parallelism, and hybrid combinations thereof. Each of these strategies\noffers its own trade-offs and exhibits optimal performance across different\nmodels and hardware topologies. Selecting the best set of strategies for a\ngiven setup is challenging because the search space grows combinatorially, and\ndebugging and testing on clusters is expensive. In this work we propose DistIR,\nan expressive intermediate representation for distributed DNN computation that\nis tailored for efficient analyses, such as simulation. This enables\nautomatically identifying the top-performing strategies without having to\nexecute on physical hardware. Unlike prior work, DistIR can naturally express\nmany distribution strategies including pipeline parallelism with arbitrary\nschedules. Our evaluation on MLP training and GPT-2 inference models\ndemonstrates how DistIR and its simulator enable fast grid searches over\ncomplex distribution spaces spanning up to 1000+ configurations, reducing\noptimization time by an order of magnitude for certain regimes.", "time": "2021-11-09T21:32:51Z", "link": "http://arxiv.org/abs/2111.05426v1", "id": "2111.05426v1", "title": "DistIR: An Intermediate Representation and Simulator for Efficient\n  Neural Network Distribution"}
{"author": "Xiangru Lian, Binhang Yuan, Xuefeng Zhu, Yulong Wang, Yongjun He, Honghuan Wu, Lei Sun, Haodong Lyu, Chengjun Liu, Xing Dong, Yiqiao Liao, Mingnan Luo, Congfei Zhang, Jingru Xie, Haonan Li, Lei Chen, Renjie Huang, Jianying Lin, Chengchun Shu, Xuezhong Qiu, Zhishan Liu, Dongying Kong, Lei Yuan, Hai Yu, Sen Yang, Ce Zhang, Ji Liu", "abstract": "Deep learning based models have dominated the current landscape of production\nrecommender systems. Furthermore, recent years have witnessed an exponential\ngrowth of the model scale--from Google's 2016 model with 1 billion parameters\nto the latest Facebook's model with 12 trillion parameters. Significant quality\nboost has come with each jump of the model capacity, which makes us believe the\nera of 100 trillion parameters is around the corner. However, the training of\nsuch models is challenging even within industrial scale data centers. This\ndifficulty is inherited from the staggering heterogeneity of the training\ncomputation--the model's embedding layer could include more than 99.99% of the\ntotal model size, which is extremely memory-intensive; while the rest neural\nnetwork is increasingly computation-intensive. To support the training of such\nhuge models, an efficient distributed training system is in urgent need. In\nthis paper, we resolve this challenge by careful co-design of both the\noptimization algorithm and the distributed system architecture. Specifically,\nin order to ensure both the training efficiency and the training accuracy, we\ndesign a novel hybrid training algorithm, where the embedding layer and the\ndense neural network are handled by different synchronization mechanisms; then\nwe build a system called Persia (short for parallel recommendation training\nsystem with hybrid acceleration) to support this hybrid training algorithm.\nBoth theoretical demonstration and empirical study up to 100 trillion\nparameters have conducted to justified the system design and implementation of\nPersia. We make Persia publicly available (at\nhttps://github.com/PersiaML/Persia) so that anyone would be able to easily\ntrain a recommender model at the scale of 100 trillion parameters.", "time": "2021-11-23T09:59:49Z", "link": "http://arxiv.org/abs/2111.05897v3", "id": "2111.05897v3", "title": "Persia: An Open, Hybrid System Scaling Deep Learning-based Recommenders\n  up to 100 Trillion Parameters"}
{"author": "Ilia Markov, Hamidreza Ramezanikebrya, Dan Alistarh", "abstract": "The ability to scale out training workloads has been one of the key\nperformance enablers of deep learning. The main scaling approach is\ndata-parallel GPU-based training, which has been boosted by hardware and\nsoftware support for highly efficient point-to-point communication, and in\nparticular via hardware bandwidth overprovisioning. Overprovisioning comes at a\ncost: there is an order of magnitude price difference between \"cloud-grade\"\nservers with such support, relative to their popular \"consumer-grade\"\ncounterparts, although single server-grade and consumer-grade GPUs can have\nsimilar computational envelopes.\n  In this paper, we show that the costly hardware overprovisioning approach can\nbe supplanted via algorithmic and system design, and propose a framework called\nCGX, which provides efficient software support for compressed communication in\nML applications, for both multi-GPU single-node training, as well as\nlarger-scale multi-node training. CGX is based on two technical advances:\n\\emph{At the system level}, it relies on a re-developed communication stack for\nML frameworks, which provides flexible, highly-efficient support for compressed\ncommunication. \\emph{At the application level}, it provides \\emph{seamless,\nparameter-free} integration with popular frameworks, so that end-users do not\nhave to modify training recipes, nor significant training code. This is\ncomplemented by a \\emph{layer-wise adaptive compression} technique which\ndynamically balances compression gains with accuracy preservation. CGX\nintegrates with popular ML frameworks, providing up to 3X speedups for\nmulti-GPU nodes based on commodity hardware, and order-of-magnitude\nimprovements in the multi-node setting, with negligible impact on accuracy.", "time": "2022-12-29T15:07:04Z", "link": "http://arxiv.org/abs/2111.08617v5", "id": "2111.08617v5", "title": "CGX: Adaptive System Support for Communication-Efficient Deep Learning"}
{"author": "Dominik Scheinert, Alireza Alamgiralem, Jonathan Bader, Jonathan Will, Thorsten Wittkopp, Lauritz Thamsen", "abstract": "With the growing amount of data, data processing workloads and the management\nof their resource usage becomes increasingly important. Since managing a\ndedicated infrastructure is in many situations infeasible or uneconomical,\nusers progressively execute their respective workloads in the cloud. As the\nconfiguration of workloads and resources is often challenging, various methods\nhave been proposed that either quickly profile towards a good configuration or\ndetermine one based on data from previous runs. Still, performance data to\ntrain such methods is often lacking and must be costly collected.\n  In this paper, we propose a collaborative approach for sharing anonymized\nworkload execution traces among users, mining them for general patterns, and\nexploiting clusters of historical workloads for future optimizations. We\nevaluate our prototype implementation for mining workload execution graphs on a\npublicly available trace dataset and demonstrate the predictive value of\nworkload clusters determined through traces only.", "time": "2022-01-16T11:27:04Z", "link": "http://arxiv.org/abs/2111.08759v2", "id": "2111.08759v2", "title": "On the Potential of Execution Traces for Batch Processing Workload\n  Optimization in Public Clouds"}
{"author": "Zhicheng Zhou, Hailong Chen, Kunhua Li, Fei Hu, Bingjie Yan, Jieren Cheng, Xuyan Wei, Bernie Liu, Xiulai Li, Fuwen Chen, Yongji Sui", "abstract": "Federated Learning (FL) since proposed has been applied in many fields, such\nas credit assessment, medical, etc. Because of the difference in the network or\ncomputing resource, the clients may not update their gradients at the same time\nthat may take a lot of time to wait or idle. That's why Asynchronous Federated\nLearning (AFL) method is needed. The main bottleneck in AFL is communication.\nHow to find a balance between the model performance and the communication cost\nis a challenge in AFL. This paper proposed a novel AFL framework VAFL. And we\nverified the performance of the algorithm through sufficient experiments. The\nexperiments show that VAFL can reduce the communication times about 51.02\\%\nwith 48.23\\% average communication compression rate and allow the model to be\nconverged faster. The code is available at\n\\url{https://github.com/RobAI-Lab/VAFL}", "time": "2021-11-18T02:52:49Z", "link": "http://arxiv.org/abs/2111.09487v1", "id": "2111.09487v1", "title": "A Novel Optimized Asynchronous Federated Learning Framework"}
{"author": "Adarsh Kumar, Kausik Subramanian, Shivaram Venkataraman, Aditya Akella", "abstract": "Many organizations employ compute clusters equipped with accelerators such as\nGPUs and TPUs for training deep learning models in a distributed fashion.\nTraining is resource-intensive, consuming significant compute, memory, and\nnetwork resources. Many prior works explore how to reduce training resource\nfootprint without impacting quality, but their focus on a subset of the\nbottlenecks (typically only the network) limits their ability to improve\noverall cluster utilization. In this work, we exploit the unique\ncharacteristics of deep learning workloads to propose Structured Partial\nBackpropagation(SPB), a technique that systematically controls the amount of\nbackpropagation at individual workers in distributed training. This\nsimultaneously reduces network bandwidth, compute utilization, and memory\nfootprint while preserving model quality. To efficiently leverage the benefits\nof SPB at cluster level, we introduce JigSaw, a SPB aware scheduler, which does\nscheduling at the iteration level for Deep Learning Training(DLT) jobs. We find\nthat JigSaw can improve large scale cluster efficiency by as high as 28\\%.", "time": "2021-11-20T20:34:26Z", "link": "http://arxiv.org/abs/2111.10672v1", "id": "2111.10672v1", "title": "Doing More by Doing Less: How Structured Partial Backpropagation\n  Improves Deep Learning Clusters"}
{"author": "Sean Augenstein, Andrew Hard, Kurt Partridge, Rajiv Mathews", "abstract": "With privacy as a motivation, Federated Learning (FL) is an increasingly used\nparadigm where learning takes place collectively on edge devices, each with a\ncache of user-generated training examples that remain resident on the local\ndevice. These on-device training examples are gathered in situ during the\ncourse of users' interactions with their devices, and thus are highly\nreflective of at least part of the inference data distribution. Yet a\ndistribution shift may still exist; the on-device training examples may lack\nfor some data inputs expected to be encountered at inference time. This paper\nproposes a way to mitigate this shift: selective usage of datacenter data,\nmixed in with FL. By mixing decentralized (federated) and centralized\n(datacenter) data, we can form an effective training data distribution that\nbetter matches the inference data distribution, resulting in more useful models\nwhile still meeting the private training data access constraints imposed by FL.", "time": "2021-11-23T20:51:24Z", "link": "http://arxiv.org/abs/2111.12150v1", "id": "2111.12150v1", "title": "Jointly Learning from Decentralized (Federated) and Centralized Data to\n  Mitigate Distribution Shift"}
{"author": "Fuxun Yu, Di Wang, Longfei Shangguan, Minjia Zhang, Xulong Tang, Chenchen Liu, Xiang Chen", "abstract": "Deep Learning (DL) models have achieved superior performance in many\napplication domains, including vision, language, medical, commercial ads,\nentertainment, etc. With the fast development, both DL applications and the\nunderlying serving hardware have demonstrated strong scaling trends, i.e.,\nModel Scaling and Compute Scaling, for example, the recent pre-trained model\nwith hundreds of billions of parameters with ~TB level memory consumption, as\nwell as the newest GPU accelerators providing hundreds of TFLOPS. With both\nscaling trends, new problems and challenges emerge in DL inference serving\nsystems, which gradually trends towards Large-scale Deep learning Serving\nsystems (LDS). This survey aims to summarize and categorize the emerging\nchallenges and optimization opportunities for large-scale deep learning serving\nsystems. By providing a novel taxonomy, summarizing the computing paradigms,\nand elaborating the recent technique advances, we hope that this survey could\nshed light on new optimization perspectives and motivate novel works in\nlarge-scale deep learning system optimization.", "time": "2022-02-18T21:07:02Z", "link": "http://arxiv.org/abs/2111.14247v2", "id": "2111.14247v2", "title": "A Survey of Large-Scale Deep Learning Serving System Optimization:\n  Challenges and Opportunities"}
{"author": "Yuke Wang, Boyuan Feng, Zheng Wang, Guyue Huang, Yufei Ding", "abstract": "Recently, graph neural networks (GNNs), as the backbone of graph-based\nmachine learning, demonstrate great success in various domains (e.g.,\ne-commerce). However, the performance of GNNs is usually unsatisfactory due to\nthe highly sparse and irregular graph-based operations. To this end, we propose\nTC-GNN, the first GNN acceleration framework based on GPU Tensor Core Units\n(TCUs). The core idea is to reconcile the \"Sparse\" GNN computation with the\nhigh-performance \"Dense\" TCUs. Specifically, we conduct an in-depth analysis of\nthe sparse operations in mainstream GNN computing frameworks. We introduce a\nnovel sparse graph translation technique to facilitate TCU processing of the\nsparse GNN workload. We implement an effective CUDA core and TCU collaboration\ndesign to fully utilize GPU resources. We integrate TC-GNN with the PyTorch\nframework for high programmability. Rigorous experiments show an average of\n1.70X speedup over the state-of-the-art DGL framework across various models and\ndatasets.", "time": "2023-05-31T19:24:58Z", "link": "http://arxiv.org/abs/2112.02052v4", "id": "2112.02052v4", "title": "TC-GNN: Bridging Sparse GNN Computation and Dense Tensor Cores on GPUs"}
{"author": "Harsh Mehta, Steffen Rendle, Walid Krichene, Li Zhang", "abstract": "We present ALX, an open-source library for distributed matrix factorization\nusing Alternating Least Squares, written in JAX. Our design allows for\nefficient use of the TPU architecture and scales well to matrix factorization\nproblems of O(B) rows/columns by scaling the number of available TPU cores. In\norder to spur future research on large scale matrix factorization methods and\nto illustrate the scalability properties of our own implementation, we also\nbuilt a real world web link prediction dataset called WebGraph. This dataset\ncan be easily modeled as a matrix factorization problem. We created several\nvariants of this dataset based on locality and sparsity properties of\nsub-graphs. The largest variant of WebGraph has around 365M nodes and training\na single epoch finishes in about 20 minutes with 256 TPU cores. We include\nspeed and performance numbers of ALX on all variants of WebGraph. Both the\nframework code and the dataset is open-sourced.", "time": "2022-03-29T20:43:42Z", "link": "http://arxiv.org/abs/2112.02194v2", "id": "2112.02194v2", "title": "ALX: Large Scale Matrix Factorization on TPUs"}
{"author": "Shreshth Tuli, Giuliano Casale, Nicholas R. Jennings", "abstract": "Building a fault-tolerant edge system that can quickly react to node\noverloads or failures is challenging due to the unreliability of edge devices\nand the strict service deadlines of modern applications. Moreover, unnecessary\ntask migrations can stress the system network, giving rise to the need for a\nsmart and parsimonious failure recovery scheme. Prior approaches often fail to\nadapt to highly volatile workloads or accurately detect and diagnose faults for\noptimal remediation. There is thus a need for a robust and proactive\nfault-tolerance mechanism to meet service level objectives. In this work, we\npropose PreGAN, a composite AI model using a Generative Adversarial Network\n(GAN) to predict preemptive migration decisions for proactive fault-tolerance\nin containerized edge deployments. PreGAN uses co-simulations in tandem with a\nGAN to learn a few-shot anomaly classifier and proactively predict migration\ndecisions for reliable computing. Extensive experiments on a Raspberry-Pi based\nedge environment show that PreGAN can outperform state-of-the-art baseline\nmethods in fault-detection, diagnosis and classification, thus achieving high\nquality of service. PreGAN accomplishes this by 5.1% more accurate fault\ndetection, higher diagnosis scores and 23.8% lower overheads compared to the\nbest method among the considered baselines.", "time": "2021-12-04T09:40:50Z", "link": "http://arxiv.org/abs/2112.02292v1", "id": "2112.02292v1", "title": "PreGAN: Preemptive Migration Prediction Network for Proactive\n  Fault-Tolerant Edge Computing"}
{"author": "Hankyul Baek, Won Joon Yun, Yunseok Kwak, Soyi Jung, Mingyue Ji, Mehdi Bennis, Jihong Park, Joongheon Kim", "abstract": "This paper aims to integrate two synergetic technologies, federated learning\n(FL) and width-adjustable slimmable neural network (SNN) architectures. FL\npreserves data privacy by exchanging the locally trained models of mobile\ndevices. By adopting SNNs as local models, FL can flexibly cope with the\ntime-varying energy capacities of mobile devices. Combining FL and SNNs is\nhowever non-trivial, particularly under wireless connections with time-varying\nchannel conditions. Furthermore, existing multi-width SNN training algorithms\nare sensitive to the data distributions across devices, so are ill-suited to\nFL. Motivated by this, we propose a communication and energy-efficient\nSNN-based FL (named SlimFL) that jointly utilizes superposition coding (SC) for\nglobal model aggregation and superposition training (ST) for updating local\nmodels. By applying SC, SlimFL exchanges the superposition of multiple width\nconfigurations that are decoded as many as possible for a given communication\nthroughput. Leveraging ST, SlimFL aligns the forward propagation of different\nwidth configurations, while avoiding the inter-width interference during\nbackpropagation. We formally prove the convergence of SlimFL. The result\nreveals that SlimFL is not only communication-efficient but also can counteract\nnon-IID data distributions and poor channel conditions, which is also\ncorroborated by simulations.", "time": "2021-12-05T11:17:17Z", "link": "http://arxiv.org/abs/2112.02543v1", "id": "2112.02543v1", "title": "Joint Superposition Coding and Training for Federated Learning over\n  Multi-Width Neural Networks"}
{"author": "Luke Melas-Kyriazi, Franklyn Wang", "abstract": "Federated learning is a rapidly-growing area of research which enables a\nlarge number of clients to jointly train a machine learning model on\nprivately-held data. One of the largest barriers to wider adoption of federated\nlearning is the communication cost of sending model updates from and to the\nclients, which is accentuated by the fact that many of these devices are\nbandwidth-constrained. In this paper, we aim to address this issue by\noptimizing networks within a subspace of their full parameter space, an idea\nknown as intrinsic dimension in the machine learning theory community. We use a\ncorrespondence between the notion of intrinsic dimension and gradient\ncompressibility to derive a family of low-bandwidth optimization algorithms,\nwhich we call intrinsic gradient compression algorithms. Specifically, we\npresent three algorithms in this family with different levels of upload and\ndownload bandwidth for use in various federated settings, along with\ntheoretical guarantees on their performance. Finally, in large-scale federated\nlearning experiments with models containing up to 100M parameters, we show that\nour algorithms perform extremely well compared to current state-of-the-art\ngradient compression methods.", "time": "2021-12-05T19:16:54Z", "link": "http://arxiv.org/abs/2112.02656v1", "id": "2112.02656v1", "title": "Intrinisic Gradient Compression for Federated Learning"}
{"author": "Lam Duc Nguyen, Shashi Raj Pandey, Soret Beatriz, Arne Broering, Petar Popovski", "abstract": "As Machine Learning (ML) models are becoming increasingly complex, one of the\ncentral challenges is their deployment at scale, such that companies and\norganizations can create value through Artificial Intelligence (AI). An\nemerging paradigm in ML is a federated approach where the learning model is\ndelivered to a group of heterogeneous agents partially, allowing agents to\ntrain the model locally with their own data. However, the problem of valuation\nof models, as well the questions of incentives for collaborative training and\ntrading of data/models, have received limited treatment in the literature. In\nthis paper, a new ecosystem of ML model trading over a trusted Blockchain-based\nnetwork is proposed. The buyer can acquire the model of interest from the ML\nmarket, and interested sellers spend local computations on their data to\nenhance that model's quality. In doing so, the proportional relation between\nthe local data and the quality of trained models is considered, and the\nvaluations of seller's data in training the models are estimated through the\ndistributed Data Shapley Value (DSV). At the same time, the trustworthiness of\nthe entire trading process is provided by the distributed Ledger Technology\n(DLT). Extensive experimental evaluation of the proposed approach shows a\ncompetitive run-time performance, with a 15\\% drop in the cost of execution,\nand fairness in terms of incentives for the participants.", "time": "2021-12-06T08:52:42Z", "link": "http://arxiv.org/abs/2112.02870v1", "id": "2112.02870v1", "title": "A Marketplace for Trading AI Models based on Blockchain and Incentives\n  for IoT Data"}
{"author": "Michael Schaarschmidt, Dominik Grewe, Dimitrios Vytiniotis, Adam Paszke, Georg Stefan Schmid, Tamara Norman, James Molloy, Jonathan Godwin, Norman Alexander Rink, Vinod Nair, Dan Belov", "abstract": "The rapid rise in demand for training large neural network architectures has\nbrought into focus the need for partitioning strategies, for example by using\ndata, model, or pipeline parallelism. Implementing these methods is\nincreasingly supported through program primitives, but identifying efficient\npartitioning strategies requires expensive experimentation and expertise. We\npresent the prototype of an automated partitioner that seamlessly integrates\ninto existing compilers and existing user workflows. Our partitioner enables\nSPMD-style parallelism that encompasses data parallelism and\nparameter/activation sharding. Through a combination of inductive tactics and\nsearch in a platform-independent partitioning IR, automap can recover expert\npartitioning strategies such as Megatron sharding for transformer layers.", "time": "2021-12-06T12:09:38Z", "link": "http://arxiv.org/abs/2112.02958v1", "id": "2112.02958v1", "title": "Automap: Towards Ergonomic Automated Parallelism for ML Models"}
{"author": "Keyu Yang, Lu Chen, Zhihao Zeng, Yunjun Gao", "abstract": "With the rapid increase of big data, distributed Machine Learning (ML) has\nbeen widely applied in training large-scale models. Stochastic Gradient Descent\n(SGD) is arguably the workhorse algorithm of ML. Distributed ML models trained\nby SGD involve large amounts of gradient communication, which limits the\nscalability of distributed ML. Thus, it is important to compress the gradients\nfor reducing communication. In this paper, we propose FastSGD, a Fast\ncompressed SGD framework for distributed ML. To achieve a high compression\nratio at a low cost, FastSGD represents the gradients as key-value pairs, and\ncompresses both the gradient keys and values in linear time complexity. For the\ngradient value compression, FastSGD first uses a reciprocal mapper to transform\noriginal values into reciprocal values, and then, it utilizes a logarithm\nquantization to further reduce reciprocal values to small integers. Finally,\nFastSGD filters reduced gradient integers by a given threshold. For the\ngradient key compression, FastSGD provides an adaptive fine-grained delta\nencoding method to store gradient keys with fewer bits. Extensive experiments\non practical ML models and datasets demonstrate that FastSGD achieves the\ncompression ratio up to 4 orders of magnitude, and accelerates the convergence\ntime up to 8x, compared with state-of-the-art methods.", "time": "2021-12-08T13:56:24Z", "link": "http://arxiv.org/abs/2112.04291v1", "id": "2112.04291v1", "title": "FastSGD: A Fast Compressed SGD Framework for Distributed Machine\n  Learning"}
{"author": "Robert Fritze, Claudia Plant", "abstract": "Choosing an appropriate programming paradigm for high-performance computing\non low-power devices can be useful to speed up calculations. Many Android\ndevices have an integrated GPU and - although not officially supported - the\nOpenCL framework can be used on Android devices for addressing these GPUs.\nOpenCL supports thread and data parallelism. Applications that use the GPU must\naccount for the fact that they can be suspended by the user or the Android\noperating system at any moment. We have created a wrapper library that allows\nto use OpenCL on Android devices. Already written OpenCL programs can be\nexecuted with almost no modification. We have used this library to compare the\nperformance of the DBSCAN and Kmeans algorithms on an integrated GPU of an\nArm-v7 tablet with other single and multithreaded implementations on the same\ndevice. We have investigated which programming paradigm and language allows the\nbest tradeoff between execution speed and energy consumption. Using the GPU for\nHPC on Android devices can help to carry out computationally intensive machine\nlearning or data mining tasks in remote areas, under harsh environmental\nconditions and in areas where energy supply is an issue.", "time": "2021-12-09T09:44:55Z", "link": "http://arxiv.org/abs/2112.04800v1", "id": "2112.04800v1", "title": "GPU backed Data Mining on Android Devices"}
{"author": "Pheeha Machaka, Olasupo Ajayi, Hloniphani Maluleke, Ferdinand Kahenga, Antoine Bagula, Kyandoghere Kyamakya", "abstract": "In current Internet-of-Things (IoT) deployments, a mix of traditional IP\nnetworking and IoT specific protocols, both relying on the TCP protocol, can be\nused to transport data from a source to a destination. Therefore, TCP-specific\nattacks, such as the Distributed Denial of Service (DDoS) using the TCP SYN\nattack, are one of the most plausible tools that attackers can use on\nCyber-Physical Systems (CPS). This may be done by launching an attack from its\nIoT subsystem, here referred to as the \"CPS-IoT\", with potential propagation to\nthe different servers located in both fog and the cloud infrastructures of the\nCPS. This study compares the effectiveness of supervised, unsupervised, and\nsemi-supervised machine learning algorithms for detecting DDoS attacks in\nCPS-IoT, particularly during data transmission to and from the physical space\nto the cyber space via the Internet. The algorithms considered are broadly\ngrouped into two: i) Detection algorithms, which include Logistic Regression\n(LGR), K-Means, and Artificial Neural Networks (ANN). We also looked into the\neffectiveness of semi-supervised hybrid learning models, which use unsupervised\nK-Means to label data, then feed the output to a supervised learning model for\nattack detection. ii.) Prediction algorithms - LGR, Kernel Ridge Regression\n(KRR) and Support Vector Regression (SVR), which were used to predict imminent\nattacks. Experimental tests were carried out and obtained results showed that\nthe hybrid model was able to achieve 100% accuracy with zero false positives;\nwhile all the prediction models were able to achieve over 94% attack prediction\naccuracy.", "time": "2022-06-20T14:06:28Z", "link": "http://arxiv.org/abs/2112.05477v2", "id": "2112.05477v2", "title": "Modelling DDoS Attacks in IoT Networks using Machine Learning"}
{"author": "Zichen Ma, Zihan Lu, Yu Lu, Wenye Li, Jinfeng Yi, Shuguang Cui", "abstract": "Federated learning is a distributed machine learning mechanism where local\ndevices collaboratively train a shared global model under the orchestration of\na central server, while keeping all private data decentralized. In the system,\nmodel parameters and its updates are transmitted instead of raw data, and thus\nthe communication bottleneck has become a key challenge. Besides, recent larger\nand deeper machine learning models also pose more difficulties in deploying\nthem in a federated environment. In this paper, we design a federated two-stage\nlearning framework that augments prototypical federated learning with a cut\nlayer on devices and uses sign-based stochastic gradient descent with the\nmajority vote method on model updates. Cut layer on devices learns informative\nand low-dimension representations of raw data locally, which helps reduce\nglobal model parameters and prevents data leakage. Sign-based SGD with the\nmajority vote method for model updates also helps alleviate communication\nlimitations. Empirically, we show that our system is an efficient and privacy\npreserving federated learning scheme and suits for general application\nscenarios.", "time": "2021-12-10T17:31:23Z", "link": "http://arxiv.org/abs/2112.05687v1", "id": "2112.05687v1", "title": "Federated Two-stage Learning with Sign-based Voting"}
{"author": "Xupeng Miao, Hailin Zhang, Yining Shi, Xiaonan Nie, Zhi Yang, Yangyu Tao, Bin Cui", "abstract": "Embedding models have been an effective learning paradigm for\nhigh-dimensional data. However, one open issue of embedding models is that\ntheir representations (latent factors) often result in large parameter space.\nWe observe that existing distributed training frameworks face a scalability\nissue of embedding models since updating and retrieving the shared embedding\nparameters from servers usually dominates the training cycle. In this paper, we\npropose HET, a new system framework that significantly improves the scalability\nof huge embedding model training. We embrace skewed popularity distributions of\nembeddings as a performance opportunity and leverage it to address the\ncommunication bottleneck with an embedding cache. To ensure consistency across\nthe caches, we incorporate a new consistency model into HET design, which\nprovides fine-grained consistency guarantees on a per-embedding basis. Compared\nto previous work that only allows staleness for read operations, HET also\nutilizes staleness for write operations. Evaluations on six representative\ntasks show that HET achieves up to 88% embedding communication reductions and\nup to 20.68x performance speedup over the state-of-the-art baselines.", "time": "2021-12-14T08:18:10Z", "link": "http://arxiv.org/abs/2112.07221v1", "id": "2112.07221v1", "title": "HET: Scaling out Huge Embedding Model Training via Cache-enabled\n  Distributed Framework"}
{"author": "Huiming Chen, Huandong Wang, Quanming Yao, Yong Li, Depeng Jin, Qiang Yang", "abstract": "Federated optimization (FedOpt), which targets at collaboratively training a\nlearning model across a large number of distributed clients, is vital for\nfederated learning. The primary concerns in FedOpt can be attributed to the\nmodel divergence and communication efficiency, which significantly affect the\nperformance. In this paper, we propose a new method, i.e., LoSAC, to learn from\nheterogeneous distributed data more efficiently. Its key algorithmic insight is\nto locally update the estimate for the global full gradient after {each}\nregular local model update. Thus, LoSAC can keep clients' information refreshed\nin a more compact way. In particular, we have studied the convergence result\nfor LoSAC. Besides, the bonus of LoSAC is the ability to defend the information\nleakage from the recent technique Deep Leakage Gradients (DLG). Finally,\nexperiments have verified the superiority of LoSAC comparing with\nstate-of-the-art FedOpt algorithms. Specifically, LoSAC significantly improves\ncommunication efficiency by more than $100\\%$ on average, mitigates the model\ndivergence problem and equips with the defense ability against DLG.", "time": "2023-02-25T09:12:31Z", "link": "http://arxiv.org/abs/2112.07839v3", "id": "2112.07839v3", "title": "LoSAC: An Efficient Local Stochastic Average Control Method for\n  Federated Optimization"}
{"author": "Yi Zhou, Parikshit Ram, Theodoros Salonidis, Nathalie Baracaldo, Horst Samulowitz, Heiko Ludwig", "abstract": "We address the relatively unexplored problem of hyper-parameter optimization\n(HPO) for federated learning (FL-HPO). We introduce Federated Loss suRface\nAggregation (FLoRA), the first FL-HPO solution framework that can address use\ncases of tabular data and gradient boosting training algorithms in addition to\nstochastic gradient descent/neural networks commonly addressed in the FL\nliterature. The framework enables single-shot FL-HPO, by first identifying a\ngood set of hyper-parameters that are used in a **single** FL training. Thus,\nit enables FL-HPO solutions with minimal additional communication overhead\ncompared to FL training without HPO. Our empirical evaluation of FLoRA for\nGradient Boosted Decision Trees on seven OpenML data sets demonstrates\nsignificant model accuracy improvements over the considered baseline, and\nrobustness to increasing number of parties involved in FL-HPO training.", "time": "2021-12-15T23:18:32Z", "link": "http://arxiv.org/abs/2112.08524v1", "id": "2112.08524v1", "title": "FLoRA: Single-shot Hyper-parameter Optimization for Federated Learning"}
{"author": "Tianfeng Liu, Yangrui Chen, Dan Li, Chuan Wu, Yibo Zhu, Jun He, Yanghua Peng, Hongzheng Chen, Hongzhi Chen, Chuanxiong Guo", "abstract": "Graph neural networks (GNNs) have extended the success of deep neural\nnetworks (DNNs) to non-Euclidean graph data, achieving ground-breaking\nperformance on various tasks such as node classification and graph property\nprediction. Nonetheless, existing systems are inefficient to train large graphs\nwith billions of nodes and edges with GPUs. The main bottlenecks are the\nprocess of preparing data for GPUs - subgraph sampling and feature retrieving.\nThis paper proposes BGL, a distributed GNN training system designed to address\nthe bottlenecks with a few key ideas. First, we propose a dynamic cache engine\nto minimize feature retrieving traffic. By a co-design of caching policy and\nthe order of sampling, we find a sweet spot of low overhead and high cache hit\nratio. Second, we improve the graph partition algorithm to reduce\ncross-partition communication during subgraph sampling. Finally, careful\nresource isolation reduces contention between different data preprocessing\nstages. Extensive experiments on various GNN models and large graph datasets\nshow that BGL significantly outperforms existing GNN training systems by 20.68x\non average.", "time": "2021-12-16T00:37:37Z", "link": "http://arxiv.org/abs/2112.08541v1", "id": "2112.08541v1", "title": "BGL: GPU-Efficient GNN Training by Optimizing Graph Data I/O and\n  Preprocessing"}
{"author": "Guanhua Ye, Hongzhi Yin, Tong Chen, Miao Xu, Quoc Viet Hung Nguyen, Jiangning Song", "abstract": "Actuated by the growing attention to personal healthcare and the pandemic,\nthe popularity of E-health is proliferating. Nowadays, enhancement on medical\ndiagnosis via machine learning models has been highly effective in many aspects\nof e-health analytics. Nevertheless, in the classic cloud-based/centralized\ne-health paradigms, all the data will be centrally stored on the server to\nfacilitate model training, which inevitably incurs privacy concerns and high\ntime delay. Distributed solutions like Decentralized Stochastic Gradient\nDescent (D-SGD) are proposed to provide safe and timely diagnostic results\nbased on personal devices. However, methods like D-SGD are subject to the\ngradient vanishing issue and usually proceed slowly at the early training\nstage, thereby impeding the effectiveness and efficiency of training. In\naddition, existing methods are prone to learning models that are biased towards\nusers with dense data, compromising the fairness when providing E-health\nanalytics for minority groups. In this paper, we propose a Decentralized Block\nCoordinate Descent (D-BCD) learning framework that can better optimize deep\nneural network-based models distributed on decentralized devices for E-health\nanalytics. Benchmarking experiments on three real-world datasets illustrate the\neffectiveness and practicality of our proposed D-BCD, where additional\nsimulation study showcases the strong applicability of D-BCD in real-life\nE-health scenarios.", "time": "2021-12-17T06:23:20Z", "link": "http://arxiv.org/abs/2112.09341v1", "id": "2112.09341v1", "title": "Personalized On-Device E-health Analytics with Decentralized Block\n  Coordinate Descent"}
{"author": "Feijie Wu, Song Guo, Haozhao Wang, Zhihao Qu, Haobo Zhang, Jie Zhang, Ziming Liu", "abstract": "In the setting of federated optimization, where a global model is aggregated\nperiodically, step asynchronism occurs when participants conduct model training\nby efficiently utilizing their computational resources. It is well acknowledged\nthat step asynchronism leads to objective inconsistency under non-i.i.d. data,\nwhich degrades the model's accuracy. To address this issue, we propose a new\nalgorithm FedaGrac, which calibrates the local direction to a predictive global\norientation. Taking advantage of the estimated orientation, we guarantee that\nthe aggregated model does not excessively deviate from the global optimum while\nfully utilizing the local updates of faster nodes. We theoretically prove that\nFedaGrac holds an improved order of convergence rate than the state-of-the-art\napproaches and eliminates the negative effect of step asynchronism. Empirical\nresults show that our algorithm accelerates the training and enhances the final\naccuracy.", "time": "2023-02-27T01:18:26Z", "link": "http://arxiv.org/abs/2112.09355v2", "id": "2112.09355v2", "title": "From Deterioration to Acceleration: A Calibration Approach to\n  Rehabilitating Step Asynchronism in Federated Optimization"}
{"author": "Md. Khaledur Rahman, Ariful Azad", "abstract": "Graph representation learning is a fast-growing field where one of the main\nobjectives is to generate meaningful representations of graphs in\nlower-dimensional spaces. The learned embeddings have been successfully applied\nto perform various prediction tasks, such as link prediction, node\nclassification, clustering, and visualization. The collective effort of the\ngraph learning community has delivered hundreds of methods, but no single\nmethod excels under all evaluation metrics such as prediction accuracy, running\ntime, scalability, etc. This survey aims to evaluate all major classes of graph\nembedding methods by considering algorithmic variations, parameter selections,\nscalability, hardware and software platforms, downstream ML tasks, and diverse\ndatasets. We organized graph embedding techniques using a taxonomy that\nincludes methods from manual feature engineering, matrix factorization, shallow\nneural networks, and deep graph convolutional networks. We evaluated these\nclasses of algorithms for node classification, link prediction, clustering, and\nvisualization tasks using widely used benchmark graphs. We designed our\nexperiments on top of PyTorch Geometric and DGL libraries and run experiments\non different multicore CPU and GPU platforms. We rigorously scrutinize the\nperformance of embedding methods under various performance metrics and\nsummarize the results. Thus, this paper may serve as a comparative guide to\nhelp users select methods that are most suitable for their tasks.", "time": "2021-12-20T07:50:26Z", "link": "http://arxiv.org/abs/2112.10372v1", "id": "2112.10372v1", "title": "A Comprehensive Analytical Survey on Unsupervised and Semi-Supervised\n  Graph Representation Learning Methods"}
{"author": "Xin Wang, Hong Shen", "abstract": "Coflow is a recently proposed networking abstraction to help improve the\ncommunication performance of data-parallel computing jobs. In multi-stage jobs,\neach job consists of multiple coflows and is represented by a Directed Acyclic\nGraph (DAG). Efficiently scheduling coflows is critical to improve the\ndata-parallel computing performance in data centers. Compared with hand-tuned\nscheduling heuristics, existing work DeepWeave [1] utilizes Reinforcement\nLearning (RL) framework to generate highly-efficient coflow scheduling policies\nautomatically. It employs a graph neural network (GNN) to encode the job\ninformation in a set of embedding vectors, and feeds a flat embedding vector\ncontaining the whole job information to the policy network. However, this\nmethod has poor scalability as it is unable to cope with jobs represented by\nDAGs of arbitrary sizes and shapes, which requires a large policy network for\nprocessing a high-dimensional embedding vector that is difficult to train. In\nthis paper, we first utilize a directed acyclic graph neural network (DAGNN) to\nprocess the input and propose a novel Pipelined-DAGNN, which can effectively\nspeed up the feature extraction process of the DAGNN. Next, we feed the\nembedding sequence composed of schedulable coflows instead of a flat embedding\nof all coflows to the policy network, and output a priority sequence, which\nmakes the size of the policy network depend on only the dimension of features\ninstead of the product of dimension and number of nodes in the job's\nDAG.Furthermore, to improve the accuracy of the priority scheduling policy, we\nincorporate the Self-Attention Mechanism into a deep RL model to capture the\ninteraction between different parts of the embedding sequence to make the\noutput priority scores relevant. Based on this model, we then develop a coflow\nscheduling algorithm for online multi-stage jobs.", "time": "2021-12-21T09:36:55Z", "link": "http://arxiv.org/abs/2112.11055v1", "id": "2112.11055v1", "title": "A Scalable Deep Reinforcement Learning Model for Online Scheduling\n  Coflows of Multi-Stage Jobs for High Performance Computing"}
{"author": "Ozan AygÃ¼n, Mohammad Kazemi, Deniz GÃ¼ndÃ¼z, Tolga M. Duman", "abstract": "Federated learning (FL) over wireless communication channels, specifically,\nover-the-air (OTA) model aggregation framework is considered. In OTA wireless\nsetups, the adverse channel effects can be alleviated by increasing the number\nof receive antennas at the parameter server (PS), which performs model\naggregation. However, the performance of OTA FL is limited by the presence of\nmobile users (MUs) located far away from the PS. In this paper, to mitigate\nthis limitation, we propose hierarchical over-the-air federated learning\n(HOTAFL), which utilizes intermediary servers (IS) to form clusters near MUs.\nWe provide a convergence analysis for the proposed setup, and demonstrate\nthrough theoretical and experimental results that local aggregation in each\ncluster before global aggregation leads to a better performance and faster\nconvergence than OTA FL.", "time": "2021-12-21T13:02:10Z", "link": "http://arxiv.org/abs/2112.11167v1", "id": "2112.11167v1", "title": "Hierarchical Over-the-Air Federated Edge Learning"}
{"author": "Andrea Fresa, Jaya Prakash Champati", "abstract": "With the emergence of edge computing, the problem of offloading jobs between\nan Edge Device (ED) and an Edge Server (ES) received significant attention in\nthe past. Motivated by the fact that an increasing number of applications are\nusing Machine Learning (ML) inference, we study the problem of offloading\ninference jobs by considering the following novel aspects: 1) in contrast to a\ntypical computational job, the processing time of an inference job depends on\nthe size of the ML model, and 2) recently proposed Deep Neural Networks (DNNs)\nfor resource-constrained devices provide the choice of scaling the model size.\nWe formulate an assignment problem with the aim of maximizing the total\ninference accuracy of n data samples available at the ED, subject to a time\nconstraint T on the makespan. We propose an approximation algorithm AMR2, and\nprove that it results in a makespan at most 2T, and achieves a total accuracy\nthat is lower by a small constant from optimal total accuracy. As proof of\nconcept, we implemented AMR2 on a Raspberry Pi, equipped with MobileNet, and is\nconnected to a server equipped with ResNet, and studied the total accuracy and\nmakespan performance of AMR2 for image classification application.", "time": "2021-12-21T18:21:24Z", "link": "http://arxiv.org/abs/2112.11413v1", "id": "2112.11413v1", "title": "Offloading Algorithms for Maximizing Inference Accuracy on Edge Device\n  Under a Time Constraint"}
{"author": "Hung T. Nguyen, Roberto Morabito, Kwang Taik Kim, Mung Chiang", "abstract": "Edge computing has revolutionized the world of mobile and wireless networks\nworld thanks to its flexible, secure, and performing characteristics. Lately,\nwe have witnessed the increasing use of it to make more performing the\ndeployment of machine learning (ML) techniques such as federated learning (FL).\nFL was debuted to improve communication efficiency compared to conventional\ndistributed machine learning (ML). The original FL assumes a central\naggregation server to aggregate locally optimized parameters and might bring\nreliability and latency issues. In this paper, we conduct an in-depth study of\nstrategies to replace this central server by a flying master that is\ndynamically selected based on the current participants and/or available\nresources at every FL round of optimization. Specifically, we compare different\nmetrics to select this flying master and assess consensus algorithms to perform\nthe selection. Our results demonstrate a significant reduction of runtime using\nour flying master FL framework compared to the original FL from measurements\nresults conducted in our EdgeAI testbed and over real 5G networks using an\noperational edge testbed.", "time": "2021-12-21T19:04:42Z", "link": "http://arxiv.org/abs/2112.11485v1", "id": "2112.11485v1", "title": "On-the-fly Resource-Aware Model Aggregation for Federated Learning in\n  Heterogeneous Edge"}
{"author": "Xiong Wang, Jiancheng Ye, John C. S. Lui", "abstract": "Mobile edge computing facilitates users to offload computation tasks to edge\nservers for meeting their stringent delay requirements. Previous works mainly\nexplore task offloading when system-side information is given (e.g., server\nprocessing speed, cellular data rate), or centralized offloading under system\nuncertainty. But both generally fall short to handle task placement involving\nmany coexisting users in a dynamic and uncertain environment. In this paper, we\ndevelop a multi-user offloading framework considering unknown yet stochastic\nsystem-side information to enable a decentralized user-initiated service\nplacement. Specifically, we formulate the dynamic task placement as an online\nmulti-user multi-armed bandit process, and propose a decentralized epoch based\noffloading (DEBO) to optimize user rewards which are subjected under network\ndelay. We show that DEBO can deduce the optimal user-server assignment, thereby\nachieving a close-to-optimal service performance and tight O(log T) offloading\nregret. Moreover, we generalize DEBO to various common scenarios such as\nunknown reward gap, dynamic entering or leaving of clients, and fair reward\ndistribution, while further exploring when users' offloaded tasks require\nheterogeneous computing resources. Particularly, we accomplish a sub-linear\nregret for each of these instances. Real measurements based evaluations\ncorroborate the superiority of our offloading schemes over state-of-the-art\napproaches in optimizing delay-sensitive rewards.", "time": "2021-12-28T04:33:33Z", "link": "http://arxiv.org/abs/2112.11818v3", "id": "2112.11818v3", "title": "Decentralized Task Offloading in Edge Computing: A Multi-User\n  Multi-Armed Bandit Approach"}
{"author": "Bingyang Chen, Tao Chen, Xingjie Zeng, Weishan Zhang, Qinghua Lu, Zhaoxiang Hou, Jiehan Zhou, Sumi Helal", "abstract": "Millions of patients suffer from rare diseases around the world. However, the\nsamples of rare diseases are much smaller than those of common diseases. In\naddition, due to the sensitivity of medical data, hospitals are usually\nreluctant to share patient information for data fusion citing privacy concerns.\nThese challenges make it difficult for traditional AI models to extract rare\ndisease features for the purpose of disease prediction. In this paper, we\novercome this limitation by proposing a novel approach for rare disease\nprediction based on federated meta-learning. To improve the prediction accuracy\nof rare diseases, we design an attention-based meta-learning (ATML) approach\nwhich dynamically adjusts the attention to different tasks according to the\nmeasured training effect of base learners. Additionally, a dynamic-weight based\nfusion strategy is proposed to further improve the accuracy of federated\nlearning, which dynamically selects clients based on the accuracy of each local\nmodel. Experiments show that with as few as five shots, our approach\nout-performs the original federated meta-learning algorithm in accuracy and\nspeed. Compared with each hospital's local model, the proposed model's average\nprediction accuracy increased by 13.28%.", "time": "2021-12-29T02:18:43Z", "link": "http://arxiv.org/abs/2112.14364v1", "id": "2112.14364v1", "title": "Feature-context driven Federated Meta-Learning for Rare Disease\n  Prediction"}
{"author": "Shicheng Gao, Jie Xu, Xiaosen Li, Fangcheng Fu, Wentao Zhang, Wen Ouyang, Yangyu Tao, Bin Cui", "abstract": "K-core decomposition is a commonly used metric to analyze graph structure or\nstudy the relative importance of nodes in complex graphs. Recent years have\nseen rapid growth in the scale of the graph, especially in industrial settings.\nFor example, our industrial partner runs popular social applications with\nbillions of users and is able to gather a rich set of user data. As a result,\napplying K-core decomposition on large graphs has attracted more and more\nattention from academics and the industry. A simple but effective method to\ndeal with large graphs is to train them in the distributed settings, and some\ndistributed K-core decomposition algorithms are also proposed. Despite their\neffectiveness, we experimentally and theoretically observe that these\nalgorithms consume too many resources and become unstable on super-large-scale\ngraphs, especially when the given resources are limited. In this paper, we deal\nwith those super-large-scale graphs and propose a divide-and-conquer strategy\non top of the distributed K-core decomposition algorithm. We evaluate our\napproach on three large graphs. The experimental results show that the\nconsumption of resources can be significantly reduced, and the calculation on\nlarge-scale graphs becomes more stable than the existing methods. For example,\nthe distributed K-core decomposition algorithm can scale to a large graph with\n136 billion edges without losing correctness with our divide-and-conquer\ntechnique.", "time": "2021-12-26T04:34:11Z", "link": "http://arxiv.org/abs/2112.14840v1", "id": "2112.14840v1", "title": "K-Core Decomposition on Super Large Graphs with Limited Resources"}
{"author": "Qing Yang, Hao Wang", "abstract": "With the booming of smart grid, The ubiquitously deployed smart meters\nconstitutes an energy internet of things. This paper develops a novel\nblockchain-based transactive energy management system for IoT-aided smart\nhomes. We consider a holistic set of options for smart homes to participate in\ntransactive energy. Smart homes can interact with the grid to perform vertical\ntransactions, e.g., feeding in extra solar energy to the grid and providing\ndemand response service to alleviate the grid load. Smart homes can also\ninteract with peer users to perform horizontal transactions, e.g., peer-to-peer\nenergy trading. However, conventional transactive energy management method\nsuffers from the drawbacks of low efficiency, privacy leakage, and single-point\nfailure. To address these challenges, we develop a privacy-preserving\ndistributed algorithm that enables users to optimally manage their energy\nusages in parallel via the smart contract on the blockchain. Further, we design\nan efficient blockchain system tailored for IoT devices and develop the smart\ncontract to support the holistic transactive energy management system. Finally,\nwe evaluate the feasibility and performance of the blockchain-based transactive\nenergy management system through extensive simulations and experiments. The\nresults show that the blockchain-based transactive energy management system is\nfeasible on practical IoT devices and reduces the overall cost by 25%.", "time": "2021-03-10T13:18:12Z", "link": "http://arxiv.org/abs/2101.03840v2", "id": "2101.03840v2", "title": "Privacy-Preserving Transactive Energy Management for IoT-aided Smart\n  Homes via Blockchain"}
{"author": "Zachary R. Atkins, Christopher J. Vogl, Achintya Madduri, Nan Duan, Agnieszka K. Miedlar, Daniel Merl", "abstract": "As photovoltaic (PV) penetration continues to rise and smart inverter\nfunctionality continues to expand, smart inverters and other distributed energy\nresources (DERs) will play increasingly important roles in distribution system\npower management and security. In this paper, it is demonstrated that a\nconstellation of smart inverters in a simulated distribution circuit can enable\nprecise voltage predictions using an asynchronous and decentralized prediction\nalgorithm. Using simulated data and a constellation of 15 inverters in a ring\ncommunication topology, the COLA algorithm is shown to accomplish the learning\ntask required for voltage magnitude prediction with far less communication\noverhead than fully connected P2P learning protocols. Additionally, a dynamic\nstopping criterion is proposed that does not require a regularizer like the\noriginal COLA stopping criterion.", "time": "2021-01-13T00:22:36Z", "link": "http://arxiv.org/abs/2101.04816v1", "id": "2101.04816v1", "title": "Distribution System Voltage Prediction from Smart Inverters using\n  Decentralized Regression"}
{"author": "Nikolay Ivanov, Qiben Yan", "abstract": "Most self-service payment terminals require network connectivity for\nprocessing electronic payments. The necessity to maintain network connectivity\nincreases costs, introduces cybersecurity risks, and significantly limits the\nnumber of places where the terminals can be installed. Leading payment service\nproviders have proposed offline payment solutions that rely on algorithmically\ngenerated payment tokens. Existing payment token solutions, however, require\ncomplex mechanisms for authentication, transaction management, and most\nimportantly, security risk management. In this paper, we present VolgaPay, a\nblockchain-based system that allows merchants to deploy secure offline payment\nterminal infrastructure that does not require collection and storage of any\nsensitive data. We design a novel payment protocol which mitigates security\nthreats for all the participants of VolgaPay, such that the maximum loss from\ngaining full access to any component by an adversary incurs only a limited\nscope of harm. We achieve significant enhancements in security, operation\nefficiency, and cost reduction via a combination of polynomial multi-hash chain\nmicropayment channels and blockchain grafting for off-chain channel state\ntransition. We implement the VolgaPay payment system, and with thorough\nevaluation and security analysis, we demonstrate that VolgaPay is capable of\ndelivering a fast, secure, and cost-efficient solution for offline payment\nterminals.", "time": "2021-07-18T16:50:57Z", "link": "http://arxiv.org/abs/2107.08490v1", "id": "2107.08490v1", "title": "System-Wide Security for Offline Payment Terminals"}
{"author": "Xu Wang, Wei Ni, Xuan Zha, Guangsheng Yu, Ren Ping Liu, Nektarios Georgalas, Andrew Reeves", "abstract": "As distributed ledgers, blockchains run consensus protocols which trade\ncapacity for consistency, especially in non-ideal networks with incomplete\nconnectivity and erroneous links. Existing studies on the tradeoff between\ncapacity and consistency are only qualitative or rely on specific assumptions.\nThis paper presents discrete-time Markov chain models to quantify the capacity\nof Proof-of-Work based public blockchains in non-ideal networks. The\ncomprehensive model is collapsed to be ergodic under the eventual consistency\nof blockchains, achieving tractability and efficient evaluations of blockchain\ncapacity. A closed-form expression for the capacity is derived in the case of\ntwo miners. Another important aspect is that we extend the ergodic model to\nanalyze the capacity under strong consistency, evaluating the robustness of\nblockchains against double-spending attacks. Validated by simulations, the\nproposed models are accurate and reveal the effect of link quality and the\ndistribution of mining rates on blockchain capacity and the ratio of stale\nblocks.", "time": "2021-06-27T05:38:13Z", "link": "http://arxiv.org/abs/2106.14149v1", "id": "2106.14149v1", "title": "Capacity Analysis of Public Blockchain"}
{"author": "Mateusz Iwo Dubaniowski, Hans Rudolf Heinimann", "abstract": "System-of-systems (SoS) approach is often used for simulating disruptions to\nbusiness and infrastructure system networks allowing for integration of several\nmodels into one simulation. However, the integration is frequently challenging\nas each system is designed individually with different characteristics, such as\ntime granularity. Understanding the impact of time granularity on propagation\nof disruptions between businesses and infrastructure systems and finding the\nappropriate granularity for the SoS simulation remain as major challenges. To\ntackle these, we explore how time granularity, recovery time, and disruption\nsize affect the propagation of disruptions between constituent systems of an\nSoS simulation. To address this issue, we developed a High Level Architecture\n(HLA) simulation of 3 networks and performed a series of simulation\nexperiments. Our results revealed that time granularity and especially recovery\ntime have huge impact on propagation of disruptions. Consequently, we developed\na model for selecting an appropriate time granularity for an SoS simulation\nbased on expected recovery time. Our simulation experiments show that time\ngranularity should be less than 1.13 of expected recovery time. We identified\nsome areas for future research centered around extending the experimental\nfactors space.", "time": "2021-03-04T10:39:37Z", "link": "http://arxiv.org/abs/2103.03247v1", "id": "2103.03247v1", "title": "Time granularity impact on propagation of disruptions in a\n  system-of-systems simulation of infrastructure and business networks"}
{"author": "Mohammad Goudarzi, Qifan Deng, Rajkumar Buyya", "abstract": "Edge/Fog computing is a novel computing paradigm that provides\nresource-limited Internet of Things (IoT) devices with scalable computing and\nstorage resources. Compared to cloud computing, edge/fog servers have fewer\nresources, but they can be accessed with higher bandwidth and less\ncommunication latency. Thus, integrating edge/fog and cloud infrastructures can\nsupport the execution of diverse latency-sensitive and computation-intensive\nIoT applications. Although some frameworks attempt to provide such integration,\nthere are still several challenges to be addressed, such as dynamic scheduling\nof different IoT applications, scalability mechanisms, multi-platform support,\nand supporting different interaction models. FogBus2, as a new python-based\nframework, offers a lightweight and distributed container-based framework to\novercome these challenges. In this chapter, we highlight key features of the\nFogBus2 framework alongside describing its main components. Besides, we provide\na step-by-step guideline to set up an integrated computing environment,\ncontaining multiple cloud service providers (Hybrid-cloud) and edge devices,\nwhich is a prerequisite for any IoT application scenario. To obtain this, a\nlow-overhead communication network among all computing resources is initiated\nby the provided scripts and configuration files. Next, we provide instructions\nand corresponding code snippets to install and run the main framework and its\nintegrated applications. Finally, we demonstrate how to implement and integrate\nseveral new IoT applications and custom scheduling and scalability policies\nwith the FogBus2 framework.", "time": "2021-08-02T01:55:41Z", "link": "http://arxiv.org/abs/2108.00591v1", "id": "2108.00591v1", "title": "Resource Management in Edge and Fog Computing using FogBus2 Framework"}
{"author": "Jamie Haddock, Benjamin Jarman, Chen Yap", "abstract": "Gossip protocols are popular methods for average consensus problems in\ndistributed computing. We prove new convergence guarantees for a variety of\nsuch protocols, including path, clique, and synchronous pairwise gossip. These\narise by exploiting the connection between these protocols and the block\nrandomized Kaczmarz method for solving linear systems. Moreover, we extend\nexisting convergence results for block randomized Kaczmarz to allow for a more\ngeneral choice of blocks, rank-deficient systems, and provide a tighter\nconvergence rate guarantee. We furthermore apply this analysis to inconsistent\nconsensus models and obtain similar guarantees. An extensive empirical analysis\nof these methods is provided for a variety of synthetic networks.", "time": "2021-10-27T17:29:37Z", "link": "http://arxiv.org/abs/2110.14609v1", "id": "2110.14609v1", "title": "Paving the Way for Consensus: Convergence of Block Gossip Algorithms"}
{"author": "Shuai Yu, Xiaowen Gong, Qian Shi, Xiaofei Wang, Xu Chen", "abstract": "Edge computing-enhanced Internet of Vehicles (EC-IoV) enables ubiquitous data\nprocessing and content sharing among vehicles and terrestrial edge computing\n(TEC) infrastructures (e.g., 5G base stations and roadside units) with little\nor no human intervention, plays a key role in the intelligent transportation\nsystems. However, EC-IoV is heavily dependent on the connections and\ninteractions between vehicles and TEC infrastructures, thus will break down in\nsome remote areas where TEC infrastructures are unavailable (e.g., desert,\nisolated islands and disaster-stricken areas). Driven by the ubiquitous\nconnections and global-area coverage, space-air-ground integrated networks\n(SAGINs) efficiently support seamless coverage and efficient resource\nmanagement, represent the next frontier for edge computing. In light of this,\nwe first review the state-of-the-art edge computing research for SAGINs in this\narticle. After discussing several existing orbital and aerial edge computing\narchitectures, we propose a framework of edge computing-enabled\nspace-air-ground integrated networks (EC-SAGINs) to support various IoV\nservices for the vehicles in remote areas. The main objective of the framework\nis to minimize the task completion time and satellite resource usage. To this\nend, a pre-classification scheme is presented to reduce the size of action\nspace, and a deep imitation learning (DIL) driven offloading and caching\nalgorithm is proposed to achieve real-time decision making. Simulation results\nshow the effectiveness of our proposed scheme. At last, we also discuss some\ntechnology challenges and future directions.", "time": "2021-01-15T10:56:23Z", "link": "http://arxiv.org/abs/2101.06056v1", "id": "2101.06056v1", "title": "EC-SAGINs: Edge Computing-enhanced Space-Air-Ground Integrated Networks\n  for Internet of Vehicles"}
{"author": "Qiong Wu, Xu Chen, Zhi Zhou, Liang Chen, Junshan Zhang", "abstract": "To meet the ever increasing mobile traffic demand in 5G era, base stations\n(BSs) have been densely deployed in radio access networks (RANs) to increase\nthe network coverage and capacity. However, as the high density of BSs is\ndesigned to accommodate peak traffic, it would consume an unnecessarily large\namount of energy if BSs are on during off-peak time. To save the energy\nconsumption of cellular networks, an effective way is to deactivate some idle\nbase stations that do not serve any traffic demand. In this paper, we develop a\ntraffic-aware dynamic BS sleep control framework, named DeepBSC, which presents\na novel data-driven learning approach to determine the BS active/sleep modes\nwhile meeting lower energy consumption and satisfactory Quality of Service\n(QoS) requirements. Specifically, the traffic demands are predicted by the\nproposed GS-STN model, which leverages the geographical and semantic\nspatial-temporal correlations of mobile traffic. With accurate mobile traffic\nforecasting, the BS sleep control problem is cast as a Markov Decision Process\nthat is solved by Actor-Critic reinforcement learning methods. To reduce the\nvariance of cost estimation in the dynamic environment, we propose a benchmark\ntransformation method that provides robust performance indicator for policy\nupdate. To expedite the training process, we adopt a Deep Deterministic Policy\nGradient (DDPG) approach, together with an explorer network, which can\nstrengthen the exploration further. Extensive experiments with a real-world\ndataset corroborate that our proposed framework significantly outperforms the\nexisting methods.", "time": "2021-01-21T01:39:42Z", "link": "http://arxiv.org/abs/2101.08391v1", "id": "2101.08391v1", "title": "Deep Reinforcement Learning with Spatio-temporal Traffic Forecasting for\n  Data-Driven Base Station Sleep Control"}
{"author": "An Zou, Jing Li, Christopher D. Gill, Xuan Zhang", "abstract": "Many emerging cyber-physical systems, such as autonomous vehicles and robots,\nrely heavily on artificial intelligence and machine learning algorithms to\nperform important system operations. Since these highly parallel applications\nare computationally intensive, they need to be accelerated by graphics\nprocessing units (GPUs) to meet stringent timing constraints. However, despite\nthe wide adoption of GPUs, efficiently scheduling multiple GPU applications\nwhile providing rigorous real-time guarantees remains a challenge. In this\npaper, we propose RTGPU, which can schedule the execution of multiple GPU\napplications in real-time to meet hard deadlines. Each GPU application can have\nmultiple CPU execution and memory copy segments, as well as GPU kernels. We\nstart with a model to explicitly account for the CPU and memory copy segments\nof these applications. We then consider the GPU architecture in the development\nof a precise timing model for the GPU kernels and leverage a technique known as\npersistent threads to implement fine-grained kernel scheduling with improved\nperformance through interleaved execution. Next, we propose a general method\nfor scheduling parallel GPU applications in real time. Finally, to schedule\nmultiple parallel GPU applications, we propose a practical real-time scheduling\nalgorithm based on federated scheduling and grid search (for GPU kernel\nsegments) with uniprocessor fixed priority scheduling (for multiple CPU and\nmemory copy segments). Our approach provides superior schedulability compared\nwith previous work, and gives real-time guarantees to meet hard deadlines for\nmultiple GPU applications according to comprehensive validation and evaluation\non a real NVIDIA GTX1080Ti GPU system.", "time": "2023-02-06T11:39:58Z", "link": "http://arxiv.org/abs/2101.10463v3", "id": "2101.10463v3", "title": "RTGPU: Real-Time GPU Scheduling of Hard Deadline Parallel Tasks with\n  Fine-Grain Utilization"}
{"author": "Zhuosheng Zhang, Jiarui Li, Shucheng Yu, Christian Makaya", "abstract": "For model privacy, local model parameters in federated learning shall be\nobfuscated before sent to the remote aggregator. This technique is referred to\nas \\emph{secure aggregation}. However, secure aggregation makes model poisoning\nattacks such backdooring more convenient considering that existing anomaly\ndetection methods mostly require access to plaintext local models. This paper\nproposes SAFELearning which supports backdoor detection for secure aggregation.\nWe achieve this through two new primitives - \\emph{oblivious random grouping\n(ORG)} and \\emph{partial parameter disclosure (PPD)}. ORG partitions\nparticipants into one-time random subgroups with group configurations oblivious\nto participants; PPD allows secure partial disclosure of aggregated subgroup\nmodels for anomaly detection without leaking individual model privacy.\nSAFELearning can significantly reduce backdoor model accuracy without\njeopardizing the main task accuracy under common backdoor strategies. Extensive\nexperiments show SAFELearning is robust against malicious and faulty\nparticipants, whilst being more efficient than the state-of-art secure\naggregation protocol in terms of both communication and computation costs.", "time": "2021-09-03T04:39:08Z", "link": "http://arxiv.org/abs/2102.02402v2", "id": "2102.02402v2", "title": "SAFELearning: Enable Backdoor Detectability In Federated Learning With\n  Secure Aggregation"}
{"author": "Francesco Quinzan, Vanja DoskoÄ, Andreas GÃ¶bel, Tobias Friedrich", "abstract": "Several large-scale machine learning tasks, such as data summarization, can\nbe approached by maximizing functions that satisfy submodularity. These\noptimization problems often involve complex side constraints, imposed by the\nunderlying application. In this paper, we develop an algorithm with\npoly-logarithmic adaptivity for non-monotone submodular maximization under\ngeneral side constraints. The adaptive complexity of a problem is the minimal\nnumber of sequential rounds required to achieve the objective.\n  Our algorithm is suitable to maximize a non-monotone submodular function\nunder a $p$-system side constraint, and it achieves a $(p +\nO(\\sqrt{p}))$-approximation for this problem, after only poly-logarithmic\nadaptive rounds and polynomial queries to the valuation oracle function.\nFurthermore, our algorithm achieves a $(p + O(1))$-approximation when the given\nside constraint is a $p$-extendible system.\n  This algorithm yields an exponential speed-up, with respect to the\nadaptivity, over any other known constant-factor approximation algorithm for\nthis problem. It also competes with previous known results in terms of the\nquery complexity. We perform various experiments on various real-world\napplications. We find that, in comparison with commonly used heuristics, our\nalgorithm performs better on these instances.", "time": "2021-02-12T12:38:03Z", "link": "http://arxiv.org/abs/2102.06486v1", "id": "2102.06486v1", "title": "Adaptive Sampling for Fast Constrained Maximization of Submodular\n  Function"}
{"author": "Rohan Sarkar, Avinash C. Kak", "abstract": "We present CheckSoft, a scalable event-driven software architecture for\nkeeping track of people-object interactions in people-centric applications such\nas airport checkpoint security areas, automated retail stores, smart libraries,\nand so on. The architecture works off the video data generated in real time by\na network of surveillance cameras. Although there are many different aspects to\nautomating these applications, the most difficult part of the overall problem\nis keeping track of the interactions between the people and the objects.\nCheckSoft uses finite-state-machine (FSM) based logic for keeping track of such\ninteractions which allows the system to quickly reject any false detections of\nthe interactions by the video cameras. CheckSoft is easily scalable since the\narchitecture is based on multi-processing in which a separate process is\nassigned to each human and to each \"storage container\" for the objects. A\nstorage container may be a shelf on which the objects are displayed or a bin in\nwhich the objects are stored, depending on the specific application in which\nCheckSoft is deployed.", "time": "2021-02-21T05:22:55Z", "link": "http://arxiv.org/abs/2102.10513v1", "id": "2102.10513v1", "title": "CheckSoft : A Scalable Event-Driven Software Architecture for Keeping\n  Track of People and Things in People-Centric Spaces"}
{"author": "Subho S. Banerjee, Saurabh Jha, Zbigniew T. Kalbarczyk, Ravishankar K. Iyer", "abstract": "Hardware performance counters (HPCs) that measure low-level architectural and\nmicroarchitectural events provide dynamic contextual information about the\nstate of the system. However, HPC measurements are error-prone due to non\ndeterminism (e.g., undercounting due to event multiplexing, or OS\ninterrupt-handling behaviors). In this paper, we present BayesPerf, a system\nfor quantifying uncertainty in HPC measurements by using a domain-driven\nBayesian model that captures microarchitectural relationships between HPCs to\njointly infer their values as probability distributions. We provide the design\nand implementation of an accelerator that allows for low-latency and low-power\ninference of the BayesPerf model for x86 and ppc64 CPUs. BayesPerf reduces the\naverage error in HPC measurements from 40.1% to 7.6% when events are being\nmultiplexed. The value of BayesPerf in real-time decision-making is illustrated\nwith a simple example of scheduling of PCIe transfers.", "time": "2021-02-22T09:00:14Z", "link": "http://arxiv.org/abs/2102.10837v1", "id": "2102.10837v1", "title": "BayesPerf: Minimizing Performance Monitoring Errors Using Bayesian\n  Statistics"}
{"author": "Liangkai Liu, Shaoshan Liu, Weisong Shi", "abstract": "Connected and autonomous vehicles (CAVs) are promising due to their potential\nsafety and efficiency benefits and have attracted massive investment and\ninterest from government agencies, industry, and academia. With more computing\nand communication resources are available, both vehicles and edge servers are\nequipped with a set of camera-based vision sensors, also known as Visual IoT\n(V-IoT) techniques, for sensing and perception. Tremendous efforts have been\nmade for achieving programmable communication, computation, and control.\nHowever, they are conducted mainly in the silo mode, limiting the\nresponsiveness and efficiency of handling challenging scenarios in the real\nworld. To improve the end-to-end performance, we envision that future CAVs\nrequire the co-design of communication, computation, and control. This paper\npresents our vision of the end-to-end design principle for CAVs, called 4C,\nwhich extends the V-IoT system by providing a unified communication,\ncomputation, and control co-design framework. With programmable communications,\nfine-grained heterogeneous computation, and efficient vehicle controls in 4C,\nCAVs can handle critical scenarios and achieve energy-efficient autonomous\ndriving. Finally, we present several challenges to achieving the vision of the\n4C framework.", "time": "2021-07-31T17:18:32Z", "link": "http://arxiv.org/abs/2107.01142v2", "id": "2107.01142v2", "title": "4C: A Computation, Communication, and Control Co-Design Framework for\n  CAVs"}
{"author": "Raphael Y. Cohen, Aaron D. Sodickson", "abstract": "Current AI-driven research in radiology requires resources and expertise that\nare often inaccessible to small and resource-limited labs. The clinicians who\nare able to participate in AI research are frequently well-funded,\nwell-staffed, and either have significant experience with AI and computing, or\nhave access to colleagues or facilities that do. Current imaging data is\nclinician-oriented and is not easily amenable to machine learning initiatives,\nresulting in inefficient, time consuming, and costly efforts that rely upon a\ncrew of data engineers and machine learning scientists, and all too often\npreclude radiologists from driving AI research and innovation. We present the\nsystem and methodology we have developed to address infrastructure and platform\nneeds, while reducing the staffing and resource barriers to entry. We emphasize\na data-first and modular approach that streamlines the AI development and\ndeployment process while providing efficient and familiar interfaces for\nradiologists, such that they can be the drivers of new AI innovations.", "time": "2021-07-06T20:32:14Z", "link": "http://arxiv.org/abs/2107.04409v1", "id": "2107.04409v1", "title": "An Orchestration Platform that Puts Radiologists in the Driver's Seat of\n  AI Innovation: A Methodological Approach"}
{"author": "Sabuzima Nayak, Ripon Patgiri, Lilapati Waikhom, Arif Ahmed", "abstract": "Edge technology aims to bring Cloud resources (specifically, the compute,\nstorage, and network) to the closed proximity of the Edge devices, i.e., smart\ndevices where the data are produced and consumed. Embedding computing and\napplication in Edge devices lead to emerging of two new concepts in Edge\ntechnology, namely, Edge computing and Edge analytics. Edge analytics uses some\ntechniques or algorithms to analyze the data generated by the Edge devices.\nWith the emerging of Edge analytics, the Edge devices have become a complete\nset. Currently, Edge analytics is unable to provide full support for the\nexecution of the analytic techniques. The Edge devices cannot execute advanced\nand sophisticated analytic algorithms following various constraints such as\nlimited power supply, small memory size, limited resources, etc. This article\naims to provide a detailed discussion on Edge analytics. A clear explanation to\ndistinguish between the three concepts of Edge technology, namely, Edge\ndevices, Edge computing, and Edge analytics, along with their issues.\nFurthermore, the article discusses the implementation of Edge analytics to\nsolve many problems in various areas such as retail, agriculture, industry, and\nhealthcare. In addition, the research papers of the state-of-the-art edge\nanalytics are rigorously reviewed in this article to explore the existing\nissues, emerging challenges, research opportunities and their directions, and\napplications.", "time": "2021-07-01T21:48:20Z", "link": "http://arxiv.org/abs/2107.06835v1", "id": "2107.06835v1", "title": "A Review on Edge Analytics: Issues, Challenges, Opportunities, Promises,\n  Future Directions, and Applications"}
{"author": "Ming Liu, Stella Ho, Mengqi Wang, Longxiang Gao, Yuan Jin, He Zhang", "abstract": "Federated Learning aims to learn machine learning models from multiple\ndecentralized edge devices (e.g. mobiles) or servers without sacrificing local\ndata privacy. Recent Natural Language Processing techniques rely on deep\nlearning and large pre-trained language models. However, both big deep neural\nand language models are trained with huge amounts of data which often lies on\nthe server side. Since text data is widely originated from end users, in this\nwork, we look into recent NLP models and techniques which use federated\nlearning as the learning framework. Our survey discusses major challenges in\nfederated natural language processing, including the algorithm challenges,\nsystem challenges as well as the privacy issues. We also provide a critical\nreview of the existing Federated NLP evaluation methods and tools. Finally, we\nhighlight the current research gaps and future directions.", "time": "2021-07-27T05:07:48Z", "link": "http://arxiv.org/abs/2107.12603v1", "id": "2107.12603v1", "title": "Federated Learning Meets Natural Language Processing: A Survey"}
{"author": "Gordon E. Moon, Hyoukjun Kwon, Geonhwa Jeong, Prasanth Chatarasi, Sivasankaran Rajamanickam, Tushar Krishna", "abstract": "There is a growing interest in custom spatial accelerators for machine\nlearning applications. These accelerators employ a spatial array of processing\nelements (PEs) interacting via custom buffer hierarchies and networks-on-chip.\nThe efficiency of these accelerators comes from employing optimized dataflow\n(i.e., spatial/temporal partitioning of data across the PEs and fine-grained\nscheduling) strategies to optimize data reuse. The focus of this work is to\nevaluate these accelerator architectures using a tiled general matrix-matrix\nmultiplication (GEMM) kernel. To do so, we develop a framework that finds\noptimized mappings (dataflow and tile sizes) for a tiled GEMM for a given\nspatial accelerator and workload combination, leveraging an analytical cost\nmodel for runtime and energy. Our evaluations over five spatial accelerators\ndemonstrate that the tiled GEMM mappings systematically generated by our\nframework achieve high performance on various GEMM workloads and accelerators.", "time": "2021-06-19T13:53:58Z", "link": "http://arxiv.org/abs/2106.10499v1", "id": "2106.10499v1", "title": "Evaluating Spatial Accelerator Architectures with Tiled Matrix-Matrix\n  Multiplication"}
{"author": "Quian Matteo Chen, Alberto Finzi, Toni Mancini, Igor Melatti, Enrico Tronci", "abstract": "In critical infrastructures like airports, much care has to be devoted in\nprotecting radio communication networks from external electromagnetic\ninterference. Protection of such mission-critical radio communication networks\nis usually tackled by exploiting radiogoniometers: at least three suitably\ndeployed radiogoniometers, and a gateway gathering information from them,\npermit to monitor and localise sources of electromagnetic emissions that are\nnot supposed to be present in the monitored area. Typically, radiogoniometers\nare connected to the gateway through relay nodes. As a result, some degree of\nfault-tolerance for the network of relay nodes is essential in order to offer a\nreliable monitoring. On the other hand, deployment of relay nodes is typically\nquite expensive. As a result, we have two conflicting requirements: minimise\ncosts while guaranteeing a given fault-tolerance. In this paper, we address the\nproblem of computing a deployment for relay nodes that minimises the relay node\nnetwork cost while at the same time guaranteeing proper working of the network\neven when some of the relay nodes (up to a given maximum number) become faulty\n(fault-tolerance). We show that, by means of a computation-intensive\npre-processing on a HPC infrastructure, the above optimisation problem can be\nencoded as a 0/1 Linear Program, becoming suitable to be approached with\nstandard Artificial Intelligence reasoners like MILP, PB-SAT, and SMT/OMT\nsolvers. Our problem formulation enables us to present experimental results\ncomparing the performance of these three solving technologies on a real case\nstudy of a relay node network deployment in areas of the Leonardo da Vinci\nAirport in Rome, Italy.", "time": "2021-06-20T12:14:36Z", "link": "http://arxiv.org/abs/2106.10685v1", "id": "2106.10685v1", "title": "MILP, pseudo-boolean, and OMT solvers for optimal fault-tolerant\n  placements of relay nodes in mission critical wireless networks"}
{"author": "Boyuan Feng, Yuke Wang, Tong Geng, Ang Li, Yufei Ding", "abstract": "Over the years, accelerating neural networks with quantization has been\nwidely studied. Unfortunately, prior efforts with diverse precisions (e.g.,\n1-bit weights and 2-bit activations) are usually restricted by limited\nprecision support on GPUs (e.g., int1 and int4). To break such restrictions, we\nintroduce the first Arbitrary Precision Neural Network framework (APNN-TC) to\nfully exploit quantization benefits on Ampere GPU Tensor Cores. Specifically,\nAPNN-TC first incorporates a novel emulation algorithm to support arbitrary\nshort bit-width computation with int1 compute primitives and XOR/AND Boolean\noperations. Second, APNN-TC integrates arbitrary precision layer designs to\nefficiently map our emulation algorithm to Tensor Cores with novel batching\nstrategies and specialized memory organization. Third, APNN-TC embodies a novel\narbitrary precision NN design to minimize memory access across layers and\nfurther improve performance. Extensive evaluations show that APNN-TC can\nachieve significant speedup over CUTLASS kernels and various NN models, such as\nResNet and VGG.", "time": "2021-11-16T23:11:50Z", "link": "http://arxiv.org/abs/2106.12169v2", "id": "2106.12169v2", "title": "APNN-TC: Accelerating Arbitrary Precision Neural Networks on Ampere GPU\n  Tensor Cores"}
{"author": "Dirk Fahland, Vadim Denisov, Wil. M. P. van der Aalst", "abstract": "To identify the causes of performance problems or to predict process\nbehavior, it is essential to have correct and complete event data. This is\nparticularly important for distributed systems with shared resources, e.g., one\ncase can block another case competing for the same machine, leading to\ninter-case dependencies in performance. However, due to a variety of reasons,\nreal-life systems often record only a subset of all events taking place. To\nunderstand and analyze the behavior and performance of processes with shared\nresources, we aim to reconstruct bounds for timestamps of events in a case that\nmust have happened but were not recorded by inference over events in other\ncases in the system. We formulate and solve the problem by systematically\nintroducing multi-entity concepts in event logs and process models. We\nintroduce a partial-order based model of a multi-entity event log and a\ncorresponding compositional model for multi-entity processes. We define\nPQR-systems as a special class of multi-entity processes with shared resources\nand queues. We then study the problem of inferring from an incomplete event log\nunobserved events and their timestamps that are globally consistent with a\nPQR-system. We solve the problem by reconstructing unobserved traces of\nresources and queues according to the PQR-model and derive bounds for their\ntimestamps using a linear program. While the problem is illustrated for\nmaterial handling systems like baggage handling systems in airports, the\napproach can be applied to other settings where recording is incomplete. The\nideas have been implemented in ProM and were evaluated using both synthetic and\nreal-life event logs.", "time": "2021-12-09T15:24:15Z", "link": "http://arxiv.org/abs/2103.00167v3", "id": "2103.00167v3", "title": "Inferring Unobserved Events in Systems With Shared Resources and Queues"}
{"author": "Abhishek Narain Singh", "abstract": "Graph network science is becoming increasingly popular, notably in big-data\nperspective where understanding individual entities for individual functional\nroles is complex and time consuming. It is likely when a set of genes are\nregulated by a set of genetic variants, the genes set is recruited for a common\nor related functional purpose. Grouping and extracting communities from network\nof associations becomes critical to understand system complexity, thus\nprioritizing genes for dis-ease and functional associations. Workload is\nreduced when studying entities one at a time. For this, we present GraphBreak,\na suite of tools for community detection application, such as for gene\nco-expression, protein interaction, regulation network, etc.Although developed\nfor use case of eQTLs regulatory genomic net-work community study -- results\nshown with our analysis with sample eQTL data. Graphbreak can be deployed for\nother studies if input data has been fed in requisite format, including but not\nlimited to gene co-expression networks, protein-protein interaction network,\nsignaling pathway and metabolic network. Graph-Break showed critical use case\nvalue in its downstream analysis for disease association of communities\ndetected. If all independent steps of community detection and analysis are a\nstep-by-step sub-part of the algorithm, GraphBreak can be considered a new\nalgorithm for community based functional characterization. Combination of\nvarious algorithmic implementation modules into a single script for this\npurpose illustrates GraphBreak novelty. Compared to other similar tools, with\nGraphBreak we can better detect communities with over-representation of its\nmember genes for statistical association with diseases, therefore target genes\nwhich can be prioritized for drug-positioning or drug-re-positioning as the\ncase be.", "time": "2021-02-24T15:16:38Z", "link": "http://arxiv.org/abs/2103.06145v1", "id": "2103.06145v1", "title": "GraphBreak: Tool for Network Community based Regulatory Medicine, Gene\n  co-expression, Linkage Disequilibrium analysis, functional annotation and\n  more"}
{"author": "Ahmed Saleh Bataineh, Jamal Bentahar, Rabeb Mizouni, Omar Abdel Wahab, Gaith Rjoub, May El Barachi", "abstract": "With the unprecedented reliance on cloud computing as the backbone for\nstoring today's big data, we argue in this paper that the role of the cloud\nshould be reshaped from being a passive virtual market to become an active\nplatform for monetizing the big data through Artificial Intelligence (AI)\nservices. The objective is to enable the cloud to be an active platform that\ncan help big data service providers reach a wider set of customers and cloud\nusers (i.e., data consumers) to be exposed to a larger and richer variety of\ndata to run their data analytic tasks. To achieve this vision, we propose a\nnovel game theoretical model, which consists of a mix of cooperative and\ncompetitive strategies. The players of the game are the big data service\nproviders, cloud computing platform, and cloud users. The strategies of the\nplayers are modeled using the two-sided market theory that takes into\nconsideration the network effects among involved parties, while integrating the\nexternalities between the cloud resources and consumer demands into the design\nof the game. Simulations conducted using Amazon and google clustered data show\nthat the proposed model improves the total surplus of all the involved parties\nin terms of cloud resources provision and monetary profits compared to the\ncurrent merchant model.", "time": "2021-04-26T17:54:31Z", "link": "http://arxiv.org/abs/2104.12762v1", "id": "2104.12762v1", "title": "Cloud computing as a platform for monetizing data services: A two-sided\n  game business model"}
{"author": "Huned Materwala, Leila Ismail", "abstract": "Cloud computing enables remote execution of users tasks. The pervasive\nadoption of cloud computing in smart cities services and applications requires\ntimely execution of tasks adhering to Quality of Services (QoS).\n  However, the increasing use of computing servers exacerbates the issues of\nhigh energy consumption, operating costs, and environmental pollution.\nMaximizing the performance and minimizing the energy in a cloud data center is\nchallenging. In this paper, we propose a performance and energy optimization\nbi-objective algorithm to tradeoff the contradicting performance and energy\nobjectives. An evolutionary algorithm-based multi-objective optimization is for\nthe first time proposed using system performance counters. The performance of\nthe proposed model is evaluated using a realistic cloud dataset in a cloud\ncomputing environment. Our experimental results achieve higher performance and\nlower energy consumption compared to a state of the art algorithm.", "time": "2021-04-25T08:55:57Z", "link": "http://arxiv.org/abs/2105.00843v1", "id": "2105.00843v1", "title": "Performance and Energy-Aware Bi-objective Tasks Scheduling for Cloud\n  Data Centers"}
{"author": "Udit Gupta, Samuel Hsia, Jeff Zhang, Mark Wilkening, Javin Pombra, Hsien-Hsin S. Lee, Gu-Yeon Wei, Carole-Jean Wu, David Brooks", "abstract": "Deep learning recommendation systems must provide high quality, personalized\ncontent under strict tail-latency targets and high system loads. This paper\npresents RecPipe, a system to jointly optimize recommendation quality and\ninference performance. Central to RecPipe is decomposing recommendation models\ninto multi-stage pipelines to maintain quality while reducing compute\ncomplexity and exposing distinct parallelism opportunities. RecPipe implements\nan inference scheduler to map multi-stage recommendation engines onto\ncommodity, heterogeneous platforms (e.g., CPUs, GPUs).While the hardware-aware\nscheduling improves ranking efficiency, the commodity platforms suffer from\nmany limitations requiring specialized hardware. Thus, we design RecPipeAccel\n(RPAccel), a custom accelerator that jointly optimizes quality, tail-latency,\nand system throughput. RPAc-cel is designed specifically to exploit the\ndistinct design space opened via RecPipe. In particular, RPAccel processes\nqueries in sub-batches to pipeline recommendation stages, implements dual\nstatic and dynamic embedding caches, a set of top-k filtering units, and a\nreconfigurable systolic array. Com-pared to prior-art and at iso-quality, we\ndemonstrate that RPAccel improves latency and throughput by 3x and 6x.", "time": "2021-05-22T17:41:29Z", "link": "http://arxiv.org/abs/2105.08820v2", "id": "2105.08820v2", "title": "RecPipe: Co-designing Models and Hardware to Jointly Optimize\n  Recommendation Quality and Performance"}
{"author": "Saif Ur Rehman, Muhammad Rashid Razzaq, Muhammad Hadi Hussian", "abstract": "In this project, we have used the computer vision algorithm SSD (Single Shot\ndetector) computer vision algorithm and trained this algorithm from the dataset\nwhich consists of 139 Pictures. Images were labeled using Intel CVAT (Computer\nVision Annotation Tool)\n  We trained this model for facial detection. We have deployed our trained\nmodel and software in the Nvidia Jetson Nano Developer kit. Model code is\nwritten in Pytorch's deep learning framework. The programming language used is\nPython.", "time": "2021-05-28T15:16:24Z", "link": "http://arxiv.org/abs/2105.13906v1", "id": "2105.13906v1", "title": "Training of SSD(Single Shot Detector) for Facial Detection using Nvidia\n  Jetson Nano"}
{"author": "Liang Luo, Jacob Nelson, Arvind Krishnamurthy, Luis Ceze", "abstract": "ML workloads are becoming increasingly popular in the cloud. Good cloud\ntraining performance is contingent on efficient parameter exchange among VMs.\nWe find that Collectives, the widely used distributed communication algorithms,\ncannot perform optimally out of the box due to the hierarchical topology of\ndatacenter networks and multi-tenancy nature of the cloudenvironment.In this\npaper, we present Cloud Collectives , a prototype that accelerates collectives\nby reordering theranks of participating VMs such that the communication pattern\ndictated by the selected collectives operation best exploits the locality in\nthe network.Collectives is non-intrusive, requires no code changes nor rebuild\nof an existing application, and runs without support from cloud providers. Our\npreliminary application of Cloud Collectives on allreduce operations in public\nclouds results in a speedup of up to 3.7x in multiple microbenchmarks and 1.3x\nin real-world workloads of distributed training of deep neural networks and\ngradient boosted decision trees using state-of-the-art frameworks.", "time": "2021-05-28T20:14:38Z", "link": "http://arxiv.org/abs/2105.14088v1", "id": "2105.14088v1", "title": "Cloud Collectives: Towards Cloud-aware Collectives forML Workloads with\n  Rank Reordering"}
{"author": "Adnan Akhunzada, Sherali Zeadally, Saif ul Islam", "abstract": "Software Defined Networks (SDNs) have dramatically simplified network\nmanagement. However, enabling pure SDNs to respond in real-time while handling\nmassive amounts of data still remains a challenging task. In contrast, fog\ncomputing has strong potential to serve large surges of data in real-time. SDN\ncontrol plane enables innovation, and greatly simplifies network operations and\nmanagement thereby providing a promising solution to implement energy and\nperformance aware SDN-enabled fog computing. Besides, power efficiency and\nperformance evaluation in SDN-enabled fog computing is an area that has not yet\nbeen fully explored by the research community. We present a novel SDN-enabled\nfog architecture to improve power efficacy and performance by leveraging\ncooperative and non-cooperative policy-based computing. Preliminary results\nfrom extensive simulation demonstrate an improvement in the power utilization\nas well as the overall performance (i.e., processing time, response time).\nFinally, we discuss several open research issues that need further\ninvestigation in the future.", "time": "2021-05-30T19:28:52Z", "link": "http://arxiv.org/abs/2105.14607v1", "id": "2105.14607v1", "title": "Power and Performance Efficient SDN-Enabled Fog Architecture"}
{"author": "Sasindu Wijeratne, Sanket Pattnaik, Zhiyu Chen, Rajgopal Kannan, Viktor Prasanna", "abstract": "Even with generational improvements in DRAM technology, memory access latency\nstill remains the major bottleneck for application accelerators, primarily due\nto limitations in memory interface IPs which cannot fully account for\nvariations in target applications, the algorithms used, and accelerator\narchitectures. Since developing memory controllers for different applications\nis time-consuming, this paper introduces a modular and programmable memory\ncontroller that can be configured for different target applications on\navailable hardware resources. The proposed memory controller efficiently\nsupports cache-line accesses along with bulk memory transfers. The user can\nconfigure the controller depending on the available logic resources on the\nFPGA, memory access pattern, and external memory specifications. The modular\ndesign supports various memory access optimization techniques including,\nrequest scheduling, internal caching, and direct memory access. These\ntechniques contribute to reducing the overall latency while maintaining high\nsustained bandwidth. We implement the system on a state-of-the-art FPGA and\nevaluate its performance using two widely studied domains: graph analytics and\ndeep learning workloads. We show improved overall memory access time up to 58%\non CNN and GCN workloads compared with commercial memory controller IPs.", "time": "2021-08-21T23:53:12Z", "link": "http://arxiv.org/abs/2108.09601v1", "id": "2108.09601v1", "title": "Programmable FPGA-based Memory Controller"}
{"author": "Hangyu Zhu, Rui Wang, Yaochu Jin, Kaitai Liang", "abstract": "Federated learning (FL) is an emerging privacy preserving machine learning\nprotocol that allows multiple devices to collaboratively train a shared global\nmodel without revealing their private local data. Non-parametric models like\ngradient boosting decision trees (GBDT) have been commonly used in FL for\nvertically partitioned data. However, all these studies assume that all the\ndata labels are stored on only one client, which may be unrealistic for\nreal-world applications. Therefore, in this work, we propose a secure vertical\nFL framework, named PIVODL, to train GBDT with data labels distributed on\nmultiple devices. Both homomorphic encryption and differential privacy are\nadopted to prevent label information from being leaked through transmitted\ngradients and leaf values. Our experimental results show that both information\nleakage and model performance degradation of the proposed PIVODL are\nnegligible.", "time": "2021-08-25T19:25:49Z", "link": "http://arxiv.org/abs/2108.11444v1", "id": "2108.11444v1", "title": "PIVODL: Privacy-preserving vertical federated learning over distributed\n  labels"}
{"author": "Kaira Samuel, Vijay Gadepally, David Jacobs, Michael Jones, Kyle McAlpin, Kyle Palko, Ben Paulk, Sid Samsi, Ho Chit Siu, Charles Yee, Jeremy Kepner", "abstract": "AI algorithms that identify maneuvers from trajectory data could play an\nimportant role in improving flight safety and pilot training. AI challenges\nallow diverse teams to work together to solve hard problems and are an\neffective tool for developing AI solutions. AI challenges are also a key driver\nof AI computational requirements. The Maneuver Identification Challenge hosted\nat maneuver-id.mit.edu provides thousands of trajectories collected from pilots\npracticing in flight simulators, descriptions of maneuvers, and examples of\nthese maneuvers performed by experienced pilots. Each trajectory consists of\npositions, velocities, and aircraft orientations normalized to a common\ncoordinate system. Construction of the data set required significant data\narchitecture to transform flight simulator logs into AI ready data, which\nincluded using a supercomputer for deduplication and data conditioning. There\nare three proposed challenges. The first challenge is separating physically\nplausible (good) trajectories from unfeasible (bad) trajectories. Human labeled\ngood and bad trajectories are provided to aid in this task. Subsequent\nchallenges are to label trajectories with their intended maneuvers and to\nassess the quality of those maneuvers.", "time": "2021-08-25T22:41:45Z", "link": "http://arxiv.org/abs/2108.11503v1", "id": "2108.11503v1", "title": "Maneuver Identification Challenge"}
{"author": "Rohit Chowdhury, Deepak Subramani", "abstract": "Autonomous marine vehicles play an essential role in many ocean science and\nengineering applications. Planning time and energy optimal paths for these\nvehicles to navigate in stochastic dynamic ocean environments is essential to\nreduce operational costs. In some missions, they must also harvest solar, wind,\nor wave energy (modeled as a stochastic scalar field) and move in optimal paths\nthat minimize net energy consumption. Markov Decision Processes (MDPs) provide\na natural framework for sequential decision-making for robotic agents in such\nenvironments. However, building a realistic model and solving the modeled MDP\nbecomes computationally expensive in large-scale real-time applications,\nwarranting the need for parallel algorithms and efficient implementation. In\nthe present work, we introduce an efficient end-to-end GPU-accelerated\nalgorithm that (i) builds the MDP model (computing transition probabilities and\nexpected one-step rewards); and (ii) solves the MDP to compute an optimal\npolicy. We develop methodical and algorithmic solutions to overcome the limited\nglobal memory of GPUs by (i) using a dynamic reduced-order representation of\nthe ocean flows, (ii) leveraging the sparse nature of the state transition\nprobability matrix, (iii) introducing a neighbouring sub-grid concept and (iv)\nproving that it is sufficient to use only the stochastic scalar field's mean to\ncompute the expected one-step rewards for missions involving energy harvesting\nfrom the environment; thereby saving memory and reducing the computational\neffort. We demonstrate the algorithm on a simulated stochastic dynamic\nenvironment and highlight that it builds the MDP model and computes the optimal\npolicy 600-1000x faster than conventional CPU implementations, making it\nsuitable for real-time use.", "time": "2021-09-20T12:39:17Z", "link": "http://arxiv.org/abs/2109.00857v2", "id": "2109.00857v2", "title": "Optimal Path Planning of Autonomous Marine Vehicles in Stochastic\n  Dynamic Ocean Flows using a GPU-Accelerated Algorithm"}
{"author": "Samuel Ackerman, Sanjib Choudhury, Nirmit Desai, Eitan Farchi, Dan Gisolfi, Andrew Hicks, Saritha Route, Diptikalyan Saha", "abstract": "API economy is driving the digital transformation of business applications\nacross the hybrid Cloud and edge environments. For such transformations to\nsucceed, end-to-end testing of the application API composition is required.\nTesting of API compositions, even in centralized Cloud environments, is\nchallenging as it requires coverage of functional as well as reliability\nrequirements. The combinatorial space of scenarios is huge, e.g., API input\nparameters, order of API execution, and network faults. Hybrid Cloud and edge\nenvironments exacerbate the challenge of API testing due to the need to\ncoordinate test execution across dynamic wide-area networks, possibly across\nnetwork boundaries. To handle this challenge, we envision a test framework\nnamed Distributed Software Test Kit (DSTK). The DSTK leverages Combinatorial\nTest Design (CTD) to cover the functional requirements and then automatically\ncovers the reliability requirements via under-the-hood closed loop between test\nexecution feedback and AI based search algorithms. In each iteration of the\nclosed loop, the search algorithms generate more reliability test scenarios to\nbe executed next. Specifically, five kinds of reliability tests are envisioned:\nout-of-order execution of APIs, network delays and faults, API performance and\nthroughput, changes in API call graph patterns, and changes in application\ntopology.", "time": "2021-09-06T15:27:36Z", "link": "http://arxiv.org/abs/2109.02540v1", "id": "2109.02540v1", "title": "Towards API Testing Across Cloud and Edge"}
{"author": "Sasindu Wijeratne, Rajgopal Kannan, Viktor Prasanna", "abstract": "Tensor decomposition has become an essential tool in many applications in\nvarious domains, including machine learning. Sparse Matricized Tensor Times\nKhatri-Rao Product (MTTKRP) is one of the most computationally expensive\nkernels in tensor computations. Despite having significant computational\nparallelism, MTTKRP is a challenging kernel to optimize due to its irregular\nmemory access characteristics. This paper focuses on a multi-faceted memory\nsystem, which explores the spatial and temporal locality of the data structures\nof MTTKRP. Further, users can reconfigure our design depending on the behavior\nof the compute units used in the FPGA accelerator. Our system efficiently\naccesses all the MTTKRP data structures while reducing the total memory access\ntime, using a distributed cache and Direct Memory Access (DMA) subsystem.\nMoreover, our work improves the memory access time by 3.5x compared with\ncommercial memory controller IPs. Also, our system shows 2x and 1.26x speedups\ncompared with cache-only and DMA-only memory systems, respectively.", "time": "2021-09-18T08:19:29Z", "link": "http://arxiv.org/abs/2109.08874v1", "id": "2109.08874v1", "title": "Reconfigurable Low-latency Memory System for Sparse Matricized Tensor\n  Times Khatri-Rao Product on FPGA"}
{"author": "SayedHassan Khatoonabadi, Shahriar Lotfi, Ayaz Isazadeh", "abstract": "Despite all the progress in Web service selection, the need for an approach\nwith a better optimality and performance still remains. This paper presents a\ngenetic algorithm by adopting the Pareto principle that is called GAP2WSS for\nselecting a Web service for each task of a composite Web service from a pool of\ncandidate Web services. In contrast to the existing approaches, all global QoS\nconstraints, interservice constraints, and transactional constraints are\nconsidered simultaneously. At first, all candidate Web services are scored and\nranked per each task using the proposed mechanism. Then, the top 20 percent of\nthe candidate Web services of each task are considered as the candidate Web\nservices of the corresponding task to reduce the problem search space. Finally,\nthe Web service selection problem is solved by focusing only on these 20\npercent candidate Web services of each task using a genetic algorithm.\nEmpirical studies demonstrate this approach leads to a higher efficiency and\nefficacy as compared with the case that all the candidate Web services are\nconsidered in solving the problem.", "time": "2021-09-21T20:41:21Z", "link": "http://arxiv.org/abs/2109.10430v1", "id": "2109.10430v1", "title": "GAP2WSS: A Genetic Algorithm based on the Pareto Principle for Web\n  Service Selection"}
{"author": "Mhd Saria Allahham, Sameh Sorour, Amr Mohamed, Aiman Erbad, Mohsen Guizani", "abstract": "Mobile Edge Learning (MEL) is a learning paradigm that enables distributed\ntraining of Machine Learning models over heterogeneous edge devices (e.g., IoT\ndevices). Multi-orchestrator MEL refers to the coexistence of multiple learning\ntasks with different datasets, each of which being governed by an orchestrator\nto facilitate the distributed training process. In MEL, the training\nperformance deteriorates without the availability of sufficient training data\nor computing resources. Therefore, it is crucial to motivate edge devices to\nbecome learners and offer their computing resources, and either offer their\nprivate data or receive the needed data from the orchestrator and participate\nin the training process of a learning task. In this work, we propose an\nincentive mechanism, where we formulate the orchestrators-learners interactions\nas a 2-round Stackelberg game to motivate the participation of the learners. In\nthe first round, the learners decide which learning task to get engaged in, and\nthen in the second round, the training parameters and the amount of data for\ntraining in case of participation such that their utility is maximized. We then\nstudy the training round analytically and derive the learners' optimal\nstrategy. Finally, numerical experiments have been conducted to evaluate the\nperformance of the proposed incentive mechanism.", "time": "2021-12-31T16:36:08Z", "link": "http://arxiv.org/abs/2109.12409v3", "id": "2109.12409v3", "title": "Motivating Learners in Multi-Orchestrator Mobile Edge Learning: A\n  Stackelberg Game Approach"}
{"author": "Beatriz Soret, Lam D. Nguyen, Jan Seeger, Arne BrÃ¶ring, Chaouki Ben Issaid, Sumudu Samarakoon, Anis El Gabli, Vivek Kulkarni, Mehdi Bennis, Petar Popovski", "abstract": "An Intelligent IoT Environment (iIoTe) is comprised of heterogeneous devices\nthat can collaboratively execute semi-autonomous IoT applications, examples of\nwhich include highly automated manufacturing cells or autonomously interacting\nharvesting machines. Energy efficiency is key in such edge environments, since\nthey are often based on an infrastructure that consists of wireless and\nbattery-run devices, e.g., e-tractors, drones, Automated Guided Vehicle (AGV)s\nand robots. The total energy consumption draws contributions from multipleiIoTe\ntechnologies that enable edge computing and communication, distributed\nlearning, as well as distributed ledgers and smart contracts. This paper\nprovides a state-of-the-art overview of these technologies and illustrates\ntheir functionality and performance, with special attention to the tradeoff\namong resources, latency, privacy and energy consumption. Finally, the paper\nprovides a vision for integrating these enabling technologies in\nenergy-efficient iIoTe and a roadmap to address the open research challenges", "time": "2021-12-24T08:40:23Z", "link": "http://arxiv.org/abs/2110.01686v2", "id": "2110.01686v2", "title": "Learning, Computing, and Trustworthiness in Intelligent IoT\n  Environments: Performance-Energy Tradeoffs"}
{"author": "Zhiyuan Yao, Yoann Desmouceaux, Mark Townsley, Thomas Heide Clausen", "abstract": "Network load balancers are important components in data centers to provide\nscalable services. Workload distribution algorithms are based on heuristics,\ne.g., Equal-Cost Multi-Path (ECMP), Weighted-Cost Multi-Path (WCMP) or naive\nmachine learning (ML) algorithms, e.g., ridge regression. Advanced ML-based\napproaches help achieve performance gain in different networking and system\nproblems. However, it is challenging to apply ML algorithms on networking\nproblems in real-life systems. It requires domain knowledge to collect features\nfrom low-latency, high-throughput, and scalable networking systems, which are\ndynamic and heterogenous. This paper proposes Aquarius to bridge the gap\nbetween ML and networking systems and demonstrates its usage in the context of\nnetwork load balancers. This paper demonstrates its ability of conducting both\noffline data analysis and online model deployment in realistic systems. The\nresults show that the ML model trained and deployed using Aquarius improves\nload balancing performance yet they also reveals more challenges to be resolved\nto apply ML for networking systems.", "time": "2021-10-27T12:47:30Z", "link": "http://arxiv.org/abs/2110.15788v1", "id": "2110.15788v1", "title": "Towards Intelligent Load Balancing in Data Centers"}
{"author": "Zhiyuan Yao, Zihan Ding, Thomas Heide Clausen", "abstract": "Network load balancers are central components in data centers, that\ndistributes workloads across multiple servers and thereby contribute to\noffering scalable services. However, when load balancers operate in dynamic\nenvironments with limited monitoring of application server loads, they rely on\nheuristic algorithms that require manual configurations for fairness and\nperformance. To alleviate that, this paper proposes a distributed asynchronous\nreinforcement learning mechanism to-with no active load balancer state\nmonitoring and limited network observations-improve the fairness of the\nworkload distribution achieved by a load balancer. The performance of proposed\nmechanism is evaluated and compared with stateof-the-art load balancing\nalgorithms in a simulator, under configurations with progressively increasing\ncomplexities. Preliminary results show promise in RLbased load balancing\nalgorithms, and identify additional challenges and future research directions,\nincluding reward function design and model scalability.", "time": "2021-10-29T07:51:26Z", "link": "http://arxiv.org/abs/2111.00008v1", "id": "2111.00008v1", "title": "Reinforced Workload Distribution Fairness"}
{"author": "Timon RÃ¼ckel, Johannes Sedlmeir, Peter Hofmann", "abstract": "Federated machine learning (FL) allows to collectively train models on\nsensitive data as only the clients' models and not their training data need to\nbe shared. However, despite the attention that research on FL has drawn, the\nconcept still lacks broad adoption in practice. One of the key reasons is the\ngreat challenge to implement FL systems that simultaneously achieve fairness,\nintegrity, and privacy preservation for all participating clients. To\ncontribute to solving this issue, our paper suggests a FL system that\nincorporates blockchain technology, local differential privacy, and\nzero-knowledge proofs. Our implementation of a proof-of-concept with multiple\nlinear regression illustrates that these state-of-the-art technologies can be\ncombined to a FL system that aligns economic incentives, trust, and\nconfidentiality requirements in a scalable and transparent system.", "time": "2021-11-11T16:08:44Z", "link": "http://arxiv.org/abs/2111.06290v1", "id": "2111.06290v1", "title": "Fairness, Integrity, and Privacy in a Scalable Blockchain-based\n  Federated Learning System"}
{"author": "Agostino Dovier, Andrea Formisano, Gopal Gupta, Manuel V. Hermenegildo, Enrico Pontelli, Ricardo Rocha", "abstract": "Multi-core and highly-connected architectures have become ubiquitous, and\nthis has brought renewed interest in language-based approaches to the\nexploitation of parallelism. Since its inception, logic programming has been\nrecognized as a programming paradigm with great potential for automated\nexploitation of parallelism. The comprehensive survey of the first twenty years\nof research in parallel logic programming, published in 2001, has served since\nas a fundamental reference to researchers and developers. The contents are\nquite valid today, but at the same time the field has continued evolving at a\nfast pace in the years that have followed. Many of these achievements and\nongoing research have been driven by the rapid pace of technological\ninnovation, that has led to advances such as very large clusters, the wide\ndiffusion of multi-core processors, the game-changing role of general-purpose\ngraphic processing units, and the ubiquitous adoption of cloud computing. This\nhas been paralleled by significant advances within logic programming, such as\ntabling, more powerful static analysis and verification, the rapid growth of\nAnswer Set Programming, and in general, more mature implementations and\nsystems. This survey provides a review of the research in parallel logic\nprogramming covering the period since 2001, thus providing a natural\ncontinuation of the previous survey. The goal of the survey is to serve not\nonly as a reference for researchers and developers of logic programming\nsystems, but also as engaging reading for anyone interested in logic and as a\nuseful source for researchers in parallel systems outside logic programming.\n  Under consideration in Theory and Practice of Logic Programming (TPLP).", "time": "2022-01-24T17:00:33Z", "link": "http://arxiv.org/abs/2111.11218v2", "id": "2111.11218v2", "title": "Parallel Logic Programming: A Sequel"}
{"author": "Yongjeong Oh, Namyoon Lee, Yo-Seb Jeon, H. Vincent Poor", "abstract": "In this paper, we present a communication-efficient federated learning\nframework inspired by quantized compressed sensing. The presented framework\nconsists of gradient compression for wireless devices and gradient\nreconstruction for a parameter server (PS). Our strategy for gradient\ncompression is to sequentially perform block sparsification, dimensional\nreduction, and quantization. Thanks to gradient sparsification and\nquantization, our strategy can achieve a higher compression ratio than one-bit\ngradient compression. For accurate aggregation of the local gradients from the\ncompressed signals at the PS, we put forth an approximate minimum mean square\nerror (MMSE) approach for gradient reconstruction using the\nexpectation-maximization generalized-approximate-message-passing (EM-GAMP)\nalgorithm. Assuming Bernoulli Gaussian-mixture prior, this algorithm\niteratively updates the posterior mean and variance of local gradients from the\ncompressed signals. We also present a low-complexity approach for the gradient\nreconstruction. In this approach, we use the Bussgang theorem to aggregate\nlocal gradients from the compressed signals, then compute an approximate MMSE\nestimate of the aggregated gradient using the EM-GAMP algorithm. We also\nprovide a convergence rate analysis of the presented framework. Using the MNIST\ndataset, we demonstrate that the presented framework achieves almost identical\nperformance with the case that performs no compression, while significantly\nreducing communication overhead for federated learning.", "time": "2021-11-30T02:13:54Z", "link": "http://arxiv.org/abs/2111.15071v1", "id": "2111.15071v1", "title": "Communication-Efficient Federated Learning via Quantized Compressed\n  Sensing"}
{"author": "Jingrong Wang, Ben Liang", "abstract": "We consider distributed online min-max resource allocation with a set of\nparallel agents and a parameter server. Our goal is to minimize the pointwise\nmaximum over a set of time-varying and decreasing cost functions, without a\npriori information about these functions. We propose a novel online algorithm,\ntermed Distributed Online resource Re-Allocation (DORA), where non-stragglers\nlearn to relinquish resource and share resource with stragglers. A notable\nfeature of DORA is that it does not require gradient calculation or projection\noperation, unlike most existing online optimization strategies. This allows it\nto substantially reduce the computation overhead in large-scale and distributed\nnetworks. We analyze the worst-case performance of DORA and derive an upper\nbound on its dynamic regret for non-convex functions. We further consider an\napplication to the bandwidth allocation problem in distributed online machine\nlearning. Our numerical study demonstrates the efficacy of the proposed\nsolution and its performance advantage over gradient- and/or projection-based\nresource allocation algorithms in reducing wall-clock time.", "time": "2023-07-20T03:40:36Z", "link": "http://arxiv.org/abs/2112.03896v3", "id": "2112.03896v3", "title": "Gradient and Projection Free Distributed Online Min-Max Resource\n  Optimization"}
{"author": "Raphael Y. Cohen, Vesela P. Kovacheva", "abstract": "Healthcare AI holds the potential to increase patient safety, augment\nefficiency and improve patient outcomes, yet research is often limited by data\naccess, cohort curation, and tooling for analysis. Collection and translation\nof electronic health record data, live data, and real-time high resolution\ndevice data can be challenging and time-consuming. The development of\nreal-world AI tools requires overcoming challenges in data acquisition, scarce\nhospital resources and high needs for data governance. These bottlenecks may\nresult in resource-heavy needs and long delays in research and development of\nAI systems. We present a system and methodology to accelerate data acquisition,\ndataset development and analysis, and AI model development. We created an\ninteractive platform that relies on a scalable microservice backend. This\nsystem can ingest 15,000 patient records per hour, where each record represents\nthousands of multimodal measurements, text notes, and high resolution data.\nCollectively, these records can approach a terabyte of data. The system can\nfurther perform cohort generation and preliminary dataset analysis in 2-5\nminutes. As a result, multiple users can collaborate simultaneously to iterate\non datasets and models in real time. We anticipate that this approach will\ndrive real-world AI model development, and, in the long run, meaningfully\nimprove healthcare delivery.", "time": "2021-12-13T18:39:10Z", "link": "http://arxiv.org/abs/2112.06883v1", "id": "2112.06883v1", "title": "A Methodology for a Scalable, Collaborative, and Resource-Efficient\n  Platform to Facilitate Healthcare AI Research"}
{"author": "Zhengye Yang, Mingfei Sun, Hongzhe Ye, Zihao Xiong, Gil Zussman, Zoran Kostic", "abstract": "Social distancing can reduce the infection rates in respiratory pandemics\nsuch as COVID-19. Traffic intersections are particularly suitable for\nmonitoring and evaluation of social distancing behavior in metropolises. We\npropose and evaluate a privacy-preserving social distancing analysis system\n(B-SDA), which uses bird's-eye view video recordings of pedestrians who cross\ntraffic intersections. We devise algorithms for video pre-processing, object\ndetection and tracking which are rooted in the known computer-vision and deep\nlearning techniques, but modified to address the problem of detecting very\nsmall objects/pedestrians captured by a highly elevated camera. We propose a\nmethod for incorporating pedestrian grouping for detection of social distancing\nviolations. B-SDA is used to compare pedestrian behavior based on pre-pandemic\nand pandemic videos in a major metropolitan area. The accomplished pedestrian\ndetection performance is $63.0\\%$ $AP_{50}$ and the tracking performance is\n$47.6\\%$ MOTA. The social distancing violation rate of $15.6\\%$ during the\npandemic is notably lower than $31.4\\%$ pre-pandemic baseline, indicating that\npedestrians followed CDC-prescribed social distancing recommendations. The\nproposed system is suitable for deployment in real-world applications.", "time": "2022-02-09T22:21:20Z", "link": "http://arxiv.org/abs/2112.07159v2", "id": "2112.07159v2", "title": "Birds Eye View Social Distancing Analysis System"}
{"author": "Shreshth Tuli, Giuliano Casale, Nicholas R. Jennings", "abstract": "Workflow scheduling is a long-studied problem in parallel and distributed\ncomputing (PDC), aiming to efficiently utilize compute resources to meet user's\nservice requirements. Recently proposed scheduling methods leverage the low\nresponse times of edge computing platforms to optimize application Quality of\nService (QoS). However, scheduling workflow applications in mobile edge-cloud\nsystems is challenging due to computational heterogeneity, changing latencies\nof mobile devices and the volatile nature of workload resource requirements. To\novercome these difficulties, it is essential, but at the same time challenging,\nto develop a long-sighted optimization scheme that efficiently models the QoS\nobjectives. In this work, we propose MCDS: Monte Carlo Learning using Deep\nSurrogate Models to efficiently schedule workflow applications in mobile\nedge-cloud computing systems. MCDS is an Artificial Intelligence (AI) based\nscheduling approach that uses a tree-based search strategy and a deep neural\nnetwork-based surrogate model to estimate the long-term QoS impact of immediate\nactions for robust optimization of scheduling decisions. Experiments on\nphysical and simulated edge-cloud testbeds show that MCDS can improve over the\nstate-of-the-art methods in terms of energy consumption, response time, SLA\nviolations and cost by at least 6.13, 4.56, 45.09 and 30.71 percent\nrespectively.", "time": "2021-12-14T10:00:01Z", "link": "http://arxiv.org/abs/2112.07269v1", "id": "2112.07269v1", "title": "MCDS: AI Augmented Workflow Scheduling in Mobile Edge Cloud Computing\n  Systems"}
{"author": "Pengzhou Chen, Tao Chen, Miqing Li", "abstract": "Software configuration tuning is essential for optimizing a given performance\nobjective (e.g., minimizing latency). Yet, due to the software's intrinsically\ncomplex configuration landscape and expensive measurement, there has been a\nrather mild success, particularly in preventing the search from being trapped\nin local optima. To address this issue, in this paper we take a different\nperspective. Instead of focusing on improving the optimizer, we work on the\nlevel of optimization model and propose a meta multi-objectivization (MMO)\nmodel that considers an auxiliary performance objective (e.g., throughput in\naddition to latency). What makes this model distinct is that we do not optimize\nthe auxiliary performance objective, but rather use it to make\nsimilarly-performing while different configurations less comparable (i.e.\nPareto nondominated to each other), thus preventing the search from being\ntrapped in local optima. Importantly, by designing a new normalization method,\nwe show how to effectively use the MMO model without worrying about its weight\n-- the only yet highly sensitive parameter that can affect its effectiveness.\nExperiments on 22 cases from 11 real-world software systems/environments\nconfirm that our MMO model with the new normalization performs better than its\nstate-of-the-art single-objective counterparts on 82% cases while achieving up\nto 2.09x speedup. For 68% of the cases, the new normalization also enables the\nMMO model to outperform the instance when using it with the normalization from\nour prior FSE work under pre-tuned best weights, saving a great amount of\nresources which would be otherwise necessary to find a good weight. We also\ndemonstrate that the MMO model with the new normalization can consolidate\nrecent model-based tuning tools on 68% of the cases with up to 1.22x speedup in\ngeneral.", "time": "2024-03-15T14:09:15Z", "link": "http://arxiv.org/abs/2112.07303v3", "id": "2112.07303v3", "title": "MMO: Meta Multi-Objectivization for Software Configuration Tuning"}
{"author": "Davood G. Samani, Mohsen Amini Salehi", "abstract": "Deep Learning-based (DL) applications are becoming increasingly popular and\nadvancing at an unprecedented pace. While many research works are being\nundertaken to enhance Deep Neural Networks (DNN) -- the centerpiece of DL\napplications -- practical deployment challenges of these applications in the\nCloud and Edge systems, and their impact on the usability of the applications\nhave not been sufficiently investigated. In particular, the impact of deploying\ndifferent virtualization platforms, offered by the Cloud and Edge, on the\nusability of DL applications (in terms of the End-to-End (E2E) inference time)\nhas remained an open question. Importantly, resource elasticity (by means of\nscale-up), CPU pinning, and processor type (CPU vs GPU) configurations have\nshown to be influential on the virtualization overhead. Accordingly, the goal\nof this research is to study the impact of these potentially decisive\ndeployment options on the E2E performance, thus, usability of the DL\napplications. To that end, we measure the impact of four popular execution\nplatforms (namely, bare-metal, virtual machine (VM), container, and container\nin VM) on the E2E inference time of four types of DL applications, upon\nchanging processor configuration (scale-up, CPU pinning) and processor types.\nThis study reveals a set of interesting and sometimes counter-intuitive\nfindings that can be used as best practices by Cloud solution architects to\nefficiently deploy DL applications in various systems. The notable finding is\nthat the solution architects must be aware of the DL application\ncharacteristics, particularly, their pre- and post-processing requirements, to\nbe able to optimally choose and configure an execution platform, determine the\nuse of GPU, and decide the efficient scale-up range.", "time": "2021-12-17T21:51:34Z", "link": "http://arxiv.org/abs/2112.09780v1", "id": "2112.09780v1", "title": "Exploring the Impact of Virtualization on the Usability of the Deep\n  Learning Applications"}
{"author": "Peng Huang, Liekang Zeng, Xu Chen, Ke Luo, Zhi Zhou, Shuai Yu", "abstract": "With the wide penetration of smart robots in multifarious fields,\nSimultaneous Localization and Mapping (SLAM) technique in robotics has\nattracted growing attention in the community. Yet collaborating SLAM over\nmultiple robots still remains challenging due to performance contradiction\nbetween the intensive graphics computation of SLAM and the limited computing\ncapability of robots. While traditional solutions resort to the powerful cloud\nservers acting as an external computation provider, we show by real-world\nmeasurements that the significant communication overhead in data offloading\nprevents its practicability to real deployment. To tackle these challenges,\nthis paper promotes the emerging edge computing paradigm into multi-robot SLAM\nand proposes RecSLAM, a multi-robot laser SLAM system that focuses on\naccelerating map construction process under the robot-edge-cloud architecture.\nIn contrast to conventional multi-robot SLAM that generates graphic maps on\nrobots and completely merges them on the cloud, RecSLAM develops a hierarchical\nmap fusion technique that directs robots' raw data to edge servers for\nreal-time fusion and then sends to the cloud for global merging. To optimize\nthe overall pipeline, an efficient multi-robot SLAM collaborative processing\nframework is introduced to adaptively optimize robot-to-edge offloading\ntailored to heterogeneous edge resource conditions, meanwhile ensuring the\nworkload balancing among the edge servers. Extensive evaluations show RecSLAM\ncan achieve up to 39% processing latency reduction over the state-of-the-art.\nBesides, a proof-of-concept prototype is developed and deployed in real scenes\nto demonstrate its effectiveness.", "time": "2022-01-24T07:19:00Z", "link": "http://arxiv.org/abs/2112.13222v2", "id": "2112.13222v2", "title": "Edge Robotics: Edge-Computing-Accelerated Multi-Robot Simultaneous\n  Localization and Mapping"}
{"author": "Junyu Shi, Wei Wan, Shengshan Hu, Jianrong Lu, Leo Yu Zhang", "abstract": "Recently emerged federated learning (FL) is an attractive distributed\nlearning framework in which numerous wireless end-user devices can train a\nglobal model with the data remained autochthonous. Compared with the\ntraditional machine learning framework that collects user data for centralized\nstorage, which brings huge communication burden and concerns about data\nprivacy, this approach can not only save the network bandwidth but also protect\nthe data privacy. Despite the promising prospect, byzantine attack, an\nintractable threat in conventional distributed network, is discovered to be\nrather efficacious against FL as well. In this paper, we conduct a\ncomprehensive investigation of the state-of-the-art strategies for defending\nagainst byzantine attacks in FL. We first provide a taxonomy for the existing\ndefense solutions according to the techniques they used, followed by an\nacross-the-board comparison and discussion. Then we propose a new byzantine\nattack method called weight attack to defeat those defense schemes, and conduct\nexperiments to demonstrate its threat. The results show that existing defense\nsolutions, although abundant, are still far from fully protecting FL. Finally,\nwe indicate possible countermeasures for weight attack, and highlight several\nchallenges and future research directions for mitigating byzantine attacks in\nFL.", "time": "2022-10-07T01:35:01Z", "link": "http://arxiv.org/abs/2112.14468v2", "id": "2112.14468v2", "title": "Challenges and Approaches for Mitigating Byzantine Attacks in Federated\n  Learning"}
{"author": "Zhiyong Sun, Anders Rantzer, Zhongkui Li, Anders Robertsson", "abstract": "In this paper we consider distributed adaptive stabilization for uncertain\nmultivariable linear systems with a time-varying diagonal matrix gain. We show\nthat uncertain multivariable linear systems are stabilizable by diagonal matrix\nhigh gains if the system matrix is an H-matrix with positive diagonal entries.\nBased on matrix measure and stability theory for diagonally dominant systems,\nwe consider two classes of uncertain linear systems, and derive a threshold\ncondition to ensure their exponential stability by a monotonically increasing\ndiagonal gain matrix. When each individual gain function in the matrix gain is\nupdated by state-dependent functions using only local state information, the\nboundedness and convergence of both system states and adaptive matrix gains are\nguaranteed. We apply the adaptive distributed stabilization approach to\nadaptive synchronization control for large-scale complex networks consisting of\nnonlinear node dynamics and time-varying coupling weights. A unified framework\nfor adaptive synchronization is proposed that includes several general design\napproaches for adaptive coupling weights to guarantee network synchronization.", "time": "2021-05-28T17:28:29Z", "link": "http://arxiv.org/abs/2105.14004v1", "id": "2105.14004v1", "title": "Distributed adaptive stabilization"}
{"author": "Qifan Deng, Rajkumar Buyya", "abstract": "Edge/Fog computing is a novel computing paradigm that provides\nresource-limited Internet of Things (IoT) devices with scalable computing and\nstorage resources. Compared to cloud computing, edge/fog servers have fewer\nresources, but they can be accessed with higher bandwidth and less\ncommunication latency. Thus, integrating edge/fog and cloud infrastructures can\nsupport the execution of diverse latency-sensitive and computation-intensive\nIoT applications. Although some frameworks attempt to provide such integration,\nthere are still several challenges to be addressed, such as dynamic scheduling\nof different IoT applications, scalability mechanisms, multi-platform support,\nand supporting different interaction models. To overcome these challenges, we\npropose a lightweight and distributed container-based framework, called\nFogBus2. It provides a mechanism for scheduling heterogeneous IoT applications\nand implements several scheduling policies. Also, it proposes an optimized\ngenetic algorithm to obtain fast convergence to well-suited solutions. Besides,\nit offers a scalability mechanism to ensure efficient responsiveness when\neither the number of IoT devices increases or the resources become\noverburdened. Also, the dynamic resource discovery mechanism of FogBus2 assists\nnew entities to quickly join the system. We have also developed two IoT\napplications, called Conway's Game of Life and Video Optical Character\nRecognition to demonstrate the effectiveness of FogBus2 for handling real-time\nand non-real-time IoT applications. Experimental results show FogBus2's\nscheduling policy improves the response time of IoT applications by 53\\%\ncompared to other policies. Also, the scalability mechanism can reduce up to\n48\\% of the queuing waiting time compared to frameworks that do not support\nscalability.", "time": "2021-08-08T03:40:44Z", "link": "http://arxiv.org/abs/2108.03562v1", "id": "2108.03562v1", "title": "Master Graduation Thesis: A Lightweight and Distributed Container-based\n  Framework"}
{"author": "Su Wang, Mengyuan Lee, Seyyedali Hosseinalipour, Roberto Morabito, Mung Chiang, Christopher G. Brinton", "abstract": "The conventional federated learning (FedL) architecture distributes machine\nlearning (ML) across worker devices by having them train local models that are\nperiodically aggregated by a server. FedL ignores two important characteristics\nof contemporary wireless networks, however: (i) the network may contain\nheterogeneous communication/computation resources, while (ii) there may be\nsignificant overlaps in devices' local data distributions. In this work, we\ndevelop a novel optimization methodology that jointly accounts for these\nfactors via intelligent device sampling complemented by device-to-device (D2D)\noffloading. Our optimization aims to select the best combination of sampled\nnodes and data offloading configuration to maximize FedL training accuracy\nsubject to realistic constraints on the network topology and device\ncapabilities. Theoretical analysis of the D2D offloading subproblem leads to\nnew FedL convergence bounds and an efficient sequential convex optimizer. Using\nthis result, we develop a sampling methodology based on graph convolutional\nnetworks (GCNs) which learns the relationship between network attributes,\nsampled nodes, and resulting offloading that maximizes FedL accuracy. Through\nevaluation on real-world datasets and network measurements from our IoT\ntestbed, we find that our methodology while sampling less than 5% of all\ndevices outperforms conventional FedL substantially both in terms of trained\nmodel accuracy and required resource utilization.", "time": "2021-01-04T05:59:50Z", "link": "http://arxiv.org/abs/2101.00787v1", "id": "2101.00787v1", "title": "Device Sampling for Heterogeneous Federated Learning: Theory,\n  Algorithms, and Implementation"}
{"author": "Christodoulos Pappas, Dimitris Chatzopoulos, Spyros Lalis, Manolis Vavalis", "abstract": "The proliferation of resourceful mobile devices that store rich,\nmultidimensional and privacy-sensitive user data motivate the design of\nfederated learning (FL), a machine-learning (ML) paradigm that enables mobile\ndevices to produce an ML model without sharing their data. However, the\nmajority of the existing FL frameworks rely on centralized entities. In this\nwork, we introduce IPLS, a fully decentralized federated learning framework\nthat is partially based on the interplanetary file system (IPFS). By using IPLS\nand connecting into the corresponding private IPFS network, any party can\ninitiate the training process of an ML model or join an ongoing training\nprocess that has already been started by another party. IPLS scales with the\nnumber of participants, is robust against intermittent connectivity and dynamic\nparticipant departures/arrivals, requires minimal resources, and guarantees\nthat the accuracy of the trained model quickly converges to that of a\ncentralized FL framework with an accuracy drop of less than one per thousand.", "time": "2021-01-06T07:44:51Z", "link": "http://arxiv.org/abs/2101.01901v1", "id": "2101.01901v1", "title": "IPLS : A Framework for Decentralized Federated Learning"}
{"author": "Sin Kit Lo, Qinghua Lu, Liming Zhu, Hye-young Paik, Xiwei Xu, Chen Wang", "abstract": "Federated learning has received fast-growing interests from academia and\nindustry to tackle the challenges of data hungriness and privacy in machine\nlearning. A federated learning system can be viewed as a large-scale\ndistributed system with different components and stakeholders as numerous\nclient devices participate in federated learning. Designing a federated\nlearning system requires software system design thinking apart from machine\nlearning knowledge. Although much effort has been put into federated learning\nfrom the machine learning technique aspects, the software architecture design\nconcerns in building federated learning systems have been largely ignored.\nTherefore, in this paper, we present a collection of architectural patterns to\ndeal with the design challenges of federated learning systems. Architectural\npatterns present reusable solutions to a commonly occurring problem within a\ngiven context during software architecture design. The presented patterns are\nbased on the results of a systematic literature review and include three client\nmanagement patterns, four model management patterns, three model training\npatterns, and four model aggregation patterns. The patterns are associated to\nthe particular state transitions in a federated learning model lifecycle,\nserving as a guidance for effective use of the patterns in the design of\nfederated learning systems.", "time": "2021-06-18T05:09:15Z", "link": "http://arxiv.org/abs/2101.02373v3", "id": "2101.02373v3", "title": "Architectural Patterns for the Design of Federated Learning Systems"}
{"author": "Malte S. Kurz", "abstract": "This paper explores serverless cloud computing for double machine learning.\nBeing based on repeated cross-fitting, double machine learning is particularly\nwell suited to exploit the high level of parallelism achievable with serverless\ncomputing. It allows to get fast on-demand estimations without additional cloud\nmaintenance effort. We provide a prototype Python implementation\n\\texttt{DoubleML-Serverless} for the estimation of double machine learning\nmodels with the serverless computing platform AWS Lambda and demonstrate its\nutility with a case study analyzing estimation times and costs.", "time": "2021-02-24T12:13:03Z", "link": "http://arxiv.org/abs/2101.04025v2", "id": "2101.04025v2", "title": "Distributed Double Machine Learning with a Serverless Architecture"}
{"author": "Jasmin Bogatinovski, Sasho Nedelkoski", "abstract": "The multi-source data generated by distributed systems, provide a holistic\ndescription of the system. Harnessing the joint distribution of the different\nmodalities by a learning model can be beneficial for critical applications for\nmaintenance of the distributed systems. One such important task is the task of\nanomaly detection where we are interested in detecting the deviation of the\ncurrent behaviour of the system from the theoretically expected. In this work,\nwe utilize the joint representation from the distributed traces and system log\ndata for the task of anomaly detection in distributed systems. We demonstrate\nthat the joint utilization of traces and logs produced better results compared\nto the single modality anomaly detection methods. Furthermore, we formalize a\nlearning task - next template prediction NTP, that is used as a generalization\nfor anomaly detection for both logs and distributed trace. Finally, we\ndemonstrate that this formalization allows for the learning of template\nembedding for both the traces and logs. The joint embeddings can be reused in\nother applications as good initialization for spans and logs.", "time": "2021-01-13T10:11:32Z", "link": "http://arxiv.org/abs/2101.04977v1", "id": "2101.04977v1", "title": "Multi-Source Anomaly Detection in Distributed IT Systems"}
{"author": "Congliang Chen, Li Shen, Fangyu Zou, Wei Liu", "abstract": "Adam is one of the most influential adaptive stochastic algorithms for\ntraining deep neural networks, which has been pointed out to be divergent even\nin the simple convex setting via a few simple counterexamples. Many attempts,\nsuch as decreasing an adaptive learning rate, adopting a big batch size,\nincorporating a temporal decorrelation technique, seeking an analogous\nsurrogate, \\textit{etc.}, have been tried to promote Adam-type algorithms to\nconverge. In contrast with existing approaches, we introduce an alternative\neasy-to-check sufficient condition, which merely depends on the parameters of\nthe base learning rate and combinations of historical second-order moments, to\nguarantee the global convergence of generic Adam for solving large-scale\nnon-convex stochastic optimization. This observation, coupled with this\nsufficient condition, gives much deeper interpretations on the divergence of\nAdam. On the other hand, in practice, mini-Adam and distributed-Adam are widely\nused without any theoretical guarantee. We further give an analysis on how the\nbatch size or the number of nodes in the distributed system affects the\nconvergence of Adam, which theoretically shows that mini-batch and distributed\nAdam can be linearly accelerated by using a larger mini-batch size or a larger\nnumber of nodes.At last, we apply the generic Adam and mini-batch Adam with the\nsufficient condition for solving the counterexample and training several neural\nnetworks on various real-world datasets. Experimental results are exactly in\naccord with our theoretical analysis.", "time": "2022-08-08T07:25:27Z", "link": "http://arxiv.org/abs/2101.05471v2", "id": "2101.05471v2", "title": "Towards Practical Adam: Non-Convexity, Convergence Theory, and\n  Mini-Batch Acceleration"}
{"author": "Beibei Zhang, Tian Xiang, Hongxuan Zhang, Te Li, Shiqiang Zhu, Jianjun Gu", "abstract": "Deep neural networks (DNNs) sustain high performance in today's data\nprocessing applications. DNN inference is resource-intensive thus is difficult\nto fit into a mobile device. An alternative is to offload the DNN inference to\na cloud server. However, such an approach requires heavy raw data transmission\nbetween the mobile device and the cloud server, which is not suitable for\nmission-critical and privacy-sensitive applications such as autopilot. To solve\nthis problem, recent advances unleash DNN services using the edge computing\nparadigm. The existing approaches split a DNN into two parts and deploy the two\npartitions to computation nodes at two edge computing tiers. Nonetheless, these\nmethods overlook collaborative device-edge-cloud computation resources.\nBesides, previous algorithms demand the whole DNN re-partitioning to adapt to\ncomputation resource changes and network dynamics. Moreover, for\nresource-demanding convolutional layers, prior works do not give a parallel\nprocessing strategy without loss of accuracy at the edge side. To tackle these\nissues, we propose D3, a dynamic DNN decomposition system for synergistic\ninference without precision loss. The proposed system introduces a heuristic\nalgorithm named horizontal partition algorithm to split a DNN into three parts.\nThe algorithm can partially adjust the partitions at run time according to\nprocessing time and network conditions. At the edge side, a vertical separation\nmodule separates feature maps into tiles that can be independently run on\ndifferent edge nodes in parallel. Extensive quantitative evaluation of five\npopular DNNs illustrates that D3 outperforms the state-of-the-art counterparts\nup to 3.4 times in end-to-end DNN inference time and reduces backbone network\ncommunication overhead up to 3.68 times.", "time": "2021-01-15T03:18:53Z", "link": "http://arxiv.org/abs/2101.05952v1", "id": "2101.05952v1", "title": "Dynamic DNN Decomposition for Lossless Synergistic Inference"}
{"author": "Rahul Arun Paropkari, Anurag Thantharate, Cory Beard", "abstract": "5G cellular networks are being deployed all over the world and this\narchitecture supports ultra-dense network (UDN) deployment. Small cells have a\nvery important role in providing 5G connectivity to the end users. Exponential\nincreases in devices, data and network demands make it mandatory for the\nservice providers to manage handovers better, to cater to the services that a\nuser desire. In contrast to any traditional handover improvement scheme, we\ndevelop a 'Deep-Mobility' model by implementing a deep learning neural network\n(DLNN) to manage network mobility, utilizing in-network deep learning and\nprediction. We use network key performance indicators (KPIs) to train our model\nto analyze network traffic and handover requirements. In this method, RF signal\nconditions are continuously observed and tracked using deep learning neural\nnetworks such as the Recurrent neural network (RNN) or Long Short-Term Memory\nnetwork (LSTM) and system level inputs are also considered in conjunction, to\ntake a collective decision for a handover. We can study multiple parameters and\ninteractions between system events along with the user mobility, which would\nthen trigger a handoff in any given scenario. Here, we show the fundamental\nmodeling approach and demonstrate usefulness of our model while investigating\nimpacts and sensitivities of certain KPIs from the user equipment (UE) and\nnetwork side.", "time": "2021-01-19T01:19:11Z", "link": "http://arxiv.org/abs/2101.06558v2", "id": "2101.06558v2", "title": "Deep-Mobility: A Deep Learning Approach for an Efficient and Reliable 5G\n  Handover"}
{"author": "Arjun Balasubramanian, Adarsh Kumar, Yuhan Liu, Han Cao, Shivaram Venkataraman, Aditya Akella", "abstract": "Deep Neural Networks (DNNs) are witnessing increased adoption in multiple\ndomains owing to their high accuracy in solving real-world problems. However,\nthis high accuracy has been achieved by building deeper networks, posing a\nfundamental challenge to the low latency inference desired by user-facing\napplications. Current low latency solutions trade-off on accuracy or fail to\nexploit the inherent temporal locality in prediction serving workloads.\n  We observe that caching hidden layer outputs of the DNN can introduce a form\nof late-binding where inference requests only consume the amount of computation\nneeded. This enables a mechanism for achieving low latencies, coupled with an\nability to exploit temporal locality. However, traditional caching approaches\nincur high memory overheads and lookup latencies, leading us to design learned\ncaches - caches that consist of simple ML models that are continuously updated.\nWe present the design of GATI, an end-to-end prediction serving system that\nincorporates learned caches for low-latency DNN inference. Results show that\nGATI can reduce inference latency by up to 7.69X on realistic workloads.", "time": "2021-01-18T22:13:08Z", "link": "http://arxiv.org/abs/2101.07344v1", "id": "2101.07344v1", "title": "Accelerating Deep Learning Inference via Learned Caches"}
{"author": "Khaled Zaouk, Fei Song, Chenghao Lyu, Yanlei Diao", "abstract": "Cloud data analytics has become an integral part of enterprise business\noperations for data-driven insight discovery. Performance modeling of cloud\ndata analytics is crucial for performance tuning and other critical operations\nin the cloud. Traditional modeling techniques fail to adapt to the high degree\nof diversity in workloads and system behaviors in this domain. In this paper,\nwe bring recent Deep Learning techniques to bear on the process of automated\nperformance modeling of cloud data analytics, with a focus on Spark data\nanalytics as representative workloads. At the core of our work is the notion of\nlearning workload embeddings (with a set of desired properties) to represent\nfundamental computational characteristics of different jobs, which enable\nperformance prediction when used together with job configurations that control\nresource allocation and other system knobs. Our work provides an in-depth study\nof different modeling choices that suit our requirements. Results of extensive\nexperiments reveal the strengths and limitations of different modeling methods,\nas well as superior performance of our best performing method over a\nstate-of-the-art modeling tool for cloud analytics.", "time": "2021-01-20T14:58:55Z", "link": "http://arxiv.org/abs/2101.08167v1", "id": "2101.08167v1", "title": "Neural-based Modeling for Performance Tuning of Spark Data Analytics"}
{"author": "Do Le Quoc, Franz Gregor, Sergei Arnautov, Roland Kunkel, Pramod Bhatotia, Christof Fetzer", "abstract": "Data-driven intelligent applications in modern online services have become\nubiquitous. These applications are usually hosted in the untrusted cloud\ncomputing infrastructure. This poses significant security risks since these\napplications rely on applying machine learning algorithms on large datasets\nwhich may contain private and sensitive information.\n  To tackle this challenge, we designed secureTF, a distributed secure machine\nlearning framework based on Tensorflow for the untrusted cloud infrastructure.\nsecureTF is a generic platform to support unmodified TensorFlow applications,\nwhile providing end-to-end security for the input data, ML model, and\napplication code. secureTF is built from ground-up based on the security\nproperties provided by Trusted Execution Environments (TEEs). However, it\nextends the trust of a volatile memory region (or secure enclave) provided by\nthe single node TEE to secure a distributed infrastructure required for\nsupporting unmodified stateful machine learning applications running in the\ncloud.\n  The paper reports on our experiences about the system design choices and the\nsystem deployment in production use-cases. We conclude with the lessons learned\nbased on the limitations of our commercially available platform, and discuss\nopen research problems for the future work.", "time": "2021-01-20T16:36:53Z", "link": "http://arxiv.org/abs/2101.08204v1", "id": "2101.08204v1", "title": "secureTF: A Secure TensorFlow Framework"}
{"author": "Jason Mohoney, Roger Waleffe, Yiheng Xu, Theodoros Rekatsinas, Shivaram Venkataraman", "abstract": "We propose a new framework for computing the embeddings of large-scale graphs\non a single machine. A graph embedding is a fixed length vector representation\nfor each node (and/or edge-type) in a graph and has emerged as the de-facto\napproach to apply modern machine learning on graphs. We identify that current\nsystems for learning the embeddings of large-scale graphs are bottlenecked by\ndata movement, which results in poor resource utilization and inefficient\ntraining. These limitations require state-of-the-art systems to distribute\ntraining across multiple machines. We propose Marius, a system for efficient\ntraining of graph embeddings that leverages partition caching and buffer-aware\ndata orderings to minimize disk access and interleaves data movement with\ncomputation to maximize utilization. We compare Marius against two\nstate-of-the-art industrial systems on a diverse array of benchmarks. We\ndemonstrate that Marius achieves the same level of accuracy but is up to one\norder of magnitude faster. We also show that Marius can scale training to\ndatasets an order of magnitude beyond a single machine's GPU and CPU memory\ncapacity, enabling training of configurations with more than a billion edges\nand 550 GB of total parameters on a single machine with 16 GB of GPU memory and\n64 GB of CPU memory. Marius is open-sourced at www.marius-project.org.", "time": "2021-05-26T00:22:46Z", "link": "http://arxiv.org/abs/2101.08358v2", "id": "2101.08358v2", "title": "Marius: Learning Massive Graph Embeddings on a Single Machine"}
{"author": "Emre Ozfatura, Kerem Ozfatura, Deniz Gunduz", "abstract": "Federated learning (FL) enables multiple clients to collaboratively train a\nshared model without disclosing their local datasets. This is achieved by\nexchanging local model updates with the help of a parameter server (PS).\nHowever, due to the increasing size of the trained models, the communication\nload due to the iterative exchanges between the clients and the PS often\nbecomes a bottleneck in the performance. Sparse communication is often employed\nto reduce the communication load, where only a small subset of the model\nupdates are communicated from the clients to the PS. In this paper, we\nintroduce a novel time-correlated sparsification (TCS) scheme, which builds\nupon the notion that sparse communication framework can be considered as\nidentifying the most significant elements of the underlying model. Hence, TCS\nseeks a certain correlation between the sparse representations used at\nconsecutive iterations in FL, so that the overhead due to encoding and\ntransmission of the sparse representation can be significantly reduced without\ncompromising the test accuracy. Through extensive simulations on the CIFAR-10\ndataset, we show that TCS can achieve centralized training accuracy with 100\ntimes sparsification, and up to 2000 times reduction in the communication load\nwhen employed together with quantization.", "time": "2021-01-21T20:15:55Z", "link": "http://arxiv.org/abs/2101.08837v1", "id": "2101.08837v1", "title": "Time-Correlated Sparsification for Communication-Efficient Federated\n  Learning"}
{"author": "Aamir Shafi, Jahanzeb Maqbool Hashmi, Hari Subramoni, Dhabaleswar K. Panda", "abstract": "Dask is a popular parallel and distributed computing framework, which rivals\nApache Spark to enable task-based scalable processing of big data. The Dask\nDistributed library forms the basis of this computing engine and provides\nsupport for adding new communication devices. It currently has two\ncommunication devices: one for TCP and the other for high-speed networks using\nUCX-Py -- a Cython wrapper to UCX. This paper presents the design and\nimplementation of a new communication backend for Dask -- called MPI4Dask --\nthat is targeted for modern HPC clusters built with GPUs. MPI4Dask exploits\nmpi4py over MVAPICH2-GDR, which is a GPU-aware implementation of the Message\nPassing Interface (MPI) standard. MPI4Dask provides point-to-point asynchronous\nI/O communication coroutines, which are non-blocking concurrent operations\ndefined using the async/await keywords from the Python's asyncio framework. Our\nlatency and throughput comparisons suggest that MPI4Dask outperforms UCX by 6x\nfor 1 Byte message and 4x for large messages (2 MBytes and beyond)\nrespectively. We also conduct comparative performance evaluation of MPI4Dask\nwith UCX using two benchmark applications: 1) sum of cuPy array with its\ntranspose, and 2) cuDF merge. MPI4Dask speeds up the overall execution time of\nthe two applications by an average of 3.47x and 3.11x respectively on an\nin-house cluster built with NVIDIA Tesla V100 GPUs for 1-6 Dask workers. We\nalso perform scalability analysis of MPI4Dask against UCX for these\napplications on TACC's Frontera (GPU) system with upto 32 Dask workers on 32\nNVIDIA Quadro RTX 5000 GPUs and 256 CPU cores. MPI4Dask speeds up the execution\ntime for cuPy and cuDF applications by an average of 1.71x and 2.91x\nrespectively for 1-32 Dask workers on the Frontera (GPU) system.", "time": "2021-01-21T22:59:08Z", "link": "http://arxiv.org/abs/2101.08878v1", "id": "2101.08878v1", "title": "Efficient MPI-based Communication for GPU-Accelerated Dask Applications"}
{"author": "Ajesh Koyatan Chathoth, Abhyuday Jagannatha, Stephen Lee", "abstract": "Internet of Things (IoT) devices are becoming increasingly popular and are\ninfluencing many application domains such as healthcare and transportation.\nThese devices are used for real-world applications such as sensor monitoring,\nreal-time control. In this work, we look at differentially private (DP) neural\nnetwork (NN) based network intrusion detection systems (NIDS) to detect\nintrusion attacks on networks of such IoT devices. Existing NN training\nsolutions in this domain either ignore privacy considerations or assume that\nthe privacy requirements are homogeneous across all users. We show that the\nperformance of existing differentially private stochastic methods degrade for\nclients with non-identical data distributions when clients' privacy\nrequirements are heterogeneous. We define a cohort-based $(\\epsilon,\\delta)$-DP\nframework that models the more practical setting of IoT device cohorts with\nnon-identical clients and heterogeneous privacy requirements. We propose two\nnovel continual-learning based DP training methods that are designed to improve\nmodel performance in the aforementioned setting. To the best of our knowledge,\nours is the first system that employs a continual learning-based approach to\nhandle heterogeneity in client privacy requirements. We evaluate our approach\non real datasets and show that our techniques outperform the baselines. We also\nshow that our methods are robust to hyperparameter changes. Lastly, we show\nthat one of our proposed methods can easily adapt to post-hoc relaxations of\nclient privacy requirements.", "time": "2021-01-25T03:33:27Z", "link": "http://arxiv.org/abs/2101.09878v1", "id": "2101.09878v1", "title": "Federated Intrusion Detection for IoT with Heterogeneous Cohort Privacy"}
{"author": "Mahdi Soleymani, Ramy E. Ali, Hessam Mahdavifar, A. Salman Avestimehr", "abstract": "We consider the problem of coded computing, where a computational task is\nperformed in a distributed fashion in the presence of adversarial workers. We\npropose techniques to break the adversarial toleration threshold barrier\npreviously known in coded computing. More specifically, we leverage\nlist-decoding techniques for folded Reed-Solomon codes and propose novel\nalgorithms to recover the correct codeword using side information. In the coded\ncomputing setting, we show how the master node can perform certain carefully\ndesigned extra computations to obtain the side information. The workload of\ncomputing this side information is negligible compared to the computations done\nby each worker. This side information is then utilized to prune the output of\nthe list decoder and uniquely recover the true outcome. We further propose\nfolded Lagrange coded computing (FLCC) to incorporate the developed techniques\ninto a specific coded computing setting. Our results show that FLCC outperforms\nLCC by breaking the barrier on the number of adversaries that can be tolerated.\nIn particular, the corresponding threshold in FLCC is improved by a factor of\ntwo asymptotically compared to that of LCC.", "time": "2021-08-19T21:37:18Z", "link": "http://arxiv.org/abs/2101.11653v2", "id": "2101.11653v2", "title": "List-Decodable Coded Computing: Breaking the Adversarial Toleration\n  Barrier"}
{"author": "Mohammad Malekzadeh, Burak Hasircioglu, Nitish Mital, Kunal Katarya, Mehmet Emre Ozfatura, Deniz GÃ¼ndÃ¼z", "abstract": "While rich medical datasets are hosted in hospitals distributed across the\nworld, concerns on patients' privacy is a barrier against using such data to\ntrain deep neural networks (DNNs) for medical diagnostics. We propose Dopamine,\na system to train DNNs on distributed datasets, which employs federated\nlearning (FL) with differentially-private stochastic gradient descent (DPSGD),\nand, in combination with secure aggregation, can establish a better trade-off\nbetween differential privacy (DP) guarantee and DNN's accuracy than other\napproaches. Results on a diabetic retinopathy~(DR) task show that Dopamine\nprovides a DP guarantee close to the centralized training counterpart, while\nachieving a better classification accuracy than FL with parallel DP where DPSGD\nis applied without coordination. Code is available at\nhttps://github.com/ipc-lab/private-ml-for-health.", "time": "2021-01-29T16:40:17Z", "link": "http://arxiv.org/abs/2101.11693v2", "id": "2101.11693v2", "title": "Dopamine: Differentially Private Federated Learning on Medical Data"}
{"author": "Xinle Liang, Yang Liu, Jiahuan Luo, Yuanqin He, Tianjian Chen, Qiang Yang", "abstract": "Federated Learning (FL) provides both model performance and data privacy for\nmachine learning tasks where samples or features are distributed among\ndifferent parties. In the training process of FL, no party has a global view of\ndata distributions or model architectures of other parties. Thus the\nmanually-designed architectures may not be optimal. In the past, Neural\nArchitecture Search (NAS) has been applied to FL to address this critical\nissue. However, existing Federated NAS approaches require prohibitive\ncommunication and computation effort, as well as the availability of\nhigh-quality labels. In this work, we present Self-supervised Vertical\nFederated Neural Architecture Search (SS-VFNAS) for automating FL where\nparticipants hold feature-partitioned data, a common cross-silo scenario called\nVertical Federated Learning (VFL). In the proposed framework, each party first\nconducts NAS using self-supervised approach to find a local optimal\narchitecture with its own data. Then, parties collaboratively improve the local\noptimal architecture in a VFL framework with supervision. We demonstrate\nexperimentally that our approach has superior performance, communication\nefficiency and privacy compared to Federated NAS and is capable of generating\nhigh-performance and highly-transferable heterogeneous architectures even with\ninsufficient overlapping samples, providing automation for those parties\nwithout deep learning expertise.", "time": "2021-02-18T02:23:50Z", "link": "http://arxiv.org/abs/2101.11896v2", "id": "2101.11896v2", "title": "Self-supervised Cross-silo Federated Neural Architecture Search"}
{"author": "Agrin Hilmkil, Sebastian Callh, Matteo Barbieri, Leon RenÃ© SÃ¼tfeld, Edvin Listo Zec, Olof Mogren", "abstract": "Federated learning (FL) is a promising approach to distributed compute, as\nwell as distributed data, and provides a level of privacy and compliance to\nlegal frameworks. This makes FL attractive for both consumer and healthcare\napplications. While the area is actively being explored, few studies have\nexamined FL in the context of larger language models and there is a lack of\ncomprehensive reviews of robustness across tasks, architectures, numbers of\nclients, and other relevant factors. In this paper, we explore the fine-tuning\nof Transformer-based language models in a federated learning setting. We\nevaluate three popular BERT-variants of different sizes (BERT, ALBERT, and\nDistilBERT) on a number of text classification tasks such as sentiment analysis\nand author identification. We perform an extensive sweep over the number of\nclients, ranging up to 32, to evaluate the impact of distributed compute on\ntask performance in the federated averaging setting. While our findings suggest\nthat the large sizes of the evaluated models are not generally prohibitive to\nfederated training, we found that the different models handle federated\naveraging to a varying degree. Most notably, DistilBERT converges significantly\nslower with larger numbers of clients, and under some circumstances, even\ncollapses to chance level performance. Investigating this issue presents an\ninteresting perspective for future research.", "time": "2021-02-01T14:31:39Z", "link": "http://arxiv.org/abs/2102.00875v1", "id": "2102.00875v1", "title": "Scaling Federated Learning for Fine-tuning of Large Language Models"}
{"author": "Xiaoyu Cao, Jinyuan Jia, Neil Zhenqiang Gong", "abstract": "Federated learning enables clients to collaboratively learn a shared global\nmodel without sharing their local training data with a cloud server. However,\nmalicious clients can corrupt the global model to predict incorrect labels for\ntesting examples. Existing defenses against malicious clients leverage\nByzantine-robust federated learning methods. However, these methods cannot\nprovably guarantee that the predicted label for a testing example is not\naffected by malicious clients. We bridge this gap via ensemble federated\nlearning. In particular, given any base federated learning algorithm, we use\nthe algorithm to learn multiple global models, each of which is learnt using a\nrandomly selected subset of clients. When predicting the label of a testing\nexample, we take majority vote among the global models. We show that our\nensemble federated learning with any base federated learning algorithm is\nprovably secure against malicious clients. Specifically, the label predicted by\nour ensemble global model for a testing example is provably not affected by a\nbounded number of malicious clients. Moreover, we show that our derived bound\nis tight. We evaluate our method on MNIST and Human Activity Recognition\ndatasets. For instance, our method can achieve a certified accuracy of 88% on\nMNIST when 20 out of 1,000 clients are malicious.", "time": "2021-10-27T02:14:38Z", "link": "http://arxiv.org/abs/2102.01854v4", "id": "2102.01854v4", "title": "Provably Secure Federated Learning against Malicious Clients"}
{"author": "Felix Sattler, Tim Korjakow, Roman Rischke, Wojciech Samek", "abstract": "Federated Distillation (FD) is a popular novel algorithmic paradigm for\nFederated Learning, which achieves training performance competitive to prior\nparameter averaging based methods, while additionally allowing the clients to\ntrain different model architectures, by distilling the client predictions on an\nunlabeled auxiliary set of data into a student model. In this work we propose\nFedAUX, an extension to FD, which, under the same set of assumptions,\ndrastically improves performance by deriving maximum utility from the unlabeled\nauxiliary data. FedAUX modifies the FD training procedure in two ways: First,\nunsupervised pre-training on the auxiliary data is performed to find a model\ninitialization for the distributed training. Second, $(\\varepsilon,\n\\delta)$-differentially private certainty scoring is used to weight the\nensemble predictions on the auxiliary data according to the certainty of each\nclient model. Experiments on large-scale convolutional neural networks and\ntransformer models demonstrate, that the training performance of FedAUX exceeds\nSOTA FL baseline methods by a substantial margin in both the iid and non-iid\nregime, further closing the gap to centralized training performance. Code is\navailable at github.com/fedl-repo/fedaux.", "time": "2021-02-04T09:53:53Z", "link": "http://arxiv.org/abs/2102.02514v1", "id": "2102.02514v1", "title": "FedAUX: Leveraging Unlabeled Auxiliary Data in Federated Learning"}
{"author": "Divyansh Jhunjhunwala, Advait Gadhikar, Gauri Joshi, Yonina C. Eldar", "abstract": "Communication of model updates between client nodes and the central\naggregating server is a major bottleneck in federated learning, especially in\nbandwidth-limited settings and high-dimensional models. Gradient quantization\nis an effective way of reducing the number of bits required to communicate each\nmodel update, albeit at the cost of having a higher error floor due to the\nhigher variance of the stochastic gradients. In this work, we propose an\nadaptive quantization strategy called AdaQuantFL that aims to achieve\ncommunication efficiency as well as a low error floor by changing the number of\nquantization levels during the course of training. Experiments on training deep\nneural networks show that our method can converge in much fewer communicated\nbits as compared to fixed quantization level setups, with little or no impact\non training and test accuracy.", "time": "2021-02-08T19:14:21Z", "link": "http://arxiv.org/abs/2102.04487v1", "id": "2102.04487v1", "title": "Adaptive Quantization of Model Updates for Communication-Efficient\n  Federated Learning"}
{"author": "Zhuoning Yuan, Zhishuai Guo, Yi Xu, Yiming Ying, Tianbao Yang", "abstract": "Deep AUC (area under the ROC curve) Maximization (DAM) has attracted much\nattention recently due to its great potential for imbalanced data\nclassification. However, the research on Federated Deep AUC Maximization (FDAM)\nis still limited. Compared with standard federated learning (FL) approaches\nthat focus on decomposable minimization objectives, FDAM is more complicated\ndue to its minimization objective is non-decomposable over individual examples.\nIn this paper, we propose improved FDAM algorithms for heterogeneous data by\nsolving the popular non-convex strongly-concave min-max formulation of DAM in a\ndistributed fashion, which can also be applied to a class of non-convex\nstrongly-concave min-max problems. A striking result of this paper is that the\ncommunication complexity of the proposed algorithm is a constant independent of\nthe number of machines and also independent of the accuracy level, which\nimproves an existing result by orders of magnitude. The experiments have\ndemonstrated the effectiveness of our FDAM algorithm on benchmark datasets, and\non medical chest X-ray images from different organizations. Our experiment\nshows that the performance of FDAM using data from multiple hospitals can\nimprove the AUC score on testing data from a single hospital for detecting\nlife-threatening diseases based on chest radiographs. The proposed method is\nimplemented in our open-sourced library LibAUC (www.libauc.org) whose github\naddress is https://github.com/Optimization-AI/ICML2021_FedDeepAUC_CODASCA.", "time": "2021-09-13T16:55:35Z", "link": "http://arxiv.org/abs/2102.04635v3", "id": "2102.04635v3", "title": "Federated Deep AUC Maximization for Heterogeneous Data with a Constant\n  Communication Complexity"}
{"author": "Dennis Bautembach, Iason Oikonomidis, Antonis Argyros", "abstract": "We present a SNN simulator which scales to millions of neurons, billions of\nsynapses, and 8 GPUs. This is made possible by 1) a novel, cache-aware spike\ntransmission algorithm 2) a model parallel multi-GPU distribution scheme and 3)\na static, yet very effective load balancing strategy. The simulator further\nfeatures an easy to use API and the ability to create custom models. We compare\nthe proposed simulator against two state of the art ones on a series of\nbenchmarks using three well-established models. We find that our simulator is\nfaster, consumes less memory, and scales linearly with the number of GPUs.", "time": "2021-04-22T18:50:23Z", "link": "http://arxiv.org/abs/2102.04681v2", "id": "2102.04681v2", "title": "Multi-GPU SNN Simulation with Static Load Balancing"}
{"author": "Haimonti Dutta, Nitin Nataraj, Saurabh Amarnath Mahindre", "abstract": "In recent years, storing large volumes of data on distributed devices has\nbecome commonplace. Applications involving sensors, for example, capture data\nin different modalities including image, video, audio, GPS and others. Novel\nalgorithms are required to learn from this rich distributed data. In this\npaper, we present consensus based multi-layer perceptrons for\nresource-constrained devices. Assuming nodes (devices) in the distributed\nsystem are arranged in a graph and contain vertically partitioned data, the\ngoal is to learn a global function that minimizes the loss. Each node learns a\nfeed-forward multi-layer perceptron and obtains a loss on data stored locally.\nIt then gossips with a neighbor, chosen uniformly at random, and exchanges\ninformation about the loss. The updated loss is used to run a back propagation\nalgorithm and adjust weights appropriately. This method enables nodes to learn\nthe global function without exchange of data in the network. Empirical results\nreveal that the consensus algorithm converges to the centralized model and has\nperformance comparable to centralized multi-layer perceptrons and tree-based\nalgorithms including random forests and gradient boosted decision trees.", "time": "2021-02-09T18:39:46Z", "link": "http://arxiv.org/abs/2102.05021v1", "id": "2102.05021v1", "title": "Consensus Based Multi-Layer Perceptrons for Edge Computing"}
{"author": "JiÅÃ­ FilipoviÄ, Jana HozzovÃ¡, Amin Nezarat, Jaroslav OÄ¾ha, Filip PetroviÄ", "abstract": "Nowadays, GPU accelerators are commonly used to speed up general-purpose\ncomputing tasks on a variety of hardware. However, due to the diversity of GPU\narchitectures and processed data, optimization of codes for a particular type\nof hardware and specific data characteristics can be extremely challenging. The\nautotuning of performance-relevant source-code parameters allows for automatic\noptimization of applications and keeps their performance portable. Although the\nautotuning process typically results in code speed-up, searching the tuning\nspace can bring unacceptable overhead if (i) the tuning space is vast and full\nof poorly-performing implementations, or (ii) the autotuning process has to be\nrepeated frequently because of changes in processed data or migration to\ndifferent hardware.\n  In this paper, we introduce a novel method for searching tuning spaces. The\nmethod takes advantage of collecting hardware performance counters (also known\nas profiling counters) during empirical tuning. Those counters are used to\nnavigate the searching process towards faster implementations. The method\nrequires the tuning space to be sampled on any GPU. It builds a\nproblem-specific model, which can be used during autotuning on various, even\npreviously unseen inputs or GPUs. Using a set of five benchmarks, we\nexperimentally demonstrate that our method can speed up autotuning when an\napplication needs to be ported to different hardware or when it needs to\nprocess data with different characteristics. We also compared our method to\nstate of the art and show that our method is superior in terms of the number of\nsearching steps and typically outperforms other searches in terms of\nconvergence time.", "time": "2021-09-17T10:48:21Z", "link": "http://arxiv.org/abs/2102.05297v2", "id": "2102.05297v2", "title": "Using hardware performance counters to speed up autotuning convergence\n  on GPUs"}
{"author": "Omid Aramoon, Pin-Yu Chen, Gang Qu, Yuan Tian", "abstract": "Due to its distributed methodology alongside its privacy-preserving features,\nFederated Learning (FL) is vulnerable to training time adversarial attacks. In\nthis study, our focus is on backdoor attacks in which the adversary's goal is\nto cause targeted misclassifications for inputs embedded with an adversarial\ntrigger while maintaining an acceptable performance on the main learning task\nat hand. Contemporary defenses against backdoor attacks in federated learning\nrequire direct access to each individual client's update which is not feasible\nin recent FL settings where Secure Aggregation is deployed. In this study, we\nseek to answer the following question, Is it possible to defend against\nbackdoor attacks when secure aggregation is in place?, a question that has not\nbeen addressed by prior arts. To this end, we propose Meta Federated Learning\n(Meta-FL), a novel variant of federated learning which not only is compatible\nwith secure aggregation protocol but also facilitates defense against backdoor\nattacks. We perform a systematic evaluation of Meta-FL on two classification\ndatasets: SVHN and GTSRB. The results show that Meta-FL not only achieves\nbetter utility than classic FL, but also enhances the performance of\ncontemporary defenses in terms of robustness against adversarial attacks.", "time": "2021-02-10T16:48:32Z", "link": "http://arxiv.org/abs/2102.05561v1", "id": "2102.05561v1", "title": "Meta Federated Learning"}
{"author": "Guojun Xiong, Gang Yan, Rahul Singh, Jian Li", "abstract": "With the increasing demand for large-scale training of machine learning\nmodels, consensus-based distributed optimization methods have recently been\nadvocated as alternatives to the popular parameter server framework. In this\nparadigm, each worker maintains a local estimate of the optimal parameter\nvector, and iteratively updates it by waiting and averaging all estimates\nobtained from its neighbors, and then corrects it on the basis of its local\ndataset. However, the synchronization phase can be time consuming due to the\nneed to wait for \\textit{stragglers}, i.e., slower workers. An efficient way to\nmitigate this effect is to let each worker wait only for updates from the\nfastest neighbors before updating its local parameter. The remaining neighbors\nare called \\textit{backup workers.} To minimize the globally training time over\nthe network, we propose a fully distributed algorithm to dynamically determine\nthe number of backup workers for each worker. We show that our algorithm\nachieves a linear speedup for convergence (i.e., convergence performance\nincreases linearly with respect to the number of workers). We conduct extensive\nexperiments on MNIST and CIFAR-10 to verify our theoretical results.", "time": "2021-02-11T21:39:53Z", "link": "http://arxiv.org/abs/2102.06280v1", "id": "2102.06280v1", "title": "Straggler-Resilient Distributed Machine Learning with Dynamic Backup\n  Workers"}
{"author": "Charlie Hou, Kiran K. Thekumparampil, Giulia Fanti, Sewoong Oh", "abstract": "We consider strongly convex-concave minimax problems in the federated\nsetting, where the communication constraint is the main bottleneck. When\nclients are arbitrarily heterogeneous, a simple Minibatch Mirror-prox achieves\nthe best performance. As the clients become more homogeneous, using multiple\nlocal gradient updates at the clients significantly improves upon Minibatch\nMirror-prox by communicating less frequently. Our goal is to design an\nalgorithm that can harness the benefit of similarity in the clients while\nrecovering the Minibatch Mirror-prox performance under arbitrary heterogeneity\n(up to log factors). We give the first federated minimax optimization algorithm\nthat achieves this goal. The main idea is to combine (i) SCAFFOLD (an algorithm\nthat performs variance reduction across clients for convex optimization) to\nerase the worst-case dependency on heterogeneity and (ii) Catalyst (a framework\nfor acceleration based on modifying the objective) to accelerate convergence\nwithout amplifying client drift. We prove that this algorithm achieves our\ngoal, and include experiments to validate the theory.", "time": "2021-02-12T02:55:36Z", "link": "http://arxiv.org/abs/2102.06333v1", "id": "2102.06333v1", "title": "Efficient Algorithms for Federated Saddle Point Optimization"}
{"author": "Rustem Islamov, Xun Qian, Peter RichtÃ¡rik", "abstract": "We develop several new communication-efficient second-order methods for\ndistributed optimization. Our first method, NEWTON-STAR, is a variant of\nNewton's method from which it inherits its fast local quadratic rate. However,\nunlike Newton's method, NEWTON-STAR enjoys the same per iteration communication\ncost as gradient descent. While this method is impractical as it relies on the\nuse of certain unknown parameters characterizing the Hessian of the objective\nfunction at the optimum, it serves as the starting point which enables us\ndesign practical variants thereof with strong theoretical guarantees. In\nparticular, we design a stochastic sparsification strategy for learning the\nunknown parameters in an iterative fashion in a communication efficient manner.\nApplying this strategy to NEWTON-STAR leads to our next method, NEWTON-LEARN,\nfor which we prove local linear and superlinear rates independent of the\ncondition number. When applicable, this method can have dramatically superior\nconvergence behavior when compared to state-of-the-art methods. Finally, we\ndevelop a globalization strategy using cubic regularization which leads to our\nnext method, CUBIC-NEWTON-LEARN, for which we prove global sublinear and linear\nconvergence rates, and a fast superlinear rate. Our results are supported with\nexperimental results on real datasets, and show several orders of magnitude\nimprovement on baseline and state-of-the-art methods in terms of communication\ncomplexity.", "time": "2021-02-14T14:06:45Z", "link": "http://arxiv.org/abs/2102.07158v1", "id": "2102.07158v1", "title": "Distributed Second Order Methods with Fast Rates and Compressed\n  Communication"}
{"author": "Ahmed M. Abdelmoniem, Chen-Yu Ho, Pantelis Papageorgiou, Muhammad Bilal, Marco Canini", "abstract": "Federated learning (FL) is becoming a popular paradigm for collaborative\nlearning over distributed, private datasets owned by non-trusting entities. FL\nhas seen successful deployment in production environments, and it has been\nadopted in services such as virtual keyboards, auto-completion, item\nrecommendation, and several IoT applications. However, FL comes with the\nchallenge of performing training over largely heterogeneous datasets, devices,\nand networks that are out of the control of the centralized FL server.\nMotivated by this inherent setting, we make a first step towards characterizing\nthe impact of device and behavioral heterogeneity on the trained model. We\nconduct an extensive empirical study spanning close to 1.5K unique\nconfigurations on five popular FL benchmarks. Our analysis shows that these\nsources of heterogeneity have a major impact on both model performance and\nfairness, thus sheds light on the importance of considering heterogeneity in FL\nsystem design.", "time": "2021-02-15T12:04:38Z", "link": "http://arxiv.org/abs/2102.07500v1", "id": "2102.07500v1", "title": "On the Impact of Device and Behavioral Heterogeneity in Federated\n  Learning"}
{"author": "Heesu Kim, Hanmin Park, Taehyun Kim, Kwanheum Cho, Eojin Lee, Soojung Ryu, Hyuk-Jae Lee, Kiyoung Choi, Jinho Lee", "abstract": "In this paper, we present GradPIM, a processing-in-memory architecture which\naccelerates parameter updates of deep neural networks training. As one of\nprocessing-in-memory techniques that could be realized in the near future, we\npropose an incremental, simple architectural design that does not invade the\nexisting memory protocol. Extending DDR4 SDRAM to utilize bank-group\nparallelism makes our operation designs in processing-in-memory (PIM) module\nefficient in terms of hardware cost and performance. Our experimental results\nshow that the proposed architecture can improve the performance of DNN training\nand greatly reduce memory bandwidth requirement while posing only a minimal\namount of overhead to the protocol and DRAM area.", "time": "2021-02-15T12:25:26Z", "link": "http://arxiv.org/abs/2102.07511v1", "id": "2102.07511v1", "title": "GradPIM: A Practical Processing-in-DRAM Architecture for Gradient\n  Descent"}
{"author": "Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, Ion Stoica", "abstract": "Model parallelism has become a necessity for training modern large-scale deep\nlanguage models. In this work, we identify a new and orthogonal dimension from\nexisting model parallel approaches: it is possible to perform pipeline\nparallelism within a single training sequence for Transformer-based language\nmodels thanks to its autoregressive property. This enables a more fine-grained\npipeline compared with previous work. With this key idea, we design TeraPipe, a\nhigh-performance token-level pipeline parallel algorithm for synchronous\nmodel-parallel training of Transformer-based language models. We develop a\nnovel dynamic programming-based algorithm to calculate the optimal pipelining\nexecution scheme given a specific model and cluster configuration. We show that\nTeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175\nbillion parameters on an AWS cluster with 48 p3.16xlarge instances compared\nwith state-of-the-art model-parallel methods. The code for reproduction can be\nfound at https://github.com/zhuohan123/terapipe", "time": "2021-09-28T05:04:28Z", "link": "http://arxiv.org/abs/2102.07988v2", "id": "2102.07988v2", "title": "TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale\n  Language Models"}
{"author": "Rachid Guerraoui, Nirupam Gupta, RafaÃ«l Pinot, SÃ©bastien Rouault, John Stephan", "abstract": "This paper addresses the problem of combining Byzantine resilience with\nprivacy in machine learning (ML). Specifically, we study if a distributed\nimplementation of the renowned Stochastic Gradient Descent (SGD) learning\nalgorithm is feasible with both differential privacy (DP) and\n$(\\alpha,f)$-Byzantine resilience. To the best of our knowledge, this is the\nfirst work to tackle this problem from a theoretical point of view. A key\nfinding of our analyses is that the classical approaches to these two\n(seemingly) orthogonal issues are incompatible. More precisely, we show that a\ndirect composition of these techniques makes the guarantees of the resulting\nSGD algorithm depend unfavourably upon the number of parameters of the ML\nmodel, making the training of large models practically infeasible. We validate\nour theoretical results through numerical experiments on publicly-available\ndatasets; showing that it is impractical to ensure DP and Byzantine resilience\nsimultaneously.", "time": "2021-06-24T15:44:21Z", "link": "http://arxiv.org/abs/2102.08166v3", "id": "2102.08166v3", "title": "Differential Privacy and Byzantine Resilience in SGD: Do They Add Up?"}
{"author": "David S. Johnson, Wolfgang Lorenz, Michael Taenzer, Stylianos Mimilakis, Sascha Grollmisch, Jakob AbeÃer, Hanna Lukashevich", "abstract": "Research on sound event detection (SED) in environmental settings has seen\nincreased attention in recent years. The large amounts of (private) domestic or\nurban audio data needed raise significant logistical and privacy concerns. The\ninherently distributed nature of these tasks, make federated learning (FL) a\npromising approach to take advantage of largescale data while mitigating\nprivacy issues. While FL has also seen increased attention recently, to the\nbest of our knowledge there is no research towards FL for SED. To address this\ngap and foster further research in this field, we create and publish novel FL\ndatasets for SED in domestic and urban environments. Furthermore, we provide\nbaseline results on the datasets in a FL context for three deep neural network\narchitectures. The results indicate that FL is a promising approach for SED,\nbut faces challenges with divergent data distributions inherent to distributed\nclient edge devices.", "time": "2021-05-31T10:32:39Z", "link": "http://arxiv.org/abs/2102.08833v3", "id": "2102.08833v3", "title": "DESED-FL and URBAN-FL: Federated Learning Datasets for Sound Event\n  Detection"}
{"author": "Jamal Toutouh, Una-May O'Reilly", "abstract": "Generative adversarial networks (GANs) exhibit training pathologies that can\nlead to convergence-related degenerative behaviors, whereas\nspatially-distributed, coevolutionary algorithms (CEAs) for GAN training, e.g.\nLipizzaner, are empirically robust to them. The robustness arises from\ndiversity that occurs by training populations of generators and discriminators\nin each cell of a toroidal grid. Communication, where signals in the form of\nparameters of the best GAN in a cell propagate in four directions: North,\nSouth, West, and East, also plays a role, by communicating adaptations that are\nboth new and fit. We propose Lipi-Ring, a distributed CEA like Lipizzaner,\nexcept that it uses a different spatial topology, i.e. a ring. Our central\nquestion is whether the different directionality of signal propagation\n(effectively migration to one or more neighbors on each side of a cell) meets\nor exceeds the performance quality and training efficiency of Lipizzaner\nExperimental analysis on different datasets (i.e, MNIST, CelebA, and COVID-19\nchest X-ray images) shows that there are no significant differences between the\nperformances of the trained generative models by both methods. However,\nLipi-Ring significantly reduces the computational time (14.2%. . . 41.2%).\nThus, Lipi-Ring offers an alternative to Lipizzaner when the computational cost\nof training matters.", "time": "2021-02-10T16:46:44Z", "link": "http://arxiv.org/abs/2102.08929v1", "id": "2102.08929v1", "title": "Signal Propagation in a Gradient-Based and Evolutionary Learning System"}
{"author": "Minghong Fang, Minghao Sun, Qi Li, Neil Zhenqiang Gong, Jin Tian, Jia Liu", "abstract": "A key challenge of big data analytics is how to collect a large volume of\n(labeled) data. Crowdsourcing aims to address this challenge via aggregating\nand estimating high-quality data (e.g., sentiment label for text) from\npervasive clients/users. Existing studies on crowdsourcing focus on designing\nnew methods to improve the aggregated data quality from unreliable/noisy\nclients. However, the security aspects of such crowdsourcing systems remain\nunder-explored to date. We aim to bridge this gap in this work. Specifically,\nwe show that crowdsourcing is vulnerable to data poisoning attacks, in which\nmalicious clients provide carefully crafted data to corrupt the aggregated\ndata. We formulate our proposed data poisoning attacks as an optimization\nproblem that maximizes the error of the aggregated data. Our evaluation results\non one synthetic and two real-world benchmark datasets demonstrate that the\nproposed attacks can substantially increase the estimation errors of the\naggregated data. We also propose two defenses to reduce the impact of malicious\nclients. Our empirical results show that the proposed defenses can\nsubstantially reduce the estimation errors of the data poisoning attacks.", "time": "2021-02-24T23:10:31Z", "link": "http://arxiv.org/abs/2102.09171v2", "id": "2102.09171v2", "title": "Data Poisoning Attacks and Defenses to Crowdsourcing Systems"}
{"author": "Constantin Philippenko, Aymeric Dieuleveut", "abstract": "We develop a new approach to tackle communication constraints in a\ndistributed learning problem with a central server. We propose and analyze a\nnew algorithm that performs bidirectional compression and achieves the same\nconvergence rate as algorithms using only uplink (from the local workers to the\ncentral server) compression. To obtain this improvement, we design MCM, an\nalgorithm such that the downlink compression only impacts local models, while\nthe global model is preserved. As a result, and contrary to previous works, the\ngradients on local servers are computed on perturbed models. Consequently,\nconvergence proofs are more challenging and require a precise control of this\nperturbation. To ensure it, MCM additionally combines model compression with a\nmemory mechanism. This analysis opens new doors, e.g. incorporating worker\ndependent randomized-models and partial participation.", "time": "2022-06-16T14:00:00Z", "link": "http://arxiv.org/abs/2102.12528v2", "id": "2102.12528v2", "title": "Preserved central model for faster bidirectional compression in\n  distributed settings"}
{"author": "Yuyang Deng, Mohammad Mahdi Kamani, Mehrdad Mahdavi", "abstract": "In this paper, we study communication efficient distributed algorithms for\ndistributionally robust federated learning via periodic averaging with adaptive\nsampling. In contrast to standard empirical risk minimization, due to the\nminimax structure of the underlying optimization problem, a key difficulty\narises from the fact that the global parameter that controls the mixture of\nlocal losses can only be updated infrequently on the global stage. To\ncompensate for this, we propose a Distributionally Robust Federated Averaging\n(DRFA) algorithm that employs a novel snapshotting scheme to approximate the\naccumulation of history gradients of the mixing parameter. We analyze the\nconvergence rate of DRFA in both convex-linear and nonconvex-linear settings.\nWe also generalize the proposed idea to objectives with regularization on the\nmixture parameter and propose a proximal variant, dubbed as DRFA-Prox, with\nprovable convergence rates. We also analyze an alternative optimization method\nfor regularized cases in strongly-convex-strongly-concave and non-convex (under\nPL condition)-strongly-concave settings. To the best of our knowledge, this\npaper is the first to solve distributionally robust federated learning with\nreduced communication, and to analyze the efficiency of local descent methods\non distributed minimax problems. We give corroborating experimental evidence\nfor our theoretical results in federated learning settings.", "time": "2021-02-25T03:32:09Z", "link": "http://arxiv.org/abs/2102.12660v1", "id": "2102.12660v1", "title": "Distributionally Robust Federated Averaging"}
{"author": "Geet Shingi, Harsh Saglani, Preeti Jain", "abstract": "Cyberattacks are a major issues and it causes organizations great financial,\nand reputation harm. However, due to various factors, the current network\nintrusion detection systems (NIDS) seem to be insufficent. Predominant NIDS\nidentifies Cyberattacks through a handcrafted dataset of rules. Although the\nrecent applications of machine learning and deep learning have alleviated the\nenormous effort in NIDS, the security of network data has always been a prime\nconcern. However, to encounter the security problem and enable sharing among\norganizations, Federated Learning (FL) scheme is employed. Although the current\nFL systems have been successful, a network's data distribution does not always\nfit into a single global model as in FL. Thus, in such cases, having a single\nglobal model in FL is no feasible. In this paper, we propose a\nSegmented-Federated Learning (Segmented-FL) learning scheme for a more\nefficient NIDS. The Segmented-FL approach employs periodic local model\nevaluation based on which the segmentation occurs. We aim to bring similar\nnetwork environments to the same group. Further, the Segmented-FL system is\ncoupled with a weighted aggregation of local model parameters based on the\nnumber of data samples a worker possesses to further augment the performance.\nThe improved performance by our system as compared to the FL and centralized\nsystems on standard dataset further validates our system and makes a strong\ncase for extending our technique across various tasks. The solution finds its\napplication in organizations that want to collaboratively learn on diverse\nnetwork environments and protect the privacy of individual datasets.", "time": "2021-07-02T07:47:05Z", "link": "http://arxiv.org/abs/2107.00881v1", "id": "2107.00881v1", "title": "Segmented Federated Learning for Adaptive Intrusion Detection System"}
{"author": "Chen Dun, Cameron R. Wolfe, Christopher M. Jermaine, Anastasios Kyrillidis", "abstract": "We propose ResIST, a novel distributed training protocol for Residual\nNetworks (ResNets). ResIST randomly decomposes a global ResNet into several\nshallow sub-ResNets that are trained independently in a distributed manner for\nseveral local iterations, before having their updates synchronized and\naggregated into the global model. In the next round, new sub-ResNets are\nrandomly generated and the process repeats until convergence. By construction,\nper iteration, ResIST communicates only a small portion of network parameters\nto each machine and never uses the full model during training. Thus, ResIST\nreduces the per-iteration communication, memory, and time requirements of\nResNet training to only a fraction of the requirements of full-model training.\nIn comparison to common protocols, like data-parallel training and\ndata-parallel training with local SGD, ResIST yields a decrease in\ncommunication and compute requirements, while being competitive with respect to\nmodel performance.", "time": "2022-03-14T14:21:25Z", "link": "http://arxiv.org/abs/2107.00961v2", "id": "2107.00961v2", "title": "ResIST: Layer-Wise Decomposition of ResNets for Distributed Training"}
{"author": "Rasheed el-Bouri, Tingting Zhu, David A. Clifton", "abstract": "Given the abundance and ease of access of personal data today, individual\nprivacy has become of paramount importance, particularly in the healthcare\ndomain. In this work, we aim to utilise patient data extracted from multiple\nhospital data centres to train a machine learning model without sacrificing\npatient privacy. We develop a scheduling algorithm in conjunction with a\nstudent-teacher algorithm that is deployed in a federated manner. This allows a\ncentral model to learn from batches of data at each federal node. The teacher\nacts between data centres to update the main task (student) algorithm using the\ndata that is stored in the various data centres. We show that the scheduler,\ntrained using meta-gradients, can effectively organise training and as a result\ntrain a machine learning model on a diverse dataset without needing explicit\naccess to the patient data. We achieve state-of-the-art performance and show\nhow our method overcomes some of the problems faced in the federated learning\nsuch as node poisoning. We further show how the scheduler can be used as a\nmechanism for transfer learning, allowing different teachers to work together\nin training a student for state-of-the-art performance.", "time": "2021-07-04T18:45:58Z", "link": "http://arxiv.org/abs/2107.01707v1", "id": "2107.01707v1", "title": "Towards Scheduling Federated Deep Learning using Meta-Gradients for\n  Inter-Hospital Learning"}
{"author": "Yipeng Zhou, Xuezheng Liu, Yao Fu, Di Wu, Chao Li, Shui Yu", "abstract": "Federated learning (FL) empowers distributed clients to collaboratively train\na shared machine learning model through exchanging parameter information.\nDespite the fact that FL can protect clients' raw data, malicious users can\nstill crack original data with disclosed parameters. To amend this flaw,\ndifferential privacy (DP) is incorporated into FL clients to disturb original\nparameters, which however can significantly impair the accuracy of the trained\nmodel. In this work, we study a crucial question which has been vastly\noverlooked by existing works: what are the optimal numbers of queries and\nreplies in FL with DP so that the final model accuracy is maximized. In FL, the\nparameter server (PS) needs to query participating clients for multiple global\niterations to complete training. Each client responds a query from the PS by\nconducting a local iteration. Our work investigates how many times the PS\nshould query clients and how many times each client should reply the PS. We\ninvestigate two most extensively used DP mechanisms (i.e., the Laplace\nmechanism and Gaussian mechanisms). Through conducting convergence rate\nanalysis, we can determine the optimal numbers of queries and replies in FL\nwith DP so that the final model accuracy can be maximized. Finally, extensive\nexperiments are conducted with publicly available datasets: MNIST and FEMNIST,\nto verify our analysis and the results demonstrate that properly setting the\nnumbers of queries and replies can significantly improve the final model\naccuracy in FL with DP.", "time": "2021-07-05T09:42:56Z", "link": "http://arxiv.org/abs/2107.01895v1", "id": "2107.01895v1", "title": "Optimizing the Numbers of Queries and Replies in Federated Learning with\n  Differential Privacy"}
{"author": "Shashikant Ilager, Rajkumar Buyya", "abstract": "This paper investigates the existing resource management approaches in Cloud\nData Centres for energy and thermal efficiency. It identifies the need for\nintegrated computing and cooling systems management and learning-based\nsolutions in resource management systems. A taxonomy on energy and thermal\nefficient resource management in data centres is proposed based on an in-depth\nanalysis of the literature. Furthermore, a detailed survey on existing\napproaches is conducted according to the taxonomy and recent advancements\nincluding machine learning-based resource management approaches and cooling\nmanagement technologies are discussed.", "time": "2021-07-06T01:49:21Z", "link": "http://arxiv.org/abs/2107.02342v1", "id": "2107.02342v1", "title": "Energy and Thermal-aware Resource Management of Cloud Data Centres: A\n  Taxonomy and Future Directions"}
{"author": "Nikhil Pratap Ghanathe, Vivek Seshadri, Rahul Sharma, Steve Wilton, Aayan Kumar", "abstract": "Recent breakthroughs in ML have produced new classes of models that allow ML\ninference to run directly on milliwatt-powered IoT devices. On one hand,\nexisting ML-to-FPGA compilers are designed for deep neural-networks on large\nFPGAs. On the other hand, general-purpose HLS tools fail to exploit properties\nspecific to ML inference, thereby resulting in suboptimal performance. We\npropose MAFIA, a tool to compile ML inference on small form-factor FPGAs for\nIoT applications. MAFIA provides native support for linear algebra operations\nand can express a variety of ML algorithms, including state-of-the-art models.\nWe show that MAFIA-generated programs outperform best-performing variant of a\ncommercial HLS compiler by 2.5x on average.", "time": "2021-07-08T07:38:23Z", "link": "http://arxiv.org/abs/2107.03653v1", "id": "2107.03653v1", "title": "MAFIA: Machine Learning Acceleration on FPGAs for IoT Applications"}
{"author": "Martina Lofqvist, JosÃ© Cano", "abstract": "There is a proliferation in the number of satellites launched each year,\nresulting in downlinking of terabytes of data each day. The data received by\nground stations is often unprocessed, making this an expensive process\nconsidering the large data sizes and that not all of the data is useful. This,\ncoupled with the increasing demand for real-time data processing, has led to a\ngrowing need for on-orbit processing solutions. In this work, we investigate\nthe performance of CNN-based object detectors on constrained devices by\napplying different image compression techniques to satellite data. We examine\nthe capabilities of the NVIDIA Jetson Nano and NVIDIA Jetson AGX Xavier;\nlow-power, high-performance computers, with integrated GPUs, small enough to\nfit on-board a nanosatellite. We take a closer look at object detection\nnetworks, including the Single Shot MultiBox Detector (SSD) and Region-based\nFully Convolutional Network (R-FCN) models that are pre-trained on DOTA - a\nLarge Scale Dataset for Object Detection in Aerial Images. The performance is\nmeasured in terms of execution time, memory consumption, and accuracy, and are\ncompared against a baseline containing a server with two powerful GPUs. The\nresults show that by applying image compression techniques, we are able to\nimprove the execution time and memory consumption, achieving a fully runnable\ndataset. A lossless compression technique achieves roughly a 10% reduction in\nexecution time and about a 3% reduction in memory consumption, with no impact\non the accuracy. While a lossy compression technique improves the execution\ntime by up to 144% and the memory consumption is reduced by as much as 97%.\nHowever, it has a significant impact on accuracy, varying depending on the\ncompression ratio. Thus the application and ratio of these compression\ntechniques may differ depending on the required level of accuracy for a\nparticular task.", "time": "2021-07-08T11:37:24Z", "link": "http://arxiv.org/abs/2107.03774v1", "id": "2107.03774v1", "title": "Optimizing Data Processing in Space for Object Detection in Satellite\n  Imagery"}
{"author": "Bo Liu, Chaowei Tan, Jiazhou Wang, Tao Zeng, Huasong Shan, Houpu Yao, Heng Huang, Peng Dai, Liefeng Bo, Yanqing Chen", "abstract": "In this paper, we present Fedlearn-Algo, an open-source privacy preserving\nmachine learning platform. We use this platform to demonstrate our research and\ndevelopment results on privacy preserving machine learning algorithms. As the\nfirst batch of novel FL algorithm examples, we release vertical federated\nkernel binary classification model and vertical federated random forest model.\nThey have been tested to be more efficient than existing vertical federated\nlearning models in our practice. Besides the novel FL algorithm examples, we\nalso release a machine communication module. The uniform data transfer\ninterface supports transferring widely used data formats between machines. We\nwill maintain this platform by adding more functional modules and algorithm\nexamples. The code is available at https://github.com/fedlearnAI/fedlearn-algo.", "time": "2021-07-30T06:13:55Z", "link": "http://arxiv.org/abs/2107.04129v2", "id": "2107.04129v2", "title": "Fedlearn-Algo: A flexible open-source privacy-preserving machine\n  learning platform"}
{"author": "Jiacheng Liang, Songze Li, Bochuan Cao, Wensi Jiang, Chaoyang He", "abstract": "We propose OmniLytics, a blockchain-based secure data trading marketplace for\nmachine learning applications. Utilizing OmniLytics, many distributed data\nowners can contribute their private data to collectively train an ML model\nrequested by some model owners, and receive compensation for data contribution.\nOmniLytics enables such model training while simultaneously providing 1) model\nsecurity against curious data owners; 2) data security against the curious\nmodel and data owners; 3) resilience to malicious data owners who provide\nfaulty results to poison model training; and 4) resilience to malicious model\nowners who intend to evade payment. OmniLytics is implemented as a blockchain\nsmart contract to guarantee the atomicity of payment. In OmniLytics, a model\nowner splits its model into the private and public parts and publishes the\npublic part on the contract. Through the execution of the contract, the\nparticipating data owners securely aggregate their locally trained models to\nupdate the model owner's public model and receive reimbursement through the\ncontract. We implement a working prototype of OmniLytics on Ethereum blockchain\nand perform extensive experiments to measure its gas cost, execution time, and\nmodel quality under various parameter combinations. For training a CNN on the\nMNIST dataset, the MO is able to boost its model accuracy from 62% to 83%\nwithin 500ms in blockchain processing time.This demonstrates the effectiveness\nof OmniLytics for practical deployment.", "time": "2021-11-15T07:18:28Z", "link": "http://arxiv.org/abs/2107.05252v4", "id": "2107.05252v4", "title": "OmniLytics: A Blockchain-based Secure Data Market for Decentralized\n  Machine Learning"}
{"author": "Jesmin Jahan Tithi, Fabrizio Petrini", "abstract": "The Word Movers Distance (WMD) measures the semantic dissimilarity between\ntwo text documents by computing the cost of optimally moving all words of a\nsource/query document to the most similar words of a target document. Computing\nWMD between two documents is costly because it requires solving an\n$O(V^3log(V))$ optimization problem where $V$ is the number of unique words in\nthe document. Fortunately, WMD can be framed as an Earth Mover's Distance (EMD)\nfor which the algorithmic complexity can be reduced to $O(V^2)$ by adding an\nentropy penalty to the optimization problem and solving it using the\nSinkhorn-Knopp algorithm. Additionally, the computation can be made highly\nparallel by adopting a batching approach, i.e., computing the WMD of a single\nquery document against multiple target documents at once.\n  Sinkhorn WMD is a key kernel used in many ML/NLP applications. and usually\ngets implemented in Python. However, a straightforward Python implementation\nmay leave significant performance on the table even though it may internally\ncall optimized C++ BLAS routines. We present a new sparse {P}arallel\n{A}lgorithm for {S}inkhorn-Knopp {W}ord-movers {D}istance to compute the\nsemantic distance of one document to many other documents by adopting the\n$O(V^2)$ EMD algorithm. We algorithmically transform $O(V^2)$ dense\ncompute-heavy EMD version into an equivalent sparse one using new fused\nSDDMM-SpMM (sparse selection of dense-dense matrix-, sparse-dense\nmatrix-multiplication) kernels. We implemented and optimized this algorithm for\ntwo very different architectures -- the new Intel Programmable Integrated\nUnified Memory Architecture (PIUMA) and Intel Xeon CPUs. We show that we were\nable to reach close to peak performance on both platforms.", "time": "2022-04-26T17:16:43Z", "link": "http://arxiv.org/abs/2107.06433v3", "id": "2107.06433v3", "title": "A New Parallel Algorithm for Sinkhorn Word-Movers Distance and Its\n  Performance on PIUMA and Xeon CPU"}
{"author": "Alaa Awad Abdellatif, Naram Mhaisen, Amr Mohamed, Aiman Erbad, Mohsen Guizani, Zaher Dawy, Wassim Nasreddine", "abstract": "Federated learning (FL) is a distributed learning methodology that allows\nmultiple nodes to cooperatively train a deep learning model, without the need\nto share their local data. It is a promising solution for telemonitoring\nsystems that demand intensive data collection, for detection, classification,\nand prediction of future events, from different locations while maintaining a\nstrict privacy constraint. Due to privacy concerns and critical communication\nbottlenecks, it can become impractical to send the FL updated models to a\ncentralized server. Thus, this paper studies the potential of hierarchical FL\nin IoT heterogeneous systems and propose an optimized solution for user\nassignment and resource allocation on multiple edge nodes. In particular, this\nwork focuses on a generic class of machine learning models that are trained\nusing gradient-descent-based schemes while considering the practical\nconstraints of non-uniformly distributed data across different users. We\nevaluate the proposed system using two real-world datasets, and we show that it\noutperforms state-of-the-art FL solutions. In particular, our numerical results\nhighlight the effectiveness of our approach and its ability to provide 4-6%\nincrease in the classification accuracy, with respect to hierarchical FL\nschemes that consider distance-based user assignment. Furthermore, the proposed\napproach could significantly accelerate FL training and reduce communication\noverhead by providing 75-85% reduction in the communication rounds between edge\nnodes and the centralized server, for the same model accuracy.", "time": "2021-07-14T08:32:39Z", "link": "http://arxiv.org/abs/2107.06548v1", "id": "2107.06548v1", "title": "Communication-Efficient Hierarchical Federated Learning for IoT\n  Heterogeneous Systems with Imbalanced Data"}
{"author": "Martin Svedin, Artur Podobas, Steven W. D. Chien, Stefano Markidis", "abstract": "One of the most promising approaches for data analysis and exploration of\nlarge data sets is Machine Learning techniques that are inspired by brain\nmodels. Such methods use alternative learning rules potentially more\nefficiently than established learning rules. In this work, we focus on the\npotential of brain-inspired ML for exploiting High-Performance Computing (HPC)\nresources to solve ML problems: we discuss the BCPNN and an HPC implementation,\ncalled StreamBrain, its computational cost, suitability to HPC systems. As an\nexample, we use StreamBrain to analyze the Higgs Boson dataset from High Energy\nPhysics and discriminate between background and signal classes in collisions of\nhigh-energy particle colliders. Overall, we reach up to 69.15% accuracy and\n76.4% Area Under the Curve (AUC) performance.", "time": "2021-08-17T13:16:38Z", "link": "http://arxiv.org/abs/2107.06676v2", "id": "2107.06676v2", "title": "Higgs Boson Classification: Brain-inspired BCPNN Learning with\n  StreamBrain"}
{"author": "Vasileios Tsouvalas, Aaqib Saeed, Tanir Ozcelebi", "abstract": "Federated Learning is a distributed machine learning paradigm dealing with\ndecentralized and personal datasets. Since data reside on devices like\nsmartphones and virtual assistants, labeling is entrusted to the clients, or\nlabels are extracted in an automated way. Specifically, in the case of audio\ndata, acquiring semantic annotations can be prohibitively expensive and\ntime-consuming. As a result, an abundance of audio data remains unlabeled and\nunexploited on users' devices. Most existing federated learning approaches\nfocus on supervised learning without harnessing the unlabeled data. In this\nwork, we study the problem of semi-supervised learning of audio models via\nself-training in conjunction with federated learning. We propose FedSTAR to\nexploit large-scale on-device unlabeled data to improve the generalization of\naudio recognition models. We further demonstrate that self-supervised\npre-trained models can accelerate the training of on-device models,\nsignificantly improving convergence to within fewer training rounds. We conduct\nexperiments on diverse public audio classification datasets and investigate the\nperformance of our models under varying percentages of labeled and unlabeled\ndata. Notably, we show that with as little as 3% labeled data available,\nFedSTAR on average can improve the recognition rate by 13.28% compared to the\nfully supervised federated model.", "time": "2022-02-25T15:46:19Z", "link": "http://arxiv.org/abs/2107.06877v2", "id": "2107.06877v2", "title": "Federated Self-Training for Semi-Supervised Audio Recognition"}
{"author": "Ian Colbert, Ken Kreutz-Delgado, Srinjoy Das", "abstract": "A novel energy-efficient edge computing paradigm is proposed for real-time\ndeep learning-based image upsampling applications. State-of-the-art deep\nlearning solutions for image upsampling are currently trained using either\nresize or sub-pixel convolution to learn kernels that generate high fidelity\nimages with minimal artifacts. However, performing inference with these learned\nconvolution kernels requires memory-intensive feature map transformations that\ndominate time and energy costs in real-time applications. To alleviate this\npressure on memory bandwidth, we confine the use of resize or sub-pixel\nconvolution to training in the cloud by transforming learned convolution\nkernels to deconvolution kernels before deploying them for inference as a\nfunctionally equivalent deconvolution. These kernel transformations, intended\nas a one-time cost when shifting from training to inference, enable a systems\ndesigner to use each algorithm in their optimal context by preserving the image\nfidelity learned when training in the cloud while minimizing data transfer\npenalties during inference at the edge. We also explore existing variants of\ndeconvolution inference algorithms and introduce a novel variant for\nconsideration. We analyze and compare the inference properties of\nconvolution-based upsampling algorithms using a quantitative model of incurred\ntime and energy costs and show that using deconvolution for inference at the\nedge improves both system latency and energy efficiency when compared to their\nsub-pixel or resize convolution counterparts.", "time": "2021-07-26T05:34:59Z", "link": "http://arxiv.org/abs/2107.07647v2", "id": "2107.07647v2", "title": "An Energy-Efficient Edge Computing Paradigm for Convolution-based Image\n  Upsampling"}
{"author": "Mohanad Odema, Nafiul Rashid, Berken Utku Demirel, Mohammad Abdullah Al Faruque", "abstract": "Edge-Cloud hierarchical systems employing intelligence through Deep Neural\nNetworks (DNNs) endure the dilemma of workload distribution within them.\nPrevious solutions proposed to distribute workloads at runtime according to the\nstate of the surroundings, like the wireless conditions. However, such\nconditions are usually overlooked at design time. This paper addresses this\nissue for DNN architectural design by presenting a novel methodology, LENS,\nwhich administers multi-objective Neural Architecture Search (NAS) for\ntwo-tiered systems, where the performance objectives are refashioned to\nconsider the wireless communication parameters. From our experimental search\nspace, we demonstrate that LENS improves upon the traditional solution's Pareto\nset by 76.47% and 75% with respect to the energy and latency metrics,\nrespectively.", "time": "2021-07-20T07:53:02Z", "link": "http://arxiv.org/abs/2107.09309v1", "id": "2107.09309v1", "title": "LENS: Layer Distribution Enabled Neural Architecture Search in\n  Edge-Cloud Hierarchies"}
{"author": "Zhize Li, Peter RichtÃ¡rik", "abstract": "Due to the high communication cost in distributed and federated learning,\nmethods relying on compressed communication are becoming increasingly popular.\nBesides, the best theoretically and practically performing gradient-type\nmethods invariably rely on some form of acceleration/momentum to reduce the\nnumber of communications (faster convergence), e.g., Nesterov's accelerated\ngradient descent (Nesterov, 1983, 2004) and Adam (Kingma and Ba, 2014). In\norder to combine the benefits of communication compression and convergence\nacceleration, we propose a \\emph{compressed and accelerated} gradient method\nbased on ANITA (Li, 2021) for distributed optimization, which we call CANITA.\nOur CANITA achieves the \\emph{first accelerated rate}\n$O\\bigg(\\sqrt{\\Big(1+\\sqrt{\\frac{\\omega^3}{n}}\\Big)\\frac{L}{\\epsilon}} +\n\\omega\\big(\\frac{1}{\\epsilon}\\big)^{\\frac{1}{3}}\\bigg)$, which improves upon\nthe state-of-the-art non-accelerated rate\n$O\\left((1+\\frac{\\omega}{n})\\frac{L}{\\epsilon} +\n\\frac{\\omega^2+\\omega}{\\omega+n}\\frac{1}{\\epsilon}\\right)$ of DIANA (Khaled et\nal., 2020) for distributed general convex problems, where $\\epsilon$ is the\ntarget error, $L$ is the smooth parameter of the objective, $n$ is the number\nof machines/devices, and $\\omega$ is the compression parameter (larger $\\omega$\nmeans more compression can be applied, and no compression implies $\\omega=0$).\nOur results show that as long as the number of devices $n$ is large (often true\nin distributed/federated learning), or the compression $\\omega$ is not very\nhigh, CANITA achieves the faster convergence rate\n$O\\Big(\\sqrt{\\frac{L}{\\epsilon}}\\Big)$, i.e., the number of communication\nrounds is $O\\Big(\\sqrt{\\frac{L}{\\epsilon}}\\Big)$ (vs.\n$O\\big(\\frac{L}{\\epsilon}\\big)$ achieved by previous works). As a result,\nCANITA enjoys the advantages of both compression (compressed communication in\neach round) and acceleration (much fewer communication rounds).", "time": "2021-11-07T08:22:38Z", "link": "http://arxiv.org/abs/2107.09461v2", "id": "2107.09461v2", "title": "CANITA: Faster Rates for Distributed Convex Optimization with\n  Communication Compression"}
{"author": "Aleksandr Beznosikov, Gesualdo Scutari, Alexander Rogozin, Alexander Gasnikov", "abstract": "We study solution methods for (strongly-)convex-(strongly)-concave\nSaddle-Point Problems (SPPs) over networks of two type - master/workers (thus\ncentralized) architectures and meshed (thus decentralized) networks. The local\nfunctions at each node are assumed to be similar, due to statistical data\nsimilarity or otherwise. We establish lower complexity bounds for a fairly\ngeneral class of algorithms solving the SPP. We show that a given suboptimality\n$\\epsilon>0$ is achieved over master/workers networks in\n$\\Omega\\big(\\Delta\\cdot \\delta/\\mu\\cdot \\log (1/\\varepsilon)\\big)$ rounds of\ncommunications, where $\\delta>0$ measures the degree of similarity of the local\nfunctions, $\\mu$ is their strong convexity constant, and $\\Delta$ is the\ndiameter of the network. The lower communication complexity bound over meshed\nnetworks reads $\\Omega\\big(1/{\\sqrt{\\rho}} \\cdot {\\delta}/{\\mu}\\cdot\\log\n(1/\\varepsilon)\\big)$, where $\\rho$ is the (normalized) eigengap of the gossip\nmatrix used for the communication between neighbouring nodes. We then propose\nalgorithms matching the lower bounds over either types of networks (up to\nlog-factors). We assess the effectiveness of the proposed algorithms on a\nrobust logistic regression problem.", "time": "2022-08-22T08:32:16Z", "link": "http://arxiv.org/abs/2107.10706v3", "id": "2107.10706v3", "title": "Distributed Saddle-Point Problems Under Similarity"}
{"author": "Fengjiao Li, Jia Liu, Bo Ji", "abstract": "In this paper, we study the problem of fair worker selection in Federated\nLearning systems, where fairness serves as an incentive mechanism that\nencourages more workers to participate in the federation. Considering the\nachieved training accuracy of the global model as the utility of the selected\nworkers, which is typically a monotone submodular function, we formulate the\nworker selection problem as a new multi-round monotone submodular maximization\nproblem with cardinality and fairness constraints. The objective is to maximize\nthe time-average utility over multiple rounds subject to an additional fairness\nrequirement that each worker must be selected for a certain fraction of time.\nWhile the traditional submodular maximization with a cardinality constraint is\nalready a well-known NP-Hard problem, the fairness constraint in the\nmulti-round setting adds an extra layer of difficulty. To address this novel\nchallenge, we propose three algorithms: Fair Continuous Greedy (FairCG1 and\nFairCG2) and Fair Discrete Greedy (FairDG), all of which satisfy the fairness\nrequirement whenever feasible. Moreover, we prove nontrivial lower bounds on\nthe achieved time-average utility under FairCG1 and FairCG2. In addition, by\ngiving a higher priority to fairness, FairDG ensures a stronger short-term\nfairness guarantee, which holds in every round. Finally, we perform extensive\nsimulations to verify the effectiveness of the proposed algorithms in terms of\nthe time-average utility and fairness satisfaction.", "time": "2021-07-25T05:17:34Z", "link": "http://arxiv.org/abs/2107.11728v1", "id": "2107.11728v1", "title": "Federated Learning with Fair Worker Selection: A Multi-Round Submodular\n  Maximization Approach"}
{"author": "Kamala Varma, Yi Zhou, Nathalie Baracaldo, Ali Anwar", "abstract": "Federated learning has arisen as a mechanism to allow multiple participants\nto collaboratively train a model without sharing their data. In these settings,\nparticipants (workers) may not trust each other fully; for instance, a set of\ncompetitors may collaboratively train a machine learning model to detect fraud.\nThe workers provide local gradients that a central server uses to update a\nglobal model. This global model can be corrupted when Byzantine workers send\nmalicious gradients, which necessitates robust methods for aggregating\ngradients that mitigate the adverse effects of Byzantine inputs. Existing\nrobust aggregation algorithms are often computationally expensive and only\neffective under strict assumptions. In this paper, we introduce LayerwisE\nGradient AggregatTiOn (LEGATO), an aggregation algorithm that is, by contrast,\nscalable and generalizable. Informed by a study of layer-specific responses of\ngradients to Byzantine attacks, LEGATO employs a dynamic gradient reweighing\nscheme that is novel in its treatment of gradients based on layer-specific\nrobustness. We show that LEGATO is more computationally efficient than multiple\nstate-of-the-art techniques and more generally robust across a variety of\nattack settings in practice. We also demonstrate LEGATO's benefits for gradient\ndescent convergence in the absence of an attack.", "time": "2021-07-26T21:34:45Z", "link": "http://arxiv.org/abs/2107.12490v1", "id": "2107.12490v1", "title": "LEGATO: A LayerwisE Gradient AggregaTiOn Algorithm for Mitigating\n  Byzantine Attacks in Federated Learning"}
{"author": "Thilanka Munasinghe, HR Pasindu", "abstract": "We propose how a developing country like Sri Lanka can benefit from\nprivacy-enabled machine learning techniques such as Federated Learning to\ndetect road conditions using crowd-sourced data collection and proposed the\nidea of implementing a Digital Twin for the national road system in Sri Lanka.\nDeveloping countries such as Sri Lanka are far behind in implementing smart\nroad systems and smart cities compared to the developed countries. The proposed\nwork discussed in this paper matches the UN Sustainable Development Goal (SDG)\n9: \"Build Resilient Infrastructure, Promote Inclusive and Sustainable\nIndustrialization and Foster Innovation\". Our proposed work discusses how the\ngovernment and private sector vehicles that conduct routine trips to collect\ncrowd-sourced data using smartphone devices to identify the road conditions and\ndetect where the potholes, surface unevenness (roughness), and other major\ndistresses are located on the roads. We explore Mobile Edge Computing (MEC)\ntechniques that can bring machine learning intelligence closer to the edge\ndevices where produced data is stored and show how the applications of\nFederated Learning can be made to detect and improve road conditions. During\nthe second phase of this study, we plan to implement a Digital Twin for the\nroad system in Sri Lanka. We intend to use data provided by both Dedicated and\nNon-Dedicated systems in the proposed Digital Twin for the road system. As of\nwriting this paper, and best to our knowledge, there is no Digital Twin system\nimplemented for roads and other infrastructure systems in Sri Lanka. The\nproposed Digital Twin will be one of the first implementations of such systems\nin Sri Lanka. Lessons learned from this pilot project will benefit other\ndeveloping countries who wish to follow the same path and make data-driven\ndecisions.", "time": "2021-07-30T11:06:32Z", "link": "http://arxiv.org/abs/2107.14551v1", "id": "2107.14551v1", "title": "Sensing and Mapping for Better Roads: Initial Plan for Using Federated\n  Learning and Implementing a Digital Twin to Identify the Road Conditions in a\n  Developing Country -- Sri Lanka"}
{"author": "He Yang", "abstract": "The longstanding goals of federated learning (FL) require rigorous privacy\nguarantees and low communication overhead while holding a relatively high model\naccuracy. However, simultaneously achieving all the goals is extremely\nchallenging. In this paper, we propose a novel framework called hierarchical\nfederated learning (H-FL) to tackle this challenge. Considering the degradation\nof the model performance due to the statistic heterogeneity of the training\ndata, we devise a runtime distribution reconstruction strategy, which\nreallocates the clients appropriately and utilizes mediators to rearrange the\nlocal training of the clients. In addition, we design a compression-correction\nmechanism incorporated into H-FL to reduce the communication overhead while not\nsacrificing the model performance. To further provide privacy guarantees, we\nintroduce differential privacy while performing local training, which injects\nmoderate amount of noise into only part of the complete model. Experimental\nresults show that our H-FL framework achieves the state-of-art performance on\ndifferent datasets for the real-world image recognition tasks.", "time": "2021-06-01T07:15:31Z", "link": "http://arxiv.org/abs/2106.00275v1", "id": "2106.00275v1", "title": "H-FL: A Hierarchical Communication-Efficient and Privacy-Protected\n  Architecture for Federated Learning"}
{"author": "Mounssif Krouka, Anis Elgabli, Chaouki ben Issaid, Mehdi Bennis", "abstract": "Split-learning (SL) has recently gained popularity due to its inherent\nprivacy-preserving capabilities and ability to enable collaborative inference\nfor devices with limited computational power. Standard SL algorithms assume an\nideal underlying digital communication system and ignore the problem of scarce\ncommunication bandwidth. However, for a large number of agents, limited\nbandwidth resources, and time-varying communication channels, the communication\nbandwidth can become the bottleneck. To address this challenge, in this work,\nwe propose a novel SL framework to solve the remote inference problem that\nintroduces an additional layer at the agent side and constrains the choices of\nthe weights and the biases to ensure over the air aggregation. Hence, the\nproposed approach maintains constant communication cost with respect to the\nnumber of agents enabling remote inference under limited bandwidth. Numerical\nresults show that our proposed algorithm significantly outperforms the digital\nimplementation in terms of communication-efficiency, especially as the number\nof agents grows large.", "time": "2021-06-02T07:49:41Z", "link": "http://arxiv.org/abs/2106.00999v1", "id": "2106.00999v1", "title": "Communication-Efficient Split Learning Based on Analog Communication and\n  Over the Air Aggregation"}
{"author": "Hao Liu, Qian Gao, Jiang Li, Xiaochao Liao, Hao Xiong, Guangxing Chen, Wenlin Wang, Guobao Yang, Zhiwei Zha, Daxiang Dong, Dejing Dou, Haoyi Xiong", "abstract": "In modern internet industries, deep learning based recommender systems have\nbecame an indispensable building block for a wide spectrum of applications,\nsuch as search engine, news feed, and short video clips. However, it remains\nchallenging to carry the well-trained deep models for online real-time\ninference serving, with respect to the time-varying web-scale traffics from\nbillions of users, in a cost-effective manner. In this work, we present JIZHI -\na Model-as-a-Service system - that per second handles hundreds of millions of\nonline inference requests to huge deep models with more than trillions of\nsparse parameters, for over twenty real-time recommendation services at Baidu,\nInc. In JIZHI, the inference workflow of every recommendation request is\ntransformed to a Staged Event-Driven Pipeline (SEDP), where each node in the\npipeline refers to a staged computation or I/O intensive task processor. With\ntraffics of real-time inference requests arrived, each modularized processor\ncan be run in a fully asynchronized way and managed separately. Besides, JIZHI\nintroduces heterogeneous and hierarchical storage to further accelerate the\nonline inference process by reducing unnecessary computations and potential\ndata access latency induced by ultra-sparse model parameters. Moreover, an\nintelligent resource manager has been deployed to maximize the throughput of\nJIZHI over the shared infrastructure by searching the optimal resource\nallocation plan from historical logs and fine-tuning the load shedding policies\nover intermediate system feedback. Extensive experiments have been done to\ndemonstrate the advantages of JIZHI from the perspectives of end-to-end service\nlatency, system-wide throughput, and resource consumption. JIZHI has helped\nBaidu saved more than ten million US dollars in hardware and utility costs\nwhile handling 200% more traffics without sacrificing inference efficiency.", "time": "2021-06-03T08:23:24Z", "link": "http://arxiv.org/abs/2106.01674v1", "id": "2106.01674v1", "title": "JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale\n  Online Inference at Baidu"}
{"author": "Jianyu Wang, Zheng Xu, Zachary Garrett, Zachary Charles, Luyang Liu, Gauri Joshi", "abstract": "The federated learning (FL) framework trains a machine learning model using\ndecentralized data stored at edge client devices by periodically aggregating\nlocally trained models. Popular optimization algorithms of FL use vanilla\n(stochastic) gradient descent for both local updates at clients and global\nupdates at the aggregating server. Recently, adaptive optimization methods such\nas AdaGrad have been studied for server updates. However, the effect of using\nadaptive optimization methods for local updates at clients is not yet\nunderstood. We show in both theory and practice that while local adaptive\nmethods can accelerate convergence, they can cause a non-vanishing solution\nbias, where the final converged solution may be different from the stationary\npoint of the global objective function. We propose correction techniques to\novercome this inconsistency and complement the local adaptive methods for FL.\nExtensive experiments on realistic federated training tasks show that the\nproposed algorithms can achieve faster convergence and higher test accuracy\nthan the baselines without local adaptivity.", "time": "2021-06-04T07:36:59Z", "link": "http://arxiv.org/abs/2106.02305v1", "id": "2106.02305v1", "title": "Local Adaptivity in Federated Learning: Convergence and Consistency"}
{"author": "Emna Baccour, Fatima Haouari, Aiman Erbad, Amr Mohamed, Kashif Bilal, Mohsen Guizani, Mounir Hamdi", "abstract": "Crowdsourced live video streaming (livecast) services such as Facebook Live,\nYouNow, Douyu and Twitch are gaining more momentum recently. Allocating the\nlimited resources in a cost-effective manner while maximizing the Quality of\nService (QoS) through real-time delivery and the provision of the appropriate\nrepresentations for all viewers is a challenging problem. In our paper, we\nintroduce a machine-learning based predictive resource allocation framework for\ngeo-distributed cloud sites, considering the delay and quality constraints to\nguarantee the maximum QoS for viewers and the minimum cost for content\nproviders. First, we present an offline optimization that decides the required\ntranscoding resources in distributed regions near the viewers with a trade-off\nbetween the QoS and the overall cost. Second, we use machine learning to build\nforecasting models that proactively predict the approximate transcoding\nresources to be reserved at each cloud site ahead of time. Finally, we develop\na Greedy Nearest and Cheapest algorithm (GNCA) to perform the resource\nallocation of real-time broadcasted videos on the rented resources. Extensive\nsimulations have shown that GNCA outperforms the state-of-the art resource\nallocation approaches for crowdsourced live streaming by achieving more than\n20% gain in terms of system cost while serving the viewers with relatively\nlower latency.", "time": "2021-06-04T11:45:09Z", "link": "http://arxiv.org/abs/2106.02420v1", "id": "2106.02420v1", "title": "An Intelligent Resource Reservation for Crowdsourced Live Video\n  Streaming Applications in Geo-Distributed Cloud Environment"}
{"author": "Tommaso Diotalevi, Antonio Falabella, Barbara Martelli, Diego Michelotto, Lucia Morganti, Daniele Bonacorsi, Luca Giommi, Simone Rossi Tisbeni", "abstract": "The distributed Grid infrastructure for High Energy Physics experiments at\nthe Large Hadron Collider (LHC) in Geneva comprises a set of computing centres,\nspread all over the world, as part of the Worldwide LHC Computing Grid (WLCG).\nIn Italy, the Tier-1 functionalities are served by the INFN-CNAF data center,\nwhich provides also computing and storage resources to more than twenty non-LHC\nexperiments. For this reason, a high amount of logs are collected each day from\nvarious sources, which are highly heterogeneous and difficult to harmonize. In\nthis contribution, a working implementation of a system that collects, parses\nand displays the log information from CNAF data sources and the investigation\nof a Machine Learning based predictive maintenance system, is presented.", "time": "2021-05-13T10:21:55Z", "link": "http://arxiv.org/abs/2106.02612v1", "id": "2106.02612v1", "title": "Collection and harmonization of system logs and prototypal Analytics\n  services with the Elastic (ELK) suite at the INFN-CNAF computing centre"}
{"author": "Mher Safaryan, Rustem Islamov, Xun Qian, Peter RichtÃ¡rik", "abstract": "Inspired by recent work of Islamov et al (2021), we propose a family of\nFederated Newton Learn (FedNL) methods, which we believe is a marked step in\nthe direction of making second-order methods applicable to FL. In contrast to\nthe aforementioned work, FedNL employs a different Hessian learning technique\nwhich i) enhances privacy as it does not rely on the training data to be\nrevealed to the coordinating server, ii) makes it applicable beyond generalized\nlinear models, and iii) provably works with general contractive compression\noperators for compressing the local Hessians, such as Top-$K$ or Rank-$R$,\nwhich are vastly superior in practice. Notably, we do not need to rely on error\nfeedback for our methods to work with contractive compressors. Moreover, we\ndevelop FedNL-PP, FedNL-CR and FedNL-LS, which are variants of FedNL that\nsupport partial participation, and globalization via cubic regularization and\nline search, respectively, and FedNL-BC, which is a variant that can further\nbenefit from bidirectional compression of gradients and models, i.e., smart\nuplink gradient and smart downlink model compression. We prove local\nconvergence rates that are independent of the condition number, the number of\ntraining data points, and compression variance. Our communication efficient\nHessian learning technique provably learns the Hessian at the optimum. Finally,\nwe perform a variety of numerical experiments that show that our FedNL methods\nhave state-of-the-art communication complexity when compared to key baselines.", "time": "2022-05-22T16:18:49Z", "link": "http://arxiv.org/abs/2106.02969v2", "id": "2106.02969v2", "title": "FedNL: Making Newton-Type Methods Applicable to Federated Learning"}
{"author": "Nhuong V. Nguyen, Sybille Legitime", "abstract": "Extreme events are occurrences whose magnitude and potential cause extensive\ndamage on people, infrastructure, and the environment. Motivated by the extreme\nnature of the current global health landscape, which is plagued by the\ncoronavirus pandemic, we seek to better understand and model extreme events.\nModeling extreme events is common in practice and plays an important role in\ntime-series prediction applications. Our goal is to (i) compare and investigate\nthe effect of some common extreme events modeling methods to explore which\nmethod can be practical in reality and (ii) accelerate the deep learning\ntraining process, which commonly uses deep recurrent neural network (RNN), by\nimplementing the asynchronous local Stochastic Gradient Descent (SGD) framework\namong multiple compute nodes. In order to verify our distributed extreme events\nmodeling, we evaluate our proposed framework on a stock data set S\\&P500, with\na standard recurrent neural network. Our intuition is to explore the (best)\nextreme events modeling method which could work well under the distributed deep\nlearning setting. Moreover, by using asynchronous distributed learning, we aim\nto significantly reduce the communication cost among the compute nodes and\ncentral server, which is the main bottleneck of almost all distributed learning\nframeworks.\n  We implement our proposed work and evaluate its performance on representative\ndata sets, such as S&P500 stock in $5$-year period. The experimental results\nvalidate the correctness of the design principle and show a significant\ntraining duration reduction upto $8$x, compared to the baseline single compute\nnode. Our results also show that our proposed work can achieve the same level\nof test accuracy, compared to the baseline setting.", "time": "2021-06-10T22:04:36Z", "link": "http://arxiv.org/abs/2106.03211v2", "id": "2106.03211v2", "title": "Distributed Learning and its Application for Time-Series Prediction"}
{"author": "Shangdi Yu, Yiqiu Wang, Yan Gu, Laxman Dhulipala, Julian Shun", "abstract": "This paper studies the hierarchical clustering problem, where the goal is to\nproduce a dendrogram that represents clusters at varying scales of a data set.\nWe propose the ParChain framework for designing parallel hierarchical\nagglomerative clustering (HAC) algorithms, and using the framework we obtain\nnovel parallel algorithms for the complete linkage, average linkage, and Ward's\nlinkage criteria. Compared to most previous parallel HAC algorithms, which\nrequire quadratic memory, our new algorithms require only linear memory, and\nare scalable to large data sets. ParChain is based on our parallelization of\nthe nearest-neighbor chain algorithm, and enables multiple clusters to be\nmerged on every round. We introduce two key optimizations that are critical for\nefficiency: a range query optimization that reduces the number of distance\ncomputations required when finding nearest neighbors of clusters, and a caching\noptimization that stores a subset of previously computed distances, which are\nlikely to be reused. Experimentally, we show that our highly-optimized\nimplementations using 48 cores with two-way hyper-threading achieve 5.8--110.1x\nspeedup over state-of-the-art parallel HAC algorithms and achieve 13.75--54.23x\nself-relative speedup. Compared to state-of-the-art algorithms, our algorithms\nrequire up to 237.3x less space. Our algorithms are able to scale to data set\nsizes with tens of millions of points, which existing algorithms are not able\nto handle.", "time": "2022-02-14T17:58:36Z", "link": "http://arxiv.org/abs/2106.04727v3", "id": "2106.04727v3", "title": "ParChain: A Framework for Parallel Hierarchical Agglomerative Clustering\n  using Nearest-Neighbor Chain"}
{"author": "Artin Spiridonoff, Alex Olshevsky, Ioannis Ch. Paschalidis", "abstract": "We consider speeding up stochastic gradient descent (SGD) by parallelizing it\nacross multiple workers. We assume the same data set is shared among $N$\nworkers, who can take SGD steps and coordinate with a central server. While it\nis possible to obtain a linear reduction in the variance by averaging all the\nstochastic gradients at every step, this requires a lot of communication\nbetween the workers and the server, which can dramatically reduce the gains\nfrom parallelism. The Local SGD method, proposed and analyzed in the earlier\nliterature, suggests machines should make many local steps between such\ncommunications. While the initial analysis of Local SGD showed it needs $\\Omega\n( \\sqrt{T} )$ communications for $T$ local gradient steps in order for the\nerror to scale proportionately to $1/(NT)$, this has been successively improved\nin a string of papers, with the state of the art requiring $\\Omega \\left( N\n\\left( \\mbox{ poly} (\\log T) \\right) \\right)$ communications. In this paper, we\nsuggest a Local SGD scheme that communicates less overall by communicating less\nfrequently as the number of iterations grows. Our analysis shows that this can\nachieve an error that scales as $1/(NT)$ with a number of communications that\nis completely independent of $T$. In particular, we show that $\\Omega(N)$\ncommunications are sufficient. Empirical evidence suggests this bound is close\nto tight as we further show that $\\sqrt{N}$ or $N^{3/4}$ communications fail to\nachieve linear speed-up in simulations. Moreover, we show that under mild\nassumptions, the main of which is twice differentiability on any neighborhood\nof the optimal solution, one-shot averaging which only uses a single round of\ncommunication can also achieve the optimal convergence rate asymptotically.", "time": "2021-10-27T15:03:25Z", "link": "http://arxiv.org/abs/2106.04759v2", "id": "2106.04759v2", "title": "Communication-efficient SGD: From Local SGD to One-Shot Averaging"}
{"author": "Mi Luo, Fei Chen, Dapeng Hu, Yifan Zhang, Jian Liang, Jiashi Feng", "abstract": "A central challenge in training classification models in the real-world\nfederated system is learning with non-IID data. To cope with this, most of the\nexisting works involve enforcing regularization in local optimization or\nimproving the model aggregation scheme at the server. Other works also share\npublic datasets or synthesized samples to supplement the training of\nunder-represented classes or introduce a certain level of personalization.\nThough effective, they lack a deep understanding of how the data heterogeneity\naffects each layer of a deep classification model. In this paper, we bridge\nthis gap by performing an experimental analysis of the representations learned\nby different layers. Our observations are surprising: (1) there exists a\ngreater bias in the classifier than other layers, and (2) the classification\nperformance can be significantly improved by post-calibrating the classifier\nafter federated training. Motivated by the above findings, we propose a novel\nand simple algorithm called Classifier Calibration with Virtual Representations\n(CCVR), which adjusts the classifier using virtual representations sampled from\nan approximated gaussian mixture model. Experimental results demonstrate that\nCCVR achieves state-of-the-art performance on popular federated learning\nbenchmarks including CIFAR-10, CIFAR-100, and CINIC-10. We hope that our simple\nyet effective method can shed some light on the future research of federated\nlearning with non-IID data.", "time": "2021-10-28T12:22:22Z", "link": "http://arxiv.org/abs/2106.05001v2", "id": "2106.05001v2", "title": "No Fear of Heterogeneity: Classifier Calibration for Federated Learning\n  with Non-IID Data"}
{"author": "Peter Macgregor, He Sun", "abstract": "Local graph clustering is an important algorithmic technique for analysing\nmassive graphs, and has been widely applied in many research fields of data\nscience. While the objective of most (local) graph clustering algorithms is to\nfind a vertex set of low conductance, there has been a sequence of recent\nstudies that highlight the importance of the inter-connection between clusters\nwhen analysing real-world datasets. Following this line of research, in this\nwork we study local algorithms for finding a pair of vertex sets defined with\nrespect to their inter-connection and their relationship with the rest of the\ngraph. The key to our analysis is a new reduction technique that relates the\nstructure of multiple sets to a single vertex set in the reduced graph. Among\nmany potential applications, we show that our algorithms successfully recover\ndensely connected clusters in the Interstate Disputes Dataset and the US\nMigration Dataset.", "time": "2021-06-09T17:40:45Z", "link": "http://arxiv.org/abs/2106.05245v1", "id": "2106.05245v1", "title": "Local Algorithms for Finding Densely Connected Clusters"}
{"author": "Artur Podobas, Martin Svedin, Steven W. D. Chien, Ivy B. Peng, Naresh Balaji Ravichandran, Pawel Herman, Anders Lansner, Stefano Markidis", "abstract": "The modern deep learning method based on backpropagation has surged in\npopularity and has been used in multiple domains and application areas. At the\nsame time, there are other -- less-known -- machine learning algorithms with a\nmature and solid theoretical foundation whose performance remains unexplored.\nOne such example is the brain-like Bayesian Confidence Propagation Neural\nNetwork (BCPNN). In this paper, we introduce StreamBrain -- a framework that\nallows neural networks based on BCPNN to be practically deployed in\nHigh-Performance Computing systems. StreamBrain is a domain-specific language\n(DSL), similar in concept to existing machine learning (ML) frameworks, and\nsupports backends for CPUs, GPUs, and even FPGAs. We empirically demonstrate\nthat StreamBrain can train the well-known ML benchmark dataset MNIST within\nseconds, and we are the first to demonstrate BCPNN on STL-10 size networks. We\nalso show how StreamBrain can be used to train with custom floating-point\nformats and illustrate the impact of using different bfloat variations on BCPNN\nusing FPGAs.", "time": "2021-06-09T20:28:18Z", "link": "http://arxiv.org/abs/2106.05373v1", "id": "2106.05373v1", "title": "StreamBrain: An HPC Framework for Brain-like Neural Networks on CPUs,\n  GPUs and FPGAs"}
{"author": "Yeshwanth Venkatesha, Youngeun Kim, Leandros Tassiulas, Priyadarshini Panda", "abstract": "As neural networks get widespread adoption in resource-constrained embedded\ndevices, there is a growing need for low-power neural systems. Spiking Neural\nNetworks (SNNs)are emerging to be an energy-efficient alternative to the\ntraditional Artificial Neural Networks (ANNs) which are known to be\ncomputationally intensive. From an application perspective, as federated\nlearning involves multiple energy-constrained devices, there is a huge scope to\nleverage energy efficiency provided by SNNs. Despite its importance, there has\nbeen little attention on training SNNs on a large-scale distributed system like\nfederated learning. In this paper, we bring SNNs to a more realistic federated\nlearning scenario. Specifically, we propose a federated learning framework for\ndecentralized and privacy-preserving training of SNNs. To validate the proposed\nfederated learning framework, we experimentally evaluate the advantages of SNNs\non various aspects of federated learning with CIFAR10 and CIFAR100 benchmarks.\nWe observe that SNNs outperform ANNs in terms of overall accuracy by over 15%\nwhen the data is distributed across a large number of clients in the federation\nwhile providing up to5.3x energy efficiency. In addition to efficiency, we also\nanalyze the sensitivity of the proposed federated SNN framework to data\ndistribution among the clients, stragglers, and gradient noise and perform a\ncomprehensive comparison with ANNs.", "time": "2021-06-11T19:00:58Z", "link": "http://arxiv.org/abs/2106.06579v1", "id": "2106.06579v1", "title": "Federated Learning with Spiking Neural Networks"}
{"author": "Ekaterina Borodich, Aleksandr Beznosikov, Abdurakhmon Sadiev, Vadim Sushko, Nikolay Savelyev, Martin TakÃ¡Ä, Alexander Gasnikov", "abstract": "Personalized Federated Learning (PFL) has witnessed remarkable advancements,\nenabling the development of innovative machine learning applications that\npreserve the privacy of training data. However, existing theoretical research\nin this field has primarily focused on distributed optimization for\nminimization problems. This paper is the first to study PFL for saddle point\nproblems encompassing a broader range of optimization problems, that require\nmore than just solving minimization problems. In this work, we consider a\nrecently proposed PFL setting with the mixing objective function, an approach\ncombining the learning of a global model together with locally distributed\nlearners. Unlike most previous work, which considered only the centralized\nsetting, we work in a more general and decentralized setup that allows us to\ndesign and analyze more practical and federated ways to connect devices to the\nnetwork. We proposed new algorithms to address this problem and provide a\ntheoretical analysis of the smooth (strongly) convex-(strongly) concave saddle\npoint problems in stochastic and deterministic cases. Numerical experiments for\nbilinear problems and neural networks with adversarial noise demonstrate the\neffectiveness of the proposed methods.", "time": "2024-04-17T17:16:55Z", "link": "http://arxiv.org/abs/2106.07289v6", "id": "2106.07289v6", "title": "Decentralized Personalized Federated Learning for Min-Max Problems"}
{"author": "Jianlei Yang, Wenzhi Fu, Xingzhou Cheng, Xucheng Ye, Pengcheng Dai, Weisheng Zhao", "abstract": "Convolutional neural networks (CNNs) have achieved great success in\nperforming cognitive tasks. However, execution of CNNs requires a large amount\nof computing resources and generates heavy memory traffic, which imposes a\nsevere challenge on computing system design. Through optimizing parallel\nexecutions and data reuse in convolution, systolic architecture demonstrates\ngreat advantages in accelerating CNN computations. However, regular internal\ndata transmission path in traditional systolic architecture prevents the\nsystolic architecture from completely leveraging the benefits introduced by\nneural network sparsity. Deployment of fine-grained sparsity on the existing\nsystolic architectures is greatly hindered by the incurred computational\noverheads. In this work, we propose S2Engine $-$ a novel systolic architecture\nthat can fully exploit the sparsity in CNNs with maximized data reuse. S2Engine\ntransmits compressed data internally and allows each processing element to\ndynamically select an aligned data from the compressed dataflow in convolution.\nCompared to the naive systolic array, S2Engine achieves about $3.2\\times$ and\nabout $3.0\\times$ improvements on speed and energy efficiency, respectively.", "time": "2021-06-15T06:08:37Z", "link": "http://arxiv.org/abs/2106.07894v1", "id": "2106.07894v1", "title": "S2Engine: A Novel Systolic Architecture for Sparse Convolutional Neural\n  Networks"}
{"author": "Aleksandr Beznosikov, Pavel Dvurechensky, Anastasia Koloskova, Valentin Samokhin, Sebastian U Stich, Alexander Gasnikov", "abstract": "We consider distributed stochastic variational inequalities (VIs) on\nunbounded domains with the problem data that is heterogeneous (non-IID) and\ndistributed across many devices. We make a very general assumption on the\ncomputational network that, in particular, covers the settings of fully\ndecentralized calculations with time-varying networks and centralized\ntopologies commonly used in Federated Learning. Moreover, multiple local\nupdates on the workers can be made for reducing the communication frequency\nbetween the workers. We extend the stochastic extragradient method to this very\ngeneral setting and theoretically analyze its convergence rate in the\nstrongly-monotone, monotone, and non-monotone (when a Minty solution exists)\nsettings. The provided rates explicitly exhibit the dependence on network\ncharacteristics (e.g., mixing time), iteration counter, data heterogeneity,\nvariance, number of devices, and other standard parameters. As a special case,\nour method and analysis apply to distributed stochastic saddle-point problems\n(SPP), e.g., to the training of Deep Generative Adversarial Networks (GANs) for\nwhich decentralized training has been reported to be extremely challenging. In\nexperiments for the decentralized training of GANs we demonstrate the\neffectiveness of our proposed approach.", "time": "2023-04-02T10:30:33Z", "link": "http://arxiv.org/abs/2106.08315v3", "id": "2106.08315v3", "title": "Decentralized Local Stochastic Extra-Gradient for Variational\n  Inequalities"}
{"author": "Mateusz Malinowski, Dimitrios Vytiniotis, Grzegorz Swirszcz, Viorica Patraucean, Joao Carreira", "abstract": "How can neural networks be trained on large-volume temporal data efficiently?\nTo compute the gradients required to update parameters, backpropagation blocks\ncomputations until the forward and backward passes are completed. For temporal\nsignals, this introduces high latency and hinders real-time learning. It also\ncreates a coupling between consecutive layers, which limits model parallelism\nand increases memory consumption. In this paper, we build upon Sideways, which\navoids blocking by propagating approximate gradients forward in time, and we\npropose mechanisms for temporal integration of information based on different\nvariants of skip connections. We also show how to decouple computation and\ndelegate individual neural modules to different devices, allowing distributed\nand parallel training. The proposed Skip-Sideways achieves low latency\ntraining, model parallelism, and, importantly, is capable of extracting\ntemporal features, leading to more stable training and improved performance on\nreal-world action recognition video datasets such as HMDB51, UCF101, and the\nlarge-scale Kinetics-600. Finally, we also show that models trained with\nSkip-Sideways generate better future frames than Sideways models, and hence\nthey can better utilize motion cues.", "time": "2021-07-12T12:52:29Z", "link": "http://arxiv.org/abs/2106.08318v2", "id": "2106.08318v2", "title": "Gradient Forward-Propagation for Large-Scale Temporal Video Modelling"}
{"author": "Vincent Cohen-Addad, Silvio Lattanzi, Slobodan MitroviÄ, Ashkan Norouzi-Fard, Nikos Parotsidis, Jakub Tarnawski", "abstract": "Correlation clustering is a central topic in unsupervised learning, with many\napplications in ML and data mining. In correlation clustering, one receives as\ninput a signed graph and the goal is to partition it to minimize the number of\ndisagreements. In this work we propose a massively parallel computation (MPC)\nalgorithm for this problem that is considerably faster than prior work. In\nparticular, our algorithm uses machines with memory sublinear in the number of\nnodes in the graph and returns a constant approximation while running only for\na constant number of rounds. To the best of our knowledge, our algorithm is the\nfirst that can provably approximate a clustering problem on graphs using only a\nconstant number of MPC rounds in the sublinear memory regime. We complement our\nanalysis with an experimental analysis of our techniques.", "time": "2021-06-15T21:45:45Z", "link": "http://arxiv.org/abs/2106.08448v1", "id": "2106.08448v1", "title": "Correlation Clustering in Constant Many Parallel Rounds"}
{"author": "Anish Acharya, Abolfazl Hashemi, Prateek Jain, Sujay Sanghavi, Inderjit S. Dhillon, Ufuk Topcu", "abstract": "Geometric median (\\textsc{Gm}) is a classical method in statistics for\nachieving a robust estimation of the uncorrupted data; under gross corruption,\nit achieves the optimal breakdown point of 0.5. However, its computational\ncomplexity makes it infeasible for robustifying stochastic gradient descent\n(SGD) for high-dimensional optimization problems. In this paper, we show that\nby applying \\textsc{Gm} to only a judiciously chosen block of coordinates at a\ntime and using a memory mechanism, one can retain the breakdown point of 0.5\nfor smooth non-convex problems, with non-asymptotic convergence rates\ncomparable to the SGD with \\textsc{Gm}.", "time": "2021-06-16T15:55:50Z", "link": "http://arxiv.org/abs/2106.08882v1", "id": "2106.08882v1", "title": "Robust Training in High Dimensions via Block Coordinate Geometric Median\n  Descent"}
{"author": "Kate Donahue, Jon Kleinberg", "abstract": "Federated learning is a distributed learning paradigm where multiple agents,\neach only with access to local data, jointly learn a global model. There has\nrecently been an explosion of research aiming not only to improve the accuracy\nrates of federated learning, but also provide certain guarantees around social\ngood properties such as total error. One branch of this research has taken a\ngame-theoretic approach, and in particular, prior work has viewed federated\nlearning as a hedonic game, where error-minimizing players arrange themselves\ninto federating coalitions. This past work proves the existence of stable\ncoalition partitions, but leaves open a wide range of questions, including how\nfar from optimal these stable solutions are. In this work, we motivate and\ndefine a notion of optimality given by the average error rates among federating\nagents (players). First, we provide and prove the correctness of an efficient\nalgorithm to calculate an optimal (error minimizing) arrangement of players.\nNext, we analyze the relationship between the stability and optimality of an\narrangement. First, we show that for some regions of parameter space, all\nstable arrangements are optimal (Price of Anarchy equal to 1). However, we show\nthis is not true for all settings: there exist examples of stable arrangements\nwith higher cost than optimal (Price of Anarchy greater than 1). Finally, we\ngive the first constant-factor bound on the performance gap between stability\nand optimality, proving that the total error of the worst stable solution can\nbe no higher than 9 times the total error of an optimal solution (Price of\nAnarchy bound of 9).", "time": "2021-06-17T15:03:51Z", "link": "http://arxiv.org/abs/2106.09580v1", "id": "2106.09580v1", "title": "Optimality and Stability in Federated Learning: A Game-theoretic\n  Approach"}
{"author": "Luofeng Liao, Li Shen, Jia Duan, Mladen Kolar, Dacheng Tao", "abstract": "Large scale convex-concave minimax problems arise in numerous applications,\nincluding game theory, robust training, and training of generative adversarial\nnetworks. Despite their wide applicability, solving such problems efficiently\nand effectively is challenging in the presence of large amounts of data using\nexisting stochastic minimax methods. We study a class of stochastic minimax\nmethods and develop a communication-efficient distributed stochastic\nextragradient algorithm, LocalAdaSEG, with an adaptive learning rate suitable\nfor solving convex-concave minimax problems in the Parameter-Server model.\nLocalAdaSEG has three main features: (i) a periodic communication strategy that\nreduces the communication cost between workers and the server; (ii) an adaptive\nlearning rate that is computed locally and allows for tuning-free\nimplementation; and (iii) theoretically, a nearly linear speed-up with respect\nto the dominant variance term, arising from the estimation of the stochastic\ngradient, is proven in both the smooth and nonsmooth convex-concave settings.\nLocalAdaSEG is used to solve a stochastic bilinear game, and train a generative\nadversarial network. We compare LocalAdaSEG against several existing optimizers\nfor minimax problems and demonstrate its efficacy through several experiments\nin both homogeneous and heterogeneous settings.", "time": "2022-09-23T04:30:21Z", "link": "http://arxiv.org/abs/2106.10022v2", "id": "2106.10022v2", "title": "Local AdaGrad-Type Algorithm for Stochastic Convex-Concave Optimization"}
{"author": "Junyuan Hong, Haotao Wang, Zhangyang Wang, Jiayu Zhou", "abstract": "Federated learning (FL) emerges as a popular distributed learning schema that\nlearns a model from a set of participating users without sharing raw data. One\nmajor challenge of FL comes with heterogeneous users, who may have\ndistributionally different (or non-iid) data and varying computation resources.\nAs federated users would use the model for prediction, they often demand the\ntrained model to be robust against malicious attackers at test time. Whereas\nadversarial training (AT) provides a sound solution for centralized learning,\nextending its usage for federated users has imposed significant challenges, as\nmany users may have very limited training data and tight computational budgets,\nto afford the data-hungry and costly AT. In this paper, we study a novel FL\nstrategy: propagating adversarial robustness from rich-resource users that can\nafford AT, to those with poor resources that cannot afford it, during federated\nlearning. We show that existing FL techniques cannot be effectively integrated\nwith the strategy to propagate robustness among non-iid users and propose an\nefficient propagation approach by the proper use of batch-normalization. We\ndemonstrate the rationality and effectiveness of our method through extensive\nexperiments. Especially, the proposed method is shown to grant federated models\nremarkable robustness even when only a small portion of users afford AT during\nlearning. Source code will be released.", "time": "2022-07-07T14:45:28Z", "link": "http://arxiv.org/abs/2106.10196v2", "id": "2106.10196v2", "title": "Federated Robustness Propagation: Sharing Robustness in Heterogeneous\n  Federated Learning"}
{"author": "Eduard Gorbunov, Alexander Borzunov, Michael Diskin, Max Ryabinin", "abstract": "Many areas of deep learning benefit from using increasingly larger neural\nnetworks trained on public data, as is the case for pre-trained models for NLP\nand computer vision. Training such models requires a lot of computational\nresources (e.g., HPC clusters) that are not available to small research groups\nand independent researchers. One way to address it is for several smaller\ngroups to pool their computational resources together and train a model that\nbenefits all participants. Unfortunately, in this case, any participant can\njeopardize the entire training run by sending incorrect updates, deliberately\nor by mistake. Training in presence of such peers requires specialized\ndistributed training algorithms with Byzantine tolerance. These algorithms\noften sacrifice efficiency by introducing redundant communication or passing\nall updates through a trusted server, making it infeasible to apply them to\nlarge-scale deep learning, where models can have billions of parameters. In\nthis work, we propose a novel protocol for secure (Byzantine-tolerant)\ndecentralized training that emphasizes communication efficiency.", "time": "2023-01-02T03:24:04Z", "link": "http://arxiv.org/abs/2106.11257v4", "id": "2106.11257v4", "title": "Secure Distributed Training at Scale"}
{"author": "Feihu Huang, Junyi Li", "abstract": "In the paper, we propose an effective and efficient Compositional Federated\nLearning (ComFedL) algorithm for solving a new compositional Federated Learning\n(FL) framework, which frequently appears in many data mining and machine\nlearning problems with a hierarchical structure such as distributionally robust\nFL and model-agnostic meta learning (MAML). Moreover, we study the convergence\nanalysis of our ComFedL algorithm under some mild conditions, and prove that it\nachieves a convergence rate of $O(\\frac{1}{\\sqrt{T}})$, where $T$ denotes the\nnumber of iteration. To the best of our knowledge, our new Compositional FL\nframework is the first work to bridge federated learning with composition\nstochastic optimization. In particular, we first transform the distributionally\nrobust FL (i.e., a minimax optimization problem) into a simple composition\noptimization problem by using KL divergence regularization. At the same time,\nwe also first transform the distribution-agnostic MAML problem (i.e., a minimax\noptimization problem) into a simple yet effective composition optimization\nproblem. Finally, we apply two popular machine learning tasks, i.e.,\ndistributionally robust FL and MAML to demonstrate the effectiveness of our\nalgorithm.", "time": "2023-07-26T23:25:26Z", "link": "http://arxiv.org/abs/2106.11264v3", "id": "2106.11264v3", "title": "Compositional federated learning: Applications in distributionally\n  robust averaging and meta learning"}
{"author": "Sin Kit Lo, Qinghua Lu, Hye-Young Paik, Liming Zhu", "abstract": "Federated learning is an emerging machine learning paradigm that enables\nmultiple devices to train models locally and formulate a global model, without\nsharing the clients' local data. A federated learning system can be viewed as a\nlarge-scale distributed system, involving different components and stakeholders\nwith diverse requirements and constraints. Hence, developing a federated\nlearning system requires both software system design thinking and machine\nlearning knowledge. Although much effort has been put into federated learning\nfrom the machine learning perspectives, our previous systematic literature\nreview on the area shows that there is a distinct lack of considerations for\nsoftware architecture design for federated learning. In this paper, we propose\nFLRA, a reference architecture for federated learning systems, which provides a\ntemplate design for federated learning-based solutions. The proposed FLRA\nreference architecture is based on an extensive review of existing patterns of\nfederated learning systems found in the literature and existing industrial\nimplementation. The FLRA reference architecture consists of a pool of\narchitectural patterns that could address the frequently recurring design\nproblems in federated learning architectures. The FLRA reference architecture\ncan serve as a design guideline to assist architects and developers with\npractical solutions for their problems, which can be further customised.", "time": "2021-06-22T06:59:19Z", "link": "http://arxiv.org/abs/2106.11570v1", "id": "2106.11570v1", "title": "FLRA: A Reference Architecture for Federated Learning Systems"}
{"author": "Celestine Mendler-DÃ¼nner, Wenshuo Guo, Stephen Bates, Michael I. Jordan", "abstract": "An increasingly common setting in machine learning involves multiple parties,\neach with their own data, who want to jointly make predictions on future test\npoints. Agents wish to benefit from the collective expertise of the full set of\nagents to make better predictions than they would individually, but may not be\nwilling to release their data or model parameters. In this work, we explore a\ndecentralized mechanism to make collective predictions at test time, leveraging\neach agent's pre-trained model without relying on external validation, model\nretraining, or data pooling. Our approach takes inspiration from the literature\nin social science on human consensus-making. We analyze our mechanism\ntheoretically, showing that it converges to inverse meansquared-error (MSE)\nweighting in the large-sample limit. To compute error bars on the collective\npredictions we propose a decentralized Jackknife procedure that evaluates the\nsensitivity of our mechanism to a single agent's prediction. Empirically, we\ndemonstrate that our scheme effectively combines models with differing quality\nacross the input space. The proposed consensus prediction achieves significant\ngains over classical model averaging, and even outperforms weighted averaging\nschemes that have access to additional validation data.", "time": "2021-06-22T18:29:58Z", "link": "http://arxiv.org/abs/2106.12012v1", "id": "2106.12012v1", "title": "Test-time Collective Prediction"}
{"author": "Zeinab Khodaverdian, Hossein Sadr, Seyed Ahmad Edalatpanah, Mojdeh Nazari Solimandarabi", "abstract": "Cloud computing service models have experienced rapid growth and inefficient\nresource usage is known as one of the greatest causes of high energy\nconsumption in cloud data centers. Resource allocation in cloud data centers\naiming to reduce energy consumption has been conducted using live migration of\nVirtual Machines (VMs) and their consolidation into the small number of\nPhysical Machines (PMs). However, the selection of the appropriate VM for\nmigration is an important challenge. To solve this issue, VMs can be classified\naccording to the pattern of user requests into sensitive or insensitive classes\nto latency, and thereafter suitable VMs can be selected for migration. In this\npaper, the combination of Convolution Neural Network (CNN) and Gated Recurrent\nUnit (GRU) is utilized for the classification of VMs in the Microsoft Azure\ndataset. Due to the fact the majority of VMs in this dataset are labeled as\ninsensitive to latency, migration of more VMs in this group not only reduces\nenergy consumption but also decreases the violation of Service Level Agreements\n(SLA). Based on the empirical results, the proposed model obtained an accuracy\nof 95.18which clearly demonstrates the superiority of our proposed model\ncompared to other existing models.", "time": "2021-06-23T05:57:51Z", "link": "http://arxiv.org/abs/2106.12178v1", "id": "2106.12178v1", "title": "Combination of Convolutional Neural Network and Gated Recurrent Unit for\n  Energy Aware Resource Allocation"}
{"author": "Hua Huang, Fanhua Shang, Yuanyuan Liu, Hongying Liu", "abstract": "Federated Learning (FL) has become an active and promising distributed\nmachine learning paradigm. As a result of statistical heterogeneity, recent\nstudies clearly show that the performance of popular FL methods (e.g., FedAvg)\ndeteriorates dramatically due to the client drift caused by local updates. This\npaper proposes a novel Federated Learning algorithm (called IGFL), which\nleverages both Individual and Group behaviors to mimic distribution, thereby\nimproving the ability to deal with heterogeneity. Unlike existing FL methods,\nour IGFL can be applied to both client and server optimization. As a\nby-product, we propose a new attention-based federated learning in the server\noptimization of IGFL. To the best of our knowledge, this is the first time to\nincorporate attention mechanisms into federated optimization. We conduct\nextensive experiments and show that IGFL can significantly improve the\nperformance of existing federated learning methods. Especially when the\ndistributions of data among individuals are diverse, IGFL can improve the\nclassification accuracy by about 13% compared with prior baselines.", "time": "2021-06-23T10:42:37Z", "link": "http://arxiv.org/abs/2106.12300v1", "id": "2106.12300v1", "title": "Behavior Mimics Distribution: Combining Individual and Group Behaviors\n  for Federated Learning"}
{"author": "Mahmoud Hossam", "abstract": "Real-time remote sensing applications like search and rescue missions,\nmilitary target detection, environmental monitoring, hazard prevention and\nother time-critical applications require onboard real time processing\ncapabilities or autonomous decision making. Some unmanned remote systems like\nsatellites are physically remote from their operators, and all control of the\nspacecraft and data returned by the spacecraft must be transmitted over a\nwireless radio link. This link may not be available for extended periods when\nthe satellite is out of line of sight of its ground station. Therefore,\nlightweight, small size and low power consumption hardware is essential for\nonboard real time processing systems. With increasing dimensionality, size and\nresolution of recent hyperspectral imaging sensors, additional challenges are\nposed upon remote sensing processing systems and more capable computing\narchitectures are needed. Graphical Processing Units (GPUs) emerged as\npromising architecture for light weight high performance computing that can\naddress these computational requirements for onboard systems. The goal of this\nstudy is to build high performance methods for onboard hyperspectral analysis.\nWe propose accelerated methods for the well-known recursive hierarchical\nsegmentation (RHSEG) clustering method, using GPUs, hybrid multicore CPU with a\nGPU and hybrid multi-core CPU/GPU clusters. RHSEG is a method developed by the\nNational Aeronautics and Space Administration (NASA), which is designed to\nprovide rich classification information with several output levels. The\nachieved speedups by parallel solutions compared to CPU sequential\nimplementations are 21x for parallel single GPU and 240x for hybrid multi-node\ncomputer clusters with 16 computing nodes. The energy consumption is reduced to\n74% using a single GPU compared to the equivalent parallel CPU cluster.", "time": "2021-05-30T09:26:03Z", "link": "http://arxiv.org/abs/2106.12942v1", "id": "2106.12942v1", "title": "High Performance Hyperspectral Image Classification using Graphics\n  Processing Units"}
{"author": "Rachit Agarwal, Tanmay Thapliyal, Sandeep Kumar Shukla", "abstract": "Smart Contracts (SCs) in Ethereum can automate tasks and provide different\nfunctionalities to a user. Such automation is enabled by the `Turing-complete'\nnature of the programming language (Solidity) in which SCs are written. This\nalso opens up different vulnerabilities and bugs in SCs that malicious actors\nexploit to carry out malicious or illegal activities on the cryptocurrency\nplatform. In this work, we study the correlation between malicious activities\nand the vulnerabilities present in SCs and find that some malicious activities\nare correlated with certain types of vulnerabilities. We then develop and study\nthe feasibility of a scoring mechanism that corresponds to the severity of the\nvulnerabilities present in SCs to determine if it is a relevant feature to\nidentify suspicious SCs. We analyze the utility of severity score towards\ndetection of suspicious SCs using unsupervised machine learning (ML) algorithms\nacross different temporal granularities and identify behavioral changes. In our\nexperiments with on-chain SCs, we were able to find a total of 1094 benign SCs\nacross different granularities which behave similar to malicious SCs, with the\ninclusion of the smart contract vulnerability scores in the feature set.", "time": "2021-06-25T04:25:23Z", "link": "http://arxiv.org/abs/2106.13422v1", "id": "2106.13422v1", "title": "Vulnerability and Transaction behavior based detection of Malicious\n  Smart Contracts"}
{"author": "Xinwei Zhang, Xiangyi Chen, Mingyi Hong, Zhiwei Steven Wu, Jinfeng Yi", "abstract": "Providing privacy protection has been one of the primary motivations of\nFederated Learning (FL). Recently, there has been a line of work on\nincorporating the formal privacy notion of differential privacy with FL. To\nguarantee the client-level differential privacy in FL algorithms, the clients'\ntransmitted model updates have to be clipped before adding privacy noise. Such\nclipping operation is substantially different from its counterpart of gradient\nclipping in the centralized differentially private SGD and has not been\nwell-understood. In this paper, we first empirically demonstrate that the\nclipped FedAvg can perform surprisingly well even with substantial data\nheterogeneity when training neural networks, which is partly because the\nclients' updates become similar for several popular deep architectures. Based\non this key observation, we provide the convergence analysis of a differential\nprivate (DP) FedAvg algorithm and highlight the relationship between clipping\nbias and the distribution of the clients' updates. To the best of our\nknowledge, this is the first work that rigorously investigates theoretical and\nempirical issues regarding the clipping operation in FL algorithms.", "time": "2021-06-25T14:47:19Z", "link": "http://arxiv.org/abs/2106.13673v1", "id": "2106.13673v1", "title": "Understanding Clipping for Federated Learning: Convergence and\n  Client-Level Differential Privacy"}
{"author": "Leon Witt, Usama Zafar, KuoYeh Shen, Felix Sattler, Dan Li, Wojciech Samek", "abstract": "The recent advent of various forms of Federated Knowledge Distillation (FD)\npaves the way for a new generation of robust and communication-efficient\nFederated Learning (FL), where mere soft-labels are aggregated, rather than\nwhole gradients of Deep Neural Networks (DNN) as done in previous FL schemes.\nThis security-per-design approach in combination with increasingly performant\nInternet of Things (IoT) and mobile devices opens up a new realm of\npossibilities to utilize private data from industries as well as from\nindividuals as input for artificial intelligence model training. Yet in\nprevious FL systems, lack of trust due to the imbalance of power between\nworkers and a central authority, the assumption of altruistic worker\nparticipation and the inability to correctly measure and compare contributions\nof workers hinder this technology from scaling beyond small groups of already\nentrusted entities towards mass adoption. This work aims to mitigate the\naforementioned issues by introducing a novel decentralized federated learning\nframework where heavily compressed 1-bit soft-labels, resembling 1-hot label\npredictions, are aggregated on a smart contract. In a context where workers'\ncontributions are now easily comparable, we modify the Peer Truth Serum for\nCrowdsourcing mechanism (PTSC) for FD to reward honest participation based on\npeer consistency in an incentive compatible fashion. Due to heavy reductions of\nboth computational complexity and storage, our framework is a fully\non-blockchain FL system that is feasible on simple smart contracts and\ntherefore blockchain agnostic. We experimentally test our new framework and\nvalidate its theoretical properties.", "time": "2021-06-27T15:51:04Z", "link": "http://arxiv.org/abs/2106.14265v1", "id": "2106.14265v1", "title": "Reward-Based 1-bit Compressed Federated Distillation on Blockchain"}
{"author": "Berkay Turan, Cesar A. Uribe, Hoi-To Wai, Mahnoosh Alizadeh", "abstract": "In this paper, we propose a first-order distributed optimization algorithm\nthat is provably robust to Byzantine failures-arbitrary and potentially\nadversarial behavior, where all the participating agents are prone to failure.\nWe model each agent's state over time as a two-state Markov chain that\nindicates Byzantine or trustworthy behaviors at different time instants. We set\nno restrictions on the maximum number of Byzantine agents at any given time. We\ndesign our method based on three layers of defense: 1) temporal robust\naggregation, 2) spatial robust aggregation, and 3) gradient normalization. We\nstudy two settings for stochastic optimization, namely Sample Average\nApproximation and Stochastic Approximation. We provide convergence guarantees\nof our method for strongly convex and smooth non-convex cost functions.", "time": "2022-06-17T18:22:14Z", "link": "http://arxiv.org/abs/2106.14956v2", "id": "2106.14956v2", "title": "Robust Distributed Optimization With Randomly Corrupted Gradients"}
{"author": "Lili Su, Jiaming Xu, Pengkun Yang", "abstract": "Federated Learning (FL) is a promising decentralized learning framework and\nhas great potentials in privacy preservation and in lowering the computation\nload at the cloud. Recent work showed that FedAvg and FedProx - the two\nwidely-adopted FL algorithms - fail to reach the stationary points of the\nglobal optimization objective even for homogeneous linear regression problems.\nFurther, it is concerned that the common model learned might not generalize\nwell locally at all in the presence of heterogeneity.\n  In this paper, we analyze the convergence and statistical efficiency of\nFedAvg and FedProx, addressing the above two concerns. Our analysis is based on\nthe standard non-parametric regression in a reproducing kernel Hilbert space\n(RKHS), and allows for heterogeneous local data distributions and unbalanced\nlocal datasets. We prove that the estimation errors, measured in either the\nempirical norm or the RKHS norm, decay with a rate of 1/t in general and\nexponentially for finite-rank kernels. In certain heterogeneous settings, these\nupper bounds also imply that both FedAvg and FedProx achieve the optimal error\nrate. To further analytically quantify the impact of the heterogeneity at each\nclient, we propose and characterize a novel notion-federation gain, defined as\nthe reduction of the estimation error for a client to join the FL. We discover\nthat when the data heterogeneity is moderate, a client with limited local data\ncan benefit from a common model with a large federation gain. Numerical\nexperiments further corroborate our theoretical findings.", "time": "2022-02-15T05:49:09Z", "link": "http://arxiv.org/abs/2106.15216v2", "id": "2106.15216v2", "title": "A Non-parametric View of FedAvg and FedProx: Beyond Stationary Points"}
{"author": "Tao Luo, Mingen Pan, Pierre Tholoniat, Asaf Cidon, Roxana Geambasu, Mathias LÃ©cuyer", "abstract": "Machine learning (ML) models trained on personal data have been shown to leak\ninformation about users. Differential privacy (DP) enables model training with\na guaranteed bound on this leakage. Each new model trained with DP increases\nthe bound on data leakage and can be seen as consuming part of a global privacy\nbudget that should not be exceeded. This budget is a scarce resource that must\nbe carefully managed to maximize the number of successfully trained models.\n  We describe PrivateKube, an extension to the popular Kubernetes datacenter\norchestrator that adds privacy as a new type of resource to be managed\nalongside other traditional compute resources, such as CPU, GPU, and memory.\nThe abstractions we design for the privacy resource mirror those defined by\nKubernetes for traditional resources, but there are also major differences. For\nexample, traditional compute resources are replenishable while privacy is not:\na CPU can be regained after a model finishes execution while privacy budget\ncannot. This distinction forces a re-design of the scheduler. We present DPF\n(Dominant Private Block Fairness) -- a variant of the popular Dominant Resource\nFairness (DRF) algorithm -- that is geared toward the non-replenishable privacy\nresource but enjoys similar theoretical properties as DRF.\n  We evaluate PrivateKube and DPF on microbenchmarks and an ML workload on\nAmazon Reviews data. Compared to existing baselines, DPF allows training more\nmodels under the same global privacy guarantee. This is especially true for DPF\nover R\\'enyi DP, a highly composable form of DP.", "time": "2021-06-29T12:43:47Z", "link": "http://arxiv.org/abs/2106.15335v1", "id": "2106.15335v1", "title": "Privacy Budget Scheduling"}
{"author": "Xingcai Zhou, Le Chang, Pengfei Xu, Shaogao Lv", "abstract": "Communication efficiency and robustness are two major issues in modern\ndistributed learning framework. This is due to the practical situations where\nsome computing nodes may have limited communication power or may behave\nadversarial behaviors. To address the two issues simultaneously, this paper\ndevelops two communication-efficient and robust distributed learning algorithms\nfor convex problems. Our motivation is based on surrogate likelihood framework\nand the median and trimmed mean operations. Particularly, the proposed\nalgorithms are provably robust against Byzantine failures, and also achieve\noptimal statistical rates for strong convex losses and convex (non-smooth)\npenalties. For typical statistical models such as generalized linear models,\nour results show that statistical errors dominate optimization errors in finite\niterations. Simulated and real data experiments are conducted to demonstrate\nthe numerical performance of our algorithms.", "time": "2021-02-28T01:38:37Z", "link": "http://arxiv.org/abs/2103.00373v1", "id": "2103.00373v1", "title": "Communication-efficient Byzantine-robust distributed learning with\n  statistical guarantee"}
{"author": "Zhize Li, SlavomÃ­r Hanzely, Peter RichtÃ¡rik", "abstract": "We propose ZeroSARAH -- a novel variant of the variance-reduced method SARAH\n(Nguyen et al., 2017) -- for minimizing the average of a large number of\nnonconvex functions $\\frac{1}{n}\\sum_{i=1}^{n}f_i(x)$. To the best of our\nknowledge, in this nonconvex finite-sum regime, all existing variance-reduced\nmethods, including SARAH, SVRG, SAGA and their variants, need to compute the\nfull gradient over all $n$ data samples at the initial point $x^0$, and then\nperiodically compute the full gradient once every few iterations (for SVRG,\nSARAH and their variants). Note that SVRG, SAGA and their variants typically\nachieve weaker convergence results than variants of SARAH: $n^{2/3}/\\epsilon^2$\nvs. $n^{1/2}/\\epsilon^2$. Thus we focus on the variant of SARAH. The proposed\nZeroSARAH and its distributed variant D-ZeroSARAH are the \\emph{first}\nvariance-reduced algorithms which \\emph{do not require any full gradient\ncomputations}, not even for the initial point. Moreover, for both standard and\ndistributed settings, we show that ZeroSARAH and D-ZeroSARAH obtain new\nstate-of-the-art convergence results, which can improve the previous best-known\nresult (given by e.g., SPIDER, SARAH, and PAGE) in certain regimes. Avoiding\nany full gradient computations (which are time-consuming steps) is important in\nmany applications as the number of data samples $n$ usually is very large.\nEspecially in the distributed setting, periodic computation of full gradient\nover all data samples needs to periodically synchronize all\nclients/devices/machines, which may be impossible or unaffordable. Thus, we\nexpect that ZeroSARAH/D-ZeroSARAH will have a practical impact in distributed\nand federated learning where full device participation is impractical.", "time": "2021-10-10T19:03:07Z", "link": "http://arxiv.org/abs/2103.01447v3", "id": "2103.01447v3", "title": "ZeroSARAH: Efficient Nonconvex Finite-Sum Optimization with Zero Full\n  Gradient Computation"}
{"author": "Sebastian U. Stich, Amirkeivan Mohtashami, Martin Jaggi", "abstract": "It has been experimentally observed that the efficiency of distributed\ntraining with stochastic gradient (SGD) depends decisively on the batch size\nand -- in asynchronous implementations -- on the gradient staleness.\nEspecially, it has been observed that the speedup saturates beyond a certain\nbatch size and/or when the delays grow too large. We identify a data-dependent\nparameter that explains the speedup saturation in both these settings. Our\ncomprehensive theoretical analysis, for strongly convex, convex and non-convex\nsettings, unifies and generalized prior work directions that often focused on\nonly one of these two aspects. In particular, our approach allows us to derive\nimproved speedup results under frequently considered sparsity assumptions. Our\ninsights give rise to theoretically based guidelines on how the learning rates\ncan be adjusted in practice. We show that our results are tight and illustrate\nkey findings in numerical experiments.", "time": "2021-03-03T12:08:23Z", "link": "http://arxiv.org/abs/2103.02351v1", "id": "2103.02351v1", "title": "Critical Parameters for Scalable Distributed Learning with Large Batches\n  and Asynchronous Updates"}
{"author": "Max Ryabinin, Eduard Gorbunov, Vsevolod Plokhotnyuk, Gennady Pekhimenko", "abstract": "Training deep neural networks on large datasets can often be accelerated by\nusing multiple compute nodes. This approach, known as distributed training, can\nutilize hundreds of computers via specialized message-passing protocols such as\nRing All-Reduce. However, running these protocols at scale requires reliable\nhigh-speed networking that is only available in dedicated clusters. In\ncontrast, many real-world applications, such as federated learning and\ncloud-based distributed training, operate on unreliable devices with unstable\nnetwork bandwidth. As a result, these applications are restricted to using\nparameter servers or gossip-based averaging protocols. In this work, we lift\nthat restriction by proposing Moshpit All-Reduce - an iterative averaging\nprotocol that exponentially converges to the global average. We demonstrate the\nefficiency of our protocol for distributed optimization with strong theoretical\nguarantees. The experiments show 1.3x speedup for ResNet-50 training on\nImageNet compared to competitive gossip-based strategies and 1.5x speedup when\ntraining ALBERT-large from scratch using preemptible compute nodes.", "time": "2022-01-11T17:46:53Z", "link": "http://arxiv.org/abs/2103.03239v4", "id": "2103.03239v4", "title": "Moshpit SGD: Communication-Efficient Decentralized Training on\n  Heterogeneous Unreliable Devices"}
{"author": "Quoc Tran-Dinh, Nhan H. Pham, Dzung T. Phan, Lam M. Nguyen", "abstract": "We develop two new algorithms, called, FedDR and asyncFedDR, for solving a\nfundamental nonconvex composite optimization problem in federated learning. Our\nalgorithms rely on a novel combination between a nonconvex Douglas-Rachford\nsplitting method, randomized block-coordinate strategies, and asynchronous\nimplementation. They can also handle convex regularizers. Unlike recent methods\nin the literature, e.g., FedSplit and FedPD, our algorithms update only a\nsubset of users at each communication round, and possibly in an asynchronous\nmanner, making them more practical. These new algorithms can handle statistical\nand system heterogeneity, which are the two main challenges in federated\nlearning, while achieving the best known communication complexity. In fact, our\nnew algorithms match the communication complexity lower bound up to a constant\nfactor under standard assumptions. Our numerical experiments illustrate the\nadvantages of our methods over existing algorithms on synthetic and real\ndatasets.", "time": "2021-10-28T14:44:59Z", "link": "http://arxiv.org/abs/2103.03452v3", "id": "2103.03452v3", "title": "FedDR -- Randomized Douglas-Rachford Splitting Algorithms for Nonconvex\n  Federated Composite Optimization"}
{"author": "Zachary Charles, Jakub KoneÄnÃ½", "abstract": "We study a family of algorithms, which we refer to as local update methods,\ngeneralizing many federated and meta-learning algorithms. We prove that for\nquadratic models, local update methods are equivalent to first-order\noptimization on a surrogate loss we exactly characterize. Moreover, fundamental\nalgorithmic choices (such as learning rates) explicitly govern a trade-off\nbetween the condition number of the surrogate loss and its alignment with the\ntrue loss. We derive novel convergence rates showcasing these trade-offs and\nhighlight their importance in communication-limited settings. Using these\ninsights, we are able to compare local update methods based on their\nconvergence/accuracy trade-off, not just their convergence to critical points\nof the empirical loss. Our results shed new light on a broad range of\nphenomena, including the efficacy of server momentum in federated learning and\nthe impact of proximal client updates.", "time": "2021-03-08T19:40:32Z", "link": "http://arxiv.org/abs/2103.05032v1", "id": "2103.05032v1", "title": "Convergence and Accuracy Trade-Offs in Federated Learning and\n  Meta-Learning"}
{"author": "Arpita Gang, Bingqing Xiang, Waheed U. Bajwa", "abstract": "Principal Subspace Analysis (PSA) -- and its sibling, Principal Component\nAnalysis (PCA) -- is one of the most popular approaches for dimensionality\nreduction in signal processing and machine learning. But centralized PSA/PCA\nsolutions are fast becoming irrelevant in the modern era of big data, in which\nthe number of samples and/or the dimensionality of samples often exceed the\nstorage and/or computational capabilities of individual machines. This has led\nto the study of distributed PSA/PCA solutions, in which the data are\npartitioned across multiple machines and an estimate of the principal subspace\nis obtained through collaboration among the machines. It is in this vein that\nthis paper revisits the problem of distributed PSA/PCA under the general\nframework of an arbitrarily connected network of machines that lacks a central\nserver. The main contributions of the paper in this regard are threefold.\nFirst, two algorithms are proposed in the paper that can be used for\ndistributed PSA/PCA, with one in the case of data partitioned across samples\nand the other in the case of data partitioned across (raw) features. Second, in\nthe case of sample-wise partitioned data, the proposed algorithm and a variant\nof it are analyzed, and their convergence to the true subspace at linear rates\nis established. Third, extensive experiments on both synthetic and real-world\ndata are carried out to validate the usefulness of the proposed algorithms. In\nparticular, in the case of sample-wise partitioned data, an MPI-based\ndistributed implementation is carried out to study the interplay between\nnetwork topology and communications cost as well as to study the effects of\nstraggler machines on the proposed algorithms.", "time": "2021-10-12T18:01:16Z", "link": "http://arxiv.org/abs/2103.06406v3", "id": "2103.06406v3", "title": "Distributed Principal Subspace Analysis for Partitioned Big Data:\n  Algorithms, Analysis, and Implementation"}
{"author": "Zebang Shen, Hamed Hassani, Satyen Kale, Amin Karbasi", "abstract": "In this paper, we initiate a study of functional minimization in Federated\nLearning. First, in the semi-heterogeneous setting, when the marginal\ndistributions of the feature vectors on client machines are identical, we\ndevelop the federated functional gradient boosting (FFGB) method that provably\nconverges to the global minimum. Subsequently, we extend our results to the\nfully-heterogeneous setting (where marginal distributions of feature vectors\nmay differ) by designing an efficient variant of FFGB called FFGB.C, with\nprovable convergence to a neighborhood of the global minimum within a radius\nthat depends on the total variation distances between the client feature\ndistributions. For the special case of square loss, but still in the fully\nheterogeneous setting, we design the FFGB.L method that also enjoys provable\nconvergence to a neighborhood of the global minimum but within a radius\ndepending on the much tighter Wasserstein-1 distances. For both FFGB.C and\nFFGB.L, the radii of convergence shrink to zero as the feature distributions\nbecome more homogeneous. Finally, we conduct proof-of-concept experiments to\ndemonstrate the benefits of our approach against natural baselines.", "time": "2021-03-11T21:49:19Z", "link": "http://arxiv.org/abs/2103.06972v1", "id": "2103.06972v1", "title": "Federated Functional Gradient Boosting"}
{"author": "Nicholas Malecki, Hye-young Paik, Aleksandar Ignjatovic, Alan Blair, Elisa Bertino", "abstract": "Federated learning enables a global machine learning model to be trained\ncollaboratively by distributed, mutually non-trusting learning agents who\ndesire to maintain the privacy of their training data and their hardware. A\nglobal model is distributed to clients, who perform training, and submit their\nnewly-trained model to be aggregated into a superior model. However, federated\nlearning systems are vulnerable to interference from malicious learning agents\nwho may desire to prevent training or induce targeted misclassification in the\nresulting global model. A class of Byzantine-tolerant aggregation algorithms\nhas emerged, offering varying degrees of robustness against these attacks,\noften with the caveat that the number of attackers is bounded by some quantity\nknown prior to training. This paper presents Simeon: a novel approach to\naggregation that applies a reputation-based iterative filtering technique to\nachieve robustness even in the presence of attackers who can exhibit arbitrary\nbehaviour. We compare Simeon to state-of-the-art aggregation techniques and\nfind that Simeon achieves comparable or superior robustness to a variety of\nattacks. Notably, we show that Simeon is tolerant to sybil attacks, where other\nalgorithms are not, presenting a key advantage of our approach.", "time": "2021-03-13T12:17:47Z", "link": "http://arxiv.org/abs/2103.07704v1", "id": "2103.07704v1", "title": "Simeon -- Secure Federated Machine Learning Through Iterative Filtering"}
{"author": "Felippe V. Zacarias, Vinicius Petrucci, Rajiv Nishtala, Paul Carpenter, Daniel MossÃ©", "abstract": "Many HPC applications suffer from a bottleneck in the shared caches,\ninstruction execution units, I/O or memory bandwidth, even though the remaining\nresources may be underutilized. It is hard for developers and runtime systems\nto ensure that all critical resources are fully exploited by a single\napplication, so an attractive technique for increasing HPC system utilization\nis to colocate multiple applications on the same server. When applications\nshare critical resources, however, contention on shared resources may lead to\nreduced application performance.\n  In this paper, we show that server efficiency can be improved by first\nmodeling the expected performance degradation of colocated applications based\non measured hardware performance counters, and then exploiting the model to\ndetermine an optimized mix of colocated applications. This paper presents a new\nintelligent resource manager and makes the following contributions: (1) a new\nmachine learning model to predict the performance degradation of colocated\napplications based on hardware counters and (2) an intelligent scheduling\nscheme deployed on an existing resource manager to enable application\nco-scheduling with minimum performance degradation. Our results show that our\napproach achieves performance improvements of 7% (avg) and 12% (max) compared\nto the standard policy commonly used by existing job managers.", "time": "2021-03-16T12:35:35Z", "link": "http://arxiv.org/abs/2103.09019v1", "id": "2103.09019v1", "title": "Intelligent colocation of HPC workloads"}
{"author": "Avishek Ghosh, Raj Kumar Maity, Arya Mazumdar, Kannan Ramchandran", "abstract": "The problem of saddle-point avoidance for non-convex optimization is quite\nchallenging in large scale distributed learning frameworks, such as Federated\nLearning, especially in the presence of Byzantine workers. The celebrated\ncubic-regularized Newton method of \\cite{nest} is one of the most elegant ways\nto avoid saddle-points in the standard centralized (non-distributed) setup. In\nthis paper, we extend the cubic-regularized Newton method to a distributed\nframework and simultaneously address several practical challenges like\ncommunication bottleneck and Byzantine attacks. Note that the issue of\nsaddle-point avoidance becomes more crucial in the presence of Byzantine\nmachines since rogue machines may create \\emph{fake local minima} near the\nsaddle-points of the loss function, also known as the saddle-point attack.\nBeing a second order algorithm, our iteration complexity is much lower than the\nfirst order counterparts. Furthermore we use compression (or sparsification)\ntechniques like $\\delta$-approximate compression for communication efficiency.\nWe obtain theoretical guarantees for our proposed scheme under several settings\nincluding approximate (sub-sampled) gradients and Hessians. Moreover, we\nvalidate our theoretical findings with experiments using standard datasets and\nseveral types of Byzantine attacks, and obtain an improvement of $25\\%$ with\nrespect to first order methods in iteration complexity.", "time": "2021-12-25T05:11:50Z", "link": "http://arxiv.org/abs/2103.09424v2", "id": "2103.09424v2", "title": "Escaping Saddle Points in Distributed Newton's Method with Communication\n  Efficiency and Byzantine Resilience"}
{"author": "Chencheng Ye, Ying Cui", "abstract": "In this paper, we investigate unconstrained and constrained sample-based\nfederated optimization, respectively. For each problem, we propose a privacy\npreserving algorithm using stochastic successive convex approximation (SSCA)\ntechniques, and show that it can converge to a Karush-Kuhn-Tucker (KKT) point.\nTo the best of our knowledge, SSCA has not been used for solving federated\noptimization, and federated optimization with nonconvex constraints has not\nbeen investigated. Next, we customize the two proposed SSCA-based algorithms to\ntwo application examples, and provide closed-form solutions for the respective\napproximate convex problems at each iteration of SSCA. Finally, numerical\nexperiments demonstrate inherent advantages of the proposed algorithms in terms\nof convergence speed, communication cost and model specification.", "time": "2021-03-17T08:38:03Z", "link": "http://arxiv.org/abs/2103.09506v1", "id": "2103.09506v1", "title": "Sample-based Federated Learning via Mini-batch SSCA"}
{"author": "Vaikkunth Mugunthan, Vignesh Gokul, Lalana Kagal, Shlomo Dubnov", "abstract": "Federated Generative Adversarial Network (FedGAN) is a\ncommunication-efficient approach to train a GAN across distributed clients\nwithout clients having to share their sensitive training data. In this paper,\nwe experimentally show that FedGAN generates biased data points under\nnon-independent-and-identically-distributed (non-iid) settings. Also, we\npropose Bias-Free FedGAN, an approach to generate bias-free synthetic datasets\nusing FedGAN. Our approach generates metadata at the aggregator using the\nmodels received from clients and retrains the federated model to achieve\nbias-free results for image synthesis. Bias-Free FedGAN has the same\ncommunication cost as that of FedGAN. Experimental results on image datasets\n(MNIST and FashionMNIST) validate our claims.", "time": "2021-04-16T03:58:13Z", "link": "http://arxiv.org/abs/2103.09876v2", "id": "2103.09876v2", "title": "Bias-Free FedGAN: A Federated Approach to Generate Bias-Free Datasets"}
{"author": "Frank Po-Chen Lin, Seyyedali Hosseinalipour, Sheikh Shams Azam, Christopher G. Brinton, Nicolo Michelusi", "abstract": "Federated learning has emerged as a popular technique for distributing\nmachine learning (ML) model training across the wireless edge. In this paper,\nwe propose two timescale hybrid federated learning (TT-HF), a\nsemi-decentralized learning architecture that combines the conventional\ndevice-to-server communication paradigm for federated learning with\ndevice-to-device (D2D) communications for model training. In TT-HF, during each\nglobal aggregation interval, devices (i) perform multiple stochastic gradient\ndescent iterations on their individual datasets, and (ii) aperiodically engage\nin consensus procedure of their model parameters through cooperative,\ndistributed D2D communications within local clusters. With a new general\ndefinition of gradient diversity, we formally study the convergence behavior of\nTT-HF, resulting in new convergence bounds for distributed ML. We leverage our\nconvergence bounds to develop an adaptive control algorithm that tunes the step\nsize, D2D communication rounds, and global aggregation period of TT-HF over\ntime to target a sublinear convergence rate of O(1/t) while minimizing network\nresource utilization. Our subsequent experiments demonstrate that TT-HF\nsignificantly outperforms the current art in federated learning in terms of\nmodel accuracy and/or network energy consumption in different scenarios where\nlocal device datasets exhibit statistical heterogeneity. Finally, our numerical\nevaluations demonstrate robustness against outages caused by fading channels,\nas well favorable performance with non-convex loss functions.", "time": "2021-09-30T07:43:25Z", "link": "http://arxiv.org/abs/2103.10481v3", "id": "2103.10481v3", "title": "Semi-Decentralized Federated Learning with Cooperative D2D Local Model\n  Aggregations"}
{"author": "Shabnam Daghaghi, Nicholas Meisburger, Mengnan Zhao, Yong Wu, Sameh Gobriel, Charlie Tai, Anshumali Shrivastava", "abstract": "Deep learning implementations on CPUs (Central Processing Units) are gaining\nmore traction. Enhanced AI capabilities on commodity x86 architectures are\ncommercially appealing due to the reuse of existing hardware and virtualization\nease. A notable work in this direction is the SLIDE system. SLIDE is a C++\nimplementation of a sparse hash table based back-propagation, which was shown\nto be significantly faster than GPUs in training hundreds of million parameter\nneural models. In this paper, we argue that SLIDE's current implementation is\nsub-optimal and does not exploit several opportunities available in modern\nCPUs. In particular, we show how SLIDE's computations allow for a unique\npossibility of vectorization via AVX (Advanced Vector Extensions)-512.\nFurthermore, we highlight opportunities for different kinds of memory\noptimization and quantizations. Combining all of them, we obtain up to 7x\nspeedup in the computations on the same hardware. Our experiments are focused\non large (hundreds of millions of parameters) recommendation and NLP models.\nOur work highlights several novel perspectives and opportunities for\nimplementing randomized algorithms for deep learning on modern CPUs. We provide\nthe code and benchmark scripts at https://github.com/RUSH-LAB/SLIDE", "time": "2021-03-06T02:13:43Z", "link": "http://arxiv.org/abs/2103.10891v1", "id": "2103.10891v1", "title": "Accelerating SLIDE Deep Learning on Modern CPUs: Vectorization,\n  Quantizations, Memory Optimizations, and More"}
{"author": "Hongyi Zhang, Jan Bosch, Helena HolmstrÃ¶m Olsson", "abstract": "With the development and the increasing interests in ML/DL fields, companies\nare eager to apply Machine Learning/Deep Learning approaches to increase\nservice quality and customer experience. Federated Learning was implemented as\nan effective model training method for distributing and accelerating\ntime-consuming model training while protecting user data privacy. However,\ncommon Federated Learning approaches, on the other hand, use a synchronous\nprotocol to conduct model aggregation, which is inflexible and unable to adapt\nto rapidly changing environments and heterogeneous hardware settings in\nreal-world scenarios. In this paper, we present an approach to real-time\nend-to-end Federated Learning combined with a novel asynchronous model\naggregation protocol. Our method is validated in an industrial use case in the\nautomotive domain, focusing on steering wheel angle prediction for autonomous\ndriving. Our findings show that asynchronous Federated Learning can\nsignificantly improve the prediction performance of local edge models while\nmaintaining the same level of accuracy as centralized machine learning.\nFurthermore, by using a sliding training window, the approach can minimize\ncommunication overhead, accelerate model training speed and consume real-time\nstreaming data, proving high efficiency when deploying ML/DL components to\nheterogeneous real-world embedded systems.", "time": "2021-09-13T07:07:50Z", "link": "http://arxiv.org/abs/2103.11879v2", "id": "2103.11879v2", "title": "Real-time End-to-End Federated Learning: An Automotive Case Study"}
{"author": "Jiaao He, Jiezhong Qiu, Aohan Zeng, Zhilin Yang, Jidong Zhai, Jie Tang", "abstract": "Mixture-of-Expert (MoE) presents a strong potential in enlarging the size of\nlanguage model to trillions of parameters. However, training trillion-scale MoE\nrequires algorithm and system co-design for a well-tuned high performance\ndistributed training system. Unfortunately, the only existing platform that\nmeets the requirements strongly depends on Google's hardware (TPU) and software\n(Mesh Tensorflow) stack, and is not open and available to the public,\nespecially GPU and PyTorch communities.\n  In this paper, we present FastMoE, a distributed MoE training system based on\nPyTorch with common accelerators. The system provides a hierarchical interface\nfor both flexible model design and easy adaption to different applications,\nsuch as Transformer-XL and Megatron-LM. Different from direct implementation of\nMoE models using PyTorch, the training speed is highly optimized in FastMoE by\nsophisticated high-performance acceleration skills. The system supports placing\ndifferent experts on multiple GPUs across multiple nodes, enabling enlarging\nthe number of experts linearly against the number of GPUs. The source of\nFastMoE is available at https://github.com/laekov/fastmoe under Apache-2\nlicense.", "time": "2021-03-24T15:27:15Z", "link": "http://arxiv.org/abs/2103.13262v1", "id": "2103.13262v1", "title": "FastMoE: A Fast Mixture-of-Expert Training System"}
{"author": "Pavlos Papadopoulos, Will Abramson, Adam J. Hall, Nikolaos Pitropakis, William J. Buchanan", "abstract": "A common privacy issue in traditional machine learning is that data needs to\nbe disclosed for the training procedures. In situations with highly sensitive\ndata such as healthcare records, accessing this information is challenging and\noften prohibited. Luckily, privacy-preserving technologies have been developed\nto overcome this hurdle by distributing the computation of the training and\nensuring the data privacy to their owners. The distribution of the computation\nto multiple participating entities introduces new privacy complications and\nrisks. In this paper, we present a privacy-preserving decentralised workflow\nthat facilitates trusted federated learning among participants. Our\nproof-of-concept defines a trust framework instantiated using decentralised\nidentity technologies being developed under Hyperledger projects\nAries/Indy/Ursa. Only entities in possession of Verifiable Credentials issued\nfrom the appropriate authorities are able to establish secure, authenticated\ncommunication channels authorised to participate in a federated learning\nworkflow related to mental health data.", "time": "2021-03-30T15:07:01Z", "link": "http://arxiv.org/abs/2103.15753v2", "id": "2103.15753v2", "title": "Privacy and Trust Redefined in Federated Machine Learning"}
{"author": "Akihito Taya, Takayuki Nishio, Masahiro Morikura, Koji Yamamoto", "abstract": "This paper proposes a fully decentralized federated learning (FL) scheme for\nInternet of Everything (IoE) devices that are connected via multi-hop networks.\nBecause FL algorithms hardly converge the parameters of machine learning (ML)\nmodels, this paper focuses on the convergence of ML models in function spaces.\nConsidering that the representative loss functions of ML tasks e.g, mean\nsquared error (MSE) and Kullback-Leibler (KL) divergence, are convex\nfunctionals, algorithms that directly update functions in function spaces could\nconverge to the optimal solution. The key concept of this paper is to tailor a\nconsensus-based optimization algorithm to work in the function space and\nachieve the global optimum in a distributed manner. This paper first analyzes\nthe convergence of the proposed algorithm in a function space, which is\nreferred to as a meta-algorithm, and shows that the spectral graph theory can\nbe applied to the function space in a manner similar to that of numerical\nvectors. Then, consensus-based multi-hop federated distillation (CMFD) is\ndeveloped for a neural network (NN) to implement the meta-algorithm. CMFD\nleverages knowledge distillation to realize function aggregation among adjacent\ndevices without parameter averaging. An advantage of CMFD is that it works even\nwith different NN models among the distributed learners. Although CMFD does not\nperfectly reflect the behavior of the meta-algorithm, the discussion of the\nmeta-algorithm's convergence property promotes an intuitive understanding of\nCMFD, and simulation evaluations show that NN models converge using CMFD for\nseveral tasks. The simulation results also show that CMFD achieves higher\naccuracy than parameter aggregation for weakly connected networks, and CMFD is\nmore stable than parameter aggregation methods.", "time": "2022-10-04T01:29:10Z", "link": "http://arxiv.org/abs/2104.00352v4", "id": "2104.00352v4", "title": "Decentralized and Model-Free Federated Learning: Consensus-Based\n  Distillation in Function Space"}
{"author": "Daniele Romanini, Adam James Hall, Pavlos Papadopoulos, Tom Titcombe, Abbas Ismail, Tudor Cebere, Robert Sandmann, Robin Roehm, Michael A. Hoeh", "abstract": "We introduce PyVertical, a framework supporting vertical federated learning\nusing split neural networks. The proposed framework allows a data scientist to\ntrain neural networks on data features vertically partitioned across multiple\nowners while keeping raw data on an owner's device. To link entities shared\nacross different datasets' partitions, we use Private Set Intersection on IDs\nassociated with data points. To demonstrate the validity of the proposed\nframework, we present the training of a simple dual-headed split neural network\nfor a MNIST classification task, with data samples vertically distributed\nacross two data owners and a data scientist.", "time": "2021-04-14T08:05:04Z", "link": "http://arxiv.org/abs/2104.00489v3", "id": "2104.00489v3", "title": "PyVertical: A Vertical Federated Learning Framework for Multi-headed\n  SplitNN"}
{"author": "Alexander Renz-Wieland, Rainer Gemulla, Zoi Kaoudi, Volker Markl", "abstract": "Parameter servers (PSs) facilitate the implementation of distributed training\nfor large machine learning tasks. In this paper, we argue that existing PSs are\ninefficient for tasks that exhibit non-uniform parameter access; their\nperformance may even fall behind that of single node baselines. We identify two\nmajor sources of such non-uniform access: skew and sampling. Existing PSs are\nill-suited for managing skew because they uniformly apply the same parameter\nmanagement technique to all parameters. They are inefficient for sampling\nbecause the PS is oblivious to the associated randomized accesses and cannot\nexploit locality. To overcome these performance limitations, we introduce NuPS,\na novel PS architecture that (i) integrates multiple management techniques and\nemploys a suitable technique for each parameter and (ii) supports sampling\ndirectly via suitable sampling primitives and sampling schemes that allow for a\ncontrolled quality--efficiency trade-off. In our experimental study, NuPS\noutperformed existing PSs by up to one order of magnitude and provided up to\nlinear scalability across multiple machine learning tasks.", "time": "2022-03-28T07:36:27Z", "link": "http://arxiv.org/abs/2104.00501v3", "id": "2104.00501v3", "title": "NuPS: A Parameter Server for Machine Learning with Non-Uniform Parameter\n  Access"}
{"author": "Yiqiu Wang, Shangdi Yu, Yan Gu, Julian Shun", "abstract": "This paper presents new parallel algorithms for generating Euclidean minimum\nspanning trees and spatial clustering hierarchies (known as HDBSCAN$^*$). Our\napproach is based on generating a well-separated pair decomposition followed by\nusing Kruskal's minimum spanning tree algorithm and bichromatic closest pair\ncomputations. We introduce a new notion of well-separation to reduce the work\nand space of our algorithm for HDBSCAN$^*$. We also present a parallel\napproximate algorithm for OPTICS based on a recent sequential algorithm by Gan\nand Tao. Finally, we give a new parallel divide-and-conquer algorithm for\ncomputing the dendrogram and reachability plots, which are used in visualizing\nclusters of different scale that arise for both EMST and HDBSCAN$^*$. We show\nthat our algorithms are theoretically efficient: they have work (number of\noperations) matching their sequential counterparts, and polylogarithmic depth\n(parallel time).\n  We implement our algorithms and propose a memory optimization that requires\nonly a subset of well-separated pairs to be computed and materialized, leading\nto savings in both space (up to 10x) and time (up to 8x). Our experiments on\nlarge real-world and synthetic data sets using a 48-core machine show that our\nfastest algorithms outperform the best serial algorithms for the problems by\n11.13--55.89x, and existing parallel algorithms by at least an order of\nmagnitude.", "time": "2021-04-02T16:05:00Z", "link": "http://arxiv.org/abs/2104.01126v1", "id": "2104.01126v1", "title": "Fast Parallel Algorithms for Euclidean Minimum Spanning Tree and\n  Hierarchical Spatial Clustering"}
{"author": "Haijin Wang, Caomingzhe Si, Junhua Zhao", "abstract": "Non-intrusive load monitoring (NILM) aims at decomposing the total reading of\nthe household power consumption into appliance-wise ones, which is beneficial\nfor consumer behavior analysis as well as energy conservation. NILM based on\ndeep learning has been a focus of research. To train a better neural network,\nit is necessary for the network to be fed with massive data containing various\nappliances and reflecting consumer behavior habits. Therefore, data cooperation\namong utilities and DNOs (distributed network operators) who own the NILM data\nhas been increasingly significant. During the cooperation, however, risks of\nconsumer privacy leakage and losses of data control rights arise. To deal with\nthe problems above, a framework to improve the performance of NILM with\nfederated learning (FL) has been set up. In the framework, model weights\ninstead of the local data are shared among utilities. The global model is\ngenerated by weighted averaging the locally-trained model weights to gather the\nlocally-trained model information. Optimal model selection help choose the\nmodel which adapts to the data from different domains best. Experiments show\nthat this proposal improves the performance of local NILM runners. The\nperformance of this framework is close to that of the centrally-trained model\nobtained by the convergent data without privacy protection.", "time": "2021-04-04T14:24:50Z", "link": "http://arxiv.org/abs/2104.01618v1", "id": "2104.01618v1", "title": "A Federated Learning Framework for Non-Intrusive Load Monitoring"}
{"author": "Yao Deng, Tiehua Zhang, Guannan Lou, Xi Zheng, Jiong Jin, Qing-Long Han", "abstract": "The rapid development of artificial intelligence, especially deep learning\ntechnology, has advanced autonomous driving systems (ADSs) by providing precise\ncontrol decisions to counterpart almost any driving event, spanning from\nanti-fatigue safe driving to intelligent route planning. However, ADSs are\nstill plagued by increasing threats from different attacks, which could be\ncategorized into physical attacks, cyberattacks and learning-based adversarial\nattacks. Inevitably, the safety and security of deep learning-based autonomous\ndriving are severely challenged by these attacks, from which the\ncountermeasures should be analyzed and studied comprehensively to mitigate all\npotential risks. This survey provides a thorough analysis of different attacks\nthat may jeopardize ADSs, as well as the corresponding state-of-the-art defense\nmechanisms. The analysis is unrolled by taking an in-depth overview of each\nstep in the ADS workflow, covering adversarial attacks for various deep\nlearning models and attacks in both physical and cyber context. Furthermore,\nsome promising research directions are suggested in order to improve deep\nlearning-based autonomous driving safety, including model robustness training,\nmodel testing and verification, and anomaly detection based on cloud/edge\nservers.", "time": "2021-04-10T02:28:02Z", "link": "http://arxiv.org/abs/2104.01789v2", "id": "2104.01789v2", "title": "Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and\n  Defenses"}
{"author": "Yaosheng Fu, Evgeny Bolotin, Niladrish Chatterjee, David Nellans, Stephen W. Keckler", "abstract": "As GPUs scale their low precision matrix math throughput to boost deep\nlearning (DL) performance, they upset the balance between math throughput and\nmemory system capabilities. We demonstrate that converged GPU design trying to\naddress diverging architectural requirements between FP32 (or larger) based HPC\nand FP16 (or smaller) based DL workloads results in sub-optimal configuration\nfor either of the application domains. We argue that a Composable On-PAckage\nGPU (COPAGPU) architecture to provide domain-specialized GPU products is the\nmost practical solution to these diverging requirements. A COPA-GPU leverages\nmulti-chip-module disaggregation to support maximal design reuse, along with\nmemory system specialization per application domain. We show how a COPA-GPU\nenables DL-specialized products by modular augmentation of the baseline GPU\narchitecture with up to 4x higher off-die bandwidth, 32x larger on-package\ncache, 2.3x higher DRAM bandwidth and capacity, while conveniently supporting\nscaled-down HPC-oriented designs. This work explores the microarchitectural\ndesign necessary to enable composable GPUs and evaluates the benefits\ncomposability can provide to HPC, DL training, and DL inference. We show that\nwhen compared to a converged GPU design, a DL-optimized COPA-GPU featuring a\ncombination of 16x larger cache capacity and 1.6x higher DRAM bandwidth scales\nper-GPU training and inference performance by 31% and 35% respectively and\nreduces the number of GPU instances by 50% in scale-out training scenarios.", "time": "2021-04-05T23:06:50Z", "link": "http://arxiv.org/abs/2104.02188v1", "id": "2104.02188v1", "title": "GPU Domain Specialization via Composable On-Package Architecture"}
{"author": "Jinu Gong, Osvaldo Simeone, Joonhyuk Kang", "abstract": "Federated Bayesian learning offers a principled framework for the definition\nof collaborative training algorithms that are able to quantify epistemic\nuncertainty and to produce trustworthy decisions. Upon the completion of\ncollaborative training, an agent may decide to exercise her legal \"right to be\nforgotten\", which calls for her contribution to the jointly trained model to be\ndeleted and discarded. This paper studies federated learning and unlearning in\na decentralized network within a Bayesian framework. It specifically develops\nfederated variational inference (VI) solutions based on the decentralized\nsolution of local free energy minimization problems within exponential-family\nmodels and on local gossip-driven communication. The proposed protocols are\ndemonstrated to yield efficient unlearning mechanisms.", "time": "2021-04-08T15:18:35Z", "link": "http://arxiv.org/abs/2104.03834v1", "id": "2104.03834v1", "title": "Bayesian Variational Federated Learning and Unlearning in Decentralized\n  Networks"}
{"author": "Dennis Rieber, Axel Acosta, Holger FrÃ¶ning", "abstract": "The success of Deep Artificial Neural Networks (DNNs) in many domains created\na rich body of research concerned with hardware accelerators for\ncompute-intensive DNN operators. However, implementing such operators\nefficiently with complex hardware intrinsics such as matrix multiply is a task\nnot yet automated gracefully. Solving this task often requires joint program\nand data layout transformations. First solutions to this problem have been\nproposed, such as TVM, UNIT or ISAMIR, which work on a loop-level\nrepresentation of operators and specify data layout and possible program\ntransformations before the embedding into the operator is performed. This\ntop-down approach creates a tension between exploration range and search space\ncomplexity, especially when also exploring data layout transformations such as\nim2col, channel packing or padding.\n  In this work, we propose a new approach to this problem. We created a\nbottom-up method that allows the joint transformation of both compuation and\ndata layout based on the found embedding. By formulating the embedding as a\nconstraint satisfaction problem over the scalar dataflow, every possible\nembedding solution is contained in the search space. Adding additional\nconstraints and optmization targets to the solver generates the subset of\npreferable solutions.\n  An evaluation using the VTA hardware accelerator with the Baidu DeepBench\ninference benchmark shows that our approach can automatically generate code\ncompetitive to reference implementations. Further, we show that dynamically\ndetermining the data layout based on intrinsic and workload is beneficial for\nhardware utilization and performance. In cases where the reference\nimplementation has low hardware utilization due to its fixed deployment\nstrategy, we achieve a geomean speedup of up to x2.813, while individual\noperators can improve as much as x170.", "time": "2021-08-26T06:29:56Z", "link": "http://arxiv.org/abs/2104.04731v4", "id": "2104.04731v4", "title": "Joint Program and Layout Transformations to enable Convolutional\n  Operators on Specialized Hardware based on Constraint Programming"}
{"author": "Riyadh Baghdadi, Massinissa Merouani, Mohamed-Hicham Leghettas, Kamel Abdous, Taha Arbaoui, Karima Benatchba, Saman Amarasinghe", "abstract": "Enabling compilers to automatically optimize code has been a longstanding\ngoal for the compiler community. Efficiently solving this problem requires\nusing precise cost models. These models predict whether applying a sequence of\ncode transformations reduces the execution time of the program. Building an\nanalytical cost model to do so is hard in modern x86 architectures due to the\ncomplexity of the microarchitecture. In this paper, we present a novel deep\nlearning based cost model for automatic code optimization. This model was\nintegrated in a search method and implemented in the Tiramisu compiler to\nselect the best code transformations. The input of the proposed model is a set\nof simple features representing the unoptimized code and a sequence of code\ntransformations. The model predicts the speedup expected when the code\ntransformations are applied. Unlike previous models, the proposed one works on\nfull programs and does not rely on any heavy feature engineering. The proposed\nmodel has only 16% of mean absolute percentage error in predicting speedups on\nfull programs. The proposed model enables Tiramisu to automatically find code\ntransformations that match or are better than state-of-the-art compilers\nwithout requiring the same level of heavy feature engineering required by those\ncompilers.", "time": "2021-04-11T08:32:42Z", "link": "http://arxiv.org/abs/2104.04955v1", "id": "2104.04955v1", "title": "A Deep Learning Based Cost Model for Automatic Code Optimization"}
{"author": "Ji Liu, Ce Zhang", "abstract": "Scalable and efficient distributed learning is one of the main driving forces\nbehind the recent rapid advancement of machine learning and artificial\nintelligence. One prominent feature of this topic is that recent progresses\nhave been made by researchers in two communities: (1) the system community such\nas database, data management, and distributed systems, and (2) the machine\nlearning and mathematical optimization community. The interaction and knowledge\nsharing between these two communities has led to the rapid development of new\ndistributed learning systems and theory.\n  In this work, we hope to provide a brief introduction of some distributed\nlearning techniques that have recently been developed, namely lossy\ncommunication compression (e.g., quantization and sparsification), asynchronous\ncommunication, and decentralized communication. One special focus in this work\nis on making sure that it can be easily understood by researchers in both\ncommunities -- On the system side, we rely on a simplified system model hiding\nmany system details that are not necessary for the intuition behind the system\nspeedups; while, on the theory side, we rely on minimal assumptions and\nsignificantly simplify the proof of some recent work to achieve comparable\nresults.", "time": "2021-04-12T07:27:07Z", "link": "http://arxiv.org/abs/2104.05245v1", "id": "2104.05245v1", "title": "Distributed Learning Systems with First-order Methods"}
{"author": "Abdullatif Albaseer, Mohamed Abdallah, Ala Al-Fuqaha, Aiman Erbad", "abstract": "Federated edge learning (FEEL) is a promising distributed learning technique\nfor next-generation wireless networks. FEEL preserves the user's privacy,\nreduces the communication costs, and exploits the unprecedented capabilities of\nedge devices to train a shared global model by leveraging a massive amount of\ndata generated at the network edge. However, FEEL might significantly shorten\nenergy-constrained participating devices' lifetime due to the power consumed\nduring the model training round. This paper proposes a novel approach that\nendeavors to minimize computation and communication energy consumption during\nFEEL rounds to address this issue. First, we introduce a modified local\ntraining algorithm that intelligently selects only the samples that enhance the\nmodel's quality based on a predetermined threshold probability. Then, the\nproblem is formulated as joint energy minimization and resource allocation\noptimization problem to obtain the optimal local computation time and the\noptimal transmission time that minimize the total energy consumption\nconsidering the worker's energy budget, available bandwidth, channel states,\nbeamforming, and local CPU speed. After that, we introduce a tractable solution\nto the formulated problem that ensures the robustness of FEEL. Our simulation\nresults show that our solution substantially outperforms the baseline FEEL\nalgorithm as it reduces the local consumed energy by up to 79%.", "time": "2021-03-30T13:34:40Z", "link": "http://arxiv.org/abs/2104.05509v1", "id": "2104.05509v1", "title": "Threshold-Based Data Exclusion Approach for Energy-Efficient Federated\n  Edge Learning"}
{"author": "Tom Titcombe, Adam J. Hall, Pavlos Papadopoulos, Daniele Romanini", "abstract": "We describe a threat model under which a split network-based federated\nlearning system is susceptible to a model inversion attack by a malicious\ncomputational server. We demonstrate that the attack can be successfully\nperformed with limited knowledge of the data distribution by the attacker. We\npropose a simple additive noise method to defend against model inversion,\nfinding that the method can significantly reduce attack efficacy at an\nacceptable accuracy trade-off on MNIST. Furthermore, we show that NoPeekNN, an\nexisting defensive method, protects different information from exposure,\nsuggesting that a combined defence is necessary to fully protect private user\ndata.", "time": "2021-04-21T11:01:25Z", "link": "http://arxiv.org/abs/2104.05743v2", "id": "2104.05743v2", "title": "Practical Defences Against Model Inversion Attacks for Split Neural\n  Networks"}
{"author": "Corey J. Nolet, Divye Gala, Edward Raff, Joe Eaton, Brad Rees, John Zedlewski, Tim Oates", "abstract": "High-performance primitives for mathematical operations on sparse vectors\nmust deal with the challenges of skewed degree distributions and limits on\nmemory consumption that are typically not issues in dense operations. We\ndemonstrate that a sparse semiring primitive can be flexible enough to support\na wide range of critical distance measures while maintaining performance and\nmemory efficiency on the GPU. We further show that this primitive is a\nfoundational component for enabling many neighborhood-based information\nretrieval and machine learning algorithms to accept sparse input. To our\nknowledge, this is the first work aiming to unify the computation of several\ncritical distance measures on the GPU under a single flexible design paradigm\nand we hope that it provides a good baseline for future research in this area.\nOur implementation is fully open source and publicly available as part of the\nRAFT library of GPU-accelerated machine learning primitives\n(https://github.com/rapidsai/raft).", "time": "2022-03-05T02:09:55Z", "link": "http://arxiv.org/abs/2104.06357v2", "id": "2104.06357v2", "title": "GPU Semiring Primitives for Sparse Neighborhood Methods"}
{"author": "Heng Zhu, Qing Ling", "abstract": "Communication between workers and the master node to collect local stochastic\ngradients is a key bottleneck in a large-scale federated learning system.\nVarious recent works have proposed to compress the local stochastic gradients\nto mitigate the communication overhead. However, robustness to malicious\nattacks is rarely considered in such a setting. In this work, we investigate\nthe problem of Byzantine-robust compressed federated learning, where the\nattacks from Byzantine workers can be arbitrarily malicious. We theoretically\npoint out that different to the attacks-free compressed stochastic gradient\ndescent (SGD), its vanilla combination with geometric median-based robust\naggregation seriously suffers from the compression noise in the presence of\nByzantine attacks. In light of this observation, we propose to reduce the\ncompression noise with gradient difference compression so as to improve the\nByzantine-robustness. We also observe the impact of the intrinsic stochastic\nnoise caused by selecting random samples, and adopt the stochastic average\ngradient algorithm (SAGA) to gradually eliminate the inner variations of\nregular workers. We theoretically prove that the proposed algorithm reaches a\nneighborhood of the optimal solution at a linear convergence rate, and the\nasymptotic learning error is in the same order as that of the state-of-the-art\nuncompressed method. Finally, numerical experiments demonstrate the\neffectiveness of the proposed method.", "time": "2022-04-11T07:50:21Z", "link": "http://arxiv.org/abs/2104.06685v2", "id": "2104.06685v2", "title": "BROADCAST: Reducing Both Stochastic and Compression Noise to Robustify\n  Communication-Efficient Federated Learning"}
{"author": "Andreas Kurth, Fabian Schuiki, Luca Benini", "abstract": "This document presents implementations of fundamental convolutional neural\nnetwork (CNN) layers on the Manticore cluster-based many-core architecture and\ndiscusses their characteristics and trade-offs.", "time": "2021-04-16T10:07:28Z", "link": "http://arxiv.org/abs/2104.08009v1", "id": "2104.08009v1", "title": "Implementing CNN Layers on the Manticore Cluster-Based Many-Core\n  Architecture"}
{"author": "Suchita Pati, Shaizeen Aga, Nuwan Jayasena, Matthew D. Sinclair", "abstract": "Transfer learning in natural language processing (NLP), as realized using\nmodels like BERT (Bi-directional Encoder Representation from Transformer), has\nsignificantly improved language representation with models that can tackle\nchallenging language problems. Consequently, these applications are driving the\nrequirements of future systems. Thus, we focus on BERT, one of the most popular\nNLP transfer learning algorithms, to identify how its algorithmic behavior can\nguide future accelerator design. To this end, we carefully profile BERT\ntraining and identify key algorithmic behaviors which are worthy of attention\nin accelerator design.\n  We observe that while computations which manifest as matrix multiplication\ndominate BERT's overall runtime, as in many convolutional neural networks,\nmemory-intensive computations also feature prominently. We characterize these\ncomputations, which have received little attention so far. Further, we also\nidentify heterogeneity in compute-intensive BERT computations and discuss\nsoftware and possible hardware mechanisms to further optimize these\ncomputations. Finally, we discuss implications of these behaviors as networks\nget larger and use distributed training environments, and how techniques such\nas micro-batching and mixed-precision training scale. Overall, our analysis\nidentifies holistic solutions to optimize systems for BERT-like models.", "time": "2021-04-14T01:06:49Z", "link": "http://arxiv.org/abs/2104.08335v1", "id": "2104.08335v1", "title": "Demystifying BERT: Implications for Accelerator Design"}
{"author": "Shijian Li, Oren Mangoubi, Lijie Xu, Tian Guo", "abstract": "Stochastic Gradient Descent (SGD) has become the de facto way to train deep\nneural networks in distributed clusters. A critical factor in determining the\ntraining throughput and model accuracy is the choice of the parameter\nsynchronization protocol. For example, while Bulk Synchronous Parallel (BSP)\noften achieves better converged accuracy, the corresponding training throughput\ncan be negatively impacted by stragglers. In contrast, Asynchronous Parallel\n(ASP) can have higher throughput, but its convergence and accuracy can be\nimpacted by stale gradients. To improve the performance of synchronization\nprotocol, recent work often focuses on designing new protocols with a heavy\nreliance on hard-to-tune hyper-parameters. In this paper, we design a hybrid\nsynchronization approach that exploits the benefits of both BSP and ASP, i.e.,\nreducing training time while simultaneously maintaining the converged accuracy.\nBased on extensive empirical profiling, we devise a collection of adaptive\npolicies that determine how and when to switch between synchronization\nprotocols. Our policies include both offline ones that target recurring jobs\nand online ones for handling transient stragglers. We implement the proposed\npolicies in a prototype system, called Sync-Switch, on top of TensorFlow, and\nevaluate the training performance with popular deep learning models and\ndatasets. Our experiments show that Sync-Switch achieves up to 5.13X throughput\nspeedup and similar converged accuracy when comparing to BSP. Further, we\nobserve that Sync-Switch achieves 3.8% higher converged accuracy with just\n1.23X the training time compared to training with ASP. Moreover, Sync-Switch\ncan be used in settings when training with ASP leads to divergence errors.\nSync-Switch achieves all of these benefits with very low overhead, e.g., the\nframework overhead can be as low as 1.7% of the total training time.", "time": "2021-04-20T00:25:37Z", "link": "http://arxiv.org/abs/2104.08364v2", "id": "2104.08364v2", "title": "Sync-Switch: Hybrid Parameter Synchronization for Distributed Deep\n  Learning"}
{"author": "Sam Partee, Matthew Ellis, Alessandro Rigazzi, Scott Bachman, Gustavo Marques, Andrew Shao, Benjamin Robbins", "abstract": "We demonstrate the first climate-scale, numerical ocean simulations improved\nthrough distributed, online inference of Deep Neural Networks (DNN) using\nSmartSim. SmartSim is a library dedicated to enabling online analysis and\nMachine Learning (ML) for traditional HPC simulations. In this paper, we detail\nthe SmartSim architecture and provide benchmarks including online inference\nwith a shared ML model on heterogeneous HPC systems. We demonstrate the\ncapability of SmartSim by using it to run a 12-member ensemble of global-scale,\nhigh-resolution ocean simulations, each spanning 19 compute nodes, all\ncommunicating with the same ML architecture at each simulation timestep. In\ntotal, 970 billion inferences are collectively served by running the ensemble\nfor a total of 120 simulated years. Finally, we show our solution is stable\nover the full duration of the model integrations, and that the inclusion of\nmachine learning has minimal impact on the simulation runtimes.", "time": "2021-04-13T19:27:28Z", "link": "http://arxiv.org/abs/2104.09355v1", "id": "2104.09355v1", "title": "Using Machine Learning at Scale in HPC Simulations with SmartSim: An\n  Application to Ocean Climate Modeling"}
{"author": "Mario Almeida, Stefanos Laskaridis, Stylianos I. Venieris, Ilias Leontiadis, Nicholas D. Lane", "abstract": "Recently, there has been an explosive growth of mobile and embedded\napplications using convolutional neural networks(CNNs). To alleviate their\nexcessive computational demands, developers have traditionally resorted to\ncloud offloading, inducing high infrastructure costs and a strong dependence on\nnetworking conditions. On the other end, the emergence of powerful SoCs is\ngradually enabling on-device execution. Nonetheless, low- and mid-tier\nplatforms still struggle to run state-of-the-art CNNs sufficiently. In this\npaper, we present DynO, a distributed inference framework that combines the\nbest of both worlds to address several challenges, such as device\nheterogeneity, varying bandwidth and multi-objective requirements. Key\ncomponents that enable this are its novel CNN-specific data packing method,\nwhich exploits the variability of precision needs in different parts of the CNN\nwhen onloading computation, and its novel scheduler that jointly tunes the\npartition point and transferred data precision at run time to adapt inference\nto its execution environment. Quantitative evaluation shows that DynO\noutperforms the current state-of-the-art, improving throughput by over an order\nof magnitude over device-only execution and up to 7.9x over competing CNN\noffloading systems, with up to 60x less data transferred.", "time": "2022-01-11T10:11:50Z", "link": "http://arxiv.org/abs/2104.09949v2", "id": "2104.09949v2", "title": "DynO: Dynamic Onloading of Deep Neural Networks from Cloud to Device"}
{"author": "Saeedeh Parsaeefard, Sayed Ehsan Etesami, Alberto Leon Garcia", "abstract": "We present a novel weighted average model based on the mixture of experts\n(MoE) concept to provide robustness in Federated learning (FL) against the\npoisoned/corrupted/outdated local models. These threats along with the non-IID\nnature of data sets can considerably diminish the accuracy of the FL model. Our\nproposed MoE-FL setup relies on the trust between users and the server where\nthe users share a portion of their public data sets with the server. The server\napplies a robust aggregation method by solving the optimization problem or the\nSoftmax method to highlight the outlier cases and to reduce their adverse\neffect on the FL process. Our experiments illustrate that MoE-FL outperforms\nthe performance of the traditional aggregation approach for high rate of\npoisoned data from attackers.", "time": "2021-04-23T16:41:04Z", "link": "http://arxiv.org/abs/2104.11700v1", "id": "2104.11700v1", "title": "Robust Federated Learning by Mixture of Experts"}
{"author": "Kun Yuan, Yiming Chen, Xinmeng Huang, Yingya Zhang, Pan Pan, Yinghui Xu, Wotao Yin", "abstract": "The scale of deep learning nowadays calls for efficient distributed training\nalgorithms. Decentralized momentum SGD (DmSGD), in which each node averages\nonly with its neighbors, is more communication efficient than vanilla Parallel\nmomentum SGD that incurs global average across all computing nodes. On the\nother hand, the large-batch training has been demonstrated critical to achieve\nruntime speedup. This motivates us to investigate how DmSGD performs in the\nlarge-batch scenario.\n  In this work, we find the momentum term can amplify the inconsistency bias in\nDmSGD. Such bias becomes more evident as batch-size grows large and hence\nresults in severe performance degradation. We next propose DecentLaM, a novel\ndecentralized large-batch momentum SGD to remove the momentum-incurred bias.\nThe convergence rate for both non-convex and strongly-convex scenarios is\nestablished. Our theoretical results justify the superiority of DecentLaM to\nDmSGD especially in the large-batch scenario. Experimental results on a variety\nof computer vision tasks and models demonstrate that DecentLaM promises both\nefficient and high-quality training.", "time": "2021-04-24T16:21:01Z", "link": "http://arxiv.org/abs/2104.11981v1", "id": "2104.11981v1", "title": "DecentLaM: Decentralized Momentum SGD for Large-batch Deep Training"}
{"author": "Saeedeh Parsaeefard, Alberto Leon Garcia", "abstract": "Due to the explosion in size and complexity of modern data sets and privacy\nconcerns of data holders, it is increasingly important to be able to solve\nmachine learning problems in distributed manners. The Alternating Direction\nMethod of Multipliers (ADMM) through the concept of consensus variables is a\npractical algorithm in this context where its diverse variations and its\nperformance have been studied in different application areas. In this paper, we\nstudy the effect of the local data sets of users in the distributed learning of\nADMM. Our aim is to deploy variational inequality (VI) to attain an unified\nview of ADMM variations. Through the simulation results, we demonstrate how\nmore general definitions of consensus parameters and introducing the uncertain\nparameters in distribute approach can help to get the better results in\nlearning processes.", "time": "2021-04-26T14:16:14Z", "link": "http://arxiv.org/abs/2104.12608v1", "id": "2104.12608v1", "title": "Generalized ADMM in Distributed Learning via Variational Inequality"}
{"author": "Sohei Itahara, Takayuki Nishio, Koji Yamamoto", "abstract": "The distributed inference framework is an emerging technology for real-time\napplications empowered by cutting-edge deep machine learning (ML) on\nresource-constrained Internet of things (IoT) devices. In distributed\ninference, computational tasks are offloaded from the IoT device to other\ndevices or the edge server via lossy IoT networks. However, narrow-band and\nlossy IoT networks cause non-negligible packet losses and retransmissions,\nresulting in non-negligible communication latency. This study solves the\nproblem of the incremental retransmission latency caused by packet loss in a\nlossy IoT network. We propose a split inference with no retransmissions (SI-NR)\nmethod that achieves high accuracy without any retransmissions, even when\npacket loss occurs. In SI-NR, the key idea is to train the ML model by\nemulating the packet loss by a dropout method, which randomly drops the output\nof hidden units in a DNN layer. This enables the SI-NR system to obtain\nrobustness against packet losses. Our ML experimental evaluation reveals that\nSI-NR obtains accurate predictions without packet retransmission at a packet\nloss rate of 60%.", "time": "2021-04-28T08:28:22Z", "link": "http://arxiv.org/abs/2104.13629v1", "id": "2104.13629v1", "title": "Packet-Loss-Tolerant Split Inference for Delay-Sensitive Deep Learning\n  in Lossy Wireless Networks"}
{"author": "Alexander Brauckmann, AndrÃ©s Goens, Jeronimo Castrillon", "abstract": "The polyhedral model allows a structured way of defining semantics-preserving\ntransformations to improve the performance of a large class of loops. Finding\nprofitable points in this space is a hard problem which is usually approached\nby heuristics that generalize from domain-expert knowledge. Existing problem\nformulations in state-of-the-art heuristics depend on the shape of particular\nloops, making it hard to leverage generic and more powerful optimization\ntechniques from the machine learning domain. In this paper, we propose PolyGym,\na shape-agnostic formulation for the space of legal transformations in the\npolyhedral model as a Markov Decision Process (MDP). Instead of using\ntransformations, the formulation is based on an abstract space of possible\nschedules. In this formulation, states model partial schedules, which are\nconstructed by actions that are reusable across different loops. With a simple\nheuristic to traverse the space, we demonstrate that our formulation is\npowerful enough to match and outperform state-of-the-art heuristics. On the\nPolybench benchmark suite, we found transformations that led to a speedup of\n3.39x over LLVM O3, which is 1.83x better than the speedup achieved by ISL. Our\ngeneric MDP formulation enables using reinforcement learning to learn\noptimization policies over a wide range of loops. This also contributes to the\nemerging field of machine learning in compilers, as it exposes a novel problem\nformulation that can push the limits of existing methods.", "time": "2021-04-29T08:04:04Z", "link": "http://arxiv.org/abs/2104.13732v2", "id": "2104.13732v2", "title": "A Reinforcement Learning Environment for Polyhedral Optimizations"}
{"author": "Fan Mo, Hamed Haddadi, Kleomenis Katevas, Eduard Marin, Diego Perino, Nicolas Kourtellis", "abstract": "We propose and implement a Privacy-preserving Federated Learning ($PPFL$)\nframework for mobile systems to limit privacy leakages in federated learning.\nLeveraging the widespread presence of Trusted Execution Environments (TEEs) in\nhigh-end and mobile devices, we utilize TEEs on clients for local training, and\non servers for secure aggregation, so that model/gradient updates are hidden\nfrom adversaries. Challenged by the limited memory size of current TEEs, we\nleverage greedy layer-wise training to train each model's layer inside the\ntrusted area until its convergence. The performance evaluation of our\nimplementation shows that $PPFL$ can significantly improve privacy while\nincurring small system overheads at the client-side. In particular, $PPFL$ can\nsuccessfully defend the trained model against data reconstruction, property\ninference, and membership inference attacks. Furthermore, it can achieve\ncomparable model utility with fewer communication rounds (0.54$\\times$) and a\nsimilar amount of network traffic (1.002$\\times$) compared to the standard\nfederated learning of a complete model. This is achieved while only introducing\nup to ~15% CPU time, ~18% memory usage, and ~21% energy consumption overhead in\n$PPFL$'s client-side.", "time": "2021-06-28T20:51:12Z", "link": "http://arxiv.org/abs/2104.14380v2", "id": "2104.14380v2", "title": "PPFL: Privacy-preserving Federated Learning with Trusted Execution\n  Environments"}
{"author": "Daniel Garcia Bernal, Lodovico Giaretta, Sarunas Girdzijauskas, Magnus Sahlgren", "abstract": "Large scale contextual representation models have significantly advanced NLP\nin recent years, understanding the semantics of text to a degree never seen\nbefore. However, they need to process large amounts of data to achieve\nhigh-quality results. Joining and accessing all these data from multiple\nsources can be extremely challenging due to privacy and regulatory reasons.\nFederated Learning can solve these limitations by training models in a\ndistributed fashion, taking advantage of the hardware of the devices that\ngenerate the data. We show the viability of training NLP models, specifically\nWord2Vec, with the Federated Learning protocol. In particular, we focus on a\nscenario in which a small number of organizations each hold a relatively large\ncorpus. The results show that neither the quality of the results nor the\nconvergence time in Federated Word2Vec deteriorates as compared to centralised\nWord2Vec.", "time": "2021-04-19T15:39:02Z", "link": "http://arxiv.org/abs/2105.00831v1", "id": "2105.00831v1", "title": "Federated Word2Vec: Leveraging Federated Learning to Encourage\n  Collaborative Representation Learning"}
{"author": "Shuo Wan, Jiaxun Lu, Pingyi Fan, Yunfeng Shao, Chenghui Peng, Khaled B. letaief", "abstract": "Federated learning (FL) has recently emerged as an important and promising\nlearning scheme in IoT, enabling devices to jointly learn a model without\nsharing their raw data sets. However, as the training data in FL is not\ncollected and stored centrally, FL training requires frequent model exchange,\nwhich is largely affected by the wireless communication network. Therein,\nlimited bandwidth and random package loss restrict interactions in training.\nMeanwhile, the insufficient message synchronization among distributed clients\ncould also affect FL convergence. In this paper, we analyze the convergence\nrate of FL training considering the joint impact of communication network and\ntraining settings. Further by considering the training costs in terms of time\nand power, the optimal scheduling problems for communication networks are\nformulated. The developed theoretical results can be used to assist the system\nparameter selections and explain the principle of how the wireless\ncommunication system could influence the distributed training process and\nnetwork scheduling.", "time": "2021-04-30T02:33:29Z", "link": "http://arxiv.org/abs/2105.00872v1", "id": "2105.00872v1", "title": "Convergence Analysis and System Design for Federated Learning over\n  Wireless Networks"}
{"author": "Chengliang Zhang, Junzhe Xia, Baichen Yang, Huancheng Puyang, Wei Wang, Ruichuan Chen, Istemi Ekin Akkus, Paarijaat Aditya, Feng Yan", "abstract": "With the advancement of machine learning (ML) and its growing awareness, many\norganizations who own data but not ML expertise (data owner) would like to pool\ntheir data and collaborate with those who have expertise but need data from\ndiverse sources to train truly generalizable models (model owner). In such\ncollaborative ML, the data owner wants to protect the privacy of its training\ndata, while the model owner desires the confidentiality of the model and the\ntraining method which may contain intellectual properties. However, existing\nprivate ML solutions, such as federated learning and split learning, cannot\nmeet the privacy requirements of both data and model owners at the same time.\n  This paper presents Citadel, a scalable collaborative ML system that protects\nthe privacy of both data owner and model owner in untrusted infrastructures\nwith the help of Intel SGX. Citadel performs distributed training across\nmultiple training enclaves running on behalf of data owners and an aggregator\nenclave on behalf of the model owner. Citadel further establishes a strong\ninformation barrier between these enclaves by means of zero-sum masking and\nhierarchical aggregation to prevent data/model leakage during collaborative\ntraining. Compared with the existing SGX-protected training systems, Citadel\nenables better scalability and stronger privacy guarantees for collaborative\nML. Cloud deployment with various ML models shows that Citadel scales to a\nlarge number of enclaves with less than 1.73X slowdown caused by SGX.", "time": "2021-11-08T02:49:52Z", "link": "http://arxiv.org/abs/2105.01281v2", "id": "2105.01281v2", "title": "Citadel: Protecting Data Privacy and Model Confidentiality for\n  Collaborative Learning with SGX"}
{"author": "Pengcheng Ren, Tongjiang Yan", "abstract": "A decentralized federated learning architecture is proposed to apply to the\nBusinesses-to-Businesses scenarios by introducing the consortium blockchain in\nthis paper. We introduce a model verification mechanism to ensure the quality\nof local models trained by participators. To analyze the latency of the system,\na latency model is constructed by considering the work flow of the\narchitecture. Finally the experiment results show that our latency model does\nwell in quantifying the actual delays.", "time": "2021-05-10T03:14:52Z", "link": "http://arxiv.org/abs/2105.04087v1", "id": "2105.04087v1", "title": "Latency Analysis of Consortium Blockchained Federated Learning"}
{"author": "Baihe Huang, Xiaoxiao Li, Zhao Song, Xin Yang", "abstract": "Federated Learning (FL) is an emerging learning scheme that allows different\ndistributed clients to train deep neural networks together without data\nsharing. Neural networks have become popular due to their unprecedented\nsuccess. To the best of our knowledge, the theoretical guarantees of FL\nconcerning neural networks with explicit forms and multi-step updates are\nunexplored. Nevertheless, training analysis of neural networks in FL is\nnon-trivial for two reasons: first, the objective loss function we are\noptimizing is non-smooth and non-convex, and second, we are even not updating\nin the gradient direction. Existing convergence results for gradient\ndescent-based methods heavily rely on the fact that the gradient direction is\nused for updating. This paper presents a new class of convergence analysis for\nFL, Federated Learning Neural Tangent Kernel (FL-NTK), which corresponds to\noverparamterized ReLU neural networks trained by gradient descent in FL and is\ninspired by the analysis in Neural Tangent Kernel (NTK). Theoretically, FL-NTK\nconverges to a global-optimal solution at a linear rate with properly tuned\nlearning parameters. Furthermore, with proper distributional assumptions,\nFL-NTK can also achieve good generalization.", "time": "2021-05-11T13:05:53Z", "link": "http://arxiv.org/abs/2105.05001v1", "id": "2105.05001v1", "title": "FL-NTK: A Neural Tangent Kernel-based Framework for Federated Learning\n  Convergence Analysis"}
{"author": "Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi", "abstract": "Recent trend towards increasing large machine learning models require both\ntraining and inference tasks to be distributed. Considering the huge cost of\ntraining these models, it is imperative to unlock optimizations in computation\nand communication to obtain best performance. However, current logical\nseparation between computation and communication kernels in deep learning\nframeworks misses the optimization opportunities across such barrier. Breaking\nthis abstraction with a holistic consideration can provide many optimizations\nto provide performance improvements in distributed workloads. Manually applying\nthese optimizations needs modifications in underlying computation and\ncommunication libraries for each scenario, which is time consuming and\nerror-prone.\n  Therefore, we present CoCoNeT, with a DSL to express a program with both\ncomputation and communication. CoCoNeT contains several machine learning aware\ntransformations to optimize a program and a compiler to generate high\nperformance kernels. Providing both computation and communication as first\nclass constructs allows users to work on a high-level abstraction and apply\npowerful optimizations, such as fusion or overlapping of communication and\ncomputation. CoCoNeT enables us to optimize data-, model-and pipeline-parallel\nworkloads in large language models with only a few lines of code. Experiments\nshow CoCoNeT significantly outperforms state-of-the-art distributed machine\nlearning implementations.", "time": "2022-03-26T16:25:41Z", "link": "http://arxiv.org/abs/2105.05720v5", "id": "2105.05720v5", "title": "Breaking the Computation and Communication Abstraction Barrier in\n  Distributed Machine Learning Workloads"}
{"author": "Julian Matschinske, Julian SpÃ¤th, Reza Nasirigerdeh, Reihaneh Torkzadehmahani, Anne Hartebrodt, BalÃ¡zs OrbÃ¡n, SÃ¡ndor FejÃ©r, Olga Zolotareva, Mohammad Bakhtiari, BÃ©la Bihari, Marcus Bloice, Nina C Donner, Walid Fdhila, Tobias Frisch, Anne-Christin Hauschild, Dominik Heider, Andreas Holzinger, Walter HÃ¶tzendorfer, Jan Hospes, Tim Kacprowski, Markus Kastelitz, Markus List, Rudolf Mayer, MÃ³nika Moga, Heimo MÃ¼ller, Anastasia Pustozerova, Richard RÃ¶ttger, Anna Saranti, Harald HHW Schmidt, Christof Tschohl, Nina K Wenke, Jan Baumbach", "abstract": "Machine Learning (ML) and Artificial Intelligence (AI) have shown promising\nresults in many areas and are driven by the increasing amount of available\ndata. However, this data is often distributed across different institutions and\ncannot be shared due to privacy concerns. Privacy-preserving methods, such as\nFederated Learning (FL), allow for training ML models without sharing sensitive\ndata, but their implementation is time-consuming and requires advanced\nprogramming skills. Here, we present the FeatureCloud AI Store for FL as an\nall-in-one platform for biomedical research and other applications. It removes\nlarge parts of this complexity for developers and end-users by providing an\nextensible AI Store with a collection of ready-to-use apps. We show that the\nfederated apps produce similar results to centralized ML, scale well for a\ntypical number of collaborators and can be combined with Secure Multiparty\nComputation (SMPC), thereby making FL algorithms safely and easily applicable\nin biomedical and clinical environments.", "time": "2021-05-12T15:31:46Z", "link": "http://arxiv.org/abs/2105.05734v1", "id": "2105.05734v1", "title": "The FeatureCloud AI Store for Federated Learning in Biomedicine and\n  Beyond"}
{"author": "Yuchen Zhong, Cong Xie, Shuai Zheng, Haibin Lin", "abstract": "Communication overhead severely hinders the scalability of distributed\nmachine learning systems. Recently, there has been a growing interest in using\ngradient compression to reduce the communication overhead of the distributed\ntraining. However, there is little understanding of applying gradient\ncompression to adaptive gradient methods. Moreover, its performance benefits\nare often limited by the non-negligible compression overhead. In this paper, we\nfirst introduce a novel adaptive gradient method with gradient compression. We\nshow that the proposed method has a convergence rate of\n$\\mathcal{O}(1/\\sqrt{T})$ for non-convex problems. In addition, we develop a\nscalable system called BytePS-Compress for two-way compression, where the\ngradients are compressed in both directions between workers and parameter\nservers. BytePS-Compress pipelines the compression and decompression on CPUs\nand achieves a high degree of parallelism. Empirical evaluations show that we\nimprove the training time of ResNet50, VGG16, and BERT-base by 5.0%, 58.1%,\n23.3%, respectively, without any accuracy loss with 25 Gb/s networking.\nFurthermore, for training the BERT models, we achieve a compression rate of\n333x compared to the mixed-precision training.", "time": "2021-05-17T13:41:47Z", "link": "http://arxiv.org/abs/2105.07829v1", "id": "2105.07829v1", "title": "Compressed Communication for Distributed Training: Adaptive Methods and\n  System"}
{"author": "Kun Yuan, Sulaiman A. Alghunaim, Xinmeng Huang", "abstract": "We consider the decentralized stochastic optimization problems, where a\nnetwork of $n$ nodes, each owning a local cost function, cooperate to find a\nminimizer of the globally-averaged cost. A widely studied decentralized\nalgorithm for this problem is decentralized SGD (D-SGD), in which each node\naverages only with its neighbors. D-SGD is efficient in single-iteration\ncommunication, but it is very sensitive to the network topology. For smooth\nobjective functions, the transient stage (which measures the number of\niterations the algorithm has to experience before achieving the linear speedup\nstage) of D-SGD is on the order of ${\\Omega}(n/(1-\\beta)^2)$ and\n$\\Omega(n^3/(1-\\beta)^4)$ for strongly and generally convex cost functions,\nrespectively, where $1-\\beta \\in (0,1)$ is a topology-dependent quantity that\napproaches $0$ for a large and sparse network. Hence, D-SGD suffers from slow\nconvergence for large and sparse networks.\n  In this work, we study the non-asymptotic convergence property of the\nD$^2$/Exact-diffusion algorithm. By eliminating the influence of data\nheterogeneity between nodes, D$^2$/Exact-diffusion is shown to have an enhanced\ntransient stage that is on the order of $\\tilde{\\Omega}(n/(1-\\beta))$ and\n$\\Omega(n^3/(1-\\beta)^2)$ for strongly and generally convex cost functions,\nrespectively. Moreover, when D$^2$/Exact-diffusion is implemented with gradient\naccumulation and multi-round gossip communications, its transient stage can be\nfurther improved to $\\tilde{\\Omega}(1/(1-\\beta)^{\\frac{1}{2}})$ and\n$\\tilde{\\Omega}(n/(1-\\beta))$ for strongly and generally convex cost functions,\nrespectively. These established results for D$^2$/Exact-Diffusion have the best\n(i.e., weakest) dependence on network topology to our knowledge compared to\nexisting decentralized algorithms. We also conduct numerical simulations to\nvalidate our theories.", "time": "2022-03-03T14:46:20Z", "link": "http://arxiv.org/abs/2105.08023v2", "id": "2105.08023v2", "title": "Removing Data Heterogeneity Influence Enhances Network Topology\n  Dependence of Decentralized SGD"}
{"author": "Jiahui Geng, Neel Kanwal, Martin Gilje Jaatun, Chunming Rong", "abstract": "We have entered the era of big data, and it is considered to be the \"fuel\"\nfor the flourishing of artificial intelligence applications. The enactment of\nthe EU General Data Protection Regulation (GDPR) raises concerns about\nindividuals' privacy in big data. Federated learning (FL) emerges as a\nfunctional solution that can help build high-performance models shared among\nmultiple parties while still complying with user privacy and data\nconfidentiality requirements. Although FL has been intensively studied and used\nin real applications, there is still limited research related to its prospects\nand applications as a FLaaS (Federated Learning as a Service) to interested 3rd\nparties. In this paper, we present a FLaaS system: DID-eFed, where FL is\nfacilitated by decentralized identities (DID) and a smart contract. DID enables\na more flexible and credible decentralized access management in our system,\nwhile the smart contract offers a frictionless and less error-prone process. We\ndescribe particularly the scenario where our DID-eFed enables the FLaaS among\nhospitals and research institutions.", "time": "2021-05-19T07:44:07Z", "link": "http://arxiv.org/abs/2105.08671v2", "id": "2105.08671v2", "title": "DID-eFed: Facilitating Federated Learning as a Service with\n  Decentralized Identities"}
{"author": "Junxiang Wang, Hongyi Li, Zheng Chai, Yongchao Wang, Yue Cheng, Liang Zhao", "abstract": "While Graph Neural Networks (GNNs) are popular in the deep learning\ncommunity, they suffer from several challenges including over-smoothing,\nover-squashing, and gradient vanishing. Recently, a series of models have\nattempted to relieve these issues by first augmenting the node features and\nthen imposing node-wise functions based on Multi-Layer Perceptron (MLP), which\nare widely referred to as GA-MLP models. However, while GA-MLP models enjoy\ndeeper architectures for better accuracy, their efficiency largely\ndeteriorates. Moreover, popular acceleration techniques such as\nstochastic-version or data-parallelism cannot be effectively applied due to the\ndependency among samples (i.e., nodes) in graphs. To address these issues, in\nthis paper, instead of data parallelism, we propose a parallel graph deep\nlearning Alternating Direction Method of Multipliers (pdADMM-G) framework to\nachieve model parallelism: parameters in each layer of GA-MLP models can be\nupdated in parallel. The extended pdADMM-G-Q algorithm reduces communication\ncosts by introducing the quantization technique. Theoretical convergence to a\n(quantized) stationary point of the pdADMM-G algorithm and the pdADMM-G-Q\nalgorithm is provided with a sublinear convergence rate $o(1/k)$, where $k$ is\nthe number of iterations. Extensive experiments demonstrate the convergence of\ntwo proposed algorithms. Moreover, they lead to a more massive speedup and\nbetter performance than all state-of-the-art comparison methods on nine\nbenchmark datasets. Last but not least, the proposed pdADMM-G-Q algorithm\nreduces communication overheads by up to $45\\%$ without loss of performance.\nOur code is available at \\url{https://github.com/xianggebenben/pdADMM-G}.", "time": "2022-11-17T02:23:25Z", "link": "http://arxiv.org/abs/2105.09837v2", "id": "2105.09837v2", "title": "Towards Quantized Model Parallelism for Graph-Augmented MLPs Based on\n  Gradient-Free ADMM Framework"}
{"author": "Huanding Zhang, Tao Shen, Fei Wu, Mingyang Yin, Hongxia Yang, Chao Wu", "abstract": "Graph neural networks (GNN) have been successful in many fields, and derived\nvarious researches and applications in real industries. However, in some\nprivacy sensitive scenarios (like finance, healthcare), training a GNN model\ncentrally faces challenges due to the distributed data silos. Federated\nlearning (FL) is a an emerging technique that can collaboratively train a\nshared model while keeping the data decentralized, which is a rational solution\nfor distributed GNN training. We term it as federated graph learning (FGL).\nAlthough FGL has received increasing attention recently, the definition and\nchallenges of FGL is still up in the air. In this position paper, we present a\ncategorization to clarify it. Considering how graph data are distributed among\nclients, we propose four types of FGL: inter-graph FL, intra-graph FL and\ngraph-structured FL, where intra-graph is further divided into horizontal and\nvertical FGL. For each type of FGL, we make a detailed discussion about the\nformulation and applications, and propose some potential challenges.", "time": "2021-05-24T05:39:24Z", "link": "http://arxiv.org/abs/2105.11099v1", "id": "2105.11099v1", "title": "Federated Graph Learning -- A Position Paper"}
{"author": "Yasmin SarcheshmehPour, Yu Tian, Linli Zhang, Alexander Jung", "abstract": "We study optimization methods to train local (or personalized) models for\ndecentralized collections of local datasets with an intrinsic network\nstructure. This network structure arises from domain-specific notions of\nsimilarity between local datasets. Examples for such notions include\nspatio-temporal proximity, statistical dependencies or functional relations.\nOur main conceptual contribution is to formulate federated learning as\ngeneralized total variation (GTV) minimization. This formulation unifies and\nconsiderably extends existing federated learning methods. It is highly flexible\nand can be combined with a broad range of parametric models, including\ngeneralized linear models or deep neural networks. Our main algorithmic\ncontribution is a fully decentralized federated learning algorithm. This\nalgorithm is obtained by applying an established primal-dual method to solve\nGTV minimization. It can be implemented as message passing and is robust\nagainst inexact computations that arise from limited computational resources\nincluding processing time or bandwidth. Our main analytic contribution is an\nupper bound on the deviation between the local model parameters learnt by our\nalgorithm and an oracle-based clustered federated learning method. This upper\nbound reveals conditions on the local models and the network structure of local\ndatasets such that GTV minimization is able to pool (nearly) homogeneous local\ndatasets.", "time": "2023-06-18T17:14:37Z", "link": "http://arxiv.org/abs/2105.12769v4", "id": "2105.12769v4", "title": "Clustered Federated Learning via Generalized Total Variation\n  Minimization"}
{"author": "Yanqi Zhang, Weizhe Hua, Zhuangzhuang Zhou, Edward Suh, Christina Delimitrou", "abstract": "Cloud applications are increasingly shifting from large monolithic services,\nto large numbers of loosely-coupled, specialized microservices. Despite their\nadvantages in terms of facilitating development, deployment, modularity, and\nisolation, microservices complicate resource management, as dependencies\nbetween them introduce backpressure effects and cascading QoS violations.\n  We present Sinan, a data-driven cluster manager for interactive cloud\nmicroservices that is online and QoS-aware. Sinan leverages a set of scalable\nand validated machine learning models to determine the performance impact of\ndependencies between microservices, and allocate appropriate resources per tier\nin a way that preserves the end-to-end tail latency target. We evaluate Sinan\nboth on dedicated local clusters and large-scale deployments on Google Compute\nEngine (GCE) across representative end-to-end applications built with\nmicroservices, such as social networks and hotel reservation sites. We show\nthat Sinan always meets QoS, while also maintaining cluster utilization high,\nin contrast to prior work which leads to unpredictable performance or\nsacrifices resource efficiency. Furthermore, the techniques in Sinan are\nexplainable, meaning that cloud operators can yield insights from the ML models\non how to better deploy and design their applications to reduce unpredictable\nperformance.", "time": "2021-05-27T19:57:51Z", "link": "http://arxiv.org/abs/2105.13424v1", "id": "2105.13424v1", "title": "Sinan: Data-Driven, QoS-Aware Cluster Management for Microservices"}
{"author": "Kaustubh Shivdikar", "abstract": "Sparse matrices, more specifically SpGEMM kernels, are commonly found in a\nwide range of applications, spanning graph-based path-finding to machine\nlearning algorithms (e.g., neural networks). A particular challenge in\nimplementing SpGEMM kernels has been the pressure placed on DRAM memory. One\napproach to tackle this problem is to use an inner product method for the\nSpGEMM kernel implementation. While the inner product produces fewer\nintermediate results, it can end up saturating the memory bandwidth, given the\nhigh number of redundant fetches of the input matrix elements. Using an outer\nproduct-based SpGEMM kernel can reduce redundant fetches, but at the cost of\nincreased overhead due to extra computation and memory accesses for\nproducing/managing partial products.\n  In this thesis, we introduce a novel SpGEMM kernel implementation based on\nthe row-wise product approach. We leverage atomic instructions to merge\nintermediate partial products as they are generated. The use of atomic\ninstructions eliminates the need to create partial product matrices.\n  To evaluate our row-wise product approach, we map an optimized SpGEMM kernel\nto a custom accelerator designed to accelerate graph-based applications. The\ntargeted accelerator is an experimental system named PIUMA, being developed by\nIntel. PIUMA provides several attractive features, including fast context\nswitching, user-configurable caches, globally addressable memory, non-coherent\ncaches, and asynchronous pipelines. We tailor our SpGEMM kernel to exploit many\nof the features of the PIUMA fabric.\n  This thesis compares our SpGEMM implementation against prior solutions, all\nmapped to the PIUMA framework. We briefly describe some of the PIUMA\narchitecture features and then delve into the details of our optimized SpGEMM\nkernel. Our SpGEMM kernel can achieve 9.4x speedup as compared to competing\napproaches.", "time": "2021-05-29T00:22:50Z", "link": "http://arxiv.org/abs/2105.14156v1", "id": "2105.14156v1", "title": "SMASH: Sparse Matrix Atomic Scratchpad Hashing"}
{"author": "Zhengda Bian, Qifan Xu, Boxiang Wang, Yang You", "abstract": "The recent Natural Language Processing techniques have been refreshing the\nstate-of-the-art performance at an incredible speed. Training huge language\nmodels is therefore an imperative demand in both industry and academy. However,\nhuge language models impose challenges to both hardware and software. Graphical\nprocessing units (GPUs) are iterated frequently to meet the exploding demand,\nand a variety of ASICs like TPUs are spawned. However, there is still a tension\nbetween the fast growth of the extremely huge models and the fact that Moore's\nlaw is approaching the end. To this end, many model parallelism techniques are\nproposed to distribute the model parameters to multiple devices, so as to\nalleviate the tension on both memory and computation. Our work is the first to\nintroduce a 3-dimensional model parallelism for expediting huge language\nmodels. By reaching a perfect load balance, our approach presents smaller\nmemory and communication cost than existing state-of-the-art 1-D and 2-D model\nparallelism. Our experiments on 64 TACC's V100 GPUs show that our 3-D\nparallelism outperforms the 1-D and 2-D parallelism with 2.32x and 1.57x\nspeedup, respectively.", "time": "2021-05-30T07:41:08Z", "link": "http://arxiv.org/abs/2105.14450v1", "id": "2105.14450v1", "title": "Maximizing Parallelism in Distributed Training for Huge Neural Networks"}
{"author": "Huanle Zhang, Jeonghoon Kim", "abstract": "Federated Learning (FL) has received a significant amount of attention in the\nindustry and research community due to its capability of keeping data on local\ndevices. To aggregate the gradients of local models to train the global model,\nexisting works require that the global model and the local models are the same.\nHowever, Internet of Things (IoT) devices are inherently diverse regarding\ncomputation speed and onboard memory. In this paper, we propose an FL framework\ntargeting the heterogeneity of IoT devices. Specifically, local models are\ncompressed from the global model, and the gradients of the compressed local\nmodels are used to update the global model. We conduct preliminary experiments\nto illustrate that our framework can facilitate the design of IoT-aware FL.", "time": "2021-05-31T02:08:36Z", "link": "http://arxiv.org/abs/2105.14675v1", "id": "2105.14675v1", "title": "Towards a Federated Learning Framework for Heterogeneous Devices of\n  Internet of Things"}
{"author": "Chunjiang Che, Xiaoli Li, Chuan Chen, Xiaoyu He, Zibin Zheng", "abstract": "Federated learning allows multiple participants to collaboratively train an\nefficient model without exposing data privacy. However, this distributed\nmachine learning training method is prone to attacks from Byzantine clients,\nwhich interfere with the training of the global model by modifying the model or\nuploading the false gradient. In this paper, we propose a novel serverless\nfederated learning framework Committee Mechanism based Federated Learning\n(CMFL), which can ensure the robustness of the algorithm with convergence\nguarantee. In CMFL, a committee system is set up to screen the uploaded local\ngradients. The committee system selects the local gradients rated by the\nelected members for the aggregation procedure through the selection strategy,\nand replaces the committee member through the election strategy. Based on the\ndifferent considerations of model performance and defense, two opposite\nselection strategies are designed for the sake of both accuracy and robustness.\nExtensive experiments illustrate that CMFL achieves faster convergence and\nbetter accuracy than the typical Federated Learning, in the meanwhile obtaining\nbetter robustness than the traditional Byzantine-tolerant algorithms, in the\nmanner of a decentralized approach. In addition, we theoretically analyze and\nprove the convergence of CMFL under different election and selection\nstrategies, which coincides with the experimental results.", "time": "2022-09-08T03:52:31Z", "link": "http://arxiv.org/abs/2108.00365v2", "id": "2108.00365v2", "title": "A Decentralized Federated Learning Framework via Committee Mechanism\n  with Convergence Guarantee"}
{"author": "Atal Narayan Sahu, Aritra Dutta, Ahmed M. Abdelmoniem, Trambak Banerjee, Marco Canini, Panos Kalnis", "abstract": "Gradient compression is a widely-established remedy to tackle the\ncommunication bottleneck in distributed training of large deep neural networks\n(DNNs). Under the error-feedback framework, Top-$k$ sparsification, sometimes\nwith $k$ as little as $0.1\\%$ of the gradient size, enables training to the\nsame model quality as the uncompressed case for a similar iteration count. From\nthe optimization perspective, we find that Top-$k$ is the communication-optimal\nsparsifier given a per-iteration $k$ element budget. We argue that to further\nthe benefits of gradient sparsification, especially for DNNs, a different\nperspective is necessary -- one that moves from per-iteration optimality to\nconsider optimality for the entire training.\n  We identify that the total error -- the sum of the compression errors for all\niterations -- encapsulates sparsification throughout training. Then, we propose\na communication complexity model that minimizes the total error under a\ncommunication budget for the entire training. We find that the hard-threshold\nsparsifier, a variant of the Top-$k$ sparsifier with $k$ determined by a\nconstant hard-threshold, is the optimal sparsifier for this model. Motivated by\nthis, we provide convex and non-convex convergence analyses for the\nhard-threshold sparsifier with error-feedback. Unlike with Top-$k$ sparsifier,\nwe show that hard-threshold has the same asymptotic convergence and linear\nspeedup property as SGD in the convex case and has no impact on the\ndata-heterogeneity in the non-convex case. Our diverse experiments on various\nDNNs and a logistic regression model demonstrated that the hard-threshold\nsparsifier is more communication-efficient than Top-$k$.", "time": "2021-08-02T14:52:42Z", "link": "http://arxiv.org/abs/2108.00951v1", "id": "2108.00951v1", "title": "Rethinking gradient sparsification as total error minimization"}
{"author": "Josep Domingo-Ferrer, Alberto Blanco-Justicia, JesÃºs ManjÃ³n, David SÃ¡nchez", "abstract": "The decentralized nature of federated learning, that often leverages the\npower of edge devices, makes it vulnerable to attacks against privacy and\nsecurity. The privacy risk for a peer is that the model update she computes on\nher private data may, when sent to the model manager, leak information on those\nprivate data. Even more obvious are security attacks, whereby one or several\nmalicious peers return wrong model updates in order to disrupt the learning\nprocess and lead to a wrong model being learned. In this paper we build a\nfederated learning framework that offers privacy to the participating peers as\nwell as security against Byzantine and poisoning attacks. Our framework\nconsists of several protocols that provide strong privacy to the participating\npeers via unlinkable anonymity and that are rationally sustainable based on the\nco-utility property. In other words, no rational party is interested in\ndeviating from the proposed protocols. We leverage the notion of co-utility to\nbuild a decentralized co-utile reputation management system that provides\nincentives for parties to adhere to the protocols. Unlike privacy protection\nvia differential privacy, our approach preserves the values of model updates\nand hence the accuracy of plain federated learning; unlike privacy protection\nvia update aggregation, our approach preserves the ability to detect bad model\nupdates while substantially reducing the computational overhead compared to\nmethods based on homomorphic encryption.", "time": "2021-08-04T08:58:24Z", "link": "http://arxiv.org/abs/2108.01913v1", "id": "2108.01913v1", "title": "Secure and Privacy-Preserving Federated Learning via Co-Utility"}
{"author": "Virginia Ochi, Ricardo Estrada, Teezal Gaji, Wendy Gadea, Emily Duong", "abstract": "Our analysis reviews and visualizes the audio features and popularity of\nsongs streamed on Spotify*. Our dataset, downloaded from Kaggle and originally\nsourced from Spotify API, consists of multiple Excel files containing\ninformation relevant to our visualization and regression analysis. The exercise\nseeks to determine the connection between the popularity of the songs and the\ndanceability. Insights to be included and factored as part of our analysis\ninclude song energy, valence, BPM, release date, and year.", "time": "2021-08-05T05:02:24Z", "link": "http://arxiv.org/abs/2108.02370v1", "id": "2108.02370v1", "title": "Spotify Danceability and Popularity Analysis using SAP"}
{"author": "Binayak Tiwari, Mei Yang, Xiaohang Wang, Yingtao Jiang", "abstract": "The increasing popularity of deep neural network (DNN) applications demands\nhigh computing power and efficient hardware accelerator architecture. DNN\naccelerators use a large number of processing elements (PEs) and on-chip memory\nfor storing weights and other parameters. As the communication backbone of a\nDNN accelerator, networks-on-chip (NoC) play an important role in supporting\nvarious dataflow patterns and enabling processing with communication\nparallelism in a DNN accelerator. However, the widely used mesh-based NoC\narchitectures inherently cannot support the efficient one-to-many and\nmany-to-one traffic largely existing in DNN workloads. In this paper, we\npropose a modified mesh architecture with a one-way/two-way streaming bus to\nspeedup one-to-many (multicast) traffic, and the use of gather packets to\nsupport many-to-one (gather) traffic. The analysis of the runtime latency of a\nconvolutional layer shows that the two-way streaming architecture achieves\nbetter improvement than the one-way streaming architecture for an Output\nStationary (OS) dataflow architecture. The simulation results demonstrate that\nthe gather packets can help to reduce the runtime latency up to 1.8 times and\nnetwork power consumption up to 1.7 times, compared with the repetitive unicast\nmethod on modified mesh architectures supporting two-way streaming.", "time": "2021-08-01T23:50:12Z", "link": "http://arxiv.org/abs/2108.02569v1", "id": "2108.02569v1", "title": "Data Streaming and Traffic Gathering in Mesh-based NoC for Deep Neural\n  Network Acceleration"}
{"author": "Andres Gomez, Andreas Tretter, Pascal Alexander Hager, Praveenth Sanmugarajah, Luca Benini, Lothar Thiele", "abstract": "Sensing systems powered by energy harvesting have traditionally been designed\nto tolerate long periods without energy. As the Internet of Things (IoT)\nevolves towards a more transient and opportunistic execution paradigm, reducing\nenergy storage costs will be key for its economic and ecologic viability.\nHowever, decreasing energy storage in harvesting systems introduces reliability\nissues. Transducers only produce intermittent energy at low voltage and current\nlevels, making guaranteed task completion a challenge. Existing ad hoc methods\novercome this by buffering enough energy either for single tasks, incurring\nlarge data-retention overheads, or for one full application cycle, requiring a\nlarge energy buffer. We present Julienning: an automated method for optimizing\nthe total energy cost of batteryless applications. Using a custom specification\nmodel, developers can describe transient applications as a set of atomically\nexecuted kernels with explicit data dependencies. Our optimization flow can\npartition data- and energy-intensive applications into multiple execution\ncycles with bounded energy consumption. By leveraging interkernel data\ndependencies, these energy-bounded execution cycles minimize the number of\nsystem activations and nonvolatile data transfers, and thus the total energy\noverhead. We validate our methodology with two batteryless cameras running\nenergy-intensive machine learning applications. Results demonstrate that\ncompared to ad hoc solutions, our method can reduce the required energy storage\nby over 94% while only incurring a 0.12% energy overhead.", "time": "2021-08-05T09:49:42Z", "link": "http://arxiv.org/abs/2108.04059v1", "id": "2108.04059v1", "title": "Memory-Aware Partitioning of Machine Learning Applications for Optimal\n  Energy Use in Batteryless Systems"}
{"author": "Yao Li, Xiaorui Liu, Jiliang Tang, Ming Yan, Kun Yuan", "abstract": "Decentralized optimization and communication compression have exhibited their\ngreat potential in accelerating distributed machine learning by mitigating the\ncommunication bottleneck in practice. While existing decentralized algorithms\nwith communication compression mostly focus on the problems with only smooth\ncomponents, we study the decentralized stochastic composite optimization\nproblem with a potentially non-smooth component. A \\underline{Prox}imal\ngradient \\underline{L}in\\underline{EA}r convergent \\underline{D}ecentralized\nalgorithm with compression, Prox-LEAD, is proposed with rigorous theoretical\nanalyses in the general stochastic setting and the finite-sum setting. Our\ntheorems indicate that Prox-LEAD works with arbitrary compression precision,\nand it tremendously reduces the communication cost almost for free. The\nsuperiorities of the proposed algorithms are demonstrated through the\ncomparison with state-of-the-art algorithms in terms of convergence\ncomplexities and numerical experiments. Our algorithmic framework also\ngenerally enlightens the compressed communication on other primal-dual\nalgorithms by reducing the impact of inexact iterations, which might be of\nindependent interest.", "time": "2021-08-12T16:50:52Z", "link": "http://arxiv.org/abs/2108.04448v2", "id": "2108.04448v2", "title": "Decentralized Composite Optimization with Compression"}
{"author": "Haoyu Zhao, Zhize Li, Peter RichtÃ¡rik", "abstract": "Federated Averaging (FedAvg, also known as Local-SGD) (McMahan et al., 2017)\nis a classical federated learning algorithm in which clients run multiple local\nSGD steps before communicating their update to an orchestrating server. We\npropose a new federated learning algorithm, FedPAGE, able to further reduce the\ncommunication complexity by utilizing the recent optimal PAGE method (Li et\nal., 2021) instead of plain SGD in FedAvg. We show that FedPAGE uses much fewer\ncommunication rounds than previous local methods for both federated convex and\nnonconvex optimization. Concretely, 1) in the convex setting, the number of\ncommunication rounds of FedPAGE is $O(\\frac{N^{3/4}}{S\\epsilon})$, improving\nthe best-known result $O(\\frac{N}{S\\epsilon})$ of SCAFFOLD (Karimireddy et\nal.,2020) by a factor of $N^{1/4}$, where $N$ is the total number of clients\n(usually is very large in federated learning), $S$ is the sampled subset of\nclients in each communication round, and $\\epsilon$ is the target error; 2) in\nthe nonconvex setting, the number of communication rounds of FedPAGE is\n$O(\\frac{\\sqrt{N}+S}{S\\epsilon^2})$, improving the best-known result\n$O(\\frac{N^{2/3}}{S^{2/3}\\epsilon^2})$ of SCAFFOLD (Karimireddy et al.,2020) by\na factor of $N^{1/6}S^{1/3}$, if the sampled clients $S\\leq \\sqrt{N}$. Note\nthat in both settings, the communication cost for each round is the same for\nboth FedPAGE and SCAFFOLD. As a result, FedPAGE achieves new state-of-the-art\nresults in terms of communication complexity for both federated convex and\nnonconvex optimization.", "time": "2021-08-10T15:41:27Z", "link": "http://arxiv.org/abs/2108.04755v1", "id": "2108.04755v1", "title": "FedPAGE: A Fast Local Stochastic Gradient Method for\n  Communication-Efficient Federated Learning"}
{"author": "Anh Tran", "abstract": "Bayesian optimization (BO) is a flexible and powerful framework that is\nsuitable for computationally expensive simulation-based applications and\nguarantees statistical convergence to the global optimum. While remaining as\none of the most popular optimization methods, its capability is hindered by the\nsize of data, the dimensionality of the considered problem, and the nature of\nsequential optimization. These scalability issues are intertwined with each\nother and must be tackled simultaneously. In this work, we propose the\nScalable$^3$-BO framework, which employs sparse GP as the underlying surrogate\nmodel to scope with Big Data and is equipped with a random embedding to\nefficiently optimize high-dimensional problems with low effective\ndimensionality. The Scalable$^3$-BO framework is further leveraged with\nasynchronous parallelization feature, which fully exploits the computational\nresource on HPC within a computational budget. As a result, the proposed\nScalable$^3$-BO framework is scalable in three independent perspectives: with\nrespect to data size, dimensionality, and computational resource on HPC. The\ngoal of this work is to push the frontiers of BO beyond its well-known\nscalability issues and minimize the wall-clock waiting time for optimizing\nhigh-dimensional computationally expensive applications. We demonstrate the\ncapability of Scalable$^3$-BO with 1 million data points, 10,000-dimensional\nproblems, with 20 concurrent workers in an HPC environment.", "time": "2021-08-12T21:05:47Z", "link": "http://arxiv.org/abs/2108.05969v1", "id": "2108.05969v1", "title": "Scalable3-BO: Big Data meets HPC - A scalable asynchronous parallel\n  high-dimensional Bayesian optimization framework on supercomputers"}
{"author": "Nanda K. Unnikrishnan, Keshab K. Parhi", "abstract": "The time required for training the neural networks increases with size,\ncomplexity, and depth. Training model parameters by backpropagation inherently\ncreates feedback loops. These loops hinder efficient pipelining and scheduling\nof the tasks within the layer and between consecutive layers. Prior approaches,\nsuch as PipeDream, have exploited the use of delayed gradient to achieve\ninter-layer pipelining. However, these approaches treat the entire\nbackpropagation as a single task; this leads to an increase in computation time\nand processor underutilization. This paper presents novel optimization\napproaches where the gradient computations with respect to the weights and the\nactivation functions are considered independently; therefore, these can be\ncomputed in parallel. This is referred to as intra-layer optimization.\nAdditionally, the gradient computation with respect to the activation function\nis further divided into two parts and distributed to two consecutive layers.\nThis leads to balanced scheduling where the computation time of each layer is\nthe same. This is referred to as inter-layer optimization. The proposed system,\nreferred to as LayerPipe, reduces the number of clock cycles required for\ntraining while maximizing processor utilization with minimal inter-processor\ncommunication overhead. LayerPipe achieves an average speedup of 25% and\nupwards of 80% with 7 to 9 processors with less communication overhead when\ncompared to PipeDream.", "time": "2021-08-14T23:51:00Z", "link": "http://arxiv.org/abs/2108.06629v1", "id": "2108.06629v1", "title": "LayerPipe: Accelerating Deep Neural Network Training by Intra-Layer and\n  Inter-Layer Gradient Pipelining and Multiprocessor Scheduling"}
{"author": "Charlie Hou, Kiran K. Thekumparampil, Giulia Fanti, Sewoong Oh", "abstract": "Federated learning (FL) aims to minimize the communication complexity of\ntraining a model over heterogeneous data distributed across many clients. A\ncommon approach is local methods, where clients take multiple optimization\nsteps over local data before communicating with the server (e.g., FedAvg).\nLocal methods can exploit similarity between clients' data. However, in\nexisting analyses, this comes at the cost of slow convergence in terms of the\ndependence on the number of communication rounds R. On the other hand, global\nmethods, where clients simply return a gradient vector in each round (e.g.,\nSGD), converge faster in terms of R but fail to exploit the similarity between\nclients even when clients are homogeneous. We propose FedChain, an algorithmic\nframework that combines the strengths of local methods and global methods to\nachieve fast convergence in terms of R while leveraging the similarity between\nclients. Using FedChain, we instantiate algorithms that improve upon previously\nknown rates in the general convex and PL settings, and are near-optimal (via an\nalgorithm-independent lower bound that we show) for problems that satisfy\nstrong convexity. Empirical results support this theoretical gain over existing\nmethods.", "time": "2023-04-16T16:53:18Z", "link": "http://arxiv.org/abs/2108.06869v5", "id": "2108.06869v5", "title": "FedChain: Chained Algorithms for Near-Optimal Communication Cost in\n  Federated Learning"}
{"author": "Gary Cheng, Karan Chadha, John Duchi", "abstract": "We propose an asymptotic framework to analyze the performance of\n(personalized) federated learning algorithms. In this new framework, we\nformulate federated learning as a multi-criterion objective, where the goal is\nto minimize each client's loss using information from all of the clients. We\nanalyze a linear regression model where, for a given client, we may\ntheoretically compare the performance of various algorithms in the\nhigh-dimensional asymptotic limit. This asymptotic multi-criterion approach\nnaturally models the high-dimensional, many-device nature of federated\nlearning. These tools make fairly precise predictions about the benefits of\npersonalization and information sharing in federated scenarios -- at least in\nour (stylized) model -- including that Federated Averaging with simple client\nfine-tuning achieves the same asymptotic risk as the more intricate\nmeta-learning and proximal-regularized approaches and outperforming Federated\nAveraging without personalization. We evaluate these predictions on federated\nversions of the EMNIST, CIFAR-100, Shakespeare, and Stack Overflow datasets,\nwhere the experiments corroborate the theoretical predictions, suggesting such\nframeworks may provide a useful guide to practical algorithmic development.", "time": "2022-02-18T05:34:57Z", "link": "http://arxiv.org/abs/2108.07313v3", "id": "2108.07313v3", "title": "Federated Asymptotics: a model to compare federated learning algorithms"}
{"author": "Virat Shejwalkar, Amir Houmansadr, Peter Kairouz, Daniel Ramage", "abstract": "While recent works have indicated that federated learning (FL) may be\nvulnerable to poisoning attacks by compromised clients, their real impact on\nproduction FL systems is not fully understood. In this work, we aim to develop\na comprehensive systemization for poisoning attacks on FL by enumerating all\npossible threat models, variations of poisoning, and adversary capabilities. We\nspecifically put our focus on untargeted poisoning attacks, as we argue that\nthey are significantly relevant to production FL deployments.\n  We present a critical analysis of untargeted poisoning attacks under\npractical, production FL environments by carefully characterizing the set of\nrealistic threat models and adversarial capabilities. Our findings are rather\nsurprising: contrary to the established belief, we show that FL is highly\nrobust in practice even when using simple, low-cost defenses. We go even\nfurther and propose novel, state-of-the-art data and model poisoning attacks,\nand show via an extensive set of experiments across three benchmark datasets\nhow (in)effective poisoning attacks are in the presence of simple defense\nmechanisms. We aim to correct previous misconceptions and offer concrete\nguidelines to conduct more accurate (and more realistic) research on this\ntopic.", "time": "2021-12-13T11:24:11Z", "link": "http://arxiv.org/abs/2108.10241v2", "id": "2108.10241v2", "title": "Back to the Drawing Board: A Critical Evaluation of Poisoning Attacks on\n  Production Federated Learning"}
{"author": "Tharindu Adikari, Stark Draper", "abstract": "We consider decentralized consensus optimization when workers sample data\nfrom non-identical distributions and perform variable amounts of work due to\nslow nodes known as stragglers. The problem of non-identical distributions and\nthe problem of variable amount of work have been previously studied separately.\nIn our work we analyze them together under a unified system model. We study the\nconvergence of the optimization algorithm when combining worker outputs under\ntwo heuristic methods: (1) weighting equally, and (2) weighting by the amount\nof work completed by each. We prove convergence of the two methods under\nperfect consensus, assuming straggler statistics are independent and identical\nacross all workers for all iterations. Our numerical results show that under\napproximate consensus the second method outperforms the first method for both\nconvex and non-convex objective functions. We make use of the theory on minimum\nvariance unbiased estimator (MVUE) to evaluate the existence of an optimal\nmethod for combining worker outputs. While we conclude that neither of the two\nheuristic methods are optimal, we also show that an optimal method does not\nexist.", "time": "2021-08-25T06:33:38Z", "link": "http://arxiv.org/abs/2108.11071v1", "id": "2108.11071v1", "title": "Decentralized optimization with non-identical sampling in presence of\n  stragglers"}
{"author": "Song-Ju Kim, Hiroyuki Yasuda, Ryoma Kitagawa, Mikio Hasegawa", "abstract": "We propose a simple channel-allocation method based on tug-of-war (TOW)\ndynamics, combined with the time scheduling based on nonlinear oscillator\nsynchronization to efficiently use of the space (channel) and time resources in\nwireless communications. This study demonstrates that synchronization groups,\nwhere each node selects a different channel, are non-uniformly distributed in\nphase space such that every distance between groups is larger than the area of\ninfluence. New type of self-organized spatiotemporal patterns can be formed for\nresource allocation according to channel rewards.", "time": "2021-08-19T07:48:53Z", "link": "http://arxiv.org/abs/2108.11979v1", "id": "2108.11979v1", "title": "Resource allocation method using tug-of-war-based synchronization"}
{"author": "Arpita Gang, Waheed U. Bajwa", "abstract": "Principal Component Analysis (PCA) is a fundamental data preprocessing tool\nin the world of machine learning. While PCA is often thought of as a\ndimensionality reduction method, the purpose of PCA is actually two-fold:\ndimension reduction and uncorrelated feature learning. Furthermore, the\nenormity of the dimensions and sample size in the modern day datasets have\nrendered the centralized PCA solutions unusable. In that vein, this paper\nreconsiders the problem of PCA when data samples are distributed across nodes\nin an arbitrarily connected network. While a few solutions for distributed PCA\nexist, those either overlook the uncorrelated feature learning aspect of the\nPCA, tend to have high communication overhead that makes them inefficient\nand/or lack `exact' or `global' convergence guarantees. To overcome these\naforementioned issues, this paper proposes a distributed PCA algorithm termed\nFAST-PCA (Fast and exAct diSTributed PCA). The proposed algorithm is efficient\nin terms of communication and is proven to converge linearly and exactly to the\nprincipal components, leading to dimension reduction as well as uncorrelated\nfeatures. The claims are further supported by experimental results.", "time": "2022-02-15T17:43:18Z", "link": "http://arxiv.org/abs/2108.12373v2", "id": "2108.12373v2", "title": "FAST-PCA: A Fast and Exact Algorithm for Distributed Principal Component\n  Analysis"}
{"author": "Zhifeng Jiang, Wei Wang, Yang Liu", "abstract": "Homomorphic encryption (HE) is a promising privacy-preserving technique for\ncross-silo federated learning (FL), where organizations perform collaborative\nmodel training on decentralized data. Despite the strong privacy guarantee,\ngeneral HE schemes result in significant computation and communication\noverhead. Prior works employ batch encryption to address this problem, but it\nis still suboptimal in mitigating communication overhead and is incompatible\nwith sparsification techniques.\n  In this paper, we propose FLASHE, an HE scheme tailored for cross-silo FL. To\ncapture the minimum requirements of security and functionality, FLASHE drops\nthe asymmetric-key design and only involves modular addition operations with\nrandom numbers. Depending on whether to accommodate sparsification techniques,\nFLASHE is optimized in computation efficiency with different approaches. We\nhave implemented FLASHE as a pluggable module atop FATE, an industrial platform\nfor cross-silo FL. Compared to plaintext training, FLASHE slightly increases\nthe training time by $\\leq6\\%$, with no communication overhead.", "time": "2021-09-29T07:30:33Z", "link": "http://arxiv.org/abs/2109.00675v2", "id": "2109.00675v2", "title": "FLASHE: Additively Symmetric Homomorphic Encryption for Cross-Silo\n  Federated Learning"}
{"author": "Hrishav Bakul Barua", "abstract": "As we are fast approaching the beginning of a paradigm shift in the field of\nscience, Data driven science (the so called fourth science paradigm) is going\nto be the driving force in research and innovation. From medicine to\nbiodiversity and astronomy to geology, all these terms are somehow going to be\naffected by this paradigm shift. The huge amount of data to be processed under\nthis new paradigm will be a major concern in the future and one will strongly\nrequire cloud based services in all the aspects of these computations (from\nstorage to compute and other services). Another aspect will be energy\nconsumption and performance of prediction jobs and tasks within such a\nscientific paradigm which will change the way one sees computation. Data\nscience has heavily impacted or rather triggered the emergence of Machine\nLearning, Signal/Image/Video processing related algorithms, Artificial\nintelligence, Robotics, health informatics, geoinformatics, and many more such\nareas of interest. Hence, we envisage an era where Data science can deliver its\npromises with the help of the existing cloud based platforms and services with\nthe addition of new services. In this article, we discuss about data driven\nscience and Machine learning and how they are going to be linked through cloud\nbased services in the future. It also discusses the rise of paradigms like\napproximate computing, quantum computing and many more in recent times and\ntheir applicability in big data processing, data science, analytics, prediction\nand machine learning in the cloud environments.", "time": "2021-09-02T17:36:24Z", "link": "http://arxiv.org/abs/2109.01661v1", "id": "2109.01661v1", "title": "Data science and Machine learning in the Clouds: A Perspective for the\n  Future"}
{"author": "Ahmed Abbas, Paul Swoboda", "abstract": "We propose a highly parallel primal-dual algorithm for the multicut (a.k.a.\ncorrelation clustering) problem, a classical graph clustering problem widely\nused in machine learning and computer vision. Our algorithm consists of three\nsteps executed recursively: (1) Finding conflicted cycles that correspond to\nviolated inequalities of the underlying multicut relaxation, (2) Performing\nmessage passing between the edges and cycles to optimize the Lagrange\nrelaxation coming from the found violated cycles producing reduced costs and\n(3) Contracting edges with high reduced costs through matrix-matrix\nmultiplications. Our algorithm produces primal solutions and lower bounds that\nestimate the distance to optimum. We implement our algorithm on GPUs and show\nresulting one to two orders-of-magnitudes improvements in execution speed\nwithout sacrificing solution quality compared to traditional sequential\nalgorithms that run on CPUs. We can solve very large scale benchmark problems\nwith up to $\\mathcal{O}(10^8)$ variables in a few seconds with small\nprimal-dual gaps. Our code is available at\nhttps://github.com/pawelswoboda/RAMA.", "time": "2022-03-11T15:28:06Z", "link": "http://arxiv.org/abs/2109.01838v3", "id": "2109.01838v3", "title": "RAMA: A Rapid Multicut Algorithm on GPU"}
{"author": "Zachary Charles, Keith Rush", "abstract": "We study whether iterated vector fields (vector fields composed with\nthemselves) are conservative. We give explicit examples of vector fields for\nwhich this self-composition preserves conservatism. Notably, this includes\ngradient vector fields of loss functions associated with some generalized\nlinear models. As we show, characterizing the set of vector fields satisfying\nthis condition leads to non-trivial geometric questions. In the context of\nfederated learning, we show that when clients have loss functions whose\ngradients satisfy this condition, federated averaging is equivalent to gradient\ndescent on a surrogate loss function. We leverage this to derive novel\nconvergence results for federated learning. By contrast, we demonstrate that\nwhen the client losses violate this property, federated averaging can yield\nbehavior which is fundamentally distinct from centralized optimization.\nFinally, we discuss theoretical and practical questions our analytical\nframework raises for federated learning.", "time": "2021-11-13T00:22:24Z", "link": "http://arxiv.org/abs/2109.03973v2", "id": "2109.03973v2", "title": "Iterated Vector Fields and Conservatism, with Applications to Federated\n  Learning"}
{"author": "Zhifeng Jiang, Wei Wang, Bo Li, Qiang Yang", "abstract": "The increasing demand for privacy-preserving collaborative learning has given\nrise to a new computing paradigm called federated learning (FL), in which\nclients collaboratively train a machine learning (ML) model without revealing\ntheir private training data. Given an acceptable level of privacy guarantee,\nthe goal of FL is to minimize the time-to-accuracy of model training. Compared\nwith distributed ML in data centers, there are four distinct challenges to\nachieving short time-to-accuracy in FL training, namely the lack of information\nfor optimization, the tradeoff between statistical and system utility, client\nheterogeneity, and large configuration space. In this paper, we survey recent\nworks in addressing these challenges and present them following a typical\ntraining workflow through three phases: client selection, configuration, and\nreporting. We also review system works including measurement studies and\nbenchmarking tools that aim to support FL developers.", "time": "2022-05-30T13:44:21Z", "link": "http://arxiv.org/abs/2109.03999v3", "id": "2109.03999v3", "title": "Towards Efficient Synchronous Federated Training: A Survey on System\n  Optimization Strategies"}
{"author": "Shulai Zhang, Zirui Li, Quan Chen, Wenli Zheng, Jingwen Leng, Minyi Guo", "abstract": "Federated learning (FL) is a distributed machine learning paradigm that\nallows clients to collaboratively train a model over their own local data. FL\npromises the privacy of clients and its security can be strengthened by\ncryptographic methods such as additively homomorphic encryption (HE). However,\nthe efficiency of FL could seriously suffer from the statistical heterogeneity\nin both the data distribution discrepancy among clients and the global\ndistribution skewness. We mathematically demonstrate the cause of performance\ndegradation in FL and examine the performance of FL over various datasets. To\ntackle the statistical heterogeneity problem, we propose a pluggable\nsystem-level client selection method named Dubhe, which allows clients to\nproactively participate in training, meanwhile preserving their privacy with\nthe assistance of HE. Experimental results show that Dubhe is comparable with\nthe optimal greedy method on the classification accuracy, with negligible\nencryption and communication overhead.", "time": "2021-09-08T13:00:46Z", "link": "http://arxiv.org/abs/2109.04253v1", "id": "2109.04253v1", "title": "Dubhe: Towards Data Unbiasedness with Homomorphic Encryption in\n  Federated Learning Client Selection"}
{"author": "Xiangyi Chen, Xiaoyun Li, Ping Li", "abstract": "In recent years, distributed optimization is proven to be an effective\napproach to accelerate training of large scale machine learning models such as\ndeep neural networks. With the increasing computation power of GPUs, the\nbottleneck of training speed in distributed training is gradually shifting from\ncomputation to communication. Meanwhile, in the hope of training machine\nlearning models on mobile devices, a new distributed training paradigm called\n``federated learning'' has become popular. The communication time in federated\nlearning is especially important due to the low bandwidth of mobile devices.\nWhile various approaches to improve the communication efficiency have been\nproposed for federated learning, most of them are designed with SGD as the\nprototype training algorithm. While adaptive gradient methods have been proven\neffective for training neural nets, the study of adaptive gradient methods in\nfederated learning is scarce. In this paper, we propose an adaptive gradient\nmethod that can guarantee both the convergence and the communication efficiency\nfor federated learning.", "time": "2021-09-10T21:14:36Z", "link": "http://arxiv.org/abs/2109.05109v1", "id": "2109.05109v1", "title": "Toward Communication Efficient Adaptive Gradient Method"}
{"author": "Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Xuyun Zhang", "abstract": "Federated learning (FL) has emerged as a promising privacy-aware paradigm\nthat allows multiple clients to jointly train a model without sharing their\nprivate data. Recently, many studies have shown that FL is vulnerable to\nmembership inference attacks (MIAs) that can distinguish the training members\nof the given model from the non-members. However, existing MIAs ignore the\nsource of a training member, i.e., the information of which client owns the\ntraining member, while it is essential to explore source privacy in FL beyond\nmembership privacy of examples from all clients. The leakage of source\ninformation can lead to severe privacy issues. For example, identification of\nthe hospital contributing to the training of an FL model for COVID-19 pandemic\ncan render the owner of a data record from this hospital more prone to\ndiscrimination if the hospital is in a high risk region. In this paper, we\npropose a new inference attack called source inference attack (SIA), which can\nderive an optimal estimation of the source of a training member. Specifically,\nwe innovatively adopt the Bayesian perspective to demonstrate that an\nhonest-but-curious server can launch an SIA to steal non-trivial source\ninformation of the training members without violating the FL protocol. The\nserver leverages the prediction loss of local models on the training members to\nachieve the attack effectively and non-intrusively. We conduct extensive\nexperiments on one synthetic and five real datasets to evaluate the key factors\nin an SIA, and the results show the efficacy of the proposed source inference\nattack.", "time": "2021-09-13T01:09:48Z", "link": "http://arxiv.org/abs/2109.05659v1", "id": "2109.05659v1", "title": "Source Inference Attacks in Federated Learning"}
{"author": "Jian Xu, Shao-Lun Huang, Linqi Song, Tian Lan", "abstract": "Gradient-based training in federated learning is known to be vulnerable to\nfaulty/malicious clients, which are often modeled as Byzantine clients. To this\nend, previous work either makes use of auxiliary data at parameter server to\nverify the received gradients (e.g., by computing validation error rate) or\nleverages statistic-based methods (e.g. median and Krum) to identify and remove\nmalicious gradients from Byzantine clients. In this paper, we remark that\nauxiliary data may not always be available in practice and focus on the\nstatistic-based approach. However, recent work on model poisoning attacks has\nshown that well-crafted attacks can circumvent most of median- and\ndistance-based statistical defense methods, making malicious gradients\nindistinguishable from honest ones. To tackle this challenge, we show that the\nelement-wise sign of gradient vector can provide valuable insight in detecting\nmodel poisoning attacks. Based on our theoretical analysis of the\n\\textit{Little is Enough} attack, we propose a novel approach called\n\\textit{SignGuard} to enable Byzantine-robust federated learning through\ncollaborative malicious gradient filtering. More precisely, the received\ngradients are first processed to generate relevant magnitude, sign, and\nsimilarity statistics, which are then collaboratively utilized by multiple\nfilters to eliminate malicious gradients before final aggregation. Finally,\nextensive experiments of image and text classification tasks are conducted\nunder recently proposed attacks and defense strategies. The numerical results\ndemonstrate the effectiveness and superiority of our proposed approach. The\ncode is available at \\textit{\\url{https://github.com/JianXu95/SignGuard}}", "time": "2023-04-29T08:24:32Z", "link": "http://arxiv.org/abs/2109.05872v2", "id": "2109.05872v2", "title": "Byzantine-robust Federated Learning through Collaborative Malicious\n  Gradient Filtering"}
{"author": "Geonhwa Jeong, Gokcen Kestor, Prasanth Chatarasi, Angshuman Parashar, Po-An Tsai, Sivasankaran Rajamanickam, Roberto Gioiosa, Tushar Krishna", "abstract": "To meet the extreme compute demands for deep learning across commercial and\nscientific applications, dataflow accelerators are becoming increasingly\npopular. While these \"domain-specific\" accelerators are not fully programmable\nlike CPUs and GPUs, they retain varying levels of flexibility with respect to\ndata orchestration, i.e., dataflow and tiling optimizations to enhance\nefficiency. There are several challenges when designing new algorithms and\nmapping approaches to execute the algorithms for a target problem on new\nhardware. Previous works have addressed these challenges individually. To\naddress this challenge as a whole, in this work, we present a HW-SW co-design\necosystem for spatial accelerators called Union within the popular MLIR\ncompiler infrastructure. Our framework allows exploring different algorithms\nand their mappings on several accelerator cost models. Union also includes a\nplug-and-play library of accelerator cost models and mappers which can easily\nbe extended. The algorithms and accelerator cost models are connected via a\nnovel mapping abstraction that captures the map space of spatial accelerators\nwhich can be systematically pruned based on constraints from the hardware,\nworkload, and mapper. We demonstrate the value of Union for the community with\nseveral case studies which examine offloading different tensor\noperations(CONV/GEMM/Tensor Contraction) on diverse accelerator architectures\nusing different mapping schemes.", "time": "2021-11-07T03:01:10Z", "link": "http://arxiv.org/abs/2109.07419v3", "id": "2109.07419v3", "title": "Union: A Unified HW-SW Co-Design Ecosystem in MLIR for Evaluating Tensor\n  Operations on Spatial Accelerators"}
{"author": "Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally, Siddharth Samsi, Jeremy Kepner", "abstract": "Over the past several years, new machine learning accelerators were being\nannounced and released every month for a variety of applications from speech\nrecognition, video object detection, assisted driving, and many data center\napplications. This paper updates the survey of AI accelerators and processors\nfrom past two years. This paper collects and summarizes the current commercial\naccelerators that have been publicly announced with peak performance and power\nconsumption numbers. The performance and power values are plotted on a scatter\ngraph, and a number of dimensions and observations from the trends on this plot\nare again discussed and analyzed. This year, we also compile a list of\nbenchmarking performance results and compute the computational efficiency with\nrespect to peak performance.", "time": "2021-09-18T15:57:47Z", "link": "http://arxiv.org/abs/2109.08957v1", "id": "2109.08957v1", "title": "AI Accelerator Survey and Trends"}
{"author": "Philippe Chatigny, Shengrui Wang, Jean-Marc Patenaude, Boris N. Oreshkin", "abstract": "We study the problem of efficiently scaling ensemble-based deep neural\nnetworks for multi-step time series (TS) forecasting on a large set of time\nseries. Current state-of-the-art deep ensemble models have high memory and\ncomputational requirements, hampering their use to forecast millions of TS in\npractical scenarios. We propose N-BEATS(P), a global parallel variant of the\nN-BEATS model designed to allow simultaneous training of multiple univariate TS\nforecasting models. Our model addresses the practical limitations of related\nmodels, reducing the training time by half and memory requirement by a factor\nof 5, while keeping the same level of accuracy in all TS forecasting settings.\nWe have performed multiple experiments detailing the various ways to train our\nmodel and have obtained results that demonstrate its capacity to generalize in\nvarious forecasting conditions and setups.", "time": "2022-01-28T16:20:34Z", "link": "http://arxiv.org/abs/2109.09705v4", "id": "2109.09705v4", "title": "Neural forecasting at scale"}
{"author": "Mahdi Soleymani, Ramy E. Ali, Hessam Mahdavifar, A. Salman Avestimehr", "abstract": "Due to the surge of cloud-assisted AI services, the problem of designing\nresilient prediction serving systems that can effectively cope with\nstragglers/failures and minimize response delays has attracted much interest.\nThe common approach for tackling this problem is replication which assigns the\nsame prediction task to multiple workers. This approach, however, is very\ninefficient and incurs significant resource overheads. Hence, a learning-based\napproach known as parity model (ParM) has been recently proposed which learns\nmodels that can generate parities for a group of predictions in order to\nreconstruct the predictions of the slow/failed workers. While this\nlearning-based approach is more resource-efficient than replication, it is\ntailored to the specific model hosted by the cloud and is particularly suitable\nfor a small number of queries (typically less than four) and tolerating very\nfew (mostly one) number of stragglers. Moreover, ParM does not handle Byzantine\nadversarial workers. We propose a different approach, named Approximate Coded\nInference (ApproxIFER), that does not require training of any parity models,\nhence it is agnostic to the model hosted by the cloud and can be readily\napplied to different data domains and model architectures. Compared with\nearlier works, ApproxIFER can handle a general number of stragglers and scales\nsignificantly better with the number of queries. Furthermore, ApproxIFER is\nrobust against Byzantine workers. Our extensive experiments on a large number\nof datasets and model architectures also show significant accuracy improvement\nby up to 58% over the parity model approaches.", "time": "2021-09-20T22:29:57Z", "link": "http://arxiv.org/abs/2109.09868v1", "id": "2109.09868v1", "title": "ApproxIFER: A Model-Agnostic Approach to Resilient and Robust Prediction\n  Serving Systems"}
{"author": "Yuxuan Sun, Fan Zhang, Junlin Zhao, Sheng Zhou, Zhisheng Niu, Deniz GÃ¼ndÃ¼z", "abstract": "Distributed computing enables large-scale computation tasks to be processed\nover multiple workers in parallel. However, the randomness of communication and\ncomputation delays across workers causes the straggler effect, which may\ndegrade the performance. Coded computation helps to mitigate the straggler\neffect, but the amount of redundant load and their assignment to the workers\nshould be carefully optimized. In this work, we consider a multi-master\nheterogeneous-worker distributed computing scenario, where multiple matrix\nmultiplication tasks are encoded and allocated to workers for parallel\ncomputation. The goal is to minimize the communication plus computation delay\nof the slowest task. We propose worker assignment, resource allocation and load\nallocation algorithms under both dedicated and fractional worker assignment\npolicies, where each worker can process the encoded tasks of either a single\nmaster or multiple masters, respectively. Then, the non-convex delay\nminimization problem is solved by employing the Markov's inequality-based\napproximation, Karush-Kuhn-Tucker conditions, and successive convex\napproximation methods. Through extensive simulations, we show that the proposed\nalgorithms can reduce the task completion delay compared to the benchmarks, and\nobserve that dedicated and fractional worker assignment policies have different\nscopes of applications.", "time": "2021-09-23T09:40:54Z", "link": "http://arxiv.org/abs/2109.11246v1", "id": "2109.11246v1", "title": "Coded Computation across Shared Heterogeneous Workers with Communication\n  Delay"}
{"author": "Lorenzo Valerio, Marco Conti, Andrea Passarella", "abstract": "Due to the pervasive diffusion of personal mobile and IoT devices, many\n\"smart environments\" (e.g., smart cities and smart factories) will be,\ngenerators of huge amounts of data. Currently, analysis of this data is\ntypically achieved through centralised cloud-based services. However, according\nto many studies, this approach may present significant issues from the\nstandpoint of data ownership, as well as wireless network capacity. In this\npaper, we exploit the fog computing paradigm to move computation close to where\ndata is produced. We exploit a well-known distributed machine learning\nframework (Hypothesis Transfer Learning), and perform data analytics on mobile\nnodes passing by IoT devices, in addition to fog gateways at the edge of the\nnetwork infrastructure. We analyse the performance of different configurations\nof the distributed learning framework, in terms of (i) accuracy obtained in the\nlearning task and (ii) energy spent to send data between the involved nodes.\nSpecifically, we consider reference wireless technologies for communication\nbetween the different types of nodes we consider, e.g. LTE, Nb-IoT, 802.15.4,\n802.11, etc. Our results show that collecting data through the mobile nodes and\nexecuting the distributed analytics using short-range communication\ntechnologies, such as 802.15.4 and 802.11, allows to strongly reduce the energy\nconsumption of the system up to $94\\%$ with a loss in accuracy w.r.t. a\ncentralised cloud solution up to $2\\%$.", "time": "2021-09-23T14:07:33Z", "link": "http://arxiv.org/abs/2109.11386v1", "id": "2109.11386v1", "title": "Energy efficient distributed analytics at the edge of the network for\n  IoT environments"}
{"author": "Di Fan, Yifan Wu, Xiaoxiao Li", "abstract": "in healthcare. However, the existing AI model may be biased in its decision\nmarking. The bias induced by data itself, such as collecting data in subgroups\nonly, can be mitigated by including more diversified data. Distributed and\ncollaborative learning is an approach to involve training models in massive,\nheterogeneous, and distributed data sources, also known as nodes. In this work,\nwe target on examining the fairness issue in Swarm Learning (SL), a recent\nedge-computing based decentralized machine learning approach, which is designed\nfor heterogeneous illnesses detection in precision medicine. SL has achieved\nhigh performance in clinical applications, but no attempt has been made to\nevaluate if SL can improve fairness. To address the problem, we present an\nempirical study by comparing the fairness among single (node) training, SL,\ncentralized training. Specifically, we evaluate on large public available skin\nlesion dataset, which contains samples from various subgroups. The experiments\ndemonstrate that SL does not exacerbate the fairness problem compared to\ncentralized training and improves both performance and fairness compared to\nsingle training. However, there still exists biases in SL model and the\nimplementation of SL is more complex than the alternative two strategies.", "time": "2021-09-24T20:20:24Z", "link": "http://arxiv.org/abs/2109.12176v1", "id": "2109.12176v1", "title": "On the Fairness of Swarm Learning in Skin Lesion Classification"}
{"author": "Edward H. Lee, Mario Michael Krell, Alexander Tsyplikhin, Victoria Rege, Errol Colak, Kristen W. Yeom", "abstract": "Differentially private SGD (DPSGD) has recently shown promise in deep\nlearning. However, compared to non-private SGD, the DPSGD algorithm places\ncomputational overheads that can undo the benefit of batching in GPUs.\nMicro-batching is a common method to alleviate this and is fully supported in\nthe TensorFlow Privacy library (TFDP). However, it degrades accuracy. We\npropose NanoBatch Privacy, a lightweight add-on to TFDP to be used on Graphcore\nIPUs by leveraging batch size of 1 (without microbatching) and gradient\naccumulation. This allows us to achieve large total batch sizes with minimal\nimpacts to throughput. Second, we illustrate using Cifar-10 how larger batch\nsizes are not necessarily optimal from a privacy versus utility perspective. On\nImageNet, we achieve more than 15x speedup over TFDP versus 8x A100s and\nsignificant speedups even across libraries such as Opacus. We also provide two\nextensions: 1) DPSGD for pipelined models and 2) per-layer clipping that is 15x\nfaster than the Opacus implementation on 8x A100s. Finally as an application\ncase study, we apply NanoBatch training for use on private Covid-19 chest CT\nprediction.", "time": "2022-06-02T22:55:16Z", "link": "http://arxiv.org/abs/2109.12191v2", "id": "2109.12191v2", "title": "NanoBatch Privacy: Enabling fast Differentially Private learning on the\n  IPU"}
{"author": "Sagar Shrestha, Xiao Fu", "abstract": "Classic and deep generalized canonical correlation analysis (GCCA) algorithms\nseek low-dimensional common representations of data entities from multiple\n``views'' (e.g., audio and image) using linear transformations and neural\nnetworks, respectively. When the views are acquired and stored at different\ncomputing agents (e.g., organizations and edge devices) and data sharing is\nundesired due to privacy or communication cost considerations, federated\nlearning-based GCCA is well-motivated. In federated learning, the views are\nkept locally at the agents and only derived, limited information exchange with\na central server is allowed. However, applying existing GCCA algorithms onto\nsuch federated learning settings may incur prohibitively high communication\noverhead. This work puts forth a communication-efficient federated learning\nframework for both linear and deep GCCA under the maximum variance (MAX-VAR)\nformulation. The overhead issue is addressed by aggressively compressing (via\nquantization) the exchanging information between the computing agents and a\ncentral controller. Compared to the unquantized version, our empirical study\nshows that the proposed algorithm enjoys a substantial reduction of\ncommunication overheads with virtually no loss in accuracy and convergence\nspeed. Rigorous convergence analyses are also presented, which is a nontrivial\neffort. Generic federated optimization results do not cover the special problem\nstructure of GCCA. Our result shows that the proposed algorithms for both\nlinear and deep GCCA converge to critical points at a sublinear rate, even\nunder heavy quantization and stochastic approximations. In addition, in the\nlinear MAX-VAR case, the quantized algorithm approaches a global optimum in a\ngeometric rate under reasonable conditions. Synthetic and real-data experiments\nare used to showcase the effectiveness of the proposed approach.", "time": "2023-04-03T19:17:47Z", "link": "http://arxiv.org/abs/2109.12400v2", "id": "2109.12400v2", "title": "Communication-Efficient Federated Linear and Deep Generalized Canonical\n  Correlation Analysis"}
{"author": "S Vineeth", "abstract": "Massive amounts of data have led to the training of large-scale machine\nlearning models on a single worker inefficient. Distributed machine learning\nmethods such as Parallel-SGD have received significant interest as a solution\nto tackle this problem. However, the performance of distributed systems does\nnot scale linearly with the number of workers due to the high network\ncommunication cost for synchronizing gradients and parameters. Researchers have\nproposed techniques such as quantization and sparsification to alleviate this\nproblem by compressing the gradients. Most of the compression schemes result in\ncompressed gradients that cannot be directly aggregated with efficient\nprotocols such as all-reduce. In this paper, we present a set of all-reduce\ncompatible gradient compression schemes which significantly reduce the\ncommunication overhead while maintaining the performance of vanilla SGD. We\npresent the results of our experiments with the CIFAR10 dataset and\nobservations derived during the process. Our compression methods perform better\nthan the in-built methods currently offered by the deep learning frameworks.\nCode is available at the repository:\n\\url{https://github.com/vineeths96/Gradient-Compression}.", "time": "2022-03-30T03:16:41Z", "link": "http://arxiv.org/abs/2109.12497v2", "id": "2109.12497v2", "title": "Unbiased Single-scale and Multi-scale Quantizers for Distributed\n  Optimization"}
{"author": "Guin Gilman, Robert J. Walls", "abstract": "We investigate the performance of the concurrency mechanisms available on\nNVIDIA's new Ampere GPU microarchitecture under deep learning training and\ninference workloads. In contrast to previous studies that treat the GPU as a\nblack box, we examine scheduling at the microarchitectural level. We find that\nthe lack of fine-grained preemption mechanisms, robust task prioritization\noptions, and contention-aware thread block placement policies limits the\neffectiveness of NVIDIA's concurrency mechanisms. In summary, the sequential\nnature of deep learning workloads and their fluctuating resource requirements\nand kernel runtimes make executing such workloads while maintaining\nconsistently high utilization and low, predictable turnaround times difficult\non current NVIDIA hardware.", "time": "2021-10-01T14:48:50Z", "link": "http://arxiv.org/abs/2110.00459v1", "id": "2110.00459v1", "title": "Characterizing Concurrency Mechanisms for NVIDIA GPUs under Deep\n  Learning Workloads"}
{"author": "Jude Haris, Perry Gibson, JosÃ© Cano, Nicolas Bohm Agostini, David Kaeli", "abstract": "Edge computing devices inherently face tight resource constraints, which is\nespecially apparent when deploying Deep Neural Networks (DNN) with high memory\nand compute demands. FPGAs are commonly available in edge devices. Since these\nreconfigurable circuits can achieve higher throughput and lower power\nconsumption than general purpose processors, they are especially well-suited\nfor DNN acceleration. However, existing solutions for designing FPGA-based DNN\naccelerators for edge devices come with high development overheads, given the\ncost of repeated FPGA synthesis passes, reimplementation in a Hardware\nDescription Language (HDL) of the simulated design, and accelerator system\nintegration.\n  In this paper we propose SECDA, a new hardware/software co-design methodology\nto reduce design time of optimized DNN inference accelerators on edge devices\nwith FPGAs. SECDA combines cost-effective SystemC simulation with hardware\nexecution, streamlining design space exploration and the development process\nvia reduced design evaluation time. As a case study, we use SECDA to\nefficiently develop two different DNN accelerator designs on a PYNQ-Z1 board, a\nplatform that includes an edge FPGA. We quickly and iteratively explore the\nsystem's hardware/software stack, while identifying and mitigating performance\nbottlenecks. We evaluate the two accelerator designs with four common DNN\nmodels, achieving an average performance speedup across models of up to\n3.5$\\times$ with a 2.9$\\times$ reduction in energy consumption over CPU-only\ninference. Our code is available at https://github.com/gicLAB/SECDA", "time": "2021-10-01T15:20:29Z", "link": "http://arxiv.org/abs/2110.00478v1", "id": "2110.00478v1", "title": "SECDA: Efficient Hardware/Software Co-Design of FPGA-based DNN\n  Accelerators for Edge Inference"}
{"author": "Alexandros Karargyris, Renato Umeton, Micah J. Sheller, Alejandro Aristizabal, Johnu George, Srini Bala, Daniel J. Beutel, Victor Bittorf, Akshay Chaudhari, Alexander Chowdhury, Cody Coleman, Bala Desinghu, Gregory Diamos, Debo Dutta, Diane Feddema, Grigori Fursin, Junyi Guo, Xinyuan Huang, David Kanter, Satyananda Kashyap, Nicholas Lane, Indranil Mallick, Pietro Mascagni, Virendra Mehta, Vivek Natarajan, Nikola Nikolov, Nicolas Padoy, Gennady Pekhimenko, Vijay Janapa Reddi, G Anthony Reina, Pablo Ribalta, Jacob Rosenthal, Abhishek Singh, Jayaraman J. Thiagarajan, Anna Wuest, Maria Xenochristou, Daguang Xu, Poonam Yadav, Michael Rosenthal, Massimo Loda, Jason M. Johnson, Peter Mattson", "abstract": "Medical AI has tremendous potential to advance healthcare by supporting the\nevidence-based practice of medicine, personalizing patient treatment, reducing\ncosts, and improving provider and patient experience. We argue that unlocking\nthis potential requires a systematic way to measure the performance of medical\nAI models on large-scale heterogeneous data. To meet this need, we are building\nMedPerf, an open framework for benchmarking machine learning in the medical\ndomain. MedPerf will enable federated evaluation in which models are securely\ndistributed to different facilities for evaluation, thereby empowering\nhealthcare organizations to assess and verify the performance of AI models in\nan efficient and human-supervised process, while prioritizing privacy. We\ndescribe the current challenges healthcare and AI communities face, the need\nfor an open platform, the design philosophy of MedPerf, its current\nimplementation status, and our roadmap. We call for researchers and\norganizations to join us in creating the MedPerf open benchmarking platform.", "time": "2021-12-29T01:57:24Z", "link": "http://arxiv.org/abs/2110.01406v3", "id": "2110.01406v3", "title": "MedPerf: Open Benchmarking Platform for Medical Artificial Intelligence\n  using Federated Evaluation"}
{"author": "Baris Yamansavascilar, Ahmet Cihat Baktir, Cagatay Sonmez, Atay Ozgovde, Cem Ersoy", "abstract": "The improvements in the edge computing technology pave the road for\ndiversified applications that demand real-time interaction. However, due to the\nmobility of the end-users and the dynamic edge environment, it becomes\nchallenging to handle the task offloading with high performance. Moreover,\nsince each application in mobile devices has different characteristics, a task\norchestrator must be adaptive and have the ability to learn the dynamics of the\nenvironment. For this purpose, we develop a deep reinforcement learning based\ntask orchestrator, DeepEdge, which learns to meet different task requirements\nwithout needing human interaction even under the heavily-loaded stochastic\nnetwork conditions in terms of mobile users and applications. Given the dynamic\noffloading requests and time-varying communication conditions, we successfully\nmodel the problem as a Markov process and then apply the Double Deep Q-Network\n(DDQN) algorithm to implement DeepEdge. To evaluate the robustness of DeepEdge,\nwe experiment with four different applications including image rendering,\ninfotainment, pervasive health, and augmented reality in the network under\nvarious loads. Furthermore, we compare the performance of our agent with the\nfour different task offloading approaches in the literature. Our results show\nthat DeepEdge outperforms its competitors in terms of the percentage of\nsatisfactorily completed tasks.", "time": "2022-03-31T17:46:28Z", "link": "http://arxiv.org/abs/2110.01863v2", "id": "2110.01863v2", "title": "DeepEdge: A Deep Reinforcement Learning based Task Orchestrator for Edge\n  Computing"}
{"author": "Logan Ward, Ganesh Sivaraman, J. Gregory Pauloski, Yadu Babuji, Ryan Chard, Naveen Dandu, Paul C. Redfern, Rajeev S. Assary, Kyle Chard, Larry A. Curtiss, Rajeev Thakur, Ian Foster", "abstract": "Scientific applications that involve simulation ensembles can be accelerated\ngreatly by using experiment design methods to select the best simulations to\nperform. Methods that use machine learning (ML) to create proxy models of\nsimulations show particular promise for guiding ensembles but are challenging\nto deploy because of the need to coordinate dynamic mixes of simulation and\nlearning tasks. We present Colmena, an open-source Python framework that allows\nusers to steer campaigns by providing just the implementations of individual\ntasks plus the logic used to choose which tasks to execute when. Colmena\nhandles task dispatch, results collation, ML model invocation, and ML model\n(re)training, using Parsl to execute tasks on HPC systems. We describe the\ndesign of Colmena and illustrate its capabilities by applying it to electrolyte\ndesign, where it both scales to 65536 CPUs and accelerates the discovery rate\nfor high-performance molecules by a factor of 100 over unguided searches.", "time": "2021-10-06T14:56:53Z", "link": "http://arxiv.org/abs/2110.02827v1", "id": "2110.02827v1", "title": "Colmena: Scalable Machine-Learning-Based Steering of Ensemble\n  Simulations for High Performance Computing"}
{"author": "Mohammad Aghapour, Aidin Ferdowsi, Walid Saad", "abstract": "In federated learning (FL), a machine learning model is trained on multiple\nnodes in a decentralized manner, while keeping the data local and not shared\nwith other nodes. However, FL requires the nodes to also send information on\nthe model parameters to a central server for aggregation. However, the\ninformation sent from the nodes to the server may reveal some details about\neach node's local data, thus raising privacy concerns. Furthermore, the\nrepetitive uplink transmission from the nodes to the server may result in a\ncommunication overhead and network congestion. To address these two challenges,\nin this paper, a novel two-bit aggregation algorithm is proposed with\nguaranteed differential privacy and reduced uplink communication overhead.\nExtensive experiments demonstrate that the proposed aggregation algorithm can\nachieve the same performance as state-of-the-art approaches on datasets such as\nMNIST, Fashion MNIST, CIFAR-10, and CIFAR-100, while ensuring differential\nprivacy and improving communication efficiency.", "time": "2021-10-06T19:03:58Z", "link": "http://arxiv.org/abs/2110.03017v1", "id": "2110.03017v1", "title": "Two-Bit Aggregation for Communication Efficient and Differentially\n  Private Federated Learning"}
{"author": "Thijs Vogels, Lie He, Anastasia Koloskova, Tao Lin, Sai Praneeth Karimireddy, Sebastian U. Stich, Martin Jaggi", "abstract": "In decentralized machine learning, workers compute model updates on their\nlocal data. Because the workers only communicate with few neighbors without\ncentral coordination, these updates propagate progressively over the network.\nThis paradigm enables distributed training on networks without all-to-all\nconnectivity, helping to protect data privacy as well as to reduce the\ncommunication cost of distributed training in data centers. A key challenge,\nprimarily in decentralized deep learning, remains the handling of differences\nbetween the workers' local data distributions. To tackle this challenge, we\nintroduce the RelaySum mechanism for information propagation in decentralized\nlearning. RelaySum uses spanning trees to distribute information exactly\nuniformly across all workers with finite delays depending on the distance\nbetween nodes. In contrast, the typical gossip averaging mechanism only\ndistributes data uniformly asymptotically while using the same communication\nvolume per step as RelaySum. We prove that RelaySGD, based on this mechanism,\nis independent of data heterogeneity and scales to many workers, enabling\nhighly accurate decentralized deep learning on heterogeneous data. Our code is\navailable at http://github.com/epfml/relaysgd.", "time": "2022-01-31T13:00:46Z", "link": "http://arxiv.org/abs/2110.04175v2", "id": "2110.04175v2", "title": "RelaySum for Decentralized Deep Learning on Heterogeneous Data"}
{"author": "Linghao Song, Yuze Chi, Jason Cong", "abstract": "Specialized accelerators provide gains of performance and efficiency in\nspecific domains of applications. Sparse data structures or/and representations\nexist in a wide range of applications. However, it is challenging to design\naccelerators for sparse applications because no architecture or\nperformance-level analytic models are able to fully capture the spectrum of the\nsparse data. Accelerator researchers rely on real execution to get precise\nfeedback for their designs. In this work, we present PYXIS, a performance\ndataset for specialized accelerators on sparse data. PYXIS collects accelerator\ndesigns and real execution performance statistics. Currently, there are 73.8 K\ninstances in PYXIS. PYXIS is open-source, and we are constantly growing PYXIS\nwith new accelerator designs and performance statistics. PYXIS can benefit\nresearchers in the fields of accelerator, architecture, performance, algorithm,\nand many related topics.", "time": "2022-02-22T01:35:34Z", "link": "http://arxiv.org/abs/2110.04280v2", "id": "2110.04280v2", "title": "Pyxis: An Open-Source Performance Dataset of Sparse Accelerators"}
{"author": "Saeed Rashidi, William Won, Sudarshan Srinivasan, Srinivas Sridharan, Tushar Krishna", "abstract": "Distributed training is a solution to reduce DNN training time by splitting\nthe task across multiple NPUs (e.g., GPU/TPU). However, distributed training\nadds communication overhead between the NPUs in order to synchronize the\ngradients and/or activation, depending on the parallelization strategy. In\nnext-generation platforms for training at scale, NPUs will be connected through\nmulti-dimensional networks with diverse, heterogeneous bandwidths. This work\nidentifies a looming challenge of keeping all network dimensions busy and\nmaximizing the network BW within the hybrid environment if we leverage\nscheduling techniques for collective communication on systems today. We propose\nThemis, a novel collective scheduling scheme that dynamically schedules\ncollectives (divided into chunks) to balance the communication loads across all\ndimensions, further improving the network BW utilization. Our results show that\non average, Themis can improve the network BW utilization of the single\nAll-Reduce by 1.72X (2.70X max), and improve the end-to-end training iteration\nperformance of real workloads such as ResNet-152, GNMT, DLRM, and\nTransformer-1T by 1.49X (2.25X max), 1.30X (1.78X max), 1.30X (1.77X max), and\n1.25X (1.53X max), respectively.", "time": "2022-07-07T04:20:56Z", "link": "http://arxiv.org/abs/2110.04478v3", "id": "2110.04478v3", "title": "Themis: A Network Bandwidth-Aware Collective Scheduling Policy for\n  Distributed Training of DL Models"}
{"author": "Arjun Ashok Rao, Hoi-To Wai", "abstract": "This paper considers decentralized optimization with application to machine\nlearning on graphs. The growing size of neural network (NN) models has\nmotivated prior works on decentralized stochastic gradient algorithms to\nincorporate communication compression. On the other hand, recent works have\ndemonstrated the favorable convergence and generalization properties of\noverparameterized NNs. In this work, we present an empirical analysis on the\nperformance of compressed decentralized stochastic gradient (DSG) algorithms\nwith overparameterized NNs. Through simulations on an MPI network environment,\nwe observe that the convergence rates of popular compressed DSG algorithms are\nrobust to the size of NNs. Our findings suggest a gap between theories and\npractice of the compressed DSG algorithms in the existing literature.", "time": "2021-10-09T09:28:37Z", "link": "http://arxiv.org/abs/2110.04523v1", "id": "2110.04523v1", "title": "An Empirical Study on Compressed Decentralized Stochastic Gradient\n  Algorithms with Overparameterized Models"}
{"author": "Shreshth Tuli, Sukhpal Singh Gill, Minxian Xu, Peter Garraghan, Rami Bahsoon, Schahram Dustdar, Rizos Sakellariou, Omer Rana, Rajkumar Buyya, Giuliano Casale, Nicholas R. Jennings", "abstract": "The worldwide adoption of cloud data centers (CDCs) has given rise to the\nubiquitous demand for hosting application services on the cloud. Further,\ncontemporary data-intensive industries have seen a sharp upsurge in the\nresource requirements of modern applications. This has led to the provisioning\nof an increased number of cloud servers, giving rise to higher energy\nconsumption and, consequently, sustainability concerns. Traditional heuristics\nand reinforcement learning based algorithms for energy-efficient cloud resource\nmanagement address the scalability and adaptability related challenges to a\nlimited extent. Existing work often fails to capture dependencies across\nthermal characteristics of hosts, resource consumption of tasks and the\ncorresponding scheduling decisions. This leads to poor scalability and an\nincrease in the compute resource requirements, particularly in environments\nwith non-stationary resource demands. To address these limitations, we propose\nan artificial intelligence (AI) based holistic resource management technique\nfor sustainable cloud computing called HUNTER. The proposed model formulates\nthe goal of optimizing energy efficiency in data centers as a multi-objective\nscheduling problem, considering three important models: energy, thermal and\ncooling. HUNTER utilizes a Gated Graph Convolution Network as a surrogate model\nfor approximating the Quality of Service (QoS) for a system state and\ngenerating optimal scheduling decisions. Experiments on simulated and physical\ncloud environments using the CloudSim toolkit and the COSCO framework show that\nHUNTER outperforms state-of-the-art baselines in terms of energy consumption,\nSLA violation, scheduling time, cost and temperature by up to 12, 35, 43, 54\nand 3 percent respectively.", "time": "2021-10-28T11:21:40Z", "link": "http://arxiv.org/abs/2110.05529v3", "id": "2110.05529v3", "title": "HUNTER: AI based Holistic Resource Management for Sustainable Cloud\n  Computing"}
{"author": "Anh Nguyen, Tuong Do, Minh Tran, Binh X. Nguyen, Chien Duong, Tu Phan, Erman Tjiputra, Quang D. Tran", "abstract": "Autonomous driving is an active research topic in both academia and industry.\nHowever, most of the existing solutions focus on improving the accuracy by\ntraining learnable models with centralized large-scale data. Therefore, these\nmethods do not take into account the user's privacy. In this paper, we present\na new approach to learn autonomous driving policy while respecting privacy\nconcerns. We propose a peer-to-peer Deep Federated Learning (DFL) approach to\ntrain deep architectures in a fully decentralized manner and remove the need\nfor central orchestration. We design a new Federated Autonomous Driving network\n(FADNet) that can improve the model stability, ensure convergence, and handle\nimbalanced data distribution problems while is being trained with federated\nlearning methods. Intensively experimental results on three datasets show that\nour approach with FADNet and DFL achieves superior accuracy compared with other\nrecent methods. Furthermore, our approach can maintain privacy by not\ncollecting user data to a central server.", "time": "2022-04-19T08:01:09Z", "link": "http://arxiv.org/abs/2110.05754v2", "id": "2110.05754v2", "title": "Deep Federated Learning for Autonomous Driving"}
{"author": "Tariq Abughofa, Ahmed A. Harby, Haruna Isah, Farhana Zulkernine", "abstract": "Community detection is an important research topic in graph analytics that\nhas a wide range of applications. A variety of static community detection\nalgorithms and quality metrics were developed in the past few years. However,\nmost real-world graphs are not static and often change over time. In the case\nof streaming data, communities in the associated graph need to be updated\neither continuously or whenever new data streams are added to the graph, which\nposes a much greater challenge in devising good community detection algorithms\nfor maintaining dynamic graphs over streaming data. In this paper, we propose\nan incremental community detection algorithm for maintaining a dynamic graph\nover streaming data. The contributions of this study include (a) the\nimplementation of a Distributed Weighted Community Clustering (DWCC) algorithm,\n(b) the design and implementation of a novel Incremental Distributed Weighted\nCommunity Clustering (IDWCC) algorithm, and (c) an experimental study to\ncompare the performance of our IDWCC algorithm with the DWCC algorithm. We\nvalidate the functionality and efficiency of our framework in processing\nstreaming data and performing large in-memory distributed dynamic graph\nanalytics. The results demonstrate that our IDWCC algorithm performs up to\nthree times faster than the DWCC algorithm for a similar accuracy.", "time": "2021-10-12T20:00:18Z", "link": "http://arxiv.org/abs/2110.06311v1", "id": "2110.06311v1", "title": "Incremental Community Detection in Distributed Dynamic Graph"}
{"author": "Kabir Nagrecha, Arun Kumar", "abstract": "Scaling up model depth and size is now a common approach to raise accuracy in\nmany deep learning (DL) applications, as evidenced by the widespread success of\nmulti-billion or even trillion parameter models in natural language processing\n(NLP) research. Despite success in DL research and at major technology\ncompanies, broader practical adoption of such large models among domain\nscientists and businesses is still bottlenecked by GPU memory limits, high\ntraining costs, and low GPU availability, even on public clouds. Model\nselection needs further compound these resource challenges: users often need to\ncompare dozens of models with different hyper-parameters or neural\narchitectures to suit their specific task and dataset. In this paper, we\npresent Hydra, a system designed to tackle such challenges by enabling\nout-of-the-box scaling for multi-large-model DL workloads on even commodity\nGPUs in a resource-efficient manner. Hydra is the first approach to\nholistically optimize the execution of multi-model workloads for large DL\nmodels. We do this by adapting prior \"model-parallel\" execution schemes to work\nwith scalable parameter offloading across the memory hierarchy and further\nhybridizing this approach with task-parallel job scheduling techniques. Hydra\ndecouples scalability of model parameters from parallelism of execution, thus\nenabling DL users to train even a 6-billion parameter model on a single\ncommodity GPU. It also fully exploits the speedup potential of task parallelism\nin multi-GPU setups, yielding near-linear strong scaling and making rigorous\nmodel selection perhaps more practical for such models. We evaluate end-to-end\nperformance by fine-tuning GPT-2 for language modeling. We find that Hydra\noffers between 50% and 100% higher training throughput than even the best\nsettings of state-of-the-art industrial frameworks such as DeepSpeed and GPipe\nfor multi-large-model training.", "time": "2022-08-03T18:50:20Z", "link": "http://arxiv.org/abs/2110.08633v7", "id": "2110.08633v7", "title": "Hydra: A System for Large Multi-Model Deep Learning"}
{"author": "Ningning Xie, Tamara Norman, Dominik Grewe, Dimitrios Vytiniotis", "abstract": "We present a novel characterization of the mapping of multiple parallelism\nforms (e.g. data and model parallelism) onto hierarchical accelerator systems\nthat is hierarchy-aware and greatly reduces the space of software-to-hardware\nmapping. We experimentally verify the substantial effect of these mappings on\nall-reduce performance (up to 448x). We offer a novel syntax-guided program\nsynthesis framework that is able to decompose reductions over one or more\nparallelism axes to sequences of collectives in a hierarchy- and mapping-aware\nway. For 69% of parallelism placements and user requested reductions, our\nframework synthesizes programs that outperform the default all-reduce\nimplementation when evaluated on different GPU hierarchies (max 2.04x, average\n1.27x). We complement our synthesis tool with a simulator exceeding 90% top-10\naccuracy, which therefore reduces the need for massive evaluations of synthesis\nresults to determine a small set of optimal programs and mappings.", "time": "2021-11-16T12:54:39Z", "link": "http://arxiv.org/abs/2110.10548v2", "id": "2110.10548v2", "title": "Synthesizing Optimal Parallelism Placement and Reduction Strategies on\n  Hierarchical Systems for Deep Learning"}
{"author": "Oliver Rausch, Tal Ben-Nun, Nikoli Dryden, Andrei Ivanov, Shigang Li, Torsten Hoefler", "abstract": "Rapid progress in deep learning is leading to a diverse set of quickly\nchanging models, with a dramatically growing demand for compute. However, as\nframeworks specialize performance optimization to patterns in popular networks,\nthey implicitly constrain novel and diverse models that drive progress in\nresearch. We empower deep learning researchers by defining a flexible and\nuser-customizable pipeline for optimizing training of arbitrary deep neural\nnetworks, based on data movement minimization. The pipeline begins with\nstandard networks in PyTorch or ONNX and transforms computation through\nprogressive lowering. We define four levels of general-purpose transformations,\nfrom local intra-operator optimizations to global data movement reduction.\nThese operate on a data-centric graph intermediate representation that\nexpresses computation and data movement at all levels of abstraction, including\nexpanding basic operators such as convolutions to their underlying\ncomputations. Central to the design is the interactive and introspectable\nnature of the pipeline. Every part is extensible through a Python API, and can\nbe tuned interactively using a GUI. We demonstrate competitive performance or\nspeedups on ten different networks, with interactive optimizations discovering\nnew opportunities in EfficientNet.", "time": "2022-08-29T23:16:19Z", "link": "http://arxiv.org/abs/2110.10802v3", "id": "2110.10802v3", "title": "A Data-Centric Optimization Framework for Machine Learning"}
{"author": "Hao Chen, Shaocheng Huang, Deyou Zhang, Ming Xiao, Mikael Skoglund, H. Vincent Poor", "abstract": "To leverage massive distributed data and computation resources, machine\nlearning in the network edge is considered to be a promising technique\nespecially for large-scale model training. Federated learning (FL), as a\nparadigm of collaborative learning techniques, has obtained increasing research\nattention with the benefits of communication efficiency and improved data\nprivacy. Due to the lossy communication channels and limited communication\nresources (e.g., bandwidth and power), it is of interest to investigate fast\nresponding and accurate FL schemes over wireless systems. Hence, we investigate\nthe problem of jointly optimized communication efficiency and resources for FL\nover wireless Internet of things (IoT) networks. To reduce complexity, we\ndivide the overall optimization problem into two sub-problems, i.e., the client\nscheduling problem and the resource allocation problem. To reduce the\ncommunication costs for FL in wireless IoT networks, a new client scheduling\npolicy is proposed by reusing stale local model parameters. To maximize\nsuccessful information exchange over networks, a Lagrange multiplier method is\nfirst leveraged by decoupling variables including power variables, bandwidth\nvariables and transmission indicators. Then a linear-search based power and\nbandwidth allocation method is developed. Given appropriate hyper-parameters,\nwe show that the proposed communication-efficient federated learning (CEFL)\nframework converges at a strong linear rate. Through extensive experiments, it\nis revealed that the proposed CEFL framework substantially boosts both the\ncommunication efficiency and learning performance of both training loss and\ntest accuracy for FL over wireless IoT networks compared to a basic FL approach\nwith uniform resource allocation.", "time": "2021-10-22T13:25:57Z", "link": "http://arxiv.org/abs/2110.11775v1", "id": "2110.11775v1", "title": "Federated Learning over Wireless IoT Networks with Optimized\n  Communication and Resources"}
{"author": "Junxiao Wang, Song Guo, Xin Xie, Heng Qi", "abstract": "We explore the problem of selectively forgetting categories from trained CNN\nclassification models in the federated learning (FL). Given that the data used\nfor training cannot be accessed globally in FL, our insights probe deep into\nthe internal influence of each channel. Through the visualization of feature\nmaps activated by different channels, we observe that different channels have a\nvarying contribution to different categories in image classification. Inspired\nby this, we propose a method for scrubbing the model clean of information about\nparticular categories. The method does not require retraining from scratch, nor\nglobal access to the data used for training. Instead, we introduce the concept\nof Term Frequency Inverse Document Frequency (TF-IDF) to quantize the class\ndiscrimination of channels. Channels with high TF-IDF scores have more\ndiscrimination on the target categories and thus need to be pruned to unlearn.\nThe channel pruning is followed by a fine-tuning process to recover the\nperformance of the pruned model. Evaluated on CIFAR10 dataset, our method\naccelerates the speed of unlearning by 8.9x for the ResNet model, and 7.9x for\nthe VGG model under no degradation in accuracy, compared to retraining from\nscratch. For CIFAR100 dataset, the speedups are 9.9x and 8.4x, respectively. We\nenvision this work as a complementary block for FL towards compliance with\nlegal and ethical criteria.", "time": "2022-01-29T03:06:56Z", "link": "http://arxiv.org/abs/2110.11794v3", "id": "2110.11794v3", "title": "Federated Unlearning via Class-Discriminative Pruning"}
{"author": "Pankaj Singh, Sudhakar Singh, P K Mishra, Rakhi Garg", "abstract": "Frequent itemset mining (FIM) is a highly computational and data intensive\nalgorithm. Therefore, parallel and distributed FIM algorithms have been\ndesigned to process large volume of data in a reduced time. Recently, a number\nof FIM algorithms have been designed on Hadoop MapReduce, a distributed big\ndata processing framework. But, due to heavy disk I/O, MapReduce is found to be\ninefficient for the highly iterative FIM algorithms. Therefore, Spark, a more\nefficient distributed data processing framework, has been developed with\nin-memory computation and resilient distributed dataset (RDD) features to\nsupport the iterative algorithms. On this framework, Apriori and FP-Growth\nbased FIM algorithms have been designed on the Spark RDD framework, but\nEclat-based algorithm has not been explored yet. In this paper, RDD-Eclat, a\nparallel Eclat algorithm on the Spark RDD framework is proposed with its five\nvariants. The proposed algorithms are evaluated on the various benchmark\ndatasets, and the experimental results show that RDD-Eclat outperforms the\nSpark-based Apriori by many times. Also, the experimental results show the\nscalability of the proposed algorithms on increasing the number of cores and\nsize of the dataset.", "time": "2021-10-22T18:51:10Z", "link": "http://arxiv.org/abs/2110.12012v1", "id": "2110.12012v1", "title": "RDD-Eclat: Approaches to Parallelize Eclat Algorithm on Spark RDD\n  Framework (Extended Version)"}
{"author": "Honglin Yuan, Warren Morningstar, Lin Ning, Karan Singhal", "abstract": "Federated learning data is drawn from a distribution of distributions:\nclients are drawn from a meta-distribution, and their data are drawn from local\ndata distributions. Thus generalization studies in federated learning should\nseparate performance gaps from unseen client data (out-of-sample gap) from\nperformance gaps from unseen client distributions (participation gap). In this\nwork, we propose a framework for disentangling these performance gaps. Using\nthis framework, we observe and explain differences in behavior across natural\nand synthetic federated datasets, indicating that dataset synthesis strategy\ncan be important for realistic simulations of generalization in federated\nlearning. We propose a semantic synthesis strategy that enables realistic\nsimulation without naturally-partitioned data. Informed by our findings, we\ncall out community suggestions for future federated learning works.", "time": "2022-03-16T05:54:12Z", "link": "http://arxiv.org/abs/2110.14216v2", "id": "2110.14216v2", "title": "What Do We Mean by Generalization in Federated Learning?"}
{"author": "Varad Pimpalkhute, Amey Pandit, Mayank Mishra, Rekha Singhal", "abstract": "Meta Learning has been in focus in recent years due to the meta-learner\nmodel's ability to adapt well and generalize to new tasks, thus, reducing both\nthe time and data requirements for learning. However, a major drawback of meta\nlearner is that, to reach to a state from where learning new tasks becomes\nfeasible with less data, it requires a large number of iterations and a lot of\ntime. We address this issue by proposing various acceleration techniques to\nspeed up meta learning algorithms such as MAML (Model Agnostic Meta Learning).\nWe present 3.73X acceleration on a well known RNN optimizer based meta learner\nproposed in literature [11]. We introduce a novel method of training tasks in\nclusters, which not only accelerates the meta learning process but also\nimproves model accuracy performance.\n  Keywords: Meta learning, RNN optimizer, AGI, Performance optimization", "time": "2021-10-27T14:27:36Z", "link": "http://arxiv.org/abs/2110.14459v1", "id": "2110.14459v1", "title": "Accelerating Gradient-based Meta Learner"}
{"author": "Shunpu Tang, Lunyuan Chen, Ke HeJunjuan Xia, Lisheng Fan, Arumugam Nallanathan", "abstract": "In this paper, we investigate how to deploy computational intelligence and\ndeep learning (DL) in edge-enabled industrial IoT networks. In this system, the\nIoT devices can collaboratively train a shared model without compromising data\nprivacy. However, due to limited resources in the industrial IoT networks,\nincluding computational power, bandwidth, and channel state, it is challenging\nfor many devices to accomplish local training and upload weights to the edge\nserver in time. To address this issue, we propose a novel multi-exit-based\nfederated edge learning (ME-FEEL) framework, where the deep model can be\ndivided into several sub-models with different depths and output prediction\nfrom the exit in the corresponding sub-model. In this way, the devices with\ninsufficient computational power can choose the earlier exits and avoid\ntraining the complete model, which can help reduce computational latency and\nenable devices to participate into aggregation as much as possible within a\nlatency threshold. Moreover, we propose a greedy approach-based exit selection\nand bandwidth allocation algorithm to maximize the total number of exits in\neach communication round. Simulation experiments are conducted on the classical\nFashion-MNIST dataset under a non-independent and identically distributed\n(non-IID) setting, and it shows that the proposed strategy outperforms the\nconventional FL. In particular, the proposed ME-FEEL can achieve an accuracy\ngain up to 32.7% in the industrial IoT networks with the severely limited\nresources.", "time": "2021-10-28T08:14:57Z", "link": "http://arxiv.org/abs/2110.14937v1", "id": "2110.14937v1", "title": "Computational Intelligence and Deep Learning for Next-Generation\n  Edge-Enabled Industrial IoT"}
{"author": "Andrei Afonin, Sai Praneeth Karimireddy", "abstract": "Is it possible to design an universal API for federated learning using which\nan ad-hoc group of data-holders (agents) collaborate with each other and\nperform federated learning? Such an API would necessarily need to be\nmodel-agnostic i.e. make no assumption about the model architecture being used\nby the agents, and also cannot rely on having representative public data at\nhand. Knowledge distillation (KD) is the obvious tool of choice to design such\nprotocols. However, surprisingly, we show that most natural KD-based federated\nlearning protocols have poor performance.\n  To investigate this, we propose a new theoretical framework, Federated Kernel\nridge regression, which can capture both model heterogeneity as well as data\nheterogeneity. Our analysis shows that the degradation is largely due to a\nfundamental limitation of knowledge distillation under data heterogeneity. We\nfurther validate our framework by analyzing and designing new protocols based\non KD. Their performance on real world experiments using neural networks,\nthough still unsatisfactory, closely matches our theoretical predictions.", "time": "2022-05-11T03:19:03Z", "link": "http://arxiv.org/abs/2110.15210v2", "id": "2110.15210v2", "title": "Towards Model Agnostic Federated Learning Using Knowledge Distillation"}
{"author": "Oualid Zari, Chuan Xu, Giovanni Neglia", "abstract": "In cross-device federated learning (FL) setting, clients such as mobiles\ncooperate with the server to train a global machine learning model, while\nmaintaining their data locally. However, recent work shows that client's\nprivate information can still be disclosed to an adversary who just eavesdrops\nthe messages exchanged between the client and the server. For example, the\nadversary can infer whether the client owns a specific data instance, which is\ncalled a passive membership inference attack. In this paper, we propose a new\npassive inference attack that requires much less computation power and memory\nthan existing methods. Our empirical results show that our attack achieves a\nhigher accuracy on CIFAR100 dataset (more than $4$ percentage points) with\nthree orders of magnitude less memory space and five orders of magnitude less\ncalculations.", "time": "2021-10-31T08:21:23Z", "link": "http://arxiv.org/abs/2111.00430v1", "id": "2111.00430v1", "title": "Efficient passive membership inference attack in federated learning"}
{"author": "Zhuofu Tao, Chen Wu, Yuan Liang, Lei He", "abstract": "Graph convolutional networks (GCNs) have been introduced to effectively\nprocess non-euclidean graph data. However, GCNs incur large amounts of\nirregularity in computation and memory access, which prevents efficient use of\ntraditional neural network accelerators. Moreover, existing dedicated GCN\naccelerators demand high memory volumes and are difficult to implement onto\nresource limited edge devices. In this work, we propose LW-GCN, a lightweight\nFPGA-based accelerator with a software-hardware co-designed process to tackle\nirregularity in computation and memory access in GCN inference. LW-GCN\ndecomposes the main GCN operations into sparse-dense matrix multiplication\n(SDMM) and dense matrix multiplication (DMM). We propose a novel compression\nformat to balance workload across PEs and prevent data hazards. Moreover, we\napply data quantization and workload tiling, and map both SDMM and DMM of GCN\ninference onto a uniform architecture on resource limited hardware. Evaluation\non GCN and GraphSAGE are performed on Xilinx Kintex-7 FPGA with three popular\ndatasets. Compared to existing CPU, GPU, and state-of-the-art FPGA-based\naccelerator, LW-GCN reduces latency by up to 60x, 12x and 1.7x and increases\npower efficiency by up to 912x., 511x and 3.87x, respectively. Furthermore,\ncompared with NVIDIA's latest edge GPU Jetson Xavier NX, LW-GCN achieves\nspeedup and energy savings of 32x and 84x, respectively.", "time": "2021-11-04T22:29:53Z", "link": "http://arxiv.org/abs/2111.03184v1", "id": "2111.03184v1", "title": "LW-GCN: A Lightweight FPGA-based Graph Convolutional Network Accelerator"}
{"author": "Andreas Grafberger, Mohak Chadha, Anshul Jindal, Jianfeng Gu, Michael Gerndt", "abstract": "The traditional cloud-centric approach for Deep Learning (DL) requires\ntraining data to be collected and processed at a central server which is often\nchallenging in privacy-sensitive domains like healthcare. Towards this, a new\nlearning paradigm called Federated Learning (FL) has been proposed that brings\nthe potential of DL to these domains while addressing privacy and data\nownership issues. FL enables remote clients to learn a shared ML model while\nkeeping the data local. However, conventional FL systems face several\nchallenges such as scalability, complex infrastructure management, and wasted\ncompute and incurred costs due to idle clients. These challenges of FL systems\nclosely align with the core problems that serverless computing and\nFunction-as-a-Service (FaaS) platforms aim to solve. These include rapid\nscalability, no infrastructure management, automatic scaling to zero for idle\nclients, and a pay-per-use billing model. To this end, we present a novel\nsystem and framework for serverless FL, called FedLess. Our system supports\nmultiple commercial and self-hosted FaaS providers and can be deployed in the\ncloud, on-premise in institutional data centers, and on edge devices. To the\nbest of our knowledge, we are the first to enable FL across a large fabric of\nheterogeneous FaaS providers while providing important features like security\nand Differential Privacy. We demonstrate with comprehensive experiments that\nthe successful training of DNNs for different tasks across up to 200 client\nfunctions and more is easily possible using our system. Furthermore, we\ndemonstrate the practical viability of our methodology by comparing it against\na traditional FL system and show that it can be cheaper and more\nresource-efficient.", "time": "2021-11-05T11:14:07Z", "link": "http://arxiv.org/abs/2111.03396v1", "id": "2111.03396v1", "title": "FedLess: Secure and Scalable Federated Learning Using Serverless\n  Computing"}
{"author": "Margalit Glasgow, Honglin Yuan, Tengyu Ma", "abstract": "Federated Averaging (FedAvg), also known as Local SGD, is one of the most\npopular algorithms in Federated Learning (FL). Despite its simplicity and\npopularity, the convergence rate of FedAvg has thus far been undetermined. Even\nunder the simplest assumptions (convex, smooth, homogeneous, and bounded\ncovariance), the best-known upper and lower bounds do not match, and it is not\nclear whether the existing analysis captures the capacity of the algorithm. In\nthis work, we first resolve this question by providing a lower bound for FedAvg\nthat matches the existing upper bound, which shows the existing FedAvg upper\nbound analysis is not improvable. Additionally, we establish a lower bound in a\nheterogeneous setting that nearly matches the existing upper bound. While our\nlower bounds show the limitations of FedAvg, under an additional assumption of\nthird-order smoothness, we prove more optimistic state-of-the-art convergence\nresults in both convex and non-convex settings. Our analysis stems from a\nnotion we call iterate bias, which is defined by the deviation of the\nexpectation of the SGD trajectory from the noiseless gradient descent\ntrajectory with the same initialization. We prove novel sharp bounds on this\nquantity, and show intuitively how to analyze this quantity from a Stochastic\nDifferential Equation (SDE) perspective.", "time": "2022-02-11T22:03:02Z", "link": "http://arxiv.org/abs/2111.03741v2", "id": "2111.03741v2", "title": "Sharp Bounds for Federated Averaging (Local SGD) and Continuous\n  Perspective"}
{"author": "Yingying Zhang, Zhengxiong Guan, Huajie Qian, Leili Xu, Hengbo Liu, Qingsong Wen, Liang Sun, Junwei Jiang, Lunting Fan, Min Ke", "abstract": "As business of Alibaba expands across the world among various industries,\nhigher standards are imposed on the service quality and reliability of big data\ncloud computing platforms which constitute the infrastructure of Alibaba Cloud.\nHowever, root cause analysis in these platforms is non-trivial due to the\ncomplicated system architecture. In this paper, we propose a root cause\nanalysis framework called CloudRCA which makes use of heterogeneous\nmulti-source data including Key Performance Indicators (KPIs), logs, as well as\ntopology, and extracts important features via state-of-the-art anomaly\ndetection and log analysis techniques. The engineered features are then\nutilized in a Knowledge-informed Hierarchical Bayesian Network (KHBN) model to\ninfer root causes with high accuracy and efficiency. Ablation study and\ncomprehensive experimental comparisons demonstrate that, compared to existing\nframeworks, CloudRCA 1) consistently outperforms existing approaches in\nf1-score across different cloud systems; 2) can handle novel types of root\ncauses thanks to the hierarchical structure of KHBN; 3) performs more robustly\nwith respect to algorithmic configurations; and 4) scales more favorably in the\ndata and feature sizes. Experiments also show that a cross-platform transfer\nlearning mechanism can be adopted to further improve the accuracy by more than\n10\\%. CloudRCA has been integrated into the diagnosis system of Alibaba Cloud\nand employed in three typical cloud computing platforms including MaxCompute,\nRealtime Compute and Hologres. It saves Site Reliability Engineers (SREs) more\nthan $20\\%$ in the time spent on resolving failures in the past twelve months\nand improves service reliability significantly.", "time": "2021-11-05T23:03:21Z", "link": "http://arxiv.org/abs/2111.03753v1", "id": "2111.03753v1", "title": "CloudRCA: A Root Cause Analysis Framework for Cloud Computing Platforms"}
{"author": "Atefeh Sohrabizadeh, Yuze Chi, Jason Cong", "abstract": "While there have been many studies on hardware acceleration for deep learning\non images, there has been a rather limited focus on accelerating deep learning\napplications involving graphs. The unique characteristics of graphs, such as\nthe irregular memory access and dynamic parallelism, impose several challenges\nwhen the algorithm is mapped to a CPU or GPU. To address these challenges while\nexploiting all the available sparsity, we propose a flexible architecture\ncalled SPA-GCN for accelerating Graph Convolutional Networks (GCN), the core\ncomputation unit in deep learning algorithms on graphs. The architecture is\nspecialized for dealing with many small graphs since the graph size has a\nsignificant impact on design considerations. In this context, we use SimGNN, a\nneural-network-based graph matching algorithm, as a case study to demonstrate\nthe effectiveness of our architecture. The experimental results demonstrate\nthat SPA-GCN can deliver a high speedup compared to a multi-core CPU\nimplementation and a GPU implementation, showing the efficiency of our design.", "time": "2021-11-10T20:47:57Z", "link": "http://arxiv.org/abs/2111.05936v1", "id": "2111.05936v1", "title": "SPA-GCN: Efficient and Flexible GCN Accelerator with an Application for\n  Graph Similarity Computation"}
{"author": "Prateek Sharma, Vikram Jadhao", "abstract": "Scientific computing applications have benefited greatly from high\nperformance computing infrastructure such as supercomputers. However, we are\nseeing a paradigm shift in the computational structure, design, and\nrequirements of these applications. Increasingly, data-driven and machine\nlearning approaches are being used to support, speed-up, and enhance scientific\ncomputing applications, especially molecular dynamics simulations.\nConcurrently, cloud computing platforms are increasingly appealing for\nscientific computing, providing \"infinite\" computing powers, easier programming\nand deployment models, and access to computing accelerators such as TPUs\n(Tensor Processing Units). This confluence of machine learning (ML) and cloud\ncomputing represents exciting opportunities for cloud and systems researchers.\nML-assisted molecular dynamics simulations are a new class of workload, and\nexhibit unique computational patterns. These simulations present new challenges\nfor low-cost and high-performance execution. We argue that transient cloud\nresources, such as low-cost preemptible cloud VMs, can be a viable platform for\nthis new workload. Finally, we present some low-hanging fruits and long-term\nchallenges in cloud resource management, and the integration of molecular\ndynamics simulations into ML platforms (such as TensorFlow).", "time": "2021-11-11T21:20:26Z", "link": "http://arxiv.org/abs/2111.06466v1", "id": "2111.06466v1", "title": "Molecular Dynamics Simulations on Cloud Computing and Machine Learning\n  Platforms"}
{"author": "Amir Hossein Estiri, Muthucumaru Maheswaran", "abstract": "Machine learning (ML) is expected to play a major role in 5G edge computing.\nVarious studies have demonstrated that ML is highly suitable for optimizing\nedge computing systems as rapid mobility and application-induced changes occur\nat the edge. For ML to provide the best solutions, it is important to\ncontinually train the ML models to include the changing scenarios. The sudden\nchanges in data distributions caused by changing scenarios (e.g., 5G base\nstation failures) is referred to as concept drift and is a major challenge to\ncontinual learning. The ML models can present high error rates while the drifts\ntake place and the errors decrease only after the model learns the\ndistributions. This problem is more pronounced in a distributed setting where\nmultiple ML models are being used for different heterogeneous datasets and the\nfinal model needs to capture all concept drifts. In this paper, we show that\nusing Attention in Federated Learning (FL) is an efficient way of handling\nconcept drifts. We use a 5G network traffic dataset to simulate concept drift\nand test various scenarios. The results indicate that Attention can\nsignificantly improve the concept drift handling capability of FL.", "time": "2021-11-14T21:48:49Z", "link": "http://arxiv.org/abs/2111.07457v1", "id": "2111.07457v1", "title": "Attentive Federated Learning for Concept Drift in Distributed 5G Edge\n  Networks"}
{"author": "Poornima Mahadevappa, Raja Kumar Murugesan", "abstract": "Edge computing provides an agile data processing platform for\nlatency-sensitive and communication-intensive applications through a\ndecentralized cloud and geographically distributed edge nodes. Gaining\ncentralized control over the edge nodes can be challenging due to security\nissues and threats. Among several security issues, data integrity attacks can\nlead to inconsistent data and intrude edge data analytics. Further\nintensification of the attack makes it challenging to mitigate and identify the\nroot cause. Therefore, this paper proposes a new concept of data quarantine\nmodel to mitigate data integrity attacks by quarantining intruders. The\nefficient security solutions in cloud, ad-hoc networks, and computer systems\nusing quarantine have motivated adopting it in edge computing. The data\nacquisition edge nodes identify the intruders and quarantine all the suspected\ndevices through dimensionality reduction. During quarantine, the proposed\nconcept builds the reputation scores to determine the falsely identified\nlegitimate devices and sanitize their affected data to regain data integrity.\nAs a preliminary investigation, this work identifies an appropriate machine\nlearning method, Linear Discriminant Analysis (LDA), for dimensionality\nreduction. The LDA results in 72.83% quarantine accuracy and 0.9 seconds\ntraining time, which is efficient than other state-of-the-art methods. In\nfuture, this would be implemented and validated with ground truth data.", "time": "2021-11-15T11:04:48Z", "link": "http://arxiv.org/abs/2111.07672v1", "id": "2111.07672v1", "title": "A Data Quarantine Model to Secure Data in Edge Computing"}
{"author": "Pinyarash Pinyoanuntapong, Prabhu Janakaraj, Ravikumar Balakrishnan, Minwoo Lee, Chen Chen, Pu Wang", "abstract": "Federated learning (FL) is a distributed machine learning technology for\nnext-generation AI systems that allows a number of workers, i.e., edge devices,\ncollaboratively learn a shared global model while keeping their data locally to\nprevent privacy leakage. Enabling FL over wireless multi-hop networks can\ndemocratize AI and make it accessible in a cost-effective manner. However, the\nnoisy bandwidth-limited multi-hop wireless connections can lead to delayed and\nnomadic model updates, which significantly slows down the FL convergence speed.\nTo address such challenges, this paper aims to accelerate FL convergence over\nwireless edge by optimizing the multi-hop federated networking performance. In\nparticular, the FL convergence optimization problem is formulated as a Markov\ndecision process (MDP). To solve such MDP, multi-agent reinforcement learning\n(MA-RL) algorithms along with domain-specific action space refining schemes are\ndeveloped, which online learn the delay-minimum forwarding paths to minimize\nthe model exchange latency between the edge devices (i.e., workers) and the\nremote server. To validate the proposed solutions, FedEdge is developed and\nimplemented, which is the first experimental framework in the literature for FL\nover multi-hop wireless edge computing networks. FedEdge allows us to fast\nprototype, deploy, and evaluate novel FL algorithms along with RL-based system\noptimization methods in real wireless devices. Moreover, a physical\nexperimental testbed is implemented by customizing the widely adopted Linux\nwireless routers and ML computing nodes.Finally, our experimentation results on\nthe testbed show that the proposed network-accelerated FL system can\npractically and significantly improve FL convergence speed, compared to the FL\nsystem empowered by the production-grade commercially available wireless\nnetworking protocol, BATMAN-Adv.", "time": "2022-05-31T13:45:32Z", "link": "http://arxiv.org/abs/2111.09410v4", "id": "2111.09410v4", "title": "EdgeML: Towards Network-Accelerated Federated Learning over Wireless\n  Edge"}
{"author": "Matias Mendieta, Taojiannan Yang, Pu Wang, Minwoo Lee, Zhengming Ding, Chen Chen", "abstract": "Federated learning (FL) is a promising strategy for performing\nprivacy-preserving, distributed learning with a network of clients (i.e., edge\ndevices). However, the data distribution among clients is often non-IID in\nnature, making efficient optimization difficult. To alleviate this issue, many\nFL algorithms focus on mitigating the effects of data heterogeneity across\nclients by introducing a variety of proximal terms, some incurring considerable\ncompute and/or memory overheads, to restrain local updates with respect to the\nglobal model. Instead, we consider rethinking solutions to data heterogeneity\nin FL with a focus on local learning generality rather than proximal\nrestriction. To this end, we first present a systematic study informed by\nsecond-order indicators to better understand algorithm effectiveness in FL.\nInterestingly, we find that standard regularization methods are surprisingly\nstrong performers in mitigating data heterogeneity effects. Based on our\nfindings, we further propose a simple and effective method, FedAlign, to\novercome data heterogeneity and the pitfalls of previous methods. FedAlign\nachieves competitive accuracy with state-of-the-art FL methods across a variety\nof settings while minimizing computation and memory overhead. Code is available\nat https://github.com/mmendiet/FedAlign", "time": "2022-04-13T18:30:03Z", "link": "http://arxiv.org/abs/2111.14213v3", "id": "2111.14213v3", "title": "Local Learning Matters: Rethinking Data Heterogeneity in Federated\n  Learning"}
{"author": "Floris-Jan Willemsen, Rob van Nieuwpoort, Ben van Werkhoven", "abstract": "Finding optimal parameter configurations for tunable GPU kernels is a\nnon-trivial exercise for large search spaces, even when automated. This poses\nan optimization task on a non-convex search space, using an expensive to\nevaluate function with unknown derivative. These characteristics make a good\ncandidate for Bayesian Optimization, which has not been applied to this problem\nbefore. However, the application of Bayesian Optimization to this problem is\nchallenging. We demonstrate how to deal with the rough, discrete, constrained\nsearch spaces, containing invalid configurations. We introduce a novel\ncontextual variance exploration factor, as well as new acquisition functions\nwith improved scalability, combined with an informed acquisition function\nselection mechanism. By comparing the performance of our Bayesian Optimization\nimplementation on various test cases to the existing search strategies in\nKernel Tuner, as well as other Bayesian Optimization implementations, we\ndemonstrate that our search strategies generalize well and consistently\noutperform other search strategies by a wide margin.", "time": "2021-11-26T11:26:26Z", "link": "http://arxiv.org/abs/2111.14991v1", "id": "2111.14991v1", "title": "Bayesian Optimization for auto-tuning GPU kernels"}
{"author": "Michael Kapralov, Silvio Lattanzi, Navid Nouri, Jakab Tardos", "abstract": "Random walks are a fundamental primitive used in many machine learning\nalgorithms with several applications in clustering and semi-supervised\nlearning. Despite their relevance, the first efficient parallel algorithm to\ncompute random walks has been introduced very recently (Lacki et al.).\nUnfortunately their method has a fundamental shortcoming: their algorithm is\nnon-local in that it heavily relies on computing random walks out of all nodes\nin the input graph, even though in many practical applications one is\ninterested in computing random walks only from a small subset of nodes in the\ngraph. In this paper, we present a new algorithm that overcomes this limitation\nby building random walk efficiently and locally at the same time. We show that\nour technique is both memory and round efficient, and in particular yields an\nefficient parallel local clustering algorithm. Finally, we complement our\ntheoretical analysis with experimental results showing that our algorithm is\nsignificantly more scalable than previous approaches.", "time": "2021-12-01T17:06:11Z", "link": "http://arxiv.org/abs/2112.00655v1", "id": "2112.00655v1", "title": "Efficient and Local Parallel Random Walks"}
{"author": "Kate Donahue, Jon Kleinberg", "abstract": "In many real-world situations, data is distributed across multiple\nself-interested agents. These agents can collaborate to build a machine\nlearning model based on data from multiple agents, potentially reducing the\nerror each experiences. However, sharing models in this way raises questions of\nfairness: to what extent can the error experienced by one agent be\nsignificantly lower than the error experienced by another agent in the same\ncoalition? In this work, we consider two notions of fairness that each may be\nappropriate in different circumstances: \"egalitarian fairness\" (which aims to\nbound how dissimilar error rates can be) and \"proportional fairness\" (which\naims to reward players for contributing more data). We similarly consider two\ncommon methods of model aggregation, one where a single model is created for\nall agents (uniform), and one where an individualized model is created for each\nagent. For egalitarian fairness, we obtain a tight multiplicative bound on how\nwidely error rates can diverge between agents collaborating (which holds for\nboth aggregation methods). For proportional fairness, we show that the\nindividualized aggregation method always gives a small player error that is\nupper bounded by proportionality. For uniform aggregation, we show that this\nupper bound is guaranteed for any individually rational coalition (where no\nplayer wishes to leave to do local learning).", "time": "2023-02-25T19:18:08Z", "link": "http://arxiv.org/abs/2112.00818v3", "id": "2112.00818v3", "title": "Models of fairness in federated learning"}
{"author": "Norman A. Rink, Adam Paszke, Dimitrios Vytiniotis, Georg Stefan Schmid", "abstract": "Modern large-scale deep learning workloads highlight the need for parallel\nexecution across many devices in order to fit model data into hardware\naccelerator memories. In these settings, array redistribution may be required\nduring a computation, but can also become a bottleneck if not done efficiently.\nIn this paper we address the problem of redistributing multi-dimensional array\ndata in SPMD computations, the most prevalent form of parallelism in deep\nlearning. We present a type-directed approach to synthesizing array\nredistributions as sequences of MPI-style collective operations. We prove\nformally that our synthesized redistributions are memory-efficient and perform\nno excessive data transfers. Array redistribution for SPMD computations using\ncollective operations has also been implemented in the context of the XLA SPMD\npartitioner, a production-grade tool for partitioning programs across\naccelerator systems. We evaluate our approach against the XLA implementation\nand find that our approach delivers a geometric mean speedup of $1.22\\times$,\nwith maximum speedups as a high as $5.7\\times$, while offering provable memory\nguarantees, making our system particularly appealing for large-scale models.", "time": "2022-11-28T15:37:22Z", "link": "http://arxiv.org/abs/2112.01075v2", "id": "2112.01075v2", "title": "Memory-efficient array redistribution through portable collective\n  communication"}
{"author": "Franziska Boenisch, Adam Dziedzic, Roei Schuster, Ali Shahin Shamsabadi, Ilia Shumailov, Nicolas Papernot", "abstract": "In federated learning (FL), data does not leave personal devices when they\nare jointly training a machine learning model. Instead, these devices share\ngradients, parameters, or other model updates, with a central party (e.g., a\ncompany) coordinating the training. Because data never \"leaves\" personal\ndevices, FL is often presented as privacy-preserving. Yet, recently it was\nshown that this protection is but a thin facade, as even a passive,\nhonest-but-curious attacker observing gradients can reconstruct data of\nindividual users contributing to the protocol. In this work, we show a novel\ndata reconstruction attack which allows an active and dishonest central party\nto efficiently extract user data from the received gradients. While prior work\non data reconstruction in FL relies on solving computationally expensive\noptimization problems or on making easily detectable modifications to the\nshared model's architecture or parameters, in our attack the central party\nmakes inconspicuous changes to the shared model's weights before sending them\nout to the users. We call the modified weights of our attack trap weights. Our\nactive attacker is able to recover user data perfectly, i.e., with zero error,\neven when this data stems from the same class. Recovery comes with near-zero\ncosts: the attack requires no complex optimization objectives. Instead, our\nattacker exploits inherent data leakage from model gradients and simply\namplifies this effect by maliciously altering the weights of the shared model\nthrough the trap weights. These specificities enable our attack to scale to\nfully-connected and convolutional deep neural networks trained with large\nmini-batches of data. For example, for the high-dimensional vision dataset\nImageNet, we perfectly reconstruct more than 50% of the training data points\nfrom mini-batches as large as 100 data points.", "time": "2023-04-12T21:27:15Z", "link": "http://arxiv.org/abs/2112.02918v2", "id": "2112.02918v2", "title": "When the Curious Abandon Honesty: Federated Learning Is Not Private"}
{"author": "Emre Ozfatura, Deniz Gunduz, H. Vincent Poor", "abstract": "In this chapter, we will mainly focus on collaborative training across\nwireless devices. Training a ML model is equivalent to solving an optimization\nproblem, and many distributed optimization algorithms have been developed over\nthe last decades. These distributed ML algorithms provide data locality; that\nis, a joint model can be trained collaboratively while the data available at\neach participating device remains local. This addresses, to some extend, the\nprivacy concern. They also provide computational scalability as they allow\nexploiting computational resources distributed across many edge devices.\nHowever, in practice, this does not directly lead to a linear gain in the\noverall learning speed with the number of devices. This is partly due to the\ncommunication bottleneck limiting the overall computation speed. Additionally,\nwireless devices are highly heterogeneous in their computational capabilities,\nand both their computation speed and communication rate can be highly\ntime-varying due to physical factors. Therefore, distributed learning\nalgorithms, particularly those to be implemented at the wireless network edge,\nmust be carefully designed taking into account the impact of time-varying\ncommunication network as well as the heterogeneous and stochastic computation\ncapabilities of devices.", "time": "2021-12-07T20:15:39Z", "link": "http://arxiv.org/abs/2112.05559v1", "id": "2112.05559v1", "title": "Collaborative Learning over Wireless Networks: An Introductory Overview"}
{"author": "Francesc Wilhelmi, Lorenza Giupponi, Paolo Dini", "abstract": "Motivated by the heterogeneous nature of devices participating in large-scale\nFederated Learning (FL) optimization, we focus on an asynchronous server-less\nFL solution empowered by blockchain technology. In contrast to mostly adopted\nFL approaches, which assume synchronous operation, we advocate an asynchronous\nmethod whereby model aggregation is done as clients submit their local updates.\nThe asynchronous setting fits well with the federated optimization idea in\npractical large-scale settings with heterogeneous clients. Thus, it potentially\nleads to higher efficiency in terms of communication overhead and idle periods.\nTo evaluate the learning completion delay of BC-enabled FL, we provide an\nanalytical model based on batch service queue theory. Furthermore, we provide\nsimulation results to assess the performance of both synchronous and\nasynchronous mechanisms. Important aspects involved in the BC-enabled FL\noptimization, such as the network size, link capacity, or user requirements,\nare put together and analyzed. As our results show, the synchronous setting\nleads to higher prediction accuracy than the asynchronous case. Nevertheless,\nasynchronous federated optimization provides much lower latency in many cases,\nthus becoming an appealing solution for FL when dealing with large datasets,\ntough timing constraints (e.g., near-real-time applications), or highly varying\ntraining data.", "time": "2022-09-05T08:30:11Z", "link": "http://arxiv.org/abs/2112.07938v3", "id": "2112.07938v3", "title": "Analysis and Evaluation of Synchronous and Asynchronous FLchain"}
{"author": "Shreshth Tuli, Giuliano Casale, Nicholas R. Jennings", "abstract": "Recently, intelligent scheduling approaches using surrogate models have been\nproposed to efficiently allocate volatile tasks in heterogeneous fog\nenvironments. Advances like deterministic surrogate models, deep neural\nnetworks (DNN) and gradient-based optimization allow low energy consumption and\nresponse times to be reached. However, deterministic surrogate models, which\nestimate objective values for optimization, do not consider the uncertainties\nin the distribution of the Quality of Service (QoS) objective function that can\nlead to high Service Level Agreement (SLA) violation rates. Moreover, the\nbrittle nature of DNN training and prevent such models from reaching minimal\nenergy or response times. To overcome these difficulties, we present a novel\nscheduler: GOSH i.e. Gradient Based Optimization using Second Order derivatives\nand Heteroscedastic Deep Surrogate Models. GOSH uses a second-order gradient\nbased optimization approach to obtain better QoS and reduce the number of\niterations to converge to a scheduling decision, subsequently lowering the\nscheduling time. Instead of a vanilla DNN, GOSH uses a Natural Parameter\nNetwork to approximate objective scores. Further, a Lower Confidence Bound\noptimization approach allows GOSH to find an optimal trade-off between greedy\nminimization of the mean latency and uncertainty reduction by employing\nerror-based exploration. Thus, GOSH and its co-simulation based extension\nGOSH*, can adapt quickly and reach better objective scores than baseline\nmethods. We show that GOSH* reaches better objective scores than GOSH, but it\nis suitable only for high resource availability settings, whereas GOSH is apt\nfor limited resource settings. Real system experiments for both GOSH and GOSH*\nshow significant improvements against the state-of-the-art in terms of energy\nconsumption, response time and SLA violations by up to 18, 27 and 82 percent,\nrespectively.", "time": "2021-12-16T14:36:34Z", "link": "http://arxiv.org/abs/2112.08916v1", "id": "2112.08916v1", "title": "GOSH: Task Scheduling Using Deep Surrogate Models in Fog Computing\n  Environments"}
{"author": "Xin Wang, Pei Guo, Xingyan Li, Aryya Gangopadhyay, Carl E. Busart, Jade Freeman, Jianwu Wang", "abstract": "Cloud computing has become a major approach to help reproduce computational\nexperiments. Yet there are still two main difficulties in reproducing batch\nbased big data analytics (including descriptive and predictive analytics) in\nthe cloud. The first is how to automate end-to-end scalable execution of\nanalytics including distributed environment provisioning, analytics pipeline\ndescription, parallel execution, and resource termination. The second is that\nan application developed for one cloud is difficult to be reproduced in another\ncloud, a.k.a. vendor lock-in problem. To tackle these problems, we leverage\nserverless computing and containerization techniques for automated scalable\nexecution and reproducibility, and utilize the adapter design pattern to enable\napplication portability and reproducibility across different clouds. We propose\nand develop an open-source toolkit that supports 1) fully automated end-to-end\nexecution and reproduction via a single command, 2) automated data and\nconfiguration storage for each execution, 3) flexible client modes based on\nuser preferences, 4) execution history query, and 5) simple reproduction of\nexisting executions in the same environment or a different environment. We did\nextensive experiments on both AWS and Azure using four big data analytics\napplications that run on virtual CPU/GPU clusters. The experiments show our\ntoolkit can achieve good execution performance, scalability, and efficient\nreproducibility for cloud-based big data analytics.", "time": "2023-03-10T02:22:13Z", "link": "http://arxiv.org/abs/2112.09762v5", "id": "2112.09762v5", "title": "Reproducible and Portable Big Data Analytics in the Cloud"}
{"author": "Seo Jin Park, Joshua Fried, Sunghyun Kim, Mohammad Alizadeh, Adam Belay", "abstract": "As emerging deep neural network (DNN) models continue to grow in size, using\nlarge GPU clusters to train DNNs is becoming an essential requirement to\nachieving acceptable training times. In this paper, we consider the case where\nfuture increases in cluster size will cause the global batch size that can be\nused to train models to reach a fundamental limit: beyond a certain point,\nlarger global batch sizes cause sample efficiency to degrade, increasing\noverall time to accuracy. As a result, to achieve further improvements in\ntraining performance, we must instead consider \"strong scaling\" strategies that\nhold the global batch size constant and allocate smaller batches to each GPU.\nUnfortunately, this makes it significantly more difficult to use cluster\nresources efficiently. We present DeepPool, a system that addresses this\nefficiency challenge through two key ideas. First, burst parallelism allocates\nlarge numbers of GPUs to foreground jobs in bursts to exploit the unevenness in\nparallelism across layers. Second, GPU multiplexing prioritizes throughput for\nforeground training jobs, while packing in background training jobs to reclaim\nunderutilized GPU resources, thereby improving cluster-wide utilization.\nTogether, these two ideas enable DeepPool to deliver a 1.2 - 2.3x improvement\nin total cluster throughput over standard data parallelism with a single task\nwhen the cluster scale is large.", "time": "2022-05-23T20:51:22Z", "link": "http://arxiv.org/abs/2112.10065v3", "id": "2112.10065v3", "title": "Efficient Strong Scaling Through Burst Parallel Training"}
{"author": "Deepesh Chaudhari, Rachit Agarwal, Sandeep Kumar Shukla", "abstract": "The temporal aspect of blockchain transactions enables us to study the\naddress's behavior and detect if it is involved in any illicit activity.\nHowever, due to the concept of change addresses (used to thwart replay\nattacks), temporal aspects are not directly applicable in the Bitcoin\nblockchain. Several pre-processing steps should be performed before such\ntemporal aspects are utilized. We are motivated to study the Bitcoin\ntransaction network and use the temporal features such as burst,\nattractiveness, and inter-event time along with several graph-based properties\nsuch as the degree of node and clustering coefficient to validate the\napplicability of already existing approaches known for other cryptocurrency\nblockchains on the Bitcoin blockchain. We generate the temporal and\nnon-temporal feature set and train the Machine Learning (ML) algorithm over\ndifferent temporal granularities to validate the state-of-the-art methods. We\nstudy the behavior of the addresses over different time granularities of the\ndataset. We identify that after applying change-address clustering, in Bitcoin,\nexisting temporal features can be extracted and ML approaches can be applied. A\ncomparative analysis of results show that the behavior of addresses in Ethereum\nand Bitcoin is similar with respect to in-degree, out-degree and inter-event\ntime. Further, we identify 3 suspects that showed malicious behavior across\ndifferent temporal granularities. These suspects are not marked as malicious in\nBitcoin.", "time": "2021-12-22T08:11:58Z", "link": "http://arxiv.org/abs/2112.11721v1", "id": "2112.11721v1", "title": "Towards Malicious address identification in Bitcoin"}
{"author": "Mohamed Ghanem, Fadi Dawoud, Habiba Gamal, Eslam Soliman, Hossam Sharara, Tamer El-Batt", "abstract": "The rapid expansion of data worldwide invites the need for more distributed\nsolutions in order to apply machine learning on a much wider scale. The\nresultant distributed learning systems can have various degrees of\ncentralization. In this work, we demonstrate our solution FLoBC for building a\ngeneric decentralized federated learning system using blockchain technology,\naccommodating any machine learning model that is compatible with gradient\ndescent optimization. We present our system design comprising the two\ndecentralized actors: trainer and validator, alongside our methodology for\nensuring reliable and efficient operation of said system. Finally, we utilize\nFLoBC as an experimental sandbox to compare and contrast the effects of\ntrainer-to-validator ratio, reward-penalty policy, and model synchronization\nschemes on the overall system performance, ultimately showing by example that a\ndecentralized federated learning system is indeed a feasible alternative to\nmore centralized architectures.", "time": "2022-10-07T00:37:59Z", "link": "http://arxiv.org/abs/2112.11873v2", "id": "2112.11873v2", "title": "FLoBC: A Decentralized Blockchain-Based Federated Learning Framework"}
{"author": "Navjot Singh, Xuanyu Cao, Suhas Diggavi, Tamer Basar", "abstract": "We consider a multi-agent network where each node has a stochastic (local)\ncost function that depends on the decision variable of that node and a random\nvariable, and further the decision variables of neighboring nodes are pairwise\nconstrained. There is an aggregate objective function for the network, composed\nadditively of the expected values of the local cost functions at the nodes, and\nthe overall goal of the network is to obtain the minimizing solution to this\naggregate objective function subject to all the pairwise constraints. This is\nto be achieved at the node level using decentralized information and local\ncomputation, with exchanges of only compressed information allowed by\nneighboring nodes. The paper develops algorithms and obtains performance bounds\nfor two different models of local information availability at the nodes: (i)\nsample feedback, where each node has direct access to samples of the local\nrandom variable to evaluate its local cost, and (ii) bandit feedback, where\nsamples of the random variables are not available, but only the values of the\nlocal cost functions at two random points close to the decision are available\nto each node. For both models, with compressed communication between neighbors,\nwe have developed decentralized saddle-point algorithms that deliver\nperformances no different (in order sense) from those without communication\ncompression; specifically, we show that deviation from the global minimum value\nand violations of the constraints are upper-bounded by\n$\\mathcal{O}(T^{-\\frac{1}{2}})$ and $\\mathcal{O}(T^{-\\frac{1}{4}})$,\nrespectively, where $T$ is the number of iterations. Numerical examples\nprovided in the paper corroborate these bounds and demonstrate the\ncommunication efficiency of the proposed method.", "time": "2021-12-23T05:54:42Z", "link": "http://arxiv.org/abs/2112.12373v1", "id": "2112.12373v1", "title": "Decentralized Multi-Task Stochastic Optimization With Compressed\n  Communications"}
{"author": "Riad Ladjel, Nicolas Anciaux, AurÃ©lien Bellet, Guillaume Scerri", "abstract": "Imagine a group of citizens willing to collectively contribute their personal\ndata for the common good to produce socially useful information, resulting from\ndata analytics or machine learning computations. Sharing raw personal data with\na centralized server performing the computation could raise concerns about\nprivacy and a perceived risk of mass surveillance. Instead, citizens may trust\neach other and their own devices to engage into a decentralized computation to\ncollaboratively produce an aggregate data release to be shared. In the context\nof secure computing nodes exchanging messages over secure channels at runtime,\na key security issue is to protect against external attackers observing the\ntraffic, whose dependence on data may reveal personal information. Existing\nsolutions are designed for the cloud setting, with the goal of hiding all\nproperties of the underlying dataset, and do not address the specific privacy\nand efficiency challenges that arise in the above context. In this paper, we\ndefine a general execution model to control the data-dependence of\ncommunications in user-side decentralized computations, in which differential\nprivacy guarantees for communication patterns in global execution plans can be\nanalyzed by combining guarantees obtained on local clusters of nodes. We\npropose a set of algorithms which allow to trade-off between privacy, utility\nand efficiency. Our formal privacy guarantees leverage and extend recent\nresults on privacy amplification by shuffling. We illustrate the usefulness of\nour proposal on two representative examples of decentralized execution plans\nwith data-dependent communications.", "time": "2021-12-23T08:30:17Z", "link": "http://arxiv.org/abs/2112.12411v1", "id": "2112.12411v1", "title": "Mitigating Leakage from Data Dependent Communications in Decentralized\n  Computing using Differential Privacy"}
{"author": "Haoyu Zhao, Konstantin Burlachenko, Zhize Li, Peter RichtÃ¡rik", "abstract": "Due to the communication bottleneck in distributed and federated learning\napplications, algorithms using communication compression have attracted\nsignificant attention and are widely used in practice. Moreover, the huge\nnumber, high heterogeneity and limited availability of clients result in high\nclient-variance. This paper addresses these two issues together by proposing\ncompressed and client-variance reduced methods COFIG and FRECON. We prove an\n$O(\\frac{(1+\\omega)^{3/2}\\sqrt{N}}{S\\epsilon^2}+\\frac{(1+\\omega)N^{2/3}}{S\\epsilon^2})$\nbound on the number of communication rounds of COFIG in the nonconvex setting,\nwhere $N$ is the total number of clients, $S$ is the number of clients\nparticipating in each round, $\\epsilon$ is the convergence error, and $\\omega$\nis the variance parameter associated with the compression operator. In case of\nFRECON, we prove an $O(\\frac{(1+\\omega)\\sqrt{N}}{S\\epsilon^2})$ bound on the\nnumber of communication rounds. In the convex setting, COFIG converges within\n$O(\\frac{(1+\\omega)\\sqrt{N}}{S\\epsilon})$ communication rounds, which, to the\nbest of our knowledge, is also the first convergence result for compression\nschemes that do not communicate with all the clients in each round. We stress\nthat neither COFIG nor FRECON needs to communicate with all the clients, and\nthey enjoy the first or faster convergence results for convex and nonconvex\nfederated learning in the regimes considered. Experimental results point to an\nempirical superiority of COFIG and FRECON over existing baselines.", "time": "2023-09-24T12:40:18Z", "link": "http://arxiv.org/abs/2112.13097v3", "id": "2112.13097v3", "title": "Faster Rates for Compressed Federated Learning with Client-Variance\n  Reduction"}
{"author": "Kun Huang, Xiao Li, Andre Milzarek, Shi Pu, Junwen Qiu", "abstract": "In this paper, we consider distributed optimization problems where $n$\nagents, each possessing a local cost function, collaboratively minimize the\naverage of the local cost functions over a connected network. To solve the\nproblem, we propose a distributed random reshuffling (D-RR) algorithm that\ninvokes the random reshuffling (RR) update in each agent. We show that D-RR\ninherits favorable characteristics of RR for both smooth strongly convex and\nsmooth nonconvex objective functions. In particular, for smooth strongly convex\nobjective functions, D-RR achieves $\\mathcal{O}(1/T^2)$ rate of convergence\n(where $T$ counts epoch number) in terms of the squared distance between the\niterate and the global minimizer. When the objective function is assumed to be\nsmooth nonconvex, we show that D-RR drives the squared norm of gradient to $0$\nat a rate of $\\mathcal{O}(1/T^{2/3})$. These convergence results match those of\ncentralized RR (up to constant factors) and outperform the distributed\nstochastic gradient descent (DSGD) algorithm if we run a relatively large\nnumber of epochs. Finally, we conduct a set of numerical experiments to\nillustrate the efficiency of the proposed D-RR method on both strongly convex\nand nonconvex distributed optimization problems.", "time": "2023-03-23T06:44:25Z", "link": "http://arxiv.org/abs/2112.15287v5", "id": "2112.15287v5", "title": "Distributed Random Reshuffling over Networks"}
{"author": "Farley Lai, Asim Kadav, Erik Kruus", "abstract": "The recent success of deep learning applications has coincided with those\nwidely available powerful computational resources for training sophisticated\nmachine learning models with huge datasets. Nonetheless, training large models\nsuch as convolutional neural networks using model parallelism (as opposed to\ndata parallelism) is challenging because the complex nature of communication\nbetween model shards makes it difficult to partition the computation\nefficiently across multiple machines with an acceptable trade-off. This paper\npresents SplitBrain, a high performance distributed deep learning framework\nsupporting hybrid data and model parallelism. Specifically, SplitBrain provides\nlayer-specific partitioning that co-locates compute intensive convolutional\nlayers while sharding memory demanding layers. A novel scalable group\ncommunication is proposed to further improve the training throughput with\nreduced communication overhead. The results show that SplitBrain can achieve\nnearly linear speedup while saving up to 67\\% of memory consumption for data\nand model parallel VGG over CIFAR-10.", "time": "2021-12-31T06:25:38Z", "link": "http://arxiv.org/abs/2112.15317v1", "id": "2112.15317v1", "title": "SplitBrain: Hybrid Data and Model Parallel Deep Learning"}
{"author": "Elie Mengin, Fabrice Rossi", "abstract": "In this paper, we present a novel algorithm to address the Network Alignment\nproblem. It is inspired from a previous message passing framework of Bayati et\nal. [2] and includes several modifications designed to significantly speed up\nthe message updates as well as to enforce their convergence. Experiments show\nthat our proposed model outperforms other state-of-the-art solvers. Finally, we\npropose an application of our method in order to address the Binary Diffing\nproblem. We show that our solution provides better assignment than the\nreference differs in almost all submitted instances and outline the importance\nof leveraging the graphical structure of binary programs.", "time": "2021-12-31T07:52:14Z", "link": "http://arxiv.org/abs/2112.15336v1", "id": "2112.15336v1", "title": "Improved Algorithm for the Network Alignment Problem with Application to\n  Binary Diffing"}
{"author": "Sami Khairy, Prasanna Balaprakash, Lin X. Cai, H. Vincent Poor", "abstract": "Non-orthogonal multiple access (NOMA) is a key technology to enable massive\nmachine type communications (mMTC) in 5G networks and beyond. In this paper,\nNOMA is applied to improve the random access efficiency in high-density\nspatially-distributed multi-cell wireless IoT networks, where IoT devices\ncontend for accessing the shared wireless channel using an adaptive\np-persistent slotted Aloha protocol. To enable a capacity-optimal network, a\nnovel formulation of random channel access management is proposed, in which the\ntransmission probability of each IoT device is tuned to maximize the geometric\nmean of users' expected capacity. It is shown that the network optimization\nobjective is high dimensional and mathematically intractable, yet it admits\nfavourable mathematical properties that enable the design of efficient\ndata-driven algorithmic solutions which do not require a priori knowledge of\nthe channel model or network topology. A centralized model-based algorithm and\na scalable distributed model-free algorithm, are proposed to optimally tune the\ntransmission probabilities of IoT devices and attain the maximum capacity. The\nconvergence of the proposed algorithms to the optimal solution is further\nestablished based on convex optimization and game-theoretic analysis. Extensive\nsimulations demonstrate the merits of the novel formulation and the efficacy of\nthe proposed algorithms.", "time": "2021-03-11T14:44:13Z", "link": "http://arxiv.org/abs/2101.00464v2", "id": "2101.00464v2", "title": "Data-Driven Random Access Optimization in Multi-Cell IoT Networks with\n  NOMA"}
{"author": "Thomas Bi, Carmelo Sferrazza, Raffaello D'Andrea", "abstract": "This paper aims to show that robots equipped with a vision-based tactile\nsensor can perform dynamic manipulation tasks without prior knowledge of all\nthe physical attributes of the objects to be manipulated. For this purpose, a\nrobotic system is presented that is able to swing up poles of different masses,\nradii and lengths, to an angle of 180 degrees, while relying solely on the\nfeedback provided by the tactile sensor. This is achieved by developing a novel\nsimulator that accurately models the interaction of a pole with the soft\nsensor. A feedback policy that is conditioned on a sensory observation history,\nand which has no prior knowledge of the physical features of the pole, is then\nlearned in the aforementioned simulation. When evaluated on the physical\nsystem, the policy is able to swing up a wide range of poles that differ\nsignificantly in their physical attributes without further adaptation. To the\nauthors' knowledge, this is the first work where a feedback policy from\nhigh-dimensional tactile observations is used to control the swing-up\nmanipulation of poles in closed-loop.", "time": "2021-05-06T06:40:40Z", "link": "http://arxiv.org/abs/2101.02680v3", "id": "2101.02680v3", "title": "Zero-shot sim-to-real transfer of tactile control policies for\n  aggressive swing-up manipulation"}
{"author": "Yifan Zhou, Peng Zhang", "abstract": "A neural ordinary differential equations network (ODE-Net)-enabled\nreachability method (Neuro-Reachability) is devised for the dynamic\nverification of networked microgrids (NMs) with unidentified subsystems and\nheterogeneous uncertainties. Three new contributions are presented: 1) An\nODENet-enabled dynamic model discovery approach is devised to construct the\ndata-driven state-space model which preserves the nonlinear and differential\nstructure of the NMs system; 2) A physics-data-integrated (PDI) NMs model is\nestablished, which empowers various NM analytics; and 3) A\nconformance-empowered reachability analysis is developed to enhance the\nreliability of the PDI-driven dynamic verification. Extensive case studies\ndemonstrate the efficacy of the ODE-Net-enabled method in microgrid dynamic\nmodel discovery, and the effectiveness of the Neuro-Reachability approach in\nverifying the NMs dynamics under multiple uncertainties and various operational\nscenarios.", "time": "2021-01-13T15:56:56Z", "link": "http://arxiv.org/abs/2101.05159v1", "id": "2101.05159v1", "title": "Neuro-Reachability of Networked Microgrids"}
{"author": "Zengyi Qin, Kaiqing Zhang, Yuxiao Chen, Jingkai Chen, Chuchu Fan", "abstract": "We study the multi-agent safe control problem where agents should avoid\ncollisions to static obstacles and collisions with each other while reaching\ntheir goals. Our core idea is to learn the multi-agent control policy jointly\nwith learning the control barrier functions as safety certificates. We propose\na novel joint-learning framework that can be implemented in a decentralized\nfashion, with generalization guarantees for certain function classes. Such a\ndecentralized framework can adapt to an arbitrarily large number of agents.\nBuilding upon this framework, we further improve the scalability by\nincorporating neural network architectures that are invariant to the quantity\nand permutation of neighboring agents. In addition, we propose a new\nspontaneous policy refinement method to further enforce the certificate\ncondition during testing. We provide extensive experiments to demonstrate that\nour method significantly outperforms other leading multi-agent control\napproaches in terms of maintaining safety and completing original tasks. Our\napproach also shows exceptional generalization capability in that the control\npolicy can be trained with 8 agents in one scenario, while being used on other\nscenarios with up to 1024 agents in complex multi-agent environments and\ndynamics.", "time": "2021-04-17T05:34:12Z", "link": "http://arxiv.org/abs/2101.05436v4", "id": "2101.05436v4", "title": "Learning Safe Multi-Agent Control with Decentralized Neural Barrier\n  Certificates"}
{"author": "Jinning Li, Liting Sun, Jianyu Chen, Masayoshi Tomizuka, Wei Zhan", "abstract": "Autonomous vehicles need to handle various traffic conditions and make safe\nand efficient decisions and maneuvers. However, on the one hand, a single\noptimization/sampling-based motion planner cannot efficiently generate safe\ntrajectories in real time, particularly when there are many interactive\nvehicles near by. On the other hand, end-to-end learning methods cannot assure\nthe safety of the outcomes. To address this challenge, we propose a\nhierarchical behavior planning framework with a set of low-level safe\ncontrollers and a high-level reinforcement learning algorithm (H-CtRL) as a\ncoordinator for the low-level controllers. Safety is guaranteed by the\nlow-level optimization/sampling-based controllers, while the high-level\nreinforcement learning algorithm makes H-CtRL an adaptive and efficient\nbehavior planner. To train and test our proposed algorithm, we built a\nsimulator that can reproduce traffic scenes using real-world datasets. The\nproposed H-CtRL is proved to be effective in various realistic simulation\nscenarios, with satisfying performance in terms of both safety and efficiency.", "time": "2021-06-09T17:53:03Z", "link": "http://arxiv.org/abs/2101.06778v2", "id": "2101.06778v2", "title": "A Safe Hierarchical Planning Framework for Complex Driving Scenarios\n  based on Reinforcement Learning"}
{"author": "Javier Fumanal-Idocin, Yu-Kai Wang, Chin-Teng Lin, Javier FernÃ¡ndez, Jose Antonio Sanz, Humberto Bustince", "abstract": "Brain Computer Interface technologies are popular methods of communication\nbetween the human brain and external devices. One of the most popular\napproaches to BCI is Motor Imagery. In BCI applications, the\nElectroEncephaloGraphy is a very popular measurement for brain dynamics because\nof its non-invasive nature. Although there is a high interest in the BCI topic,\nthe performance of existing systems is still far from ideal, due to the\ndifficulty of performing pattern recognition tasks in EEG signals. BCI systems\nare composed of a wide range of components that perform signal pre-processing,\nfeature extraction and decision making. In this paper, we define a BCI\nFramework, named Enhanced Fusion Framework, where we propose three different\nideas to improve the existing MI-based BCI frameworks. Firstly, we include aan\nadditional pre-processing step of the signal: a differentiation of the EEG\nsignal that makes it time-invariant. Secondly, we add an additional frequency\nband as feature for the system and we show its effect on the performance of the\nsystem. Finally, we make a profound study of how to make the final decision in\nthe system. We propose the usage of both up to six types of different\nclassifiers and a wide range of aggregation functions (including classical\naggregations, Choquet and Sugeno integrals and their extensions and overlap\nfunctions) to fuse the information given by the considered classifiers. We have\ntested this new system on a dataset of 20 volunteers performing motor\nimagery-based brain-computer interface experiments. On this dataset, the new\nsystem achieved a 88.80% of accuracy. We also propose an optimized version of\nour system that is able to obtain up to 90,76%. Furthermore, we find that the\npair Choquet/Sugeno integrals and overlap functions are the ones providing the\nbest results.", "time": "2021-06-02T08:41:37Z", "link": "http://arxiv.org/abs/2101.06968v2", "id": "2101.06968v2", "title": "Motor-Imagery-Based Brain Computer Interface using Signal Derivation and\n  Aggregation Functions"}
{"author": "Umit Cali, Murat Kuzlu, Vinayak Sharma, Manisa Pipattanasomporn, Ferhat Ozgur Catak", "abstract": "During the last two decades, distributed energy systems, especially renewable\nenergy sources (RES), have become more economically viable with increasing\nmarket share and penetration levels on power systems. In addition to\ndecarbonization and decentralization of energy systems, digitalization has also\nbecome very important. The use of artificial intelligence (AI), advanced\noptimization algorithms, Industrial Internet of Things (IIoT), and other\ndigitalization frameworks makes modern power system assets more intelligent,\nwhile vulnerable to cybersecurity risks. This paper proposes the concept of the\nInternet of Predictable Things (IoPT) that incorporates advanced data analytics\nand machine learning methods to increase the resiliency of cyber-physical\nsystems against cybersecurity risks. The proposed concept is demonstrated using\na cyber-physical system testbed under a variety of cyber attack scenarios as a\nproof of concept (PoC).", "time": "2021-01-19T19:01:56Z", "link": "http://arxiv.org/abs/2101.07816v1", "id": "2101.07816v1", "title": "Internet of Predictable Things (IoPT) Framework to Increase\n  Cyber-Physical System Resiliency"}
{"author": "Andrea Favrin, Vladislav Nenchev, Angelo Cenedese", "abstract": "While automated driving technology has achieved a tremendous progress, the\nscalable and rigorous testing and verification of safe automated and autonomous\ndriving vehicles remain challenging. This paper proposes a learning-based\nfalsification framework for testing the implementation of an automated or\nself-driving function in simulation. We assume that the function specification\nis associated with a violation metric on possible scenarios. Prior knowledge is\nincorporated to limit the scenario parameter variance and in a model-based\nfalsifier to guide and improve the learning process. For an exemplary adaptive\ncruise controller, the presented framework yields non-trivial falsifying\nscenarios with higher reward, compared to scenarios obtained by purely\nlearning-based or purely model-based falsification approaches.", "time": "2021-01-25T19:51:38Z", "link": "http://arxiv.org/abs/2101.10377v1", "id": "2101.10377v1", "title": "Learning to falsify automated driving vehicles with prior knowledge"}
{"author": "Erin Lanus, Ivan Hernandez, Adam Dachowicz, Laura Freeman, Melanie Grande, Andrew Lang, Jitesh H. Panchal, Anthony Patrick, Scott Welch", "abstract": "Test and evaluation is a necessary process for ensuring that engineered\nsystems perform as intended under a variety of conditions, both expected and\nunexpected. In this work, we consider the unique challenges of developing a\nunifying test and evaluation framework for complex ensembles of cyber-physical\nsystems with embedded artificial intelligence. We propose a framework that\nincorporates test and evaluation throughout not only the development life\ncycle, but continues into operation as the system learns and adapts in a noisy,\nchanging, and contended environment. The framework accounts for the challenges\nof testing the integration of diverse systems at various hierarchical scales of\ncomposition while respecting that testing time and resources are limited. A\ngeneric use case is provided for illustrative purposes and research directions\nemerging as a result of exploring the use case via the framework are suggested.", "time": "2021-01-25T21:42:27Z", "link": "http://arxiv.org/abs/2101.10430v1", "id": "2101.10430v1", "title": "Test and Evaluation Framework for Multi-Agent Systems of Autonomous\n  Intelligent Agents"}
{"author": "Fares Fourati, Mohamed-Slim Alouini", "abstract": "Satellite communication offers the prospect of service continuity over\nuncovered and under-covered areas, service ubiquity, and service scalability.\nHowever, several challenges must first be addressed to realize these benefits,\nas the resource management, network control, network security, spectrum\nmanagement, and energy usage of satellite networks are more challenging than\nthat of terrestrial networks. Meanwhile, artificial intelligence (AI),\nincluding machine learning, deep learning, and reinforcement learning, has been\nsteadily growing as a research field and has shown successful results in\ndiverse applications, including wireless communication. In particular, the\napplication of AI to a wide variety of satellite communication aspects have\ndemonstrated excellent potential, including beam-hopping, anti-jamming, network\ntraffic forecasting, channel modeling, telemetry mining, ionospheric\nscintillation detecting, interference managing, remote sensing, behavior\nmodeling, space-air-ground integrating, and energy managing. This work thus\nprovides a general overview of AI, its diverse sub-fields, and its\nstate-of-the-art algorithms. Several challenges facing diverse aspects of\nsatellite communication systems are then discussed, and their proposed and\npotential AI-based solutions are presented. Finally, an outlook of field is\ndrawn, and future steps are suggested.", "time": "2021-01-25T13:01:16Z", "link": "http://arxiv.org/abs/2101.10899v1", "id": "2101.10899v1", "title": "Artificial Intelligence for Satellite Communication: A Review"}
{"author": "Rohan Thakker, Nikhilesh Alatur, David D. Fan, Jesus Tordesillas, Michael Paton, Kyohei Otsu, Olivier Toupet, Ali-akbar Agha-mohammadi", "abstract": "We propose a framework for resilient autonomous navigation in perceptually\nchallenging unknown environments with mobility-stressing elements such as\nuneven surfaces with rocks and boulders, steep slopes, negative obstacles like\ncliffs and holes, and narrow passages. Environments are GPS-denied and\nperceptually-degraded with variable lighting from dark to lit and obscurants\n(dust, fog, smoke). Lack of prior maps and degraded communication eliminates\nthe possibility of prior or off-board computation or operator intervention.\nThis necessitates real-time on-board computation using noisy sensor data. To\naddress these challenges, we propose a resilient architecture that exploits\nredundancy and heterogeneity in sensing modalities. Further resilience is\nachieved by triggering recovery behaviors upon failure. We propose a fast\nsettling algorithm to generate robust multi-fidelity traversability estimates\nin real-time. The proposed approach was deployed on multiple physical systems\nincluding skid-steer and tracked robots, a high-speed RC car and legged robots,\nas a part of Team CoSTAR's effort to the DARPA Subterranean Challenge, where\nthe team won 2nd and 1st place in the Tunnel and Urban Circuits, respectively.", "time": "2021-01-26T22:13:01Z", "link": "http://arxiv.org/abs/2101.11110v1", "id": "2101.11110v1", "title": "Autonomous Off-road Navigation over Extreme Terrains with\n  Perceptually-challenging Conditions"}
{"author": "Manuel Eggimann, Abbas Rahimi, Luca Benini", "abstract": "Hyperdimensional computing (HDC) is a brain-inspired computing paradigm based\non high-dimensional holistic representations of vectors. It recently gained\nattention for embedded smart sensing due to its inherent error-resiliency and\nsuitability to highly parallel hardware implementations. In this work, we\npropose a programmable all-digital CMOS implementation of a fully autonomous\nHDC accelerator for always-on classification in energy-constrained sensor\nnodes. By using energy-efficient standard cell memory (SCM), the design is\neasily cross-technology mappable. It achieves extremely low power, 5 $\\mu W$ in\ntypical applications, and an energy-efficiency improvement over the\nstate-of-the-art (SoA) digital architectures of up to 3$\\times$ in post-layout\nsimulations for always-on wearable tasks such as EMG gesture recognition. As\npart of the accelerator's architecture, we introduce novel hardware-friendly\nembodiments of common HDC-algorithmic primitives, which results in 3.3$\\times$\ntechnology scaled area reduction over the SoA, achieving the same accuracy\nlevels in all examined targets. The proposed architecture also has a fully\nconfigurable datapath using microcode optimized for HDC stored on an integrated\nSCM based configuration memory, making the design \"general-purpose\" in terms of\nHDC algorithm flexibility. This flexibility allows usage of the accelerator\nacross novel HDC tasks, for instance, a newly designed HDC applied to the task\nof ball bearing fault detection.", "time": "2021-02-04T17:41:29Z", "link": "http://arxiv.org/abs/2102.02758v1", "id": "2102.02758v1", "title": "A 5 Î¼W Standard Cell Memory-based Configurable Hyperdimensional\n  Computing Accelerator for Always-on Smart Sensing"}
{"author": "Tiansheng Huang, Weiwei Lin, Xiaobin Hong, Xiumin Wang, Qingbo Wu, Rui Li, Ching-Hsien Hsu, Albert Y. Zomaya", "abstract": "With astonishing speed, bandwidth, and scale, Mobile Edge Computing (MEC) has\nplayed an increasingly important role in the next generation of connectivity\nand service delivery. Yet, along with the massive deployment of MEC servers,\nthe ensuing energy issue is now on an increasingly urgent agenda. In the\ncurrent context, the large scale deployment of renewable-energy-supplied MEC\nservers is perhaps the most promising solution for the incoming energy issue.\nNonetheless, as a result of the intermittent nature of their power sources,\nthese special design MEC server must be more cautious about their energy usage,\nin a bid to maintain their service sustainability as well as service standard.\nTargeting optimization on a single-server MEC scenario, we in this paper\npropose NAFA, an adaptive processor frequency adjustment solution, to enable an\neffective plan of the server's energy usage. By learning from the historical\ndata revealing request arrival and energy harvest pattern, the deep\nreinforcement learning-based solution is capable of making intelligent\nschedules on the server's processor frequency, so as to strike a good balance\nbetween service sustainability and service quality. The superior performance of\nNAFA is substantiated by real-data-based experiments, wherein NAFA demonstrates\nup to 20% increase in average request acceptance ratio and up to 50% reduction\nin average request processing time.", "time": "2021-10-09T15:10:17Z", "link": "http://arxiv.org/abs/2102.05449v3", "id": "2102.05449v3", "title": "Adaptive Processor Frequency Adjustment for Mobile Edge Computing with\n  Intermittent Energy Supply"}
{"author": "Tai Cheng Yang", "abstract": "This short article presents an opinion that control system study up to date\ncan be divided into four generations; namely, 1 transfer function based; 2\nstate-space based; 3 networked control systems; and 4 control in the new AI\nera.", "time": "2021-02-11T18:38:19Z", "link": "http://arxiv.org/abs/2102.08190v1", "id": "2102.08190v1", "title": "Four Generations of Control Theory Development ?"}
{"author": "Inga Jatzkowski, Till Menzel, Ansgar Bock, Markus Maurer", "abstract": "Automated vehicles need to be aware of the capabilities they currently\npossess. Skill graphs are directed acylic graphs in which a vehicle's\ncapabilities and the dependencies between these capabilities are modeled. The\nskills a vehicle requires depend on the behaviors the vehicle has to perform\nand the operational design domain (ODD) of the vehicle. Skill graphs were\noriginally proposed for online monitoring of the current capabilities of an\nautomated vehicle. They have also been shown to be useful during other parts of\nthe development process, e.g. system design, system verification. Skill graph\nconstruction is an iterative, expert-based, manual process with little to no\nguidelines. This process is, thus, prone to errors and inconsistencies\nespecially regarding the propagation of changes in the vehicle's intended ODD\ninto the skill graphs. In order to circumnavigate this problem, we propose to\nformalize expert knowledge regarding skill graph construction into a knowledge\nbase and automate the construction process. Thus, all changes in the vehicle's\nODD are reflected in the skill graphs automatically leading to a reduction in\ninconsistencies and errors in the constructed skill graphs.", "time": "2021-08-03T07:03:47Z", "link": "http://arxiv.org/abs/2102.08827v2", "id": "2102.08827v2", "title": "A Knowledge-based Approach for the Automatic Construction of Skill\n  Graphs for Online Monitoring"}
{"author": "Seyedali Meghdadi, Guido Tack, Ariel Liebman, Nicolas LangrenÃ©, Christoph Bergmeir", "abstract": "To support N-1 pre-fault transient stability assessment, this paper\nintroduces a new data collection method in a data-driven algorithm\nincorporating the knowledge of power system dynamics. The domain knowledge on\nhow the disturbance effect will propagate from the fault location to the rest\nof the network is leveraged to recognise the dominant conditions that determine\nthe stability of a system. Accordingly, we introduce a new concept called\nFault-Affected Area, which provides crucial information regarding the unstable\nregion of operation. This information is embedded in an augmented dataset to\ntrain an ensemble model using an instance transfer learning framework. The test\nresults on the IEEE 39-bus system verify that this model can accurately predict\nthe stability of previously unseen operational scenarios while reducing the\nrisk of false prediction of unstable instances compared to standard approaches.", "time": "2021-02-20T09:10:29Z", "link": "http://arxiv.org/abs/2102.10296v1", "id": "2102.10296v1", "title": "Versatile and Robust Transient Stability Assessment via Instance\n  Transfer Learning"}
{"author": "Zhengyu Liu, Jingliang Duan, Wenxuan Wang, Shengbo Eben Li, Yuming Yin, Ziyu Lin, Qi Sun, Bo Cheng", "abstract": "This paper proposes an off-line algorithm, called Recurrent Model Predictive\nControl (RMPC), to solve general nonlinear finite-horizon optimal control\nproblems. Unlike traditional Model Predictive Control (MPC) algorithms, it can\nmake full use of the current computing resources and adaptively select the\nlongest model prediction horizon. Our algorithm employs a recurrent function to\napproximate the optimal policy, which maps the system states and reference\nvalues directly to the control inputs. The number of prediction steps is equal\nto the number of recurrent cycles of the learned policy function. With an\narbitrary initial policy function, the proposed RMPC algorithm can converge to\nthe optimal policy by directly minimizing the designed loss function. We\nfurther prove the convergence and optimality of the RMPC algorithm thorough\nBellman optimality principle, and demonstrate its generality and efficiency\nusing two numerical examples.", "time": "2021-02-23T15:01:36Z", "link": "http://arxiv.org/abs/2102.11736v1", "id": "2102.11736v1", "title": "Recurrent Model Predictive Control"}
{"author": "Scott Gilroy, Derek Lau, Lizhi Yang, Ed Izaguirre, Kristen Biermayer, Anxing Xiao, Mengti Sun, Ayush Agrawal, Jun Zeng, Zhongyu Li, Koushil Sreenath", "abstract": "Quadrupeds are strong candidates for navigating challenging environments\nbecause of their agile and dynamic designs. This paper presents a methodology\nthat extends the range of exploration for quadrupedal robots by creating an\nend-to-end navigation framework that exploits walking and jumping modes. To\nobtain a dynamic jumping maneuver while avoiding obstacles,\ndynamically-feasible trajectories are optimized offline through\ncollocation-based optimization where safety constraints are imposed. Such\noptimization schematic allows the robot to jump through window-shaped obstacles\nby considering both obstacles in the air and on the ground. The resulted\njumping mode is utilized in an autonomous navigation pipeline that leverages a\nsearch-based global planner and a local planner to enable the robot to reach\nthe goal location by walking. A state machine together with a decision making\nstrategy allows the system to switch behaviors between walking around obstacles\nor jumping through them. The proposed framework is experimentally deployed and\nvalidated on a quadrupedal robot, a Mini Cheetah, to enable the robot to\nautonomously navigate through an environment while avoiding obstacles and\njumping over a maximum height of 13 cm to pass through a window-shaped opening\nin order to reach its goal.", "time": "2021-07-01T23:40:30Z", "link": "http://arxiv.org/abs/2107.00773v1", "id": "2107.00773v1", "title": "Autonomous Navigation for Quadrupedal Robots with Optimized Jumping\n  through Constrained Obstacles"}
{"author": "Xiaoyan Cao, Yao Yao, Lanqing Li, Wanpeng Zhang, Zhicheng An, Zhong Zhang, Li Xiao, Shihui Guo, Xiaoyu Cao, Meihong Wu, Dijun Luo", "abstract": "Agriculture is the foundation of human civilization. However, the rapid\nincrease of the global population poses a challenge on this cornerstone by\ndemanding more food. Modern autonomous greenhouses, equipped with sensors and\nactuators, provide a promising solution to the problem by empowering precise\ncontrol for high-efficient food production. However, the optimal control of\nautonomous greenhouses is challenging, requiring decision-making based on\nhigh-dimensional sensory data, and the scaling of production is limited by the\nscarcity of labor capable of handling this task. With the advances of\nartificial intelligence (AI), the internet of things (IoT), and cloud computing\ntechnologies, we are hopeful to provide a solution to automate and smarten\ngreenhouse control to address the above challenges. In this paper, we propose a\nsmart agriculture solution named iGrow, for autonomous greenhouse control\n(AGC): (1) for the first time, we formulate the AGC problem as a Markov\ndecision process (MDP) optimization problem; (2) we design a neural\nnetwork-based simulator incorporated with the incremental mechanism to simulate\nthe complete planting process of an autonomous greenhouse, which provides a\ntestbed for the optimization of control strategies; (3) we propose a\nclosed-loop bi-level optimization algorithm, which can dynamically re-optimize\nthe greenhouse control strategy with newly observed data during real-world\nproduction. We not only conduct simulation experiments but also deploy iGrow in\nreal scenarios, and experimental results demonstrate the effectiveness and\nsuperiority of iGrow in autonomous greenhouse simulation and optimal control.\nParticularly, compelling results from the tomato pilot project in real\nautonomous greenhouses show that our solution significantly increases crop\nyield (+10.15\\%) and net profit (+92.70\\%) with statistical significance\ncompared to planting experts.", "time": "2022-03-14T11:53:30Z", "link": "http://arxiv.org/abs/2107.05464v2", "id": "2107.05464v2", "title": "IGrow: A Smart Agriculture Solution to Autonomous Greenhouse Control"}
{"author": "Salah U. Kadir, Subir Majumder, Ajay D. Chhokra, Abhishek Dubey, Himanshu Neema, Aron Laszka, Anurag K. Srivastava", "abstract": "Power grid operation subject to an extreme event requires decision-making by\nhuman operators under stressful condition with high cognitive load. Decision\nsupport under adverse dynamic events, specially if forecasted, can be\nsupplemented by intelligent proactive control. Power system operation during\nwildfires require resiliency-driven proactive control for load shedding, line\nswitching and resource allocation considering the dynamics of the wildfire and\nfailure propagation. However, possible number of line- and load-switching in a\nlarge system during an event make traditional prediction-driven and stochastic\napproaches computationally intractable, leading operators to often use greedy\nalgorithms. We model and solve the proactive control problem as a Markov\ndecision process and introduce an integrated testbed for spatio-temporal\nwildfire propagation and proactive power-system operation. We transform the\nenormous wildfire-propagation observation space and utilize it as part of a\nheuristic for proactive de-energization of transmission assets. We integrate\nthis heuristic with a reinforcement-learning based proactive policy for\ncontrolling the generating assets. Our approach allows this controller to\nprovide setpoints for a part of the generation fleet, while a myopic operator\ncan determine the setpoints for the remaining set, which results in a symbiotic\naction. We evaluate our approach utilizing the IEEE 24-node system mapped on a\nhypothetical terrain. Our results show that the proposed approach can help the\noperator to reduce load loss during an extreme event, reduce power flow through\nlines that are to be de-energized, and reduce the likelihood of infeasible\npower-flow solutions, which would indicate violation of short-term thermal\nlimits of transmission lines.", "time": "2021-07-12T22:04:12Z", "link": "http://arxiv.org/abs/2107.05756v1", "id": "2107.05756v1", "title": "Reinforcement Learning based Proactive Control for Transmission Grid\n  Resilience to Wildfire"}
{"author": "Jakob Stigenberg, Vidit Saxena, Soma Tayamon, Euhanna Ghadimi", "abstract": "Fifth-generation (5G) New Radio (NR) cellular networks support a wide range\nof new services, many of which require an application-specific quality of\nservice (QoS), e.g. in terms of a guaranteed minimum bit-rate or a maximum\ntolerable delay. Therefore, scheduling multiple parallel data flows, each\nserving a unique application instance, is bound to become an even more\nchallenging task compared to the previous generations. Leveraging recent\nadvances in deep reinforcement learning, in this paper, we propose a QoS-Aware\nDeep Reinforcement learning Agent (QADRA) scheduler for NR networks. In\ncontrast to state-of-the-art scheduling heuristics, the QADRA scheduler\nexplicitly optimizes for the QoS satisfaction rate while simultaneously\nmaximizing the network performance. Moreover, we train our algorithm end-to-end\non these objectives. We evaluate QADRA in a full scale, near-product, system\nlevel NR simulator and demonstrate a significant boost in network performance.\nIn our particular evaluation scenario, the QADRA scheduler improves network\nthroughput by 30% while simultaneously maintaining the QoS satisfaction rate of\nVoIP users served by the network, compared to state-of-the-art baselines.", "time": "2021-07-14T09:18:39Z", "link": "http://arxiv.org/abs/2107.06570v1", "id": "2107.06570v1", "title": "QoS-Aware Scheduling in New Radio Using Deep Reinforcement Learning"}
{"author": "Rafail Ismayilov, Renato L. G. Cavalcante, SÅawomir StaÅczak", "abstract": "We propose a deep learning-based method that uses spatial and temporal\ninformation extracted from the sub-6GHz band to predict/track beams in the\nmillimeter-wave (mmWave) band. In more detail, we consider a dual-band\ncommunication system operating in both the sub-6GHz and mmWave bands. The\nobjective is to maximize the achievable mutual information in the mmWave band\nwith a hybrid analog/digital architecture where analog precoders (RF precoders)\nare taken from a finite codebook. Finding a RF precoder using conventional\nsearch methods incurs large signalling overhead, and the signalling scales with\nthe number of RF chains and the resolution of the phase shifters. To overcome\nthe issue of large signalling overhead in the mmWave band, the proposed method\nexploits the spatiotemporal correlation between sub-6GHz and mmWave bands, and\nit predicts/tracks the RF precoders in the mmWave band from sub-6GHz channel\nmeasurements. The proposed method provides a smaller candidate set so that\nperforming a search over that set significantly reduces the signalling overhead\ncompared with conventional search heuristics. Simulations show that the\nproposed method can provide reasonable achievable rates while significantly\nreducing the signalling overhead.", "time": "2021-07-16T12:10:32Z", "link": "http://arxiv.org/abs/2107.07843v1", "id": "2107.07843v1", "title": "Deep Learning Based Hybrid Precoding in Dual-Band Communication Systems"}
{"author": "Peide Cai, Hengli Wang, Huaiyang Huang, Yuxuan Liu, Ming Liu", "abstract": "Autonomous car racing is a challenging task in the robotic control area.\nTraditional modular methods require accurate mapping, localization and\nplanning, which makes them computationally inefficient and sensitive to\nenvironmental changes. Recently, deep-learning-based end-to-end systems have\nshown promising results for autonomous driving/racing. However, they are\ncommonly implemented by supervised imitation learning (IL), which suffers from\nthe distribution mismatch problem, or by reinforcement learning (RL), which\nrequires a huge amount of risky interaction data. In this work, we present a\ngeneral deep imitative reinforcement learning approach (DIRL), which\nsuccessfully achieves agile autonomous racing using visual inputs. The driving\nknowledge is acquired from both IL and model-based RL, where the agent can\nlearn from human teachers as well as perform self-improvement by safely\ninteracting with an offline world model. We validate our algorithm both in a\nhigh-fidelity driving simulation and on a real-world 1/20-scale RC-car with\nlimited onboard computation. The evaluation results demonstrate that our method\noutperforms previous IL and RL methods in terms of sample efficiency and task\nperformance. Demonstration videos are available at\nhttps://caipeide.github.io/autorace-dirl/", "time": "2021-07-18T00:00:48Z", "link": "http://arxiv.org/abs/2107.08325v1", "id": "2107.08325v1", "title": "Vision-Based Autonomous Car Racing Using Deep Imitative Reinforcement\n  Learning"}
{"author": "Rumia Masburah, Sayan Sinha, Rajib Lochan Jana, Soumyajit Dey, Qi Zhu", "abstract": "Building loads consume roughly 40% of the energy produced in developed\ncountries, a significant part of which is invested towards building\ntemperature-control infrastructure. Therein, renewable resource-based\nmicrogrids offer a greener and cheaper alternative. This communication explores\nthe possible co-design of microgrid power dispatch and building HVAC (heating,\nventilation and air conditioning system) actuations with the objective of\neffective temperature control under minimised operating cost. For this, we\nattempt control designs with various levels of abstractions based on\ninformation available about microgrid and HVAC system models using the Deep\nReinforcement Learning (DRL) technique. We provide control architectures that\nconsider model information ranging from completely determined system models to\nsystems with fully unknown parameter settings and illustrate the advantages of\nDRL for the design prescriptions.", "time": "2021-07-18T06:39:52Z", "link": "http://arxiv.org/abs/2107.08378v1", "id": "2107.08378v1", "title": "Co-designing Intelligent Control of Building HVACs and Microgrids"}
{"author": "C. G. Marcelino, G. M. C. Leite, C. A. D. M Delgado, L. B. de Oliveira, E. F. Wanner, S. JimÃ©nez-FernÃ¡ndez, S. Salcedo-Sanz", "abstract": "This paper tackles the short-term hydro-power unit commitment problem in a\nmulti-reservoir system - a cascade-based operation scenario. For this, we\npropose a new mathematical modelling in which the goal is to maximize the total\nenergy production of the hydro-power plant in a sub-daily operation, and,\nsimultaneously, to maximize the total water content (volume) of reservoirs. For\nsolving the problem, we discuss the Multi-objective Evolutionary Swarm\nHybridization (MESH) algorithm, a recently proposed multi-objective swarm\nintelligence-based optimization method which has obtained very competitive\nresults when compared to existing evolutionary algorithms in specific\napplications. The MESH approach has been applied to find the optimal water\ndischarge and the power produced at the maximum reservoir volume for all\npossible combinations of turbines in a hydro-power plant. The performance of\nMESH has been compared with that of well-known evolutionary approaches such as\nNSGA-II, NSGA-III, SPEA2, and MOEA/D in a realistic problem considering data\nfrom a hydro-power energy system with two cascaded hydro-power plants in\nBrazil. Results indicate that MESH showed a superior performance than\nalternative multi-objective approaches in terms of efficiency and accuracy,\nproviding a profit of \\$412,500 per month in a projection analysis carried out.", "time": "2021-07-28T16:24:51Z", "link": "http://arxiv.org/abs/2107.09718v2", "id": "2107.09718v2", "title": "An Efficient Multi-objective Evolutionary Approach for Solving the\n  Operation of Multi-Reservoir System Scheduling in Hydro-Power Plants"}
{"author": "Krishan Rana, Vibhavari Dasagi, Jesse Haviland, Ben Talbot, Michael Milford, Niko SÃ¼nderhauf", "abstract": "We present Bayesian Controller Fusion (BCF): a hybrid control strategy that\ncombines the strengths of traditional hand-crafted controllers and model-free\ndeep reinforcement learning (RL). BCF thrives in the robotics domain, where\nreliable but suboptimal control priors exist for many tasks, but RL from\nscratch remains unsafe and data-inefficient. By fusing uncertainty-aware\ndistributional outputs from each system, BCF arbitrates control between them,\nexploiting their respective strengths. We study BCF on two real-world robotics\ntasks involving navigation in a vast and long-horizon environment, and a\ncomplex reaching task that involves manipulability maximisation. For both these\ndomains, simple handcrafted controllers exist that can solve the task at hand\nin a risk-averse manner but do not necessarily exhibit the optimal solution\ngiven limitations in analytical modelling, controller miscalibration and task\nvariation. As exploration is naturally guided by the prior in the early stages\nof training, BCF accelerates learning, while substantially improving beyond the\nperformance of the control prior, as the policy gains more experience. More\nimportantly, given the risk-aversity of the control prior, BCF ensures safe\nexploration and deployment, where the control prior naturally dominates the\naction distribution in states unknown to the policy. We additionally show BCF's\napplicability to the zero-shot sim-to-real setting and its ability to deal with\nout-of-distribution states in the real world. BCF is a promising approach\ntowards combining the complementary strengths of deep RL and traditional\nrobotic control, surpassing what either can achieve independently. The code and\nsupplementary video material are made publicly available at\nhttps://krishanrana.github.io/bcf.", "time": "2023-04-03T05:32:36Z", "link": "http://arxiv.org/abs/2107.09822v3", "id": "2107.09822v3", "title": "Bayesian Controller Fusion: Leveraging Control Priors in Deep\n  Reinforcement Learning for Robotics"}
{"author": "Ren Hu, Qifeng Li", "abstract": "The multi-period dynamics of energy storage (ES), intermittent renewable\ngeneration and uncontrollable power loads, make the optimization of power\nsystem operation (PSO) challenging. A multi-period optimal PSO under\nuncertainty is formulated using the chance-constrained optimization (CCO)\nmodeling paradigm, where the constraints include the nonlinear energy storage\nand AC power flow models. Based on the emerging scenario optimization method\nwhich does not rely on pre-known probability distribution functions, this paper\ndevelops a novel solution method for this challenging CCO problem. The proposed\nmeth-od is computationally effective for mainly two reasons. First, the\noriginal AC power flow constraints are approximated by a set of\nlearning-assisted quadratic convex inequalities based on a generalized least\nabsolute shrinkage and selection operator. Second, considering the physical\npatterns of data and motived by learning-based sampling, the strategic sampling\nmethod is developed to significantly reduce the required number of scenarios\nthrough different sampling strategies. The simulation results on IEEE standard\nsystems indicate that 1) the proposed strategic sampling significantly improves\nthe computational efficiency of the scenario-based approach for solving the\nchance-constrained optimal PSO problem, 2) the data-driven convex approximation\nof power flow can be promising alternatives of nonlinear and nonconvex AC power\nflow.", "time": "2021-07-21T11:21:50Z", "link": "http://arxiv.org/abs/2107.10013v1", "id": "2107.10013v1", "title": "Optimal Operation of Power Systems with Energy Storage under\n  Uncertainty: A Scenario-based Method with Strategic Sampling"}
{"author": "Liang Sun, Leonardo Escamilla", "abstract": "This paper addresses task-allocation problems with uncertainty in situational\nawareness for distributed autonomous robots (DARs). The uncertainty propagation\nover a task-allocation process is done by using the Unscented transform that\nuses the Sigma-Point sampling mechanism. It has great potential to be employed\nfor generic task-allocation schemes, in the sense that there is no need to\nmodify an existing task-allocation method that has been developed without\nconsidering the uncertainty in the situational awareness. The proposed\nframework was tested in a simulated environment where the decision-maker needs\nto determine an optimal allocation of multiple locations assigned to multiple\nmobile flying robots whose locations come as random variables of known mean and\ncovariance. The simulation result shows that the proposed stochastic task\nallocation approach generates an assignment with 30% less overall cost than the\none without considering the uncertainty.", "time": "2021-07-21T20:43:05Z", "link": "http://arxiv.org/abs/2107.10350v1", "id": "2107.10350v1", "title": "Uncertainty-Aware Task Allocation for Distributed Autonomous Robots"}
{"author": "Mathieu Muniglia, SÃ©bastien Verel, Jean-Charles Le Pallec, Jean-Michel Do", "abstract": "In the context of the introduction of intermittent renewable energies, we\npropose to optimize the main variables of the control rods of a nuclear power\nplant to improve its capability to load-follow. The design problem is a\nblack-box combinatorial optimization problem with expensive evaluation based on\na multi-physics simulator. Therefore, we use a parallel asynchronous\nmaster-worker Evolutionary Algorithm scaling up to thousand computing units.\nOne main issue is the tuning of the algorithm parameters. A fitness landscape\nanalysis is conducted on this expensive real-world problem to show that it\nwould be possible to tune the mutation parameters according to the low-cost\nestimation of the fitness landscape features.", "time": "2021-07-06T14:35:25Z", "link": "http://arxiv.org/abs/2107.11201v1", "id": "2107.11201v1", "title": "A Fitness Landscape View on the Tuning of an Asynchronous Master-Worker\n  EA for Nuclear Reactor Design"}
{"author": "Dimitri Bertsekas", "abstract": "In this paper we propose an on-line policy iteration (PI) algorithm for\nfinite-state infinite horizon discounted dynamic programming, whereby the\npolicy improvement operation is done on-line, only for the states that are\nencountered during operation of the system. This allows the continuous\nupdating/improvement of the current policy, thus resulting in a form of on-line\nPI that incorporates the improved controls into the current policy as new\nstates and controls are generated. The algorithm converges in a finite number\nof stages to a type of locally optimal policy, and suggests the possibility of\nvariants of PI and multiagent PI where the policy improvement is simplified.\nMoreover, the algorithm can be used with on-line replanning, and is also\nwell-suited for on-line PI algorithms with value and policy approximations.", "time": "2021-06-01T19:50:22Z", "link": "http://arxiv.org/abs/2106.00746v1", "id": "2106.00746v1", "title": "On-Line Policy Iteration for Infinite Horizon Dynamic Programming"}
{"author": "Xin Tao, Jonas MÃ¥rtensson, HÃ¥kan Warnquist, Anna PernestÃ¥l", "abstract": "New autonomous driving technologies are emerging every day and some of them\nhave been commercially applied in the real world. While benefiting from these\ntechnologies, autonomous trucks are facing new challenges in short-term\nmaintenance planning, which directly influences the truck operator's profit. In\nthis paper, we implement a vehicle health management system by addressing the\nmaintenance planning issues of autonomous trucks on a transport mission. We\nalso present a maintenance planning model using a risk-based decision-making\nmethod, which identifies the maintenance decision with minimal economic risk of\nthe truck company. Both availability losses and maintenance costs are\nconsidered when evaluating the economic risk. We demonstrate the proposed model\nby numerical experiments illustrating real-world scenarios. In the experiments,\ncompared to three baseline methods, the expected economic risk of the proposed\nmethod is reduced by up to $47\\%$. We also conduct sensitivity analyses of\ndifferent model parameters. The analyses show that the economic risk\nsignificantly decreases when the estimation accuracy of remaining useful life,\nthe maximal allowed time of delivery delay before order cancellation, or the\nnumber of workshops increases. The experiment results contribute to identifying\nfuture research and development attentions of autonomous trucks from an\neconomic perspective.", "time": "2021-05-28T10:31:51Z", "link": "http://arxiv.org/abs/2106.01871v1", "id": "2106.01871v1", "title": "Short-term Maintenance Planning of Autonomous Trucks for Minimizing\n  Economic Risk"}
{"author": "Alvaro Velasquez, Ismail Alkhouri, Andre Beckus, Ashutosh Trivedi, George Atia", "abstract": "Given a Markov decision process (MDP) and a linear-time ($\\omega$-regular or\nLTL) specification, the controller synthesis problem aims to compute the\noptimal policy that satisfies the specification. More recently, problems that\nreason over the asymptotic behavior of systems have been proposed through the\nlens of steady-state planning. This entails finding a control policy for an MDP\nsuch that the Markov chain induced by the solution policy satisfies a given set\nof constraints on its steady-state distribution. This paper studies a\ngeneralization of the controller synthesis problem for a linear-time\nspecification under steady-state constraints on the asymptotic behavior. We\npresent an algorithm to find a deterministic policy satisfying $\\omega$-regular\nand steady-state constraints by characterizing the solutions as an integer\nlinear program, and experimentally evaluate our approach.", "time": "2022-02-07T13:32:28Z", "link": "http://arxiv.org/abs/2106.02951v2", "id": "2106.02951v2", "title": "Controller Synthesis for Omega-Regular and Steady-State Specifications"}
{"author": "Fanlin Meng, Qian Ma, Zixu Liu, Xiao-Jun Zeng", "abstract": "In this paper, we propose a realistic multiple dynamic pricing approach to\ndemand response in the retail market. First, an adaptive clustering-based\ncustomer segmentation framework is proposed to categorize customers into\ndifferent groups to enable the effective identification of usage patterns.\nSecond, customized demand models with important market constraints which\ncapture the price-demand relationship explicitly, are developed for each group\nof customers to improve the model accuracy and enable meaningful pricing.\nThird, the multiple pricing based demand response is formulated as a profit\nmaximization problem subject to realistic market constraints. The overall aim\nof the proposed scalable and practical method aims to achieve 'right' prices\nfor 'right' customers so as to benefit various stakeholders in the system such\nas grid operators, customers and retailers. The proposed multiple pricing\nframework is evaluated via simulations based on real-world datasets.", "time": "2021-06-10T16:47:15Z", "link": "http://arxiv.org/abs/2106.05905v1", "id": "2106.05905v1", "title": "Multiple Dynamic Pricing for Demand Response with Adaptive\n  Clustering-based Customer Segmentation in Smart Grids"}
{"author": "Linan Huang, Shumeng Jia, Emily Balcetis, Quanyan Zhu", "abstract": "Attacks exploiting the innate and the acquired vulnerabilities of human users\nhave posed severe threats to cybersecurity. This work proposes ADVERT, a\nhuman-technical solution that generates adaptive visual aids in real-time to\nprevent users from inadvertence and reduce their susceptibility to phishing\nattacks. Based on the eye-tracking data, we extract visual states and attention\nstates as system-level sufficient statistics to characterize the user's visual\nbehaviors and attention status. By adopting a data-driven approach and two\nlearning feedback of different time scales, this work lays out a theoretical\nfoundation to analyze, evaluate, and particularly modify humans' attention\nprocesses while they vet and recognize phishing emails. We corroborate the\neffectiveness, efficiency, and robustness of ADVERT through a case study based\non the data set collected from human subject experiments conducted at New York\nUniversity. The results show that the visual aids can statistically increase\nthe attention level and improve the accuracy of phishing recognition from 74.6%\nto a minimum of 86%. The meta-adaptation can further improve the accuracy to\n91.5% (resp. 93.7%) in less than 3 (resp. 50) tuning stages.", "time": "2022-07-08T23:16:17Z", "link": "http://arxiv.org/abs/2106.06907v3", "id": "2106.06907v3", "title": "ADVERT: An Adaptive and Data-Driven Attention Enhancement Mechanism for\n  Phishing Prevention"}
{"author": "Sabino Francesco Roselli, Martin Fabian, Knut Ãkesson", "abstract": "The Vehicle Routing Problem (VRP) is the combinatorial optimization problem\nof designing routes for vehicles to visit customers in such a fashion that a\ncost function, typically the number of vehicles, or the total travelled\ndistance is minimized. The problem finds applications in industrial scenarios,\nfor example where Automated Guided Vehicles run through the plant to deliver\ncomponents from the warehouse. This specific problem, henceforth called the\nElectric Conflict-Free Vehicle Routing Problem (CF-EVRP), involves constraints\nsuch as limited operating range of the vehicles, time windows on the delivery\nto the customers, and limited capacity on the number of vehicles the road\nsegments can accommodate at the same time. Such a complex system results in a\nlarge model that cannot easily be solved to optimality in reasonable time. We\ntherefore developed a compositional model that breaks down the problem into\nsmaller and simpler sub-problems and provides sub-optimal, feasible solutions\nto the original problem. The algorithm exploits the strengths of SMT solvers,\nwhich proved in our previous work to be an efficient approach to deal with\nscheduling problems. Compared to a monolithic model for the CF-EVRP, written in\nthe SMT standard language and solved using a state-of-the-art SMT solver the\ncompositional model was found to be significantly faster.", "time": "2021-06-30T07:18:03Z", "link": "http://arxiv.org/abs/2106.07387v2", "id": "2106.07387v2", "title": "An SMT Based Compositional Algorithm to Solve a Conflict-Free Electric\n  Vehicle Routing Problem"}
{"author": "Johannes Kruse, Benjamin SchÃ¤fer, Dirk Witthaut", "abstract": "Deterministic frequency deviations (DFDs) critically affect power grid\nfrequency quality and power system stability. A better understanding of these\nevents is urgently needed as frequency deviations have been growing in the\nEuropean grid in recent years. DFDs are partially explained by the rapid\nadjustment of power generation following the intervals of electricity trading,\nbut this intuitive picture fails especially before and around noonday. In this\narticle, we provide a detailed analysis of DFDs and their relation to external\nfeatures using methods from explainable Artificial Intelligence. We establish a\nmachine learning model that well describes the daily cycle of DFDs and\nelucidate key interdependencies using SHapley Additive exPlanations (SHAP).\nThereby, we identify solar ramps as critical to explain patterns in the Rate of\nChange of Frequency (RoCoF).", "time": "2021-06-14T08:30:33Z", "link": "http://arxiv.org/abs/2106.09538v1", "id": "2106.09538v1", "title": "Exploring deterministic frequency deviations with explainable AI"}
{"author": "Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz", "abstract": "As AI systems are integrated into high stakes social domains, researchers now\nexamine how to design and operate them in a safe and ethical manner. However,\nthe criteria for identifying and diagnosing safety risks in complex social\ncontexts remain unclear and contested. In this paper, we examine the vagueness\nin debates about the safety and ethical behavior of AI systems. We show how\nthis vagueness cannot be resolved through mathematical formalism alone, instead\nrequiring deliberation about the politics of development as well as the context\nof deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness\nin terms of distinct design challenges at key stages in AI system development.\nThe resulting framework of Hard Choices in Artificial Intelligence (HCAI)\nempowers developers by 1) identifying points of overlap between design\ndecisions and major sociotechnical challenges; 2) motivating the creation of\nstakeholder feedback channels so that safety issues can be exhaustively\naddressed. As such, HCAI contributes to a timely debate about the status of AI\ndevelopment in democratic societies, arguing that deliberation should be the\ngoal of AI Safety, not just the procedure by which it is ensured.", "time": "2021-06-10T09:49:34Z", "link": "http://arxiv.org/abs/2106.11022v1", "id": "2106.11022v1", "title": "Hard Choices in Artificial Intelligence"}
{"author": "Jiaxun Cui, William Macke, Harel Yedidsion, Daniel Urieli, Peter Stone", "abstract": "Traffic congestion is a major challenge in modern urban settings. The\nindustry-wide development of autonomous and automated vehicles (AVs) motivates\nthe question of how can AVs contribute to congestion reduction. Past research\nhas shown that in small scale mixed traffic scenarios with both AVs and\nhuman-driven vehicles, a small fraction of AVs executing a controlled\nmultiagent driving policy can mitigate congestion. In this paper, we scale up\nexisting approaches and develop new multiagent driving policies for AVs in\nscenarios with greater complexity. We start by showing that a congestion metric\nused by past research is manipulable in open road network scenarios where\nvehicles dynamically join and leave the road. We then propose using a different\nmetric that is robust to manipulation and reflects open network traffic\nefficiency. Next, we propose a modular transfer reinforcement learning\napproach, and use it to scale up a multiagent driving policy to outperform\nhuman-like traffic and existing approaches in a simulated realistic scenario,\nwhich is an order of magnitude larger than past scenarios (hundreds instead of\ntens of vehicles). Additionally, our modular transfer learning approach saves\nup to 80% of the training time in our experiments, by focusing its data\ncollection on key locations in the network. Finally, we show for the first time\na distributed multiagent policy that improves congestion over human-driven\ntraffic. The distributed approach is more realistic and practical, as it relies\nsolely on existing sensing and actuation capabilities, and does not require\nadding new communication infrastructure.", "time": "2022-07-06T20:37:48Z", "link": "http://arxiv.org/abs/2103.00058v2", "id": "2103.00058v2", "title": "Scalable Multiagent Driving Policies For Reducing Traffic Congestion"}
{"author": "David D. Fan, Kyohei Otsu, Yuki Kubo, Anushri Dixit, Joel Burdick, Ali-Akbar Agha-Mohammadi", "abstract": "Although ground robotic autonomy has gained widespread usage in structured\nand controlled environments, autonomy in unknown and off-road terrain remains a\ndifficult problem. Extreme, off-road, and unstructured environments such as\nundeveloped wilderness, caves, and rubble pose unique and challenging problems\nfor autonomous navigation. To tackle these problems we propose an approach for\nassessing traversability and planning a safe, feasible, and fast trajectory in\nreal-time. Our approach, which we name STEP (Stochastic Traversability\nEvaluation and Planning), relies on: 1) rapid uncertainty-aware mapping and\ntraversability evaluation, 2) tail risk assessment using the Conditional\nValue-at-Risk (CVaR), and 3) efficient risk and constraint-aware kinodynamic\nmotion planning using sequential quadratic programming-based (SQP) model\npredictive control (MPC). We analyze our method in simulation and validate its\nefficacy on wheeled and legged robotic platforms exploring extreme terrains\nincluding an abandoned subway and an underground lava tube.", "time": "2021-06-25T19:45:43Z", "link": "http://arxiv.org/abs/2103.02828v2", "id": "2103.02828v2", "title": "STEP: Stochastic Traversability Evaluation and Planning for Risk-Aware\n  Off-road Navigation"}
{"author": "Zahra Ghodsi, Siva Kumar Sastry Hari, Iuri Frosio, Timothy Tsai, Alejandro Troccoli, Stephen W. Keckler, Siddharth Garg, Anima Anandkumar", "abstract": "Extracting interesting scenarios from real-world data as well as generating\nfailure cases is important for the development and testing of autonomous\nsystems. We propose efficient mechanisms to both characterize and generate\ntesting scenarios using a state-of-the-art driving simulator. For any scenario,\nour method generates a set of possible driving paths and identifies all the\npossible safe driving trajectories that can be taken starting at different\ntimes, to compute metrics that quantify the complexity of the scenario. We use\nour method to characterize real driving data from the Next Generation\nSimulation (NGSIM) project, as well as adversarial scenarios generated in\nsimulation. We rank the scenarios by defining metrics based on the complexity\nof avoiding accidents and provide insights into how the AV could have minimized\nthe probability of incurring an accident. We demonstrate a strong correlation\nbetween the proposed metrics and human intuition.", "time": "2021-03-12T17:00:23Z", "link": "http://arxiv.org/abs/2103.07403v1", "id": "2103.07403v1", "title": "Generating and Characterizing Scenarios for Safety Testing of Autonomous\n  Vehicles"}
{"author": "Donghwan Lee, Niao He, Seungjae Lee, Panagiota Karava, Jianghai Hu", "abstract": "The building sector consumes the largest energy in the world, and there have\nbeen considerable research interests in energy consumption and comfort\nmanagement of buildings. Inspired by recent advances in reinforcement learning\n(RL), this paper aims at assessing the potential of RL in building climate\ncontrol problems with occupant interaction. We apply a recent RL approach,\ncalled DDPG (deep deterministic policy gradient), for the continuous building\ncontrol tasks and assess its performance with simulation studies in terms of\nits ability to handle (a) the partial state observability due to sensor\nlimitations; (b) complex stochastic system with high-dimensional state-spaces,\nwhich are jointly continuous and discrete; (c) uncertainties due to ambient\nweather conditions, occupant's behavior, and comfort feelings. Especially, the\npartial observability and uncertainty due to the occupant interaction\nsignificantly complicate the control problem. Through simulation studies, the\npolicy learned by DDPG demonstrates reasonable performance and computational\ntractability.", "time": "2021-03-14T13:04:04Z", "link": "http://arxiv.org/abs/2103.07919v1", "id": "2103.07919v1", "title": "Simulation Studies on Deep Reinforcement Learning for Building Control\n  with Human Interaction"}
{"author": "Mingwei Shi", "abstract": "Policy gradient is an effective way to estimate continuous action on the\nenvironment. This paper, it about explaining the mathematical formula and code\nimplementation. In the end, comparing between the rotation angle of the stick\non CartPole , and the angle of the Autonomous vehicle when turning, and\nutilizing the Bicycle Model, a simple Kinematic dynamic model, are the purpose\nto discover the similarity between these two models, so as to facilitate the\nmodel transfer from CartPole to the F1tenth Autonomous vehicle.", "time": "2021-03-15T14:09:25Z", "link": "http://arxiv.org/abs/2103.08396v1", "id": "2103.08396v1", "title": "Gradient Policy on \"CartPole\" game and its' expansibility to F1Tenth\n  Autonomous Vehicles"}
{"author": "Takfarinas Saber, Anthony Ventresque, Joao Marques-Silva, James Thorburn, Liam Murphy", "abstract": "Machine Reassignment is a challenging problem for constraint programming (CP)\nand mixed-integer linear programming (MILP) approaches, especially given the\nsize of data centres. The multi-objective version of the Machine Reassignment\nProblem is even more challenging and it seems unlikely for CP or MILP to obtain\ngood results in this context. As a result, the first approaches to address this\nproblem have been based on other optimisation methods, including\nmetaheuristics. In this paper we study under which conditions a mixed-integer\noptimisation solver, such as IBM ILOG CPLEX, can be used for the\nMulti-objective Machine Reassignment Problem. We show that it is useful only\nfor small or medium-scale data centres and with some relaxations, such as an\noptimality tolerance gap and a limited number of directions explored in the\nsearch space. Building on this study, we also investigate a hybrid approach,\nfeeding a metaheuristic with the results of CPLEX, and we show that the gains\nare important in terms of quality of the set of Pareto solutions (+126.9%\nagainst the metaheuristic alone and +17.8% against CPLEX alone) and number of\nsolutions (8.9 times more than CPLEX), while the processing time increases only\nby 6% in comparison to CPLEX for execution times larger than 100 seconds.", "time": "2021-03-18T17:46:57Z", "link": "http://arxiv.org/abs/2103.10410v1", "id": "2103.10410v1", "title": "MILP for the Multi-objective VM Reassignment Problem"}
{"author": "Xiaoyu Yang, Huiyun Li", "abstract": "In dense and dynamic scenarios, planning a safe and comfortable trajectory is\nfull of challenges when traffic participants are driving at high speed. The\nclassic graph search and sampling methods first perform path planning and then\nconfigure the corresponding speed, which lacks a strategy to deal with the\nhigh-speed obstacles. Decoupling optimization methods perform motion planning\nin the S-L and S-T domains respectively. These methods require a large free\nconfiguration space to plan the lane change trajectory. In dense dynamic\nscenes, it is easy to cause the failure of trajectory planning and be cut in by\nothers, causing slow driving speed and bring safety hazards. We analyze the\ncollision relationship in the spatio-temporal domain, and propose an\ninstantaneous analysis model which only analyzes the collision relationship at\nthe same time. In the model, the collision-free constraints in 3D\nspatio-temporal domain is projected to the 2D space domain to remove redundant\nconstraints and reduce computational complexity. Experimental results show that\nour method can plan a safe and comfortable lane-changing trajectory in dense\ndynamic scenarios. At the same time, it improves traffic efficiency and\nincreases ride comfort.", "time": "2021-03-19T17:10:50Z", "link": "http://arxiv.org/abs/2103.10909v1", "id": "2103.10909v1", "title": "IA Planner: Motion Planning Using Instantaneous Analysis for Autonomous\n  Vehicle in the Dense Dynamic Scenarios on Highways"}
{"author": "Zhe Xu, Xiaoming Duan", "abstract": "Pandemics can bring a range of devastating consequences to public health and\nthe world economy. Identifying the most effective control strategies has been\nthe imperative task all around the world. Various public health control\nstrategies have been proposed and tested against pandemic diseases (e.g.,\nCOVID-19). We study two specific pandemic control models: the susceptible,\nexposed, infectious, recovered (SEIR) model with vaccination control; and the\nSEIR model with shield immunity control. We express the pandemic control\nrequirement in metric temporal logic (MTL) formulas. We then develop an\niterative approach for synthesizing the optimal control strategies with MTL\nspecifications. We provide simulation results in two different scenarios for\nrobust control of the COVID-19 pandemic: one for vaccination control, and\nanother for shield immunity control, with the model parameters estimated from\ndata in Lombardy, Italy. The results show that the proposed synthesis approach\ncan generate control inputs such that the time-varying numbers of individuals\nin each category (e.g., infectious, immune) satisfy the MTL specifications with\nrobustness against initial state and parameter uncertainties.", "time": "2021-03-26T04:46:04Z", "link": "http://arxiv.org/abs/2103.14262v1", "id": "2103.14262v1", "title": "Robust Pandemic Control Synthesis with Formal Specifications: A Case\n  Study on COVID-19 Pandemic"}
{"author": "Zhe Xu, Yichen Zhang", "abstract": "In this paper, we present a provably correct controller synthesis approach\nfor switched stochastic control systems with metric temporal logic (MTL)\nspecifications with provable probabilistic guarantees. We first present the\nstochastic control bisimulation function for switched stochastic control\nsystems, which bounds the trajectory divergence between the switched stochastic\ncontrol system and its nominal deterministic control system in a probabilistic\nfashion. We then develop a method to compute optimal control inputs by solving\nan optimization problem for the nominal trajectory of the deterministic control\nsystem with robustness against initial state variations and stochastic\nuncertainties. We implement our robust stochastic controller synthesis approach\non both a four-bus power system and a nine-bus power system under generation\nloss disturbances, with MTL specifications expressing requirements for the grid\nfrequency deviations, wind turbine generator rotor speed variations and the\npower flow constraints at different power lines.", "time": "2021-03-26T04:50:29Z", "link": "http://arxiv.org/abs/2103.14264v1", "id": "2103.14264v1", "title": "Provably Correct Controller Synthesis of Switched Stochastic Systems\n  with Metric Temporal Logic Specifications: A Case Study on Power Systems"}
{"author": "Alexander Reske, Jan Carius, Yuntao Ma, Farbod Farshidian, Marco Hutter", "abstract": "We present a learning algorithm for training a single policy that imitates\nmultiple gaits of a walking robot. To achieve this, we use and extend MPC-Net,\nwhich is an Imitation Learning approach guided by Model Predictive Control\n(MPC). The strategy of MPC-Net differs from many other approaches since its\nobjective is to minimize the control Hamiltonian, which derives from the\nprinciple of optimality. To represent the policies, we employ a\nmixture-of-experts network (MEN) and observe that the performance of a policy\nimproves if each expert of a MEN specializes in controlling exactly one mode of\na hybrid system, such as a walking robot. We introduce new loss functions for\nsingle- and multi-gait policies to achieve this kind of expert selection\nbehavior. Moreover, we benchmark our algorithm against Behavioral Cloning and\nthe original MPC implementation on various rough terrain scenarios. We validate\nour approach on hardware and show that a single learned policy can replace its\nteacher to control multiple gaits.", "time": "2021-03-26T08:48:53Z", "link": "http://arxiv.org/abs/2103.14331v1", "id": "2103.14331v1", "title": "Imitation Learning from MPC for Quadrupedal Multi-Gait Control"}
{"author": "Jie Fu", "abstract": "We present a formal language for specifying qualitative preferences over\ntemporal goals and a preference-based planning method in stochastic systems.\nUsing automata-theoretic modeling, the proposed specification allows us to\nexpress preferences over different sets of outcomes, where each outcome\ndescribes a set of temporal sequences of subgoals. We define the value of\npreference satisfaction given a stochastic process over possible outcomes and\ndevelop an algorithm for time-constrained probabilistic planning in labeled\nMarkov decision processes where an agent aims to maximally satisfy its\npreference formula within a pre-defined finite time duration. We present\nexperimental results using a stochastic gridworld example and discuss possible\nextensions of the proposed preference model.", "time": "2021-03-26T14:26:40Z", "link": "http://arxiv.org/abs/2103.14489v1", "id": "2103.14489v1", "title": "Probabilistic Planning with Preferences over Temporal Goals"}
{"author": "Thimal Kempitiya, Seppo Sierla, Daswin De Silva, Matti Yli-Ojanpera, Damminda Alahakoon, Valeriy Vyatkin", "abstract": "The global ambitions of a carbon-neutral society necessitate a stable and\nrobust smart grid that capitalises on frequency reserves of renewable energy.\nFrequency reserves are resources that adjust power production or consumption in\nreal time to react to a power grid frequency deviation. Revenue generation\nmotivates the availability of these resources for managing such deviations.\nHowever, limited research has been conducted on data-driven decisions and\noptimal bidding strategies for trading such capacities in multiple frequency\nreserves markets. We address this limitation by making the following research\ncontributions. Firstly, a generalised model is designed based on an extensive\nstudy of critical characteristics of global frequency reserves markets.\nSecondly, three bidding strategies are proposed, based on this market model, to\ncapitalise on price peaks in multi-stage markets. Two strategies are proposed\nfor non-reschedulable loads, in which case the bidding strategy aims to select\nthe market with the highest anticipated price, and the third bidding strategy\nfocuses on rescheduling loads to hours on which highest reserve market prices\nare anticipated. The third research contribution is an Artificial Intelligence\n(AI) based bidding optimization framework that implements these three\nstrategies, with novel uncertainty metrics that supplement data-driven price\nprediction. Finally, the framework is evaluated empirically using a case study\nof multiple frequency reserves markets in Finland. The results from this\nevaluation confirm the effectiveness of the proposed bidding strategies and the\nAI-based bidding optimization framework in terms of cumulative revenue\ngeneration, leading to an increased availability of frequency reserves.", "time": "2021-04-05T12:04:29Z", "link": "http://arxiv.org/abs/2104.01865v1", "id": "2104.01865v1", "title": "An Artificial Intelligence Framework for Bidding Optimization with\n  Uncertainty in Multiple Frequency Reserve Markets"}
{"author": "A. Sadik Satir, Umut Demir, Gulay Goktas Sever, N. Kemal Ure", "abstract": "In this work, we propose a novel missile guidance algorithm that combines\ndeep learning based trajectory prediction with nonlinear model predictive\ncontrol. Although missile guidance and threat interception is a well-studied\nproblem, existing algorithms' performance degrades significantly when the\ntarget is pulling high acceleration attack maneuvers while rapidly changing its\ndirection. We argue that since most threats execute similar attack maneuvers,\nthese nonlinear trajectory patterns can be processed with modern machine\nlearning methods to build high accuracy trajectory prediction algorithms. We\ntrain a long short-term memory network (LSTM) based on a class of simulated\nstructured agile attack patterns, then combine this predictor with quadratic\nprogramming based nonlinear model predictive control (NMPC). Our method, named\nnonlinear model based predictive control with target acceleration predictions\n(NMPC-TAP), significantly outperforms compared approaches in terms of miss\ndistance, for the scenarios where the target/threat is executing agile\nmaneuvers.", "time": "2021-04-06T13:20:36Z", "link": "http://arxiv.org/abs/2104.02491v1", "id": "2104.02491v1", "title": "Nonlinear Model Based Guidance with Deep Learning Based Target\n  Trajectory Prediction Against Aerial Agile Attack Patterns"}
{"author": "Suhail Alsalehi, Noushin Mehdipour, Ezio Bartocci, Calin Belta", "abstract": "We propose a framework for solving control synthesis problems for multi-agent\nnetworked systems required to satisfy spatio-temporal specifications. We use\nSpatio-Temporal Reach and Escape Logic (STREL) as a specification language. For\nthis logic, we define smooth quantitative semantics, which captures the degree\nof satisfaction of a formula by a multi-agent team. We use the novel\nquantitative semantics to map control synthesis problems with STREL\nspecifications to optimization problems and propose a combination of heuristic\nand gradient-based methods to solve such problems. As this method might not\nmeet the requirements of a real-time implementation, we develop a machine\nlearning technique that uses the results of the off-line optimizations to train\na neural network that gives the control inputs at current states. We illustrate\nthe effectiveness of the proposed framework by applying it to a model of a\nrobotic team required to satisfy a spatial-temporal specification under\ncommunication constraints.", "time": "2021-04-06T18:08:09Z", "link": "http://arxiv.org/abs/2104.02737v1", "id": "2104.02737v1", "title": "Neural Network-based Control for Multi-Agent Systems from\n  Spatio-Temporal Specifications"}
{"author": "Philippe Hansen-Estruch, Wenling Shang, Lerrel Pinto, Pieter Abbeel, Stas Tiomkin", "abstract": "Learning the dynamics of a physical system wherein an autonomous agent\noperates is an important task. Often these systems present apparent geometric\nstructures. For instance, the trajectories of a robotic manipulator can be\nbroken down into a collection of its transitional and rotational motions, fully\ncharacterized by the corresponding Lie groups and Lie algebras. In this work,\nwe take advantage of these structures to build effective dynamical models that\nare amenable to sample-based learning. We hypothesize that learning the\ndynamics on a Lie algebra vector space is more effective than learning a direct\nstate transition model. To verify this hypothesis, we introduce the Group\nEnhanced Model (GEM). GEMs significantly outperform conventional transition\nmodels on tasks of long-term prediction, planning, and model-based\nreinforcement learning across a diverse suite of standard continuous-control\nenvironments, including Walker, Hopper, Reacher, Half-Cheetah, Inverted\nPendulums, Ant, and Humanoid. Furthermore, plugging GEM into existing state of\nthe art systems enhances their performance, which we demonstrate on the PETS\nsystem. This work sheds light on a connection between learning of dynamics and\nLie group properties, which opens doors for new research directions and\npractical applications along this direction. Our code is publicly available at:\nhttps://tinyurl.com/GEMMBRL.", "time": "2021-04-07T01:08:18Z", "link": "http://arxiv.org/abs/2104.02844v1", "id": "2104.02844v1", "title": "GEM: Group Enhanced Model for Learning Dynamical Control Systems"}
{"author": "Luis Pineda, Brandon Amos, Amy Zhang, Nathan O. Lambert, Roberto Calandra", "abstract": "Model-based reinforcement learning is a compelling framework for\ndata-efficient learning of agents that interact with the world. This family of\nalgorithms has many subcomponents that need to be carefully selected and tuned.\nAs a result the entry-bar for researchers to approach the field and to deploy\nit in real-world tasks can be daunting. In this paper, we present MBRL-Lib -- a\nmachine learning library for model-based reinforcement learning in continuous\nstate-action spaces based on PyTorch. MBRL-Lib is designed as a platform for\nboth researchers, to easily develop, debug and compare new algorithms, and\nnon-expert user, to lower the entry-bar of deploying state-of-the-art\nalgorithms. MBRL-Lib is open-source at\nhttps://github.com/facebookresearch/mbrl-lib.", "time": "2021-04-20T17:58:22Z", "link": "http://arxiv.org/abs/2104.10159v1", "id": "2104.10159v1", "title": "MBRL-Lib: A Modular Library for Model-based Reinforcement Learning"}
{"author": "Zongyuan Shen, James P. Wilson, Ryan Harvey, Shalabh Gupta", "abstract": "This paper presents a novel algorithm, called MRRT, which uses multiple\nrapidly-exploring random trees for fast online replanning of autonomous\nvehicles in dynamic environments with moving obstacles. The proposed algorithm\nis built upon the RRT algorithm with a multi-tree structure. At the beginning,\nthe RRT algorithm is applied to find the initial solution based on partial\nknowledge of the environment. Then, the robot starts to execute this path. At\neach iteration, the new obstacle configurations are collected by the robot's\nsensor and used to replan the path. This new information can come from unknown\nstatic obstacles (e.g., seafloor layout) as well as moving obstacles. Then, to\naccommodate the environmental changes, two procedures are adopted: 1) edge\npruning, and 2) tree regrowing. Specifically, the edge pruning procedure checks\nthe collision status through the tree and only removes the invalid edges while\nmaintaining the tree structure of already-explored regions. Due to removal of\ninvalid edges, the tree could be broken into multiple disjoint trees. As such,\nthe RRT algorithm is applied to regrow the trees. Specifically, a sample is\ncreated randomly and joined to all the disjoint trees in its local neighborhood\nby connecting to the nearest nodes. Finally, a new solution is found for the\nrobot. The advantages of the proposed MRRT algorithm are as follows: i) retains\nthe maximal tree structure by only pruning the edges which collide with the\nobstacles, ii) guarantees probabilistic completeness, and iii) is computational\nefficient for fast replanning since all disjoint trees are maintained for\nfuture connections and expanded simultaneously.", "time": "2021-04-22T13:41:48Z", "link": "http://arxiv.org/abs/2104.11059v1", "id": "2104.11059v1", "title": "MRRT: Multiple Rapidly-Exploring Random Trees for Fast Online Replanning\n  in Dynamic Environments"}
{"author": "Syed Eqbal Alam, Fabian Wirth, Jia Yuan Yu", "abstract": "In a federated setting, agents coordinate with a central agent or a server to\nsolve an optimization problem in which agents do not share their information\nwith each other. Wirth and his co-authors, in a recent paper, describe how the\nbasic additive-increase multiplicative-decrease (AIMD) algorithm can be\nmodified in a straightforward manner to solve a class of optimization problems\nfor federated settings for a single shared resource with no inter-agent\ncommunication. The AIMD algorithm is one of the most successful distributed\nresource allocation algorithms currently deployed in practice. It is best known\nas the backbone of the Internet and is also widely explored in other\napplication areas. We extend the single-resource algorithm to multiple\nheterogeneous shared resources that emerge in smart cities, sharing economy,\nand many other applications. Our main results show the convergence of the\naverage allocations to the optimal values. We model the system as a\nnon-homogeneous Markov chain with place-dependent probabilities. Furthermore,\nsimulation results are presented to demonstrate the efficacy of the algorithms\nand to highlight the main features of our analysis.", "time": "2021-05-24T14:44:00Z", "link": "http://arxiv.org/abs/2104.12828v2", "id": "2104.12828v2", "title": "Multi-resource allocation for federated settings: A non-homogeneous\n  Markov chain model"}
{"author": "Murat Cubuktepe, FrantiÅ¡ek Blahoudek, Ufuk Topcu", "abstract": "We study the problem of minimizing the resource capacity of autonomous agents\ncooperating to achieve a shared task. More specifically, we consider high-level\nplanning for a team of homogeneous agents that operate under resource\nconstraints in stochastic environments and share a common goal: given a set of\ntarget locations, ensure that each location will be visited infinitely often by\nsome agent almost surely. We formalize the dynamics of agents by consumption\nMarkov decision processes. In a consumption Markov decision process, the agent\nhas a resource of limited capacity. Each action of the agent may consume some\namount of the resource. To avoid exhaustion, the agent can replenish its\nresource to full capacity in designated reload states. The resource capacity\nrestricts the capabilities of the agent. The objective is to assign target\nlocations to agents, and each agent is only responsible for visiting the\nassigned subset of target locations repeatedly. Moreover, the assignment must\nensure that the agents can carry out their tasks with minimal resource\ncapacity. We reduce the problem of finding target assignments for a team of\nagents with the lowest possible capacity to an equivalent graph-theoretical\nproblem. We develop an algorithm that solves this graph problem in time that is\n\\emph{polynomial} in the number of agents, target locations, and size of the\nconsumption Markov decision process. We demonstrate the applicability and\nscalability of the algorithm in a scenario where hundreds of unmanned\nunderwater vehicles monitor hundreds of locations in environments with\nstochastic ocean currents.", "time": "2021-05-04T00:30:02Z", "link": "http://arxiv.org/abs/2105.01225v1", "id": "2105.01225v1", "title": "Polynomial-Time Algorithms for Multi-Agent Minimal-Capacity Planning"}
{"author": "FrantiÅ¡ek Blahoudek, Petr NovotnÃ½, Melkior Ornik, Pranay Thangeda, Ufuk Topcu", "abstract": "We consider qualitative strategy synthesis for the formalism called\nconsumption Markov decision processes. This formalism can model dynamics of an\nagents that operates under resource constraints in a stochastic environment.\nThe presented algorithms work in time polynomial with respect to the\nrepresentation of the model and they synthesize strategies ensuring that a\ngiven set of goal states will be reached (once or infinitely many times) with\nprobability 1 without resource exhaustion. In particular, when the amount of\nresource becomes too low to safely continue in the mission, the strategy\nchanges course of the agent towards one of a designated set of reload states\nwhere the agent replenishes the resource to full capacity; with sufficient\namount of resource, the agent attempts to fulfill the mission again.\n  We also present two heuristics that attempt to reduce expected time that the\nagent needs to fulfill the given mission, a parameter important in practical\nplanning. The presented algorithms were implemented and numerical examples\ndemonstrate (i) the effectiveness (in terms of computation time) of the\nplanning approach based on consumption Markov decision processes and (ii) the\npositive impact of the two heuristics on planning in a realistic example.", "time": "2021-05-05T14:59:30Z", "link": "http://arxiv.org/abs/2105.02099v1", "id": "2105.02099v1", "title": "Efficient Strategy Synthesis for MDPs with Resource Constraints"}
{"author": "Zichao Meng, Ye Guo, Wenjun Tang, Hongbin Sun, Wenqi Huang", "abstract": "A multivariate density forecast model based on deep learning is designed in\nthis paper to forecast the joint cumulative distribution functions (JCDFs) of\nmultiple security margins in power systems. Differing from existing\nmultivariate density forecast models, the proposed method requires no a priori\nhypotheses on the distribution of forecasting targets. In addition, based on\nthe universal approximation capability of neural networks, the value domain of\nthe proposed approach has been proven to include all continuous JCDFs. The\nforecasted JCDF is further employed to calculate the deterministic security\nassessment index evaluating the security level of future power system\noperations. Numerical tests verify the superiority of the proposed method over\ncurrent multivariate density forecast models. The deterministic security\nassessment index is demonstrated to be more informative for operators than\nsecurity margins as well.", "time": "2021-05-07T03:30:05Z", "link": "http://arxiv.org/abs/2105.03047v1", "id": "2105.03047v1", "title": "A Multivariate Density Forecast Approach for Online Power System\n  Security Assessment"}
{"author": "Jiazhi Song, Inna Sharf", "abstract": "This paper presents a framework that allows online\ndynamic-stability-constrained optimal trajectory planning of a mobile\nmanipulator robot working on rough terrain. First, the kinematics model of a\nmobile manipulator robot, and the Zero Moment Point (ZMP) stability measure are\npresented as theoretical background. Then, a sampling-based quasi-static\nplanning algorithm modified for stability guarantee and traction optimization\nin continuous dynamic motion is presented along with a mathematical proof. The\nrobot's quasi-static path is then used as an initial guess to warm-start a\nnonlinear optimal control solver which may otherwise have difficulties finding\na solution to the stability-constrained formulation efficiently. The\nperformance and computational efficiency of the framework are demonstrated\nthrough an application to a simulated timber harvesting mobile manipulator\nmachine working on varying terrain. The results demonstrate feasibility of\nonline trajectory planning on varying terrain while satisfying the dynamic\nstability constraint.", "time": "2021-05-10T14:21:59Z", "link": "http://arxiv.org/abs/2105.04396v1", "id": "2105.04396v1", "title": "Stability Constrained Mobile Manipulation Planning on Rough Terrain"}
{"author": "Anne Collin, Artur Bilka, Scott Pendleton, Radboud Duintjer Tebbens", "abstract": "Autonomous Vehicles (AVs) are complex systems that drive in uncertain\nenvironments and potentially navigate unforeseeable situations. Safety of these\nsystems requires not only an absence of malfunctions but also high performance\nof functions in many different scenarios. The ISO/PAS 21448 [1] guidance\nrecommends a process to ensure the Safety of the Intended Functionality (SOTIF)\nfor road vehicles. This process starts with a functional specification that\nfully describes the intended functionality and further includes the\nverification and validation that the AV meets this specification. For the path\nplanning function, defining the correct sequence of control actions for each\nvehicle in all potential driving situations is intractable. In this paper, the\nauthors provide a link between the Rulebooks framework, presented by [2], and\nthe SOTIF process. We establish that Rulebooks provide a functional description\nof the path planning task in an AV and discuss the potential usage of the\nmethod for verification and validation.", "time": "2021-05-10T16:11:15Z", "link": "http://arxiv.org/abs/2105.04472v1", "id": "2105.04472v1", "title": "Safety of the Intended Driving Behavior Using Rulebooks"}
{"author": "Xianyuan Zhan, Xiangyu Zhu, Haoran Xu", "abstract": "The recent offline reinforcement learning (RL) studies have achieved much\nprogress to make RL usable in real-world systems by learning policies from\npre-collected datasets without environment interaction. Unfortunately, existing\noffline RL methods still face many practical challenges in real-world system\ncontrol tasks, such as computational restriction during agent training and the\nrequirement of extra control flexibility. The model-based planning framework\nprovides an attractive alternative. However, most model-based planning\nalgorithms are not designed for offline settings. Simply combining the\ningredients of offline RL with existing methods either provides\nover-restrictive planning or leads to inferior performance. We propose a new\nlight-weighted model-based offline planning framework, namely MOPP, which\ntackles the dilemma between the restrictions of offline learning and\nhigh-performance planning. MOPP encourages more aggressive trajectory rollout\nguided by the behavior policy learned from data, and prunes out problematic\ntrajectories to avoid potential out-of-distribution samples. Experimental\nresults show that MOPP provides competitive performance compared with existing\nmodel-based offline planning and RL approaches.", "time": "2022-04-21T08:43:40Z", "link": "http://arxiv.org/abs/2105.07351v3", "id": "2105.07351v3", "title": "Model-Based Offline Planning with Trajectory Pruning"}
{"author": "Hyghor Miranda Cortes, Paulo Eduardo Santos, Joao Inacio da Silva Filho", "abstract": "The constant increase in the amount and complexity of information obtained\nfrom IT data networkelements, for its correct monitoring and management, is a\nreality. The same happens to data net-works in electrical systems that provide\neffective supervision and control of substations and hydro-electric plants.\nContributing to this fact is the growing number of installations and new\nenvironmentsmonitored by such data networks and the constant evolution of the\ntechnologies involved. This sit-uation potentially leads to incomplete and/or\ncontradictory data, issues that must be addressed inorder to maintain a good\nlevel of monitoring and, consequently, management of these systems. Inthis\npaper, a prototype of an expert system is developed to monitor the status of\nequipment of datanetworks in electrical systems, which deals with\ninconsistencies without trivialising the inferences.This is accomplished in the\ncontext of the remote control of hydroelectric plants and substationsby a\nRegional Operation Centre (ROC). The expert system is developed with algorithms\ndefinedupon a combination of Fuzzy logic and Paraconsistent Annotated Logic\nwith Annotation of TwoValues (PAL2v) in order to analyse uncertain signals and\ngenerate the operating conditions (faulty,normal, unstable or inconsistent /\nindeterminate) of the equipment that are identified as importantfor the remote\ncontrol of hydroelectric plants and substations. A prototype of this expert\nsystemwas installed on a virtualised server with CLP500 software (from the\nEFACEC manufacturer) thatwas applied to investigate scenarios consisting of a\nRegional (Brazilian) Operation Centre, with aGeneric Substation and a Generic\nHydroelectric Plant, representing a remote control environment.", "time": "2021-05-24T01:08:17Z", "link": "http://arxiv.org/abs/2105.07579v2", "id": "2105.07579v2", "title": "Monitoring electrical systems data-network equipment by means of Fuzzy\n  and Paraconsistent Annotated Logic"}
{"author": "Pedro Cisneros-Velarde, Francesco Bullo", "abstract": "Much recent interest has focused on the design of optimization algorithms\nfrom the discretization of an associated optimization flow, i.e., a system of\ndifferential equations (ODEs) whose trajectories solve an associated\noptimization problem. Such a design approach poses an important problem: how to\nfind a principled methodology to design and discretize appropriate ODEs. This\npaper aims to provide a solution to this problem through the use of contraction\ntheory. We first introduce general mathematical results that explain how\ncontraction theory guarantees the stability of the implicit and explicit Euler\nintegration methods. Then, we propose a novel system of ODEs, namely the\nAccelerated-Contracting-Nesterov flow, and use contraction theory to establish\nit is an optimization flow with exponential convergence rate, from which the\nlinear convergence rate of its associated optimization algorithm is immediately\nestablished. Remarkably, a simple explicit Euler discretization of this flow\ncorresponds to the Nesterov acceleration method. Finally, we present how our\napproach leads to performance guarantees in the design of optimization\nalgorithms for time-varying optimization problems.", "time": "2022-01-31T20:39:07Z", "link": "http://arxiv.org/abs/2105.08832v3", "id": "2105.08832v3", "title": "A Contraction Theory Approach to Optimization Algorithms from\n  Acceleration Flows"}
{"author": "Linyu Lin, Paridhi Athe, Pascal Rouxelin, Maria Avramova, Abhinav Gupta, Robert Youngblood, Nam Dinh", "abstract": "The Nearly Autonomous Management and Control System (NAMAC) is a\ncomprehensive control system that assists plant operations by furnishing\ncontrol recommendations to operators in a broad class of situations. This study\nrefines a NAMAC system for making reasonable recommendations during complex\nloss-of-flow scenarios with a validated Experimental Breeder Reactor II\nsimulator, digital twins improved by machine-learning algorithms, a\nmulti-attribute decision-making scheme, and a discrepancy checker for\nidentifying unexpected recommendation effects. We assessed the performance of\neach NAMAC component, while we demonstrated and evaluated the capability of\nNAMAC in a class of loss-of-flow scenarios.", "time": "2021-05-23T23:05:07Z", "link": "http://arxiv.org/abs/2105.11039v1", "id": "2105.11039v1", "title": "Digital-Twin-Based Improvements to Diagnosis, Prognosis, Strategy\n  Assessment, and Discrepancy Checking in a Nearly Autonomous Management and\n  Control System"}
{"author": "Jonathan Dumas, Colin Cointe, Antoine Wehenkel, Antonio Sutera, Xavier Fettweis, Bertrand CornÃ©lusse", "abstract": "This paper addresses the energy management of a grid-connected renewable\ngeneration plant coupled with a battery energy storage device in the capacity\nfirming market, designed to promote renewable power generation facilities in\nsmall non-interconnected grids. The core contribution is to propose a\nprobabilistic forecast-driven strategy, modeled as a min-max-min robust\noptimization problem with recourse. It is solved using a Benders-dual cutting\nplane algorithm and a column and constraints generation algorithm in a\ntractable manner. A dynamic risk-averse parameters selection strategy based on\nthe quantile forecasts distribution is proposed to improve the results. A\nsecondary contribution is to use a recently developed deep learning model known\nas normalizing flows to generate quantile forecasts of renewable generation for\nthe robust optimization problem. This technique provides a general mechanism\nfor defining expressive probability distributions, only requiring the\nspecification of a base distribution and a series of bijective transformations.\nOverall, the robust approach improves the results over a deterministic approach\nwith nominal point forecasts by finding a trade-off between conservative and\nrisk-seeking policies. The case study uses the photovoltaic generation\nmonitored on-site at the University of Li\\`ege (ULi\\`ege), Belgium.", "time": "2021-10-19T11:22:15Z", "link": "http://arxiv.org/abs/2105.13801v5", "id": "2105.13801v5", "title": "A Probabilistic Forecast-Driven Strategy for a Risk-Aware Participation\n  in the Capacity Firming Market: extended version"}
{"author": "Jan KÅetÃ­nskÃ½", "abstract": "Decision-making policies for agents are often synthesized with the constraint\nthat a formal specification of behaviour is satisfied. Here we focus on\ninfinite-horizon properties. On the one hand, Linear Temporal Logic (LTL) is a\npopular example of a formalism for qualitative specifications. On the other\nhand, Steady-State Policy Synthesis (SSPS) has recently received considerable\nattention as it provides a more quantitative and more behavioural perspective\non specifications, in terms of the frequency with which states are visited.\nFinally, rewards provide a classic framework for quantitative properties. In\nthis paper, we study Markov decision processes (MDP) with the specification\ncombining all these three types. The derived policy maximizes the reward among\nall policies ensuring the LTL specification with the given probability and\nadhering to the steady-state constraints. To this end, we provide a unified\nsolution reducing the multi-type specification to a multi-dimensional long-run\naverage reward. This is enabled by Limit-Deterministic B\\\"uchi Automata (LDBA),\nrecently studied in the context of LTL model checking on MDP, and allows for an\nelegant solution through a simple linear programme. The algorithm also extends\nto the general $\\omega$-regular properties and runs in time polynomial in the\nsizes of the MDP as well as the LDBA.", "time": "2021-05-31T11:35:42Z", "link": "http://arxiv.org/abs/2105.14894v1", "id": "2105.14894v1", "title": "LTL-Constrained Steady-State Policy Synthesis"}
{"author": "K. Ritsuka, Karen Rudie", "abstract": "An epistemic model for decentralized discrete-event systems with non-binary\ncontrol is presented. This framework combines existing work on conditional\ncontrol decisions with existing work on formal reasoning about knowledge in\ndiscrete-event systems. The novelty in the model presented is that the\nnecessary and sufficient conditions for problem solvability encapsulate the\nactions that supervisors must take. This direct coupling between knowledge and\naction -- in a formalism that mimics natural language -- makes it easier, when\nthe problem conditions fail, to determine how the problem requirements should\nbe revised.", "time": "2022-05-30T16:27:48Z", "link": "http://arxiv.org/abs/2108.02000v3", "id": "2108.02000v3", "title": "Do What You Know: Coupling Knowledge with Action in Discrete-Event\n  Systems"}
{"author": "Parag Narkhede, Rahee Walambe, Shashi Poddar, Ketan Kotecha", "abstract": "This paper presents a novel method for attitude estimation of an object in 3D\nspace by incremental learning of the Long-Short Term Memory (LSTM) network.\nGyroscope, accelerometer, and magnetometer are few widely used sensors in\nattitude estimation applications. Traditionally, multi-sensor fusion methods\nsuch as the Extended Kalman Filter and Complementary Filter are employed to\nfuse the measurements from these sensors. However, these methods exhibit\nlimitations in accounting for the uncertainty, unpredictability, and dynamic\nnature of the motion in real-world situations. In this paper, the inertial\nsensors data are fed to the LSTM network which are then updated incrementally\nto incorporate the dynamic changes in motion occurring in the run time. The\nrobustness and efficiency of the proposed framework is demonstrated on the\ndataset collected from a commercially available inertial measurement unit. The\nproposed framework offers a significant improvement in the results compared to\nthe traditional method, even in the case of a highly dynamic environment. The\nLSTM framework-based attitude estimation approach can be deployed on a standard\nAI-supported processing module for real-time applications.", "time": "2021-08-04T09:03:53Z", "link": "http://arxiv.org/abs/2108.03173v1", "id": "2108.03173v1", "title": "Incremental learning of LSTM framework for sensor fusion in attitude\n  estimation"}
{"author": "Antonio Vitale, Alpha Renner, Celine Nauer, Davide Scaramuzza, Yulia Sandamirskaya", "abstract": "Event-based vision sensors achieve up to three orders of magnitude better\nspeed vs. power consumption trade off in high-speed control of UAVs compared to\nconventional image sensors. Event-based cameras produce a sparse stream of\nevents that can be processed more efficiently and with a lower latency than\nimages, enabling ultra-fast vision-driven control. Here, we explore how an\nevent-based vision algorithm can be implemented as a spiking neuronal network\non a neuromorphic chip and used in a drone controller. We show how seamless\nintegration of event-based perception on chip leads to even faster control\nrates and lower latency. In addition, we demonstrate how online adaptation of\nthe SNN controller can be realised using on-chip learning. Our spiking neuronal\nnetwork on chip is the first example of a neuromorphic vision-based controller\nsolving a high-speed UAV control task. The excellent scalability of processing\nin neuromorphic hardware opens the possibility to solve more challenging visual\ntasks in the future and integrate visual perception in fast control loops.", "time": "2021-08-19T07:46:25Z", "link": "http://arxiv.org/abs/2108.03694v2", "id": "2108.03694v2", "title": "Event-driven Vision and Control for UAVs on a Neuromorphic Chip"}
{"author": "Philipp Foehn, Angel Romero, Davide Scaramuzza", "abstract": "Quadrotors are among the most agile flying robots. However, planning\ntime-optimal trajectories at the actuation limit through multiple waypoints\nremains an open problem. This is crucial for applications such as inspection,\ndelivery, search and rescue, and drone racing. Early works used polynomial\ntrajectory formulations, which do not exploit the full actuator potential\nbecause of their inherent smoothness. Recent works resorted to numerical\noptimization but require waypoints to be allocated as costs or constraints at\nspecific discrete times. However, this time allocation is a priori unknown and\nrenders previous works incapable of producing truly time-optimal trajectories.\nTo generate truly time-optimal trajectories, we propose a solution to the time\nallocation problem while exploiting the full quadrotor's actuator potential. We\nachieve this by introducing a formulation of progress along the trajectory,\nwhich enables the simultaneous optimization of the time allocation and the\ntrajectory itself. We compare our method against related approaches and\nvalidate it in real-world flights in one of the world's largest motion-capture\nsystems, where we outperform human expert drone pilots in a drone-racing task.", "time": "2021-10-01T13:03:54Z", "link": "http://arxiv.org/abs/2108.04537v3", "id": "2108.04537v3", "title": "Time-Optimal Planning for Quadrotor Waypoint Flight"}
{"author": "Liuhui Ding, Dachuan Li, Bowen Liu, Wenxing Lan, Bing Bai, Qi Hao, Weipeng Cao, Ke Pei", "abstract": "Uncertainties in Deep Neural Network (DNN)-based perception and vehicle's\nmotion pose challenges to the development of safe autonomous driving vehicles.\nIn this paper, we propose a safe motion planning framework featuring the\nquantification and propagation of DNN-based perception uncertainties and motion\nuncertainties. Contributions of this work are twofold: (1) A Bayesian Deep\nNeural network model which detects 3D objects and quantitatively captures the\nassociated aleatoric and epistemic uncertainties of DNNs; (2) An\nuncertainty-aware motion planning algorithm (PU-RRT) that accounts for\nuncertainties in object detection and ego-vehicle's motion. The proposed\napproaches are validated via simulated complex scenarios built in CARLA.\nExperimental results show that the proposed motion planning scheme can cope\nwith uncertainties of DNN-based perception and vehicle motion, and improve the\noperational safety of autonomous vehicles while still achieving desirable\nefficiency.", "time": "2021-08-11T09:41:54Z", "link": "http://arxiv.org/abs/2108.05118v1", "id": "2108.05118v1", "title": "Capture Uncertainties in Deep Neural Networks for Safe Operation of\n  Autonomous Driving Vehicles"}
