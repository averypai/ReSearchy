{"author": "Wisal Mukhtiar, Waliiya Rizwan, Aneela Habib, Yasir Saleem Afridi, Laiq Hasan, Kashif Ahmad", "abstract": "In recent years, social media has been widely explored as a potential source\nof communication and information in disasters and emergency situations. Several\ninteresting works and case studies of disaster analytics exploring different\naspects of natural disasters have been already conducted. Along with the great\npotential, disaster analytics comes with several challenges mainly due to the\nnature of social media content. In this paper, we explore one such challenge\nand propose a text classification framework to deal with Twitter noisy data.\nMore specifically, we employed several transformers both individually and in\ncombination, so as to differentiate between relevant and non-relevant Twitter\nposts, achieving the highest F1-score of 0.87.", "time": "2023-01-01T01:34:15Z", "link": "http://arxiv.org/abs/2301.00320v1", "id": "2301.00320v1", "title": "Relevance Classification of Flood-related Twitter Posts via Multiple\n  Transformers"}
{"author": "Muhammad Suleman, Muhammad Asif, Tayyab Zamir, Ayaz Mehmood, Jebran Khan, Nasir Ahmad, Kashif Ahmad", "abstract": "This paper presents our solutions for the MediaEval 2022 task on DisasterMM.\nThe task is composed of two subtasks, namely (i) Relevance Classification of\nTwitter Posts (RCTP), and (ii) Location Extraction from Twitter Texts (LETT).\nThe RCTP subtask aims at differentiating flood-related and non-relevant social\nposts while LETT is a Named Entity Recognition (NER) task and aims at the\nextraction of location information from the text. For RCTP, we proposed four\ndifferent solutions based on BERT, RoBERTa, Distil BERT, and ALBERT obtaining\nan F1-score of 0.7934, 0.7970, 0.7613, and 0.7924, respectively. For LETT, we\nused three models namely BERT, RoBERTa, and Distil BERTA obtaining an F1-score\nof 0.6256, 0.6744, and 0.6723, respectively.", "time": "2023-01-01T01:36:32Z", "link": "http://arxiv.org/abs/2301.00321v1", "id": "2301.00321v1", "title": "Floods Relevancy and Identification of Location from Twitter Posts using\n  NLP Techniques"}
{"author": "Xingwu Sun, Hongyin Tang, chengzhong Xu", "abstract": "Neural models with an encoder-decoder framework provide a feasible solution\nto Question Generation (QG). However, after analyzing the model vocabulary we\nfind that current models (both RNN-based and pre-training based) have more than\n23\\% inflected forms. As a result, the encoder will generate separate\nembeddings for the inflected forms, leading to a waste of training data and\nparameters. Even worse, in decoding these models are vulnerable to irrelevant\nnoise and they suffer from high computational costs. In this paper, we propose\nan approach to enhance the performance of QG by fusing word transformation.\nFirstly, we identify the inflected forms of words from the input of encoder,\nand replace them with the root words, letting the encoder pay more attention to\nthe repetitive root words. Secondly, we propose to adapt QG as a combination of\nthe following actions in the encode-decoder framework: generating a question\nword, copying a word from the source sequence or generating a word\ntransformation type. Such extension can greatly decrease the size of predicted\nwords in the decoder as well as noise. We apply our approach to a typical\nRNN-based model and \\textsc{UniLM} to get the improved versions. We conduct\nextensive experiments on SQuAD and MS MARCO datasets. The experimental results\nshow that the improved versions can significantly outperform the corresponding\nbaselines in terms of BLEU, ROUGE-L and METEOR as well as time cost.", "time": "2023-01-01T13:08:11Z", "link": "http://arxiv.org/abs/2301.00397v1", "id": "2301.00397v1", "title": "Inflected Forms Are Redundant in Question Generation Models"}
{"author": "Farshad Noravesh", "abstract": "In the present paper, semantic parsing challenges are briefly introduced and\nQDMR formalism in semantic parsing is implemented using sequence to sequence\nmodel with attention but uses only part of speech(POS) as a representation of\nwords of a sentence to make the training as simple and as fast as possible and\nalso avoiding curse of dimensionality as well as overfitting. It is shown how\nsemantic operator prediction could be augmented with other models like the\nCopyNet model or the recursive neural net model.", "time": "2023-01-01T13:20:57Z", "link": "http://arxiv.org/abs/2301.00399v1", "id": "2301.00399v1", "title": "Semantic Operator Prediction and Applications"}
{"author": "Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "To the best of our knowledge, this paper made the first attempt to answer\nwhether word segmentation is necessary for Vietnamese sentiment classification.\nTo do this, we presented five pre-trained monolingual S4- based language models\nfor Vietnamese, including one model without word segmentation, and four models\nusing RDRsegmenter, uitnlp, pyvi, or underthesea toolkits in the pre-processing\ndata phase. According to comprehensive experimental results on two corpora,\nincluding the VLSP2016-SA corpus of technical article reviews from the news and\nsocial media and the UIT-VSFC corpus of the educational survey, we have two\nsuggestions. Firstly, using traditional classifiers like Naive Bayes or Support\nVector Machines, word segmentation maybe not be necessary for the Vietnamese\nsentiment classification corpus, which comes from the social domain. Secondly,\nword segmentation is necessary for Vietnamese sentiment classification when\nword segmentation is used before using the BPE method and feeding into the deep\nlearning model. In this way, the RDRsegmenter is the stable toolkit for word\nsegmentation among the uitnlp, pyvi, and underthesea toolkits.", "time": "2023-01-01T15:04:47Z", "link": "http://arxiv.org/abs/2301.00418v1", "id": "2301.00418v1", "title": "Is word segmentation necessary for Vietnamese sentiment classification?"}
{"author": "Quoc-Loc Duong, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "RTE is a significant problem and is a reasonably active research community.\nThe proposed research works on the approach to this problem are pretty diverse\nwith many different directions. For Vietnamese, the RTE problem is moderately\nnew, but this problem plays a vital role in natural language understanding\nsystems. Currently, methods to solve this problem based on contextual word\nrepresentation learning models have given outstanding results. However,\nVietnamese is a semantically rich language. Therefore, in this paper, we want\nto present an experiment combining semantic word representation through the SRL\ntask with context representation of BERT relative models for the RTE problem.\nThe experimental results give conclusions about the influence and role of\nsemantic representation on Vietnamese in understanding natural language. The\nexperimental results show that the semantic-aware contextual representation\nmodel has about 1% higher performance than the model that does not incorporate\nsemantic representation. In addition, the effects on the data domain in\nVietnamese are also higher than those in English. This result also shows the\npositive influence of SRL on RTE problem in Vietnamese.", "time": "2023-01-01T15:13:25Z", "link": "http://arxiv.org/abs/2301.00422v1", "id": "2301.00422v1", "title": "Leveraging Semantic Representations Combined with Contextual Word\n  Representations for Recognizing Textual Entailment in Vietnamese"}
{"author": "Hang Thi-Thu Le, Viet-Duc Ho, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen", "abstract": "Machine Reading Comprehension has become one of the most advanced and popular\nresearch topics in the fields of Natural Language Processing in recent years.\nThe classification of answerability questions is a relatively significant\nsub-task in machine reading comprehension; however, there haven't been many\nstudies. Retro-Reader is one of the studies that has solved this problem\neffectively. However, the encoders of most traditional machine reading\ncomprehension models in general and Retro-Reader, in particular, have not been\nable to exploit the contextual semantic information of the context completely.\nInspired by SemBERT, we use semantic role labels from the SRL task to add\nsemantics to pre-trained language models such as mBERT, XLM-R, PhoBERT. This\nexperiment was conducted to compare the influence of semantics on the\nclassification of answerability for the Vietnamese machine reading\ncomprehension. Additionally, we hope this experiment will enhance the encoder\nfor the Retro-Reader model's Sketchy Reading Module. The improved Retro-Reader\nmodel's encoder with semantics was first applied to the Vietnamese Machine\nReading Comprehension task and obtained positive results.", "time": "2023-01-01T15:28:27Z", "link": "http://arxiv.org/abs/2301.00429v1", "id": "2301.00429v1", "title": "Integrating Semantic Information into Sketchy Reading Module of\n  Retro-Reader for Vietnamese Machine Reading Comprehension"}
{"author": "Sudhansu Bala Das, Divyajoti Panda, Tapas Kumar Mishra, Bidyut Kr. Patra", "abstract": "Machine Translation (MT) system generally aims at automatic representation of\nsource language into target language retaining the originality of context using\nvarious Natural Language Processing (NLP) techniques. Among various NLP\nmethods, Statistical Machine Translation(SMT). SMT uses probabilistic and\nstatistical techniques to analyze information and conversion. This paper\ncanvasses about the development of bilingual SMT models for translating English\nto fifteen low-resource Indian Languages (ILs) and vice versa. At the outset,\nall 15 languages are briefed with a short description related to our\nexperimental need. Further, a detailed analysis of Samanantar and OPUS dataset\nfor model building, along with standard benchmark dataset (Flores-200) for\nfine-tuning and testing, is done as a part of our experiment. Different\npreprocessing approaches are proposed in this paper to handle the noise of the\ndataset. To create the system, MOSES open-source SMT toolkit is explored.\nDistance reordering is utilized with the aim to understand the rules of grammar\nand context-dependent adjustments through a phrase reordering categorization\nframework. In our experiment, the quality of the translation is evaluated using\nstandard metrics such as BLEU, METEOR, and RIBES", "time": "2023-01-02T06:23:12Z", "link": "http://arxiv.org/abs/2301.00539v1", "id": "2301.00539v1", "title": "Statistical Machine Translation for Indic Languages"}
{"author": "Hamed Vahdat-Nejad, Mohammad Ghasem Akbari, Fatemeh Salmani, Faezeh Azizi, Hamid-Reza Nili-Sani", "abstract": "With Twitter's growth and popularity, a huge number of views are shared by\nusers on various topics, making this platform a valuable information source on\nvarious political, social, and economic issues. This paper investigates English\ntweets on the Russia-Ukraine war to analyze trends reflecting users' opinions\nand sentiments regarding the conflict. The tweets' positive and negative\nsentiments are analyzed using a BERT-based model, and the time series\nassociated with the frequency of positive and negative tweets for various\ncountries is calculated. Then, we propose a method based on the neighborhood\naverage for modeling and clustering the time series of countries. The\nclustering results provide valuable insight into public opinion regarding this\nconflict. Among other things, we can mention the similar thoughts of users from\nthe United States, Canada, the United Kingdom, and most Western European\ncountries versus the shared views of Eastern European, Scandinavian, Asian, and\nSouth American nations toward the conflict.", "time": "2023-01-02T11:32:47Z", "link": "http://arxiv.org/abs/2301.00604v1", "id": "2301.00604v1", "title": "Russia-Ukraine war: Modeling and Clustering the Sentiments Trends of\n  Various Countries"}
{"author": "Tahereh Firoozi, Hamid Mohammadi, Mark J. Gierl", "abstract": "Research on automated essay scoring has become increasing important because\nit serves as a method for evaluating students' written-responses at scale.\nScalable methods for scoring written responses are needed as students migrate\nto online learning environments resulting in the need to evaluate large numbers\nof written-response assessments. The purpose of this study is to describe and\nevaluate three active learning methods than can be used to minimize the number\nof essays that must be scored by human raters while still providing the data\nneeded to train a modern automated essay scoring system. The three active\nlearning methods are the uncertainty-based, the topological-based, and the\nhybrid method. These three methods were used to select essays included as part\nof the Automated Student Assessment Prize competition that were then classified\nusing a scoring model that was training with the bidirectional encoder\nrepresentations from transformer language model. All three active learning\nmethods produced strong results, with the topological-based method producing\nthe most efficient classification. Growth rate accuracy was also evaluated. The\nactive learning methods produced different levels of efficiency under different\nsample size allocations but, overall, all three methods were highly efficient\nand produced classifications that were similar to one another.", "time": "2023-04-13T23:17:58Z", "link": "http://arxiv.org/abs/2301.00628v2", "id": "2301.00628v2", "title": "Using Active Learning Methods to Strategically Select Essays for\n  Automated Scoring"}
{"author": "Zhongtao Chen, Chenghu Mi, Siwei Duo, Jingfei He, Yatong Zhou", "abstract": "Text clustering and topic extraction are two important tasks in text mining.\nUsually, these two tasks are performed separately. For topic extraction to\nfacilitate clustering, we can first project texts into a topic space and then\nperform a clustering algorithm to obtain clusters. To promote topic extraction\nby clustering, we can first obtain clusters with a clustering algorithm and\nthen extract cluster-specific topics. However, this naive strategy ignores the\nfact that text clustering and topic extraction are strongly correlated and\nfollow a chicken-and-egg relationship. Performing them separately fails to make\nthem mutually benefit each other to achieve the best overall performance. In\nthis paper, we propose an unsupervised text clustering and topic extraction\nframework (ClusTop) which integrates text clustering and topic extraction into\na unified framework and can achieve high-quality clustering result and extract\ntopics from each cluster simultaneously. Our framework includes four\ncomponents: enhanced language model training, dimensionality reduction,\nclustering and topic extraction, where the enhanced language model can be\nviewed as a bridge between clustering and topic extraction. On one hand, it\nprovides text embeddings with a strong cluster structure which facilitates\neffective text clustering; on the other hand, it pays high attention on the\ntopic related words for topic extraction because of its self-attention\narchitecture. Moreover, the training of enhanced language model is\nunsupervised. Experiments on two datasets demonstrate the effectiveness of our\nframework and provide benchmarks for different model combinations in this\nframework.", "time": "2023-01-03T03:26:26Z", "link": "http://arxiv.org/abs/2301.00818v1", "id": "2301.00818v1", "title": "ClusTop: An unsupervised and integrated text clustering and topic\n  extraction framework"}
{"author": "Xiuying Chen, Mingzhe Li, Shen Gao, Zhangming Chan, Dongyan Zhao, Xin Gao, Xiangliang Zhang, Rui Yan", "abstract": "Nowadays, time-stamped web documents related to a general news query floods\nspread throughout the Internet, and timeline summarization targets concisely\nsummarizing the evolution trajectory of events along the timeline. Unlike\ntraditional document summarization, timeline summarization needs to model the\ntime series information of the input events and summarize important events in\nchronological order. To tackle this challenge, in this paper, we propose a\nUnified Timeline Summarizer (UTS) that can generate abstractive and extractive\ntimeline summaries in time order. Concretely, in the encoder part, we propose a\ngraph-based event encoder that relates multiple events according to their\ncontent dependency and learns a global representation of each event. In the\ndecoder part, to ensure the chronological order of the abstractive summary, we\npropose to extract the feature of event-level attention in its generation\nprocess with sequential information remained and use it to simulate the\nevolutionary attention of the ground truth summary. The event-level attention\ncan also be used to assist in extracting summary, where the extracted summary\nalso comes in time sequence. We augment the previous Chinese large-scale\ntimeline summarization dataset and collect a new English timeline dataset.\nExtensive experiments conducted on these datasets and on the out-of-domain\nTimeline 17 dataset show that UTS achieves state-of-the-art performance in\nterms of both automatic and human evaluations.", "time": "2023-01-02T20:29:40Z", "link": "http://arxiv.org/abs/2301.00867v1", "id": "2301.00867v1", "title": "Follow the Timeline! Generating Abstractive and Extractive Timeline\n  Summary in Chronological Order"}
{"author": "Steven H. Wang, Antoine Scardigli, Leonard Tang, Wei Chen, Dimitry Levkin, Anya Chen, Spencer Ball, Thomas Woodside, Oliver Zhang, Dan Hendrycks", "abstract": "Reading comprehension of legal text can be a particularly challenging task\ndue to the length and complexity of legal clauses and a shortage of\nexpert-annotated datasets. To address this challenge, we introduce the Merger\nAgreement Understanding Dataset (MAUD), an expert-annotated reading\ncomprehension dataset based on the American Bar Association's 2021 Public\nTarget Deal Points Study, with over 39,000 examples and over 47,000 total\nannotations. Our fine-tuned Transformer baselines show promising results, with\nmodels performing well above random on most questions. However, on a large\nsubset of questions, there is still room for significant improvement. As the\nonly expert-annotated merger agreement dataset, MAUD is valuable as a benchmark\nfor both the legal profession and the NLP community.", "time": "2023-11-24T14:24:01Z", "link": "http://arxiv.org/abs/2301.00876v3", "id": "2301.00876v3", "title": "MAUD: An Expert-Annotated Legal NLP Dataset for Merger Agreement\n  Understanding"}
{"author": "Samiran Gode, Supreeth Bare, Bhiksha Raj, Hyungon Yoo", "abstract": "Our paper aims to analyze political polarization in US political system using\nLanguage Models, and thereby help candidates make an informed decision. The\navailability of this information will help voters understand their candidates\nviews on the economy, healthcare, education and other social issues. Our main\ncontributions are a dataset extracted from Wikipedia that spans the past 120\nyears and a Language model based method that helps analyze how polarized a\ncandidate is. Our data is divided into 2 parts, background information and\npolitical information about a candidate, since our hypothesis is that the\npolitical views of a candidate should be based on reason and be independent of\nfactors such as birthplace, alma mater, etc. We further split this data into 4\nphases chronologically, to help understand if and how the polarization amongst\ncandidates changes. This data has been cleaned to remove biases. To understand\nthe polarization we begin by showing results from some classical language\nmodels in Word2Vec and Doc2Vec. And then use more powerful techniques like the\nLongformer, a transformer based encoder, to assimilate more information and\nfind the nearest neighbors of each candidate based on their political view and\ntheir background.", "time": "2023-01-02T22:15:04Z", "link": "http://arxiv.org/abs/2301.00891v1", "id": "2301.00891v1", "title": "Understanding Political Polarisation using Language Models: A dataset\n  and method"}
{"author": "Mingzhe Li, Xiuying Chen, Weiheng Liao, Yang Song, Tao Zhang, Dongyan Zhao, Rui Yan", "abstract": "Interview has been regarded as one of the most crucial step for recruitment.\nTo fully prepare for the interview with the recruiters, job seekers usually\npractice with mock interviews between each other. However, such a mock\ninterview with peers is generally far away from the real interview experience:\nthe mock interviewers are not guaranteed to be professional and are not likely\nto behave like a real interviewer. Due to the rapid growth of online\nrecruitment in recent years, recruiters tend to have online interviews, which\nmakes it possible to collect real interview data from real interviewers. In\nthis paper, we propose a novel application named EZInterviewer, which aims to\nlearn from the online interview data and provides mock interview services to\nthe job seekers. The task is challenging in two ways: (1) the interview data\nare now available but still of low-resource; (2) to generate meaningful and\nrelevant interview dialogs requires thorough understanding of both resumes and\njob descriptions. To address the low-resource challenge, EZInterviewer is\ntrained on a very small set of interview dialogs. The key idea is to reduce the\nnumber of parameters that rely on interview dialogs by disentangling the\nknowledge selector and dialog generator so that most parameters can be trained\nwith ungrounded dialogs as well as the resume data that are not low-resource.\nEvaluation results on a real-world job interview dialog dataset indicate that\nwe achieve promising results to generate mock interviews. With the help of\nEZInterviewer, we hope to make mock interview practice become easier for job\nseekers.", "time": "2023-01-03T07:00:30Z", "link": "http://arxiv.org/abs/2301.00972v1", "id": "2301.00972v1", "title": "EZInterviewer: To Improve Job Interview Performance with Mock Interview\n  Generator"}
{"author": "Longxu Dou, Yan Gao, Xuqi Liu, Mingyang Pan, Dingzirui Wang, Wanxiang Che, Dechen Zhan, Min-Yen Kan, Jian-Guang Lou", "abstract": "In this paper, we study the problem of knowledge-intensive text-to-SQL, in\nwhich domain knowledge is necessary to parse expert questions into SQL queries\nover domain-specific tables. We formalize this scenario by building a new\nChinese benchmark KnowSQL consisting of domain-specific questions covering\nvarious domains. We then address this problem by presenting formulaic\nknowledge, rather than by annotating additional data examples. More concretely,\nwe construct a formulaic knowledge bank as a domain knowledge base and propose\na framework (ReGrouP) to leverage this formulaic knowledge during parsing.\nExperiments using ReGrouP demonstrate a significant 28.2% improvement overall\non KnowSQL.", "time": "2023-01-03T12:37:47Z", "link": "http://arxiv.org/abs/2301.01067v1", "id": "2301.01067v1", "title": "Towards Knowledge-Intensive Text-to-SQL Semantic Parsing with Formulaic\n  Knowledge"}
{"author": "Matúš Pikuliak, Marián Šimko", "abstract": "This position paper discusses the problem of multilingual evaluation. Using\nsimple statistics, such as average language performance, might inject\nlinguistic biases in favor of dominant language families into evaluation\nmethodology. We argue that a qualitative analysis informed by comparative\nlinguistics is needed for multilingual results to detect this kind of bias. We\nshow in our case study that results in published works can indeed be\nlinguistically biased and we demonstrate that visualization based on URIEL\ntypological database can detect it.", "time": "2023-01-03T18:23:42Z", "link": "http://arxiv.org/abs/2301.01269v1", "id": "2301.01269v1", "title": "Average Is Not Enough: Caveats of Multilingual Evaluation"}
{"author": "Xiaoying Mou, Haiming Chen", "abstract": "Extended regular expressions with counting and interleaving are widely used\nin practice. However the related theoretical studies for this kind of\nexpressions currently cannot meet the need of practical work. This paper\ndevelops syntax definitions for extended deterministic expressions and their\nsubclasses, hope to completely solve the long-standing problem that there are\nno syntax definitions for this kind of expressions, which has become an\nimportant reason for restricting the use of extended expressions.", "time": "2023-02-02T05:22:04Z", "link": "http://arxiv.org/abs/2301.01621v3", "id": "2301.01621v3", "title": "Grammar construction methods for extended deterministic expressions"}
{"author": "Dennis Aumiller, Michael Gertz", "abstract": "Previous state-of-the-art models for lexical simplification consist of\ncomplex pipelines with several components, each of which requires deep\ntechnical knowledge and fine-tuned interaction to achieve its full potential.\nAs an alternative, we describe a frustratingly simple pipeline based on\nprompted GPT-3 responses, beating competing approaches by a wide margin in\nsettings with few training instances. Our best-performing submission to the\nEnglish language track of the TSAR-2022 shared task consists of an ``ensemble''\nof six different prompt templates with varying context levels. As a\nlate-breaking result, we further detail a language transfer technique that\nallows simplification in languages other than English. Applied to the Spanish\nand Portuguese subset, we achieve state-of-the-art results with only minor\nmodification to the original prompts. Aside from detailing the implementation\nand setup, we spend the remainder of this work discussing the particularities\nof prompting and implications for future work. Code for the experiments is\navailable online at https://github.com/dennlinger/TSAR-2022-Shared-Task", "time": "2023-01-05T15:22:05Z", "link": "http://arxiv.org/abs/2301.01764v2", "id": "2301.01764v2", "title": "UniHD at TSAR-2022 Shared Task: Is Compute All We Need for Lexical\n  Simplification?"}
{"author": "A. Seza Doğruöz, Sunayana Sitaram, Barbara E. Bullock, Almeida Jacqueline Toribio", "abstract": "The analysis of data in which multiple languages are represented has gained\npopularity among computational linguists in recent years. So far, much of this\nresearch focuses mainly on the improvement of computational methods and largely\nignores linguistic and social aspects of C-S discussed across a wide range of\nlanguages within the long-established literature in linguistics. To fill this\ngap, we offer a survey of code-switching (C-S) covering the literature in\nlinguistics with a reflection on the key issues in language technologies. From\nthe linguistic perspective, we provide an overview of structural and functional\npatterns of C-S focusing on the literature from European and Indian contexts as\nhighly multilingual areas. From the language technologies perspective, we\ndiscuss how massive language models fail to represent diverse C-S types due to\nlack of appropriate training data, lack of robust evaluation benchmarks for C-S\n(across multilingual situations and types of C-S) and lack of end-to-end\nsystems that cover sociolinguistic aspects of C-S as well. Our survey will be a\nstep towards an outcome of mutual benefit for computational scientists and\nlinguists with a shared interest in multilingualism and C-S.", "time": "2023-01-05T09:08:04Z", "link": "http://arxiv.org/abs/2301.01967v1", "id": "2301.01967v1", "title": "A Survey of Code-switching: Linguistic and Social Perspectives for\n  Language Technologies"}